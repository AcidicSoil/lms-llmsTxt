# llms-full (private-aware)
> Built by GitHub fetches (raw or authenticated). Large files may be truncated.

--- artifacts/lmstudio-ai/docs/docs-llms.txt ---
# Docs

> This repository serves as comprehensive documentation for LM Studio, an AI model management platform. Its primary purpose is to provide clear guidance on using the platform's features through tutorials, API references, and technical specifications. The documentation aims to help both developers and end-users understand how to configure models, interact with APIs, and leverage advanced capabilities like speculative decoding and plugin systems.

**Remember:**
- Markdown-based documentation structure
- Frontmatter metadata for content organization
- Multi-language code examples (Python/TypeScript)
- API references for developers
- Model management workflows (loading/publishing models)
- Plugin system architecture

## Docs
- [Project Setup](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/1_getting-started/project-setup.md): install & quickstart.
- [Quickstart](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/_2_rest/quickstart.md): install & quickstart.
- [Repl](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/1_getting-started/repl.md): install & quickstart.

## Tutorials
- [Examples](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/3_plugins/2_prompt-preprocessor/examples.md): worked example.

## API
- [Api Changelog](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/api-changelog.md): API reference.
- [Act](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/_7_api-reference/act.md): API reference.
- [Act](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/7_api-reference/_act.md): API reference.
- [Chat](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/_7_api-reference/chat.md): API reference.
- [Chat](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/7_api-reference/_chat.md): API reference.
- [Complete](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/_7_api-reference/complete.md): API reference.
- [Complete](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/7_api-reference/_complete.md): API reference.
- [Count Tokens](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/_7_api-reference/count-tokens.md): API reference.
- [Count Tokens](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/7_api-reference/_count-tokens.md): API reference.
- [Embed](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/_7_api-reference/embed.md): API reference.

## Optional
- [Contributing](https://raw.githubusercontent.com/lmstudio-ai/docs/main/CONTRIBUTING.md): docs page.
- [README](https://raw.githubusercontent.com/lmstudio-ai/docs/main/README.md): docs page.
- [Template Dont Edit](https://raw.githubusercontent.com/lmstudio-ai/docs/main/_template_dont_edit.md): docs page.

##  Configuration
- [Inference](https://raw.githubusercontent.com/lmstudio-ai/docs/main/_configuration/inference.md): docs page.
- [Lm Runtimes](https://raw.githubusercontent.com/lmstudio-ai/docs/main/_configuration/lm-runtimes.md): docs page.
- [Load](https://raw.githubusercontent.com/lmstudio-ai/docs/main/_configuration/_load.md): docs page.

## 0 App
- [0 Root](https://raw.githubusercontent.com/lmstudio-ai/docs/main/0_app/0_root/index.md): docs page.
- [1 Basics](https://raw.githubusercontent.com/lmstudio-ai/docs/main/0_app/1_basics/index.md): docs page.
- [2 Mcp](https://raw.githubusercontent.com/lmstudio-ai/docs/main/0_app/2_mcp/index.md): docs page.
- [3 Modelyaml](https://raw.githubusercontent.com/lmstudio-ai/docs/main/0_app/3_modelyaml/index.md): docs page.
- [3 Presets](https://raw.githubusercontent.com/lmstudio-ai/docs/main/0_app/3_presets/index.md): docs page.
- [Branching](https://raw.githubusercontent.com/lmstudio-ai/docs/main/0_app/5_advanced/_branching.md): docs page.
- [Chat](https://raw.githubusercontent.com/lmstudio-ai/docs/main/0_app/1_basics/chat.md): docs page.
- [Connect Apps](https://raw.githubusercontent.com/lmstudio-ai/docs/main/0_app/1_basics/_connect-apps.md): docs page.
- [Context](https://raw.githubusercontent.com/lmstudio-ai/docs/main/0_app/5_advanced/_context.md): docs page.
- [Deeplink](https://raw.githubusercontent.com/lmstudio-ai/docs/main/0_app/2_mcp/deeplink.md): docs page.

## 1 Developer
- [1 Developer](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/index.md): docs page.
- [ 2 Rest](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/_2_rest/index.md): docs page.
- [0 Core](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/0_core/index.md): docs page.
- [3 Openai Compat](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/3_openai-compat/index.md): docs page.
- [Embeddings](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/_embeddings.md): docs page.
- [Authentication](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/0_core/_authentication.md): docs page.
- [Chat](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/_2_rest/chat.md): docs page.
- [Chat Completions](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/3_openai-compat/chat-completions.md): docs page.
- [Completions](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/3_openai-compat/completions.md): docs page.
- [Download](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_developer/_2_rest/download.md): docs page.

## 1 Python
- [1 Python](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/index.md): docs page.
- [3 Embedding](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/3_embedding/index.md): docs page.
- [4 Tokenization](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/4_tokenization/index.md): docs page.
- [Act](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/2_agent/act.md): docs page.
- [Apply Prompt Template](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/_more/_apply-prompt-template.md): docs page.
- [Cancelling Predictions](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/1_llm-prediction/cancelling-predictions.md): docs page.
- [Chat Completion](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/1_llm-prediction/chat-completion.md): docs page.
- [Completion](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/1_llm-prediction/completion.md): docs page.
- [Download Models](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/5_manage-models/_download-models.md): docs page.
- [Get Context Length](https://raw.githubusercontent.com/lmstudio-ai/docs/main/1_python/6_model-info/get-context-length.md): docs page.

## 2 Typescript
- [2 Typescript](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/index.md): docs page.
- [3 Plugins](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/3_plugins/index.md): docs page.
- [4 Embedding](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/4_embedding/index.md): docs page.
- [5 Tokenization](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/5_tokenization/index.md): docs page.
- [1 Tools Provider](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/3_plugins/1_tools-provider/index.md): docs page.
- [2 Prompt Preprocessor](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/3_plugins/2_prompt-preprocessor/index.md): docs page.
- [3 Generator](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/3_plugins/3_generator/index.md): docs page.
- [4 Custom Configuration](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/3_plugins/4_custom-configuration/index.md): docs page.
- [5 Publish Plugins](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/3_plugins/5_publish-plugins/index.md): docs page.
- [Project Setup](https://raw.githubusercontent.com/lmstudio-ai/docs/main/2_typescript/project-setup.md): docs page.

## 3 Cli
- [3 Cli](https://raw.githubusercontent.com/lmstudio-ai/docs/main/3_cli/index.md): docs page.
- [Get](https://raw.githubusercontent.com/lmstudio-ai/docs/main/3_cli/get.md): docs page.
- [Lms Load](https://raw.githubusercontent.com/lmstudio-ai/docs/main/3_cli/_lms-load.md): docs page.
- [Load](https://raw.githubusercontent.com/lmstudio-ai/docs/main/3_cli/load.md): docs page.
- [Log Stream](https://raw.githubusercontent.com/lmstudio-ai/docs/main/3_cli/log-stream.md): docs page.
- [Ls](https://raw.githubusercontent.com/lmstudio-ai/docs/main/3_cli/ls.md): docs page.
- [Ps](https://raw.githubusercontent.com/lmstudio-ai/docs/main/3_cli/ps.md): docs page.
- [Push](https://raw.githubusercontent.com/lmstudio-ai/docs/main/3_cli/push.md): docs page.
- [Server Start](https://raw.githubusercontent.com/lmstudio-ai/docs/main/3_cli/server-start.md): docs page.
- [Server Status](https://raw.githubusercontent.com/lmstudio-ai/docs/main/3_cli/server-status.md): docs page.


--- artifacts/lmstudio-ai/docs/docs-llms-full.txt ---
# llms-full (private-aware)
> Built by GitHub fetches (raw or authenticated). Large files may be truncated.

--- 1_python/1_getting-started/project-setup.md ---
---
title: "Project Setup"
sidebar_title: "Project Setup"
description: "Set up your `lmstudio-python` app or script."
index: 2
---

`lmstudio` is a library published on PyPI that allows you to use `lmstudio-python` in your own projects.
It is open source and developed on GitHub.
You can find the source code [here](https://github.com/lmstudio-ai/lmstudio-python).

## Installing `lmstudio-python`

As it is published to PyPI, `lmstudio-python` may be installed using `pip`
or your preferred project dependency manager (`pdm` and `uv` are shown, but other
Python project management tools offer similar dependency addition commands).

```lms_code_snippet
  variants:
    pip:
      language: bash
      code: |
        pip install lmstudio
    pdm:
      language: bash
      code: |
        pdm add lmstudio
    uv:
      language: bash
      code: |
        uv add lmstudio
```

## Customizing the server API host and TCP port

All of the examples in the documentation assume that the server API is running locally
on one of the default application ports (Note: in Python SDK versions prior to 1.5.0, the
SDK also required that the optional HTTP REST server be enabled).

The network location of the server API can be overridden by
passing a `"host:port"` string when creating the client instance.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms
        SERVER_API_HOST = "localhost:1234"

        # This must be the *first* convenience API interaction (otherwise the SDK
        # implicitly creates a client that accesses the default server API host)
        lms.configure_default_client(SERVER_API_HOST)

        # Note: the dedicated configuration API was added in lmstudio-python 1.3.0
        # For compatibility with earlier SDK versions, it is still possible to use
        # lms.get_default_client(SERVER_API_HOST) to configure the default client

    "Python (scoped resource API)":
      language: python
      code: |
        import lmstudio as lms
        SERVER_API_HOST = "localhost:1234"

        # When using the scoped resource API, each client instance
        # can be configured to use a specific server API host
        with lms.Client(SERVER_API_HOST) as client:
            model = client.llm.model()

            for fragment in model.respond_stream("What is the meaning of life?"):
                print(fragment.content, end="", flush=True)
            print() # Advance to a new line at the end of the response

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms
        SERVER_API_HOST = "localhost:1234"

        # When using the asynchronous API, each client instance
        # can be configured to use a specific server API host
        async with lms.AsyncClient(SERVER_API_HOST) as client:
            model = await client.llm.model()

            for fragment in await model.respond_stream("What is the meaning of life?"):
                print(fragment.content, end="", flush=True)
            print() # Advance to a new line at the end of the response
```

### Checking a specified API server host is running

*Required Python SDK version*: **1.5.0**

While the most common connection pattern is to let the SDK raise an exception if it can't
connect to the specified API server host, the SDK also supports running the API check directly
without creating an SDK client instance first:

```lms_code_snippet
  variants:
    "Python (synchronous API)":
      language: python
      code: |
        import lmstudio as lms
        SERVER_API_HOST = "localhost:1234"

        if lms.Client.is_valid_api_host(SERVER_API_HOST):
            print(f"An LM Studio API server instance is available at {SERVER_API_HOST}")
        else:
            print("No LM Studio API server instance found at {SERVER_API_HOST}")

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms
        SERVER_API_HOST = "localhost:1234"

        if await lms.AsyncClient.is_valid_api_host(SERVER_API_HOST):
            print(f"An LM Studio API server instance is available at {SERVER_API_HOST}")
        else:
            print("No LM Studio API server instance found at {SERVER_API_HOST}")
```


### Determining the default local API server port

*Required Python SDK version*: **1.5.0**

When no API server host is specified, the SDK queries a number of ports on the local loopback
interface for a running API server instance. This scan is repeated for each new client instance
created. Rather than letting the SDK perform this scan implicitly, the SDK also supports running
the scan explicitly, and passing in the reported API server details when creating clients:

```lms_code_snippet
  variants:
    "Python (synchronous API)":
      language: python
      code: |
        import lmstudio as lms

        api_host = lms.Client.find_default_local_api_host()
        if api_host is not None:
            print(f"An LM Studio API server instance is available at {api_host}")
          else:
            print("No LM Studio API server instance found on any of the default local ports")

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms

        api_host = await lms.AsyncClient.find_default_local_api_host()
        if api_host is not None:
            print(f"An LM Studio API server instance is available at {api_host}")
          else:
            print("No LM Studio API server instance found on any of the default local ports")
```


--- 1_developer/_2_rest/quickstart.md ---
---
title: Get up and running with the LM Studio API
sidebar_title: Quickstart
description: Download a model and start a simple Chat session using the REST API
fullPage: false
index: 2
---

## Start the server

[Install](/download) and launch LM Studio.

Then ensure the server is running through the toggle at the top left of the Developer page, or through [lms](/docs/cli) in the terminal:

```bash
lms server start
```

By default, the server is available at `http://127.0.0.1:1234`.

## Download a model

Use the download endpoint to download models by identifier from the [LM Studio model catalog](https://lmstudio.ai/models), or by Hugging Face model URL.

```lms_code_snippet
title: Download Qwen 3
variants:
  curl:
    language: bash
    code: |
      curl http://127.0.0.1:1234/api/v1/models/download \
        -H "Content-Type: application/json" \
        -d '{
          "model": "qwen/qwen3-4b-2507"
        }'
```

The response will return a `job_id` that you can use to track download progress. 

```lms_code_snippet
title: Track download
variants:
  curl:
    language: bash
    code: |
      curl \
        http://127.0.0.1:1234/api/v1/models/download/status/{job_id}
```

See the [download](/docs/developer/rest/download) and [download status](/docs/developer/rest/download-status) docs for more details.

## Chat with a model

Use the chat endpoint to send a message to a model. By default, the model will be automatically loaded if it is not already.

The `/api/v1/chat` endpoint is stateful, which means you do not need to pass the full history in every request. Read more about it [here](/docs/developer/rest/stateful-chats).

```lms_code_snippet
title: Simple chat
variants:
  curl:
    language: bash
    code: |
      curl http://127.0.0.1:1234/api/v1/chat \
        -H "Content-Type: application/json" \
        -d '{
          "model": "qwen/qwen3-4b-2507",
          "input": "Write a short haiku about sunrise."
        }'
```

See the full [chat](/docs/developer/rest/chat) docs for more details.

## Use MCP servers


Enable the model interact with remote Model Context Protocol (MCP) servers in `api/v1/chat` by specifying servers in the `remote_mcp_servers` field.

```lms_code_snippet
title: Use a remote MCP server
variants:
  curl:
    language: bash
    code: |
      curl http://127.0.0.1:1234/api/v1/chat \
        -H "Content-Type: application/json" \
        -d '{    
          "model": "qwen/qwen3-4b-2507",
          "input": "What is the top trending model on huggingface?",
          "remote_mcp_servers": [
            {                   
              "server_label": "huggingface", 
              "server_url": "https://huggingface.co/mcp",
              "allowed_tools": ["model_search"]
            }                    
          ]
        }'
```

You can also use locally configured MCP plugins (from your `mcp.json`) via the `plugins` field. Using locally run MCP plugins requires authentication via an API token passed through the `X-LM-API-Token` header. Read more about authentication [here](/docs/developer/core/authentication).

```lms_code_snippet
title: Use an MCP plugin
variants:
  curl:
    language: bash
    code: |
      curl http://127.0.0.1:1234/api/v1/chat \
        -H "Content-Type: application/json" \
        -H "X-LM-API-Token: $LM_API_TOKEN" \
        -d '{
          "model": "qwen/qwen3-4b-2507",
          "input": "Navigate to lmstudio.ai",
          "plugins": [
            {
              "id": "mcp/playwright",
              "allowed_tools": ["browser_navigate"]
            }
          ]
        }'
```

See the full [chat](/docs/developer/rest/chat) docs for more details.


--- 1_python/1_getting-started/repl.md ---
---
title: "Using `lmstudio-python` in REPL"
sidebar_title: "REPL Usage"
description: "You can use `lmstudio-python` in REPL (Read-Eval-Print Loop) to interact with LLMs, manage models, and more."
index: 2
---

To simplify interactive use, `lmstudio-python` offers a convenience API which manages
its resources via `atexit` hooks, allowing a default synchronous client session
to be used across multiple interactive commands.

This convenience API is shown in the examples throughout the documentation as the
`Python (convenience API)` tab (alongside the `Python (scoped resource API)` examples,
which use `with` statements to ensure deterministic cleanup of network communication
resources).

The convenience API allows the standard Python REPL, or more flexible alternatives like
Juypter Notebooks, to be used to interact with AI models loaded into LM Studio. For
example:

```lms_code_snippet
  title: "Python REPL"
  variants:
    "Interactive chat session":
      language: python
      code: |
        >>> import lmstudio as lms
        >>> loaded_models = lms.list_loaded_models()
        >>> for idx, model in enumerate(loaded_models):
        ...     print(f"{idx:>3} {model}")
        ...
          0 LLM(identifier='qwen2.5-7b-instruct')
        >>> model = loaded_models[0]
        >>> chat = lms.Chat("You answer questions concisely")
        >>> chat = lms.Chat("You answer questions concisely")
        >>> chat.add_user_message("Tell me three fruits")
        UserMessage(content=[TextData(text='Tell me three fruits')])
        >>> print(model.respond(chat, on_message=chat.append))
        Banana, apple, orange.
        >>> chat.add_user_message("Tell me three more fruits")
        UserMessage(content=[TextData(text='Tell me three more fruits')])
        >>> print(model.respond(chat, on_message=chat.append))
        Mango, strawberry, avocado.
        >>> chat.add_user_message("How many fruits have you told me?")
        UserMessage(content=[TextData(text='How many fruits have you told me?')])
        >>> print(model.respond(chat, on_message=chat.append))
        You asked for three initial fruits and three more, so I've listed a total of six fruits.

```

While not primarily intended for use this way, the SDK's asynchronous structured concurrency API
is compatible with the asynchronous Python REPL that is launched by `python -m asyncio`.
For example:

```lms_code_snippet
  title: "Python REPL"
  variants:
    "Asynchronous chat session":
      language: python
      code: |
        # Note: assumes use of the "python -m asyncio" asynchronous REPL (or equivalent)
        # Requires Python SDK version 1.5.0 or later
        >>> from contextlib import AsyncExitStack
        >>> import lmstudio as lms
        >>> resources = AsyncExitStack()
        >>> client = await resources.enter_async_context(lms.AsyncClient())
        >>> loaded_models = await client.llm.list_loaded()
        >>> for idx, model in enumerate(loaded_models):
        ...     print(f"{idx:>3} {model}")
        ...
          0 AsyncLLM(identifier='qwen2.5-7b-instruct-1m')
        >>> model = loaded_models[0]
        >>> chat = lms.Chat("You answer questions concisely")
        >>> chat.add_user_message("Tell me three fruits")
        UserMessage(content=[TextData(text='Tell me three fruits')])
        >>> print(await model.respond(chat, on_message=chat.append))
        Apple, banana, and orange.
        >>> chat.add_user_message("Tell me three more fruits")
        UserMessage(content=[TextData(text='Tell me three more fruits')])
        >>> print(await model.respond(chat, on_message=chat.append))
        Mango, strawberry, and pineapple.
        >>> chat.add_user_message("How many fruits have you told me?")
        UserMessage(content=[TextData(text='How many fruits have you told me?')])
        >>> print(await model.respond(chat, on_message=chat.append))
        You asked for three fruits initially, then three more, so I've listed six fruits in total.

```


--- 2_typescript/3_plugins/2_prompt-preprocessor/examples.md ---
---
title: "Examples"
+description: "Example prompt preprocessors for LM Studio plugins"
index: 2
---

### Example: Inject Current Time

The following is an example preprocessor that injects the current time before each user message.

```lms_code_snippet
  title: "src/promptPreprocessor.ts"
  variants:
    TypeScript:
      language: typescript
      code: |
        import { type PromptPreprocessorController, type ChatMessage } from "@lmstudio/sdk";
        export async function preprocess(ctl: PromptPreprocessorController, userMessage: ChatMessage) {
          const textContent = userMessage.getText();
          const transformed = `Current time: ${new Date().toString()}\n\n${textContent}`;
          return transformed;
        }
```

### Example: Replace Trigger Words

Another example you can do it with simple text only processing is by replacing certain trigger words. For example, you can replace a `@init` trigger with a special initialization message.

```lms_code_snippet
  title: "src/promptPreprocessor.ts"
  variants:
    TypeScript:
      language: typescript
      code: |
        import { type PromptPreprocessorController, type ChatMessage, text } from "@lmstudio/sdk";

        const mySpecialInstructions = text`
          Here are some special instructions...
        `;

        export async function preprocess(ctl: PromptPreprocessorController, userMessage: ChatMessage) {
          const textContent = userMessage.getText();
          const transformed = textContent.replaceAll("@init", mySpecialInstructions);
          return transformed;
        }
```


--- 1_developer/api-changelog.md ---
---
title: API Changelog
description: Updates and changes to the LM Studio API.
index: 2
---

---

###### LM Studio 0.3.29 • 2025‑10‑06

### OpenAI `/v1/responses` and variant listing

- New OpenAI‑compatible endpoint: `POST /v1/responses`.
  - Stateful interactions via `previous_response_id`.
  - Custom tool calling and Remote MCP support (opt‑in).
  - Reasoning support with `reasoning.effort` for `openai/gpt‑oss‑20b`.
  - Streaming via SSE when `stream: true`.
- CLI: `lms ls --variants` lists all variants for multi‑variant models.
- Docs: [/docs/developer/openai-compat](/docs/developer/openai-compat). Full release notes: [/blog/lmstudio-v0.3.29](/blog/lmstudio-v0.3.29).

---

###### LM Studio 0.3.27 • 2025‑09‑24

### CLI: model resource estimates, status, and interrupts

- New: `lms load --estimate-only <model>` prints estimated GPU and total memory before loading. Honors `--context-length` and `--gpu`, and uses an improved estimator that now accounts for flash attention and vision models.
- `lms chat`: press `Ctrl+C` to interrupt an ongoing prediction.
- `lms ps --json` now reports each model's generation status and the number of queued prediction requests.
- CLI color contrast improved for light mode.
- See docs: [/docs/cli/load](/docs/cli/load). Full release notes: [/blog/lmstudio-v0.3.27](/blog/lmstudio-v0.3.27).

---

###### LM Studio 0.3.26 • 2025‑09‑15

### CLI log streaming: server + model

- `lms log stream` now supports multiple sources and filters.
  - `--source server` streams HTTP server logs (startup, endpoints, status)
  - `--source model --filter input,output` streams formatted user input and model output
  - Append `--json` for machine‑readable logs; `--stats` adds tokens/sec and related metrics (model source)
- See usage and examples: [/docs/cli/log-stream](/docs/cli/log-stream). Full release notes: [/blog/lmstudio-v0.3.26](/blog/lmstudio-v0.3.26).

---

###### LM Studio 0.3.25 • 2025‑09‑04

### New model support (API)

- Added support for NVIDIA Nemotron‑Nano‑v2 with tool‑calling via the OpenAI‑compatible endpoints [‡](/blog/lmstudio-v0.3.25).
- Added support for Google EmbeddingGemma for the `/v1/embeddings` endpoint [‡](/blog/lmstudio-v0.3.25).

---

###### LM Studio 0.3.24 • 2025‑08‑28

### Seed‑OSS tool‑calling and template fixes

- Added support for ByteDance/Seed‑OSS including tool‑calling and prompt‑template compatibility fixes in the OpenAI‑compatible API [‡](/blog/lmstudio-v0.3.24).
- Fixed cases where tool calls were not parsed for certain prompt templates [‡](/blog/lmstudio-v0.3.24).

---

###### LM Studio 0.3.23 • 2025‑08‑12

### Reasoning content and tool‑calling reliability

- For `gpt‑oss` on `POST /v1/chat/completions`, reasoning content moves out of `message.content` and into `choices.message.reasoning` (non‑streaming) and `choices.delta.reasoning` (streaming), aligning with `o3‑mini` [‡](/blog/lmstudio-v0.3.23).
- Tool names are normalized (e.g., snake_case) before being provided to the model to improve tool‑calling reliability [‡](/blog/lmstudio-v0.3.23).
- Fixed errors for certain tools‑containing requests to `POST /v1/chat/completions` (e.g., "reading 'properties'") and non‑streaming tool‑call failures [‡](/blog/lmstudio-v0.3.23).

---

###### LM Studio 0.3.19 • 2025‑07‑21

### Bug fixes for streaming and tool calls

- Corrected usage statistics returned by OpenAI‑compatible streaming responses [‡](https://lmstudio.ai/blog/lmstudio-v0.3.19#:~:text=,OpenAI%20streaming%20responses%20were%20incorrect).
- Improved handling of parallel tool calls via the streaming API [‡](https://lmstudio.ai/blog/lmstudio-v0.3.19#:~:text=,API%20were%20not%20handled%20correctly).
- Fixed parsing of correct tool calls for certain Mistral models [‡](https://lmstudio.ai/blog/lmstudio-v0.3.19#:~:text=,Ryzen%20AI%20PRO%20300%20series).

---

###### LM Studio 0.3.18 • 2025‑07‑10

### Streaming options and tool‑calling improvements

- Added support for the `stream_options` object on OpenAI‑compatible endpoints. Setting `stream_options.include_usage` to `true` returns prompt and completion token usage during streaming [‡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=%2A%20Added%20support%20for%20%60,to%20support%20more%20prompt%20templates).
- Errors returned from streaming endpoints now follow the correct format expected by OpenAI clients [‡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,with%20proper%20chat%20templates).
- Tool‑calling support added for Mistral v13 tokenizer models, using proper chat templates [‡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,with%20proper%20chat%20templates).
- The `response_format.type` field now accepts `"text"` in chat‑completion requests [‡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,that%20are%20split%20across%20multiple).
- Fixed bugs where parallel tool calls split across multiple chunks were dropped and where root‑level `$defs` in tool definitions were stripped [‡](https://lmstudio.ai/blog/lmstudio-v0.3.18#:~:text=,being%20stripped%20in%20tool%20definitions).

---

###### LM Studio 0.3.17 • 2025‑06‑25

### Tool‑calling reliability and token‑count updates

- Token counts now include the system prompt and tool definitions [‡](https://lmstudio.ai/blog/lmstudio-v0.3.17#:~:text=,have%20a%20URL%20in%20the). This makes usage reporting more accurate for both the UI and the API.
- Tool‑call argument tokens are streamed as they are generated [‡](https://lmstudio.ai/blog/lmstudio-v0.3.17#:~:text=Build%206), improving responsiveness when using streamed function calls.
- Various fixes improve MCP and tool‑calling reliability, including correct handling of tools that omit a `parameters` object and preventing hangs when an MCP server reloads [‡](https://lmstudio.ai/blog/lmstudio-v0.3.17#:~:text=,tool%20calls%20would%20hang%20indefinitely).

---

###### LM Studio 0.3.16 • 2025‑05‑23

### Model capabilities in `GET /models`

- The OpenAI‑compatible REST API (`/api/v0`) now returns a `capabilities` array in the `GET /models` response. Each model lists its supported capabilities (e.g. `"tool_use"`) [‡](https://lmstudio.ai/blog/lmstudio-v0.3.16#:~:text=,response) so clients can programmatically discover tool‑enabled models.
- Fixed a streaming bug where an empty function name string was appended after the first packet of streamed tool calls [‡](https://lmstudio.ai/blog/lmstudio-v0.3.16#:~:text=%2A%20Bugfix%3A%20%5BOpenAI,packet%20of%20streamed%20function%20calls).

---

###### [👾 LM Studio 0.3.15](/blog/lmstudio-v0.3.15) • 2025-04-24

### Improved Tool Use API Support

OpenAI-like REST API now supports the `tool_choice` parameter:

```json
{
  "tool_choice": "auto" // or "none", "required"
}
```

- `"tool_choice": "none"` — Model will not call tools
- `"tool_choice": "auto"` — Model decides
- `"tool_choice": "required"` — Model must call tools (llama.cpp only)

Chunked responses now set `"finish_reason": "tool_calls"` when appropriate.

---

###### [👾 LM Studio 0.3.14](/blog/lmstudio-v0.3.14) • 2025-03-27

### [API/SDK] Preset Support

RESTful API and SDKs support specifying presets in requests.

_(example needed)_

###### [👾 LM Studio 0.3.10](/blog/lmstudio-v0.3.10) • 2025-02-18

### Speculative Decoding API

Enable speculative decoding in API requests with `"draft_model"`:

```json
{
  "model": "deepseek-r1-distill-qwen-7b",
  "draft_model": "deepseek-r1-distill-qwen-0.5b",
  "messages": [ ... ]
}
```

Responses now include a `stats` object for speculative decoding:

```json
"stats": {
  "tokens_per_second": ...,
  "draft_model": "...",
  "total_draft_tokens_count": ...,
  "accepted_draft_tokens_count": ...,
  "rejected_draft_tokens_count": ...,
  "ignored_draft_tokens_count": ...
}
```

---

###### [👾 LM Studio 0.3.9](blog/lmstudio-v0.3.9) • 2025-01-30

### Idle TTL and Auto Evict

Set a TTL (in seconds) for models loaded via API requests (docs article: [Idle TTL and Auto-Evict](/docs/developer/core/ttl-and-auto-evict))

```diff
curl http://localhost:1234/api/v0/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-r1-distill-qwen-7b",
    "messages": [ ... ]
+   "ttl": 300,
}'
```

With `lms`:

```
lms load --ttl <seconds>
```

### Separate `reasoning_content` in Chat Completion responses

For DeepSeek R1 models, get reasoning content in a separate field. See more [here](/blog/lmstudio-v0.3.9#separate-reasoningcontent-in-chat-completion-responses).

Turn this on in App Settings > Developer.

---

###### [👾 LM Studio 0.3.6](blog/lmstudio-v0.3.6) • 2025-01-06

### Tool and Function Calling API

Use any LLM that supports Tool Use and Function Calling through the OpenAI-like API.

Docs: [Tool Use and Function Calling](/docs/developer/core/tools).

---

###### [👾 LM Studio 0.3.5](blog/lmstudio-v0.3.5) • 2024-10-22

### Introducing `lms get`: download models from the terminal

You can now download models directly from the terminal using a keyword

```bash
lms get deepseek-r1
```

or a full Hugging Face URL

```bash
lms get <hugging face url>
```

To filter for MLX models only, add `--mlx` to the command.

```bash
lms get deepseek-r1 --mlx
```


--- 1_python/_7_api-reference/act.md ---
---
title: "`.act()`"
sidebar_title: "`.act()`"
description: ".act() - API reference for automatic tool use in a multi-turn chat conversation"
index: 3
---

`.act()` is a method that generates automatic tool use in a multi-turn chat conversation.

--- 2_typescript/7_api-reference/_act.md ---
---
title: "`.act()`"
sidebar_title: "`.act()`"
description: ".act() - API reference for automatic tool use in a multi-turn chat conversation"
index: 3
---

`.act()` is a method that generates automatic tool use in a multi-turn chat conversation.

--- 1_python/_7_api-reference/chat.md ---
---
title: "`Chat`"
sidebar_title: "`Chat`"
description: "`Chat` - API reference for representing a chat conversation with an LLM"
index: 5
---

...


--- 2_typescript/7_api-reference/_chat.md ---
---
title: "`Chat`"
sidebar_title: "`Chat`"
description: "`Chat` - API reference for representing a chat conversation with an LLM"
index: 5
---

...


--- 1_python/_7_api-reference/complete.md ---
---
title: "`.complete()`"
sidebar_title: "`.complete()`"
description: ".complete() - API reference for generating text completions from a loaded language model"
index: 4
---

`.complete()` is a method that generates text completions from a loaded language model.

--- 2_typescript/7_api-reference/_complete.md ---
---
title: "`.complete()`"
sidebar_title: "`.complete()`"
description: ".complete() - API reference for generating text completions from a loaded language model"
index: 4
---

`.complete()` is a method that generates text completions from a loaded language model.

--- 1_python/_7_api-reference/count-tokens.md ---
---
title: "`.countTokens()`"
sidebar_title: "`.countTokens()`"
description: ".countTokens() - API reference for counting tokens in a string using a model's tokenizer"
---

`.countTokens()` is a method that counts the number of tokens in a string using a model's tokenizer. This method is useful for situations such as assessing whether a string could fit in the model's remaining context window.

--- 2_typescript/7_api-reference/_count-tokens.md ---
---
title: "`.countTokens()`"
sidebar_title: "`.countTokens()`"
description: ".countTokens() - API reference for counting tokens in a string using a model's tokenizer"
---

`.countTokens()` is a method that counts the number of tokens in a string using a model's tokenizer. This method is useful for situations such as assessing whether a string could fit in the model's remaining context window.

--- 1_python/_7_api-reference/embed.md ---
---
title: "`.embed()`"
sidebar_title: "`.embed()`"
description: ".embed() - API reference for generating embeddings from a loaded embedding model"
---

`.embed()` is a method that generates embeddings from a loaded embedding model.

--- CONTRIBUTING.md ---



--- README.md ---
[← Home](/README.md)

# LM Studio Documentation

Content from this repo is served at [lmstudio.ai/docs](https://lmstudio.ai/docs).

## Parsing rules

- Folders under docs/ are treated as sections
- Files under a section folder are treated as articles
- You can nest folders to create sub-sections
- `0_root` is a special folder that contains the root-level articles
- `index.md` is a special file that is treated as the section's landing page (served on `/docs/<section>/`)
- you can control the ordering of sections by naming them with a number prefix (e.g. `1_api`, `2_guides`, etc.)
- you can control the ordering of articles by setting the `index` field in the frontmatter
  - `index: -1` means "place this article at the end of the list"
- files and folders starting with `_` are ignored

## How to add new documentation articles

1. Create a new markdown file in the appropriate folder

2. Populate the file with the following frontmatter:

```yaml
---
title: Keyboard Shortcuts
description: Learn about the keyboard shortcuts in LM Studio
index: 2
---
```

##### Optionally set `sidebar_title:` to override the title in the sidebar

## Custom markdown component

### -> See examples in `_template_dont_edit.md`

### Multi-Language Code Snippets

See it in action on a model page: https://lmstudio.ai/model/deepseek-r1-qwen-7b

Configurations that look good:

1. title + 1 variant
2. no title + 2+ variants

````
```lms_code_snippet
  variants:
    TypeScript:
      language: typescript
      code: |
        // Multi-line TypeScript code
        function hello() {
          console.log("hey")
          return "world"
        }

    Python:
      language: python
      code: |
        # Multi-line Python code
        def hello():
            print("hey")
            return "world"
```
````

````
```lms_code_snippet
  title: "generator.py"
  variants:
    Python:
      language: python
      code: |
        # Multi-line Python code
        def hello():
            print("hey")
            return "world"
```
````

### Horizontal Stack (HStack)

Useful for placing a text block next to a code block (for example for a code explanation, or step-by-step guide)

````
```lms_hstack

# Column 1

~~~js
console.log("Hello from the code block");
~~~

:::split:::

# Column 2
Second column markdown content here

```
````

### Params

List of formatted parameters

````
### Parameters 
```lms_params
- name: "[path]"
  type: "string"
  optional: true
  description: "The path of the model to load. If not provided, you will be prompted to select one"
- name: "--ttl"
  type: "number"
  optional: true
  description: "If provided, when the model is not used for this number of seconds, it will be unloaded"
- name: "--gpu"
  type: "string"
  optional: true
  description: "How much to offload to the GPU. Values: 0-1, off, max"
- name: "--context-length"
  type: "number"
  optional: true
  description: "The number of tokens to consider as context when generating text"
- name: "--identifier"
  type: "string"
  optional: true
  description: "The identifier to assign to the loaded model for API reference"
```
````


--- _template_dont_edit.md ---
---
title: Introduction
description: Quick start
---

Welcome to the LM Studio documentation!

## Code Snippets

Configurations that look good:

1. title + 1 variant
2. no title + 2+ variants

```lms_code_snippet
  variants:
    TypeScript:
      language: typescript
      code: |
        // Multi-line TypeScript code
        function hello() {
          console.log("hey")
          return "world"
        }

    Python:
      language: python
      code: |
        # Multi-line Python code
        def hello():
            print("hey")
            return "world"
```

```lms_code_snippet
  title: "generator.py"
  variants:
    Python:
      language: python
      code: |
        # Multi-line Python code
        def hello():
            print("hey")
            return "world"
```

<br></br>

```lms_hstack

# Column 1

~~~js
console.log("Hello from the code block");
~~~

:::split:::

# Column 2
Second column markdown content here

```

<br><br>

```ts
// index.ts
import { LMStudioClient } from "@lmstudio/sdk";

// Create a client to connect to LM Studio, then load a model
async function main() {
  const client = new LMStudioClient();
  const model = await client.llm.load("meta-llama-3-8b");

  const prediction = model.predict("Once upon a time, there was a");

  for await (const text of prediction) {
    process.stdout.write(text);
  }
}

main();
```

```lms_notice
You can jump to Settings from anywhere in the app by pressing `cmd` + `,` on macOS or `ctrl` + `,` on Windows/Linux.
```

```lms_protip
You can jump to Settings from anywhere in the app by pressing `cmd` + `,` on macOS or `ctrl` + `,` on Windows/Linux.
```

```lms_warning
You can jump to Settings from anywhere in the app by pressing `cmd` + `,` on macOS or `ctrl` + `,` on Windows/Linux.
```

### Params

List of formatted parameters

### Parameters 

```lms_params
- name: "[path]"
  type: "string"
  optional: true
  description: "The path of the model to load. If not provided, you will be prompted to select one"
- name: "--ttl"
  type: "number"
  optional: true
  description: "If provided, when the model is not used for this number of seconds, it will be unloaded"
- name: "--gpu"
  type: "string"
  optional: true
  description: "How much to offload to the GPU. Values: 0-1, off, max"
- name: "--context-length"
  type: "number"
  optional: true
  description: "The number of tokens to consider as context when generating text"
- name: "--identifier"
  type: "string"
  optional: true
  description: "The identifier to assign to the loaded model for API reference"
```

## What is LM Studio?

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam auctor, nunc nec
suscipit ultricies, `code which is inline` nunc nunc ultricies nunc, nec suscipit nunc nunc nec. Nullam.

```lms_download_options
repository: user/model
options:
  - name: "meta-llama-3-8b"
    description: "Meta Llama 3 8B"
    version: "1.0.0"
    size: "1.2 GB"
    download: "https://example.com/meta-llama-3-8b.zip"
    license: "MIT"

  - name: "meta-llama-3-8b"
    description: "Meta Llama 3 8B"
    version: "1.0.0"
    size: "1.2 GB"
    download: "https://example.com/meta-llama-3-8b.zip"
    license: "MIT"
```

<img src="/assets/hero-dark-classic@2x.png" alt="LM Studio" data-caption="Some caption and a [link](https://lmstudio.ai)" />

## Main Features

Some of the main features of LM Studio are:

| Feature       | Description                                                           |
| ------------- | --------------------------------------------------------------------- |
| **Feature 1** | Lorem ipsum dolor sit amet, consectetur adipiscing elit.              |
| **Feature 2** | Nullam auctor, nunc nec suscipit ultricies, nunc nunc ultricies nunc. |
| **Feature 3** | Nec suscipit nunc nunc nec. Nullam.                                   |

## How to use this documentation

This documentation is divided into the following sections:

- **Quick Start**: Get started with LM Studio in minutes.
- **API Reference**: Learn how to use the LM Studio API.


--- _configuration/inference.md ---
---
title: Inference parameters
description: Configurable parameters for inference in LM Studio
---

### Page

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam auctor, nunc nec suscipit ultricies, nunc nunc ultricies nunc, nec suscipit nunc nunc nec. Nullam.

--- _configuration/lm-runtimes.md ---
---
title: LM Runtimes
description: Downloading, installing, and using different LM Runtimes in LM Studio
---

### Page

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam auctor, nunc nec suscipit ultricies, nunc nunc ultricies nunc, nec suscipit nunc nunc nec. Nullam.

--- _configuration/_load.md ---
---
title: Load parameters
description: Configurable parameters for load in LM Studio
---

### Page

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam auctor, nunc nec suscipit ultricies, nunc nunc ultricies nunc, nec suscipit nunc nunc nec. Nullam.

--- 0_app/0_root/index.md ---
---
title: Welcome to LM Studio Docs!
sidebar_title: Welcome
description: Learn how to run Llama, DeepSeek, Qwen, Phi, and other LLMs locally with LM Studio.
index: 1
---

To get LM Studio, head over to the [Downloads page](/download) and download an installer for your operating system.

LM Studio is available for macOS, Windows, and Linux.

## What can I do with LM Studio?

1. Download and run local LLMs like gpt-oss or Llama, Qwen
2. Use a simple and flexible chat interface
3. Connect MCP servers and use them with local models
4. Search & download functionality (via Hugging Face 🤗)
5. Serve local models on OpenAI-like endpoints, locally and on the network
6. Manage your local models, prompts, and configurations

## System requirements

LM Studio generally supports Apple Silicon Macs, x64/ARM64 Windows PCs, and x64 Linux PCs.

Consult the [System Requirements](app/system-requirements) page for more detailed information.

## Run llama.cpp (GGUF) or MLX models

LM Studio supports running LLMs on Mac, Windows, and Linux using [`llama.cpp`](https://github.com/ggerganov/llama.cpp).

On Apple Silicon Macs, LM Studio also supports running LLMs using Apple's [`MLX`](https://github.com/ml-explore/mlx).

To install or manage LM Runtimes, press `⌘` `Shift` `R` on Mac or `Ctrl` `Shift` `R` on Windows/Linux.

## LM Studio as an MCP client

You can install MCP servers in LM Studio and use them with your local models.

See the docs for more: [Use MCP server](/docs/app/plugins/mcp).

If you're develping an MCP server, check out [Add to LM Studio Button](/docs/app/plugins/mcp/deeplink).

## Run an LLM like `gpt-oss`, `Llama`, `Qwen`, `Mistral`, or `DeepSeek R1` on your computer

To run an LLM on your computer you first need to download the model weights.

You can do this right within LM Studio! See [Download an LLM](app/basics/download-model) for guidance.

## Chat with documents entirely offline on your computer

You can attach documents to your chat messages and interact with them entirely offline, also known as "RAG".

Read more about how to use this feature in the [Chat with Documents](app/basics/rag) guide.

## Use LM Studio's API from your own apps and scripts

LM Studio provides a REST API that you can use to interact with your local models from your own apps and scripts.

- [OpenAI Compatibility API](api/openai-api)
- [LM Studio REST API (beta)](api/rest-api)

<br />

## Community

Join the LM Studio community on [Discord](https://discord.gg/aPQfnNkxGC) to ask questions, share knowledge, and get help from other users and the LM Studio team.


--- 0_app/1_basics/index.md ---
---
title: Get started with LM Studio
sidebar_title: Overview
description: Download and run Large Language Models like Qwen, Mistral, Gemma, or gpt-oss in LM Studio.
index: 1
---

Double check computer meets the minimum [system requirements](/docs/system-requirements).

```lms_info
You might sometimes see terms such as `open-source models` or `open-weights models`. Different models might be released under different licenses and varying degrees of 'openness'. In order to run a model locally, you need to be able to get access to its "weights", often distributed as one or more files that end with `.gguf`, `.safetensors` etc.
```

<hr>

## Getting up and running

First, **install the latest version of LM Studio**. You can get it from [here](/download).

Once you're all set up, you need to **download your first LLM**.

### 1. Download an LLM to your computer

Head over to the Discover tab to download models. Pick one of the curated options or search for models by search query (e.g. `"Llama"`). See more in-depth information about downloading models [here](/docs/basics/download-models).

<img src="/assets/docs/discover.png" style="width: 500px; margin-top:30px" data-caption="The Discover tab in LM Studio" />

### 2. Load a model to memory

Head over to the **Chat** tab, and

1. Open the model loader
2. Select one of the models you downloaded (or [sideloaded](/docs/advanced/sideload)).
3. Optionally, choose load configuration parameters.

<img src="/assets/docs/loader.png" data-caption="Quickly open the model loader with `cmd` + `L` on macOS or `ctrl` + `L` on Windows/Linux" />

##### What does loading a model mean?

Loading a model typically means allocating memory to be able to accommodate the model's weights and other parameters in your computer's RAM.

### 3. Chat!

Once the model is loaded, you can start a back-and-forth conversation with the model in the Chat tab.

<img src="/assets/docs/chat.png" data-caption="LM Studio on macOS" />

<hr>

### Community

Chat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).


--- 0_app/2_mcp/index.md ---
---
title: Use MCP Servers
description: Connect MCP servers to LM Studio
index: 1
---

Starting LM Studio 0.3.17, LM Studio acts as an **Model Context Protocol (MCP) Host**. This means you can connect MCP servers to the app and make them available to your models.

### Be cautious

Never install MCPs from untrusted sources.

```lms_warning
Some MCP servers can run arbitrary code, access your local files, and use your network connection. Always be cautious when installing and using MCP servers. If you don't trust the source, don't install it.
```

# Use MCP servers in LM Studio

Starting 0.3.17 (b10), LM Studio supports both local and remote MCP servers. You can add MCPs by editing the app's `mcp.json` file or via the ["Add to LM Studio" Button](mcp/deeplink), when available. LM Studio currently follows Cursor's `mcp.json` notation.

## Install new servers: `mcp.json`

Switch to the "Program" tab in the right hand sidebar. Click `Install > Edit mcp.json`.

<img src="/assets/docs/install-mcp.png"  data-caption="" style="width: 80%;" className="" />

This will open the `mcp.json` file in the in-app editor. You can add MCP servers by editing this file.

<img src="/assets/docs/mcp-editor.png"  data-caption="Edit mcp.json using the in-app editor" style="width: 100%;" className="" />

### Example MCP to try: Hugging Face MCP Server

This MCP server provides access to functions like model and dataset search.

<div className="w-fit">
  <a style="background: rgb(255,255,255)" href="https://lmstudio.ai/install-mcp?name=hf-mcp-server&config=eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL21jcCIsImhlYWRlcnMiOnsiQXV0aG9yaXphdGlvbiI6IkJlYXJlciA8WU9VUl9IRl9UT0tFTj4ifX0%3D">
    <LightVariant>
      <img src="https://files.lmstudio.ai/deeplink/mcp-install-light.svg" alt="Add MCP Server hf-mcp-server to LM Studio" />
    </LightVariant>
    <DarkVariant>
      <img src="https://files.lmstudio.ai/deeplink/mcp-install-dark.svg" alt="Add MCP Server hf-mcp-server to LM Studio" />
    </DarkVariant>
  </a>
</div>

```json
{
  "mcpServers": {
    "hf-mcp-server": {
      "url": "https://huggingface.co/mcp",
      "headers": {
        "Authorization": "Bearer <YOUR_HF_TOKEN>"
      }
    }
  }
}
```

###### You will need to replace `<YOUR_HF_TOKEN>` with your actual Hugging Face token. Learn more [here](https://huggingface.co/docs/hub/en/security-tokens).

Use the [deeplink button](mcp/deeplink), or copy the JSON snippet above and paste it into your `mcp.json` file.

---

## Gotchas and Troubleshooting

- Never install MCP servers from untrusted sources. Some MCPs can have far reaching access to your system.

- Some MCP servers were designed to be used with Claude, ChatGPT, Gemini and might use excessive amounts of tokens.

  - Watch out for this. It may quickly bog down your local model and trigger frequent context overflows.

- When adding MCP servers manually, copy only the content after `"mcpServers": {` and before the closing `}`.


--- 0_app/3_modelyaml/index.md ---
---
title: "Introduction to `model.yaml`"
description: Describe models with the cross-platform `model.yaml` specification.
index: 5
socialCard: 
  url: https://files.lmstudio.ai/modelyaml-card.jpg
  alt: "model.yaml logo"
---

`Draft`

[`model.yaml`](https://modelyaml.org) describes a model and all of its variants in a single portable file. Models in LM Studio's [model catalog](https://lmstudio.ai/models) are all implemented using model.yaml.

This allows abstracting away the underlying format (GGUF, MLX, etc) and presenting a single entry point for a given model. Furthermore, the model.yaml file supports baking in additional metadata, load and inference options, and even custom logic (e.g. enable/disable thinking).

**You can clone existing model.yaml files on the LM Studio Hub and even [publish your own](./modelyaml/publish)!**

## Core fields

### `model`

The canonical identifier in the form `publisher/model`.

```yaml
model: qwen/qwen3-8b
```

### `base`

Points to the "concrete" model files or other virtual models. Each entry uses a unique `key` and one or more `sources` from which the file can be fetched.

The snippet below demonstrates a case where the model (`qwen/qwen3-8b`) can resolve to one of 3 different concrete models.

```yaml
model: qwen/qwen3-8b
base:
  - key: lmstudio-community/qwen3-8b-gguf
    sources:
      - type: huggingface
        user: lmstudio-community
        repo: Qwen3-8B-GGUF
  - key: lmstudio-community/qwen3-8b-mlx-4bit
    sources:
      - type: huggingface
        user: lmstudio-community
        repo: Qwen3-8B-MLX-4bit
  - key: lmstudio-community/qwen3-8b-mlx-8bit
    sources:
      - type: huggingface
        user: lmstudio-community
        repo: Qwen3-8B-MLX-8bit
```

Concrete model files refer to the actual weights.

### `metadataOverrides`

Overrides the base model's metadata. This is useful for presentation purposes, for example in LM Studio's model catalog or in app model search. It is not used for any functional changes to the model.

```yaml
metadataOverrides:
  domain: llm
  architectures:
    - qwen3
  compatibilityTypes:
    - gguf
    - safetensors
  paramsStrings:
    - 8B
  minMemoryUsageBytes: 4600000000
  contextLengths:
    - 40960
  vision: false
  reasoning: true
  trainedForToolUse: true
```

### `config`

Use this to "bake in" default runtime settings (such as sampling parameters) and even load time options.
This works similarly to [Per Model Defaults](/docs/app/advanced/per-model).

- `operation:` inference time parameters
- `load:` load time parameters

```yaml
config:
  operation:
    fields:
      - key: llm.prediction.topKSampling
        value: 20
      - key: llm.prediction.temperature
        value: 0.7
  load:
    fields:
      - key: llm.load.contextLength
        value: 42690
```

### `customFields`

Define model-specific custom fields.

```yaml
customFields:
  - key: enableThinking
    displayName: Enable Thinking
    description: Controls whether the model will think before replying
    type: boolean
    defaultValue: true
    effects:
      - type: setJinjaVariable
        variable: enable_thinking
```

In order for the above example to work, the jinja template needs to have a variable named `enable_thinking`.

## Complete example

Taken from https://lmstudio.ai/models/qwen/qwen3-8b

```yaml
# model.yaml is an open standard for defining cross-platform, composable AI models
# Learn more at https://modelyaml.org
model: qwen/qwen3-8b
base:
  - key: lmstudio-community/qwen3-8b-gguf
    sources:
      - type: huggingface
        user: lmstudio-community
        repo: Qwen3-8B-GGUF
  - key: lmstudio-community/qwen3-8b-mlx-4bit
    sources:
      - type: huggingface
        user: lmstudio-community
        repo: Qwen3-8B-MLX-4bit
  - key: lmstudio-community/qwen3-8b-mlx-8bit
    sources:
      - type: huggingface
        user: lmstudio-community
        repo: Qwen3-8B-MLX-8bit
metadataOverrides:
  domain: llm
  architectures:
    - qwen3
  compatibilityTypes:
    - gguf
    - safetensors
  paramsStrings:
    - 8B
  minMemoryUsageBytes: 4600000000
  contextLengths:
    - 40960
  vision: false
  reasoning: true
  trainedForToolUse: true
config:
  operation:
    fields:
      - key: llm.prediction.topKSampling
        value: 20
      - key: llm.prediction.minPSampling
        value:
          checked: true
          value: 0
customFields:
  - key: enableThinking
    displayName: Enable Thinking
    description: Controls whether the model will think before replying
    type: boolean
    defaultValue: true
    effects:
      - type: setJinjaVariable
        variable: enable_thinking
```

The [GitHub specification](https://github.com/modelyaml/modelyaml) contains further details and the latest schema.


--- 0_app/3_presets/index.md ---
---
title: Config Presets
sidebar_title: Overview
description: Save your system prompts and other parameters as Presets for easy reuse across chats.
index: 1
---

Presets are a way to bundle together a system prompt and other parameters into a single configuration that can be easily reused across different chats.

New in 0.3.15: You can [import](/docs/app/presets/import) Presets from file or URL, and even [publish](/docs/app/presets/publish) your own Presets to share with others on to the LM Studio Hub.
<hr>

## Saving, resetting, and deselecting Presets

Below is the anatomy of the Preset manager:

<img src="/assets/docs/preset-widget-anatomy.png" style="width:70%" data-caption="The anatomy of the Preset manager in the settings sidebar.">

## Importing, Publishing, and Updating Downloaded Presets

Presets are JSON files. You can share them by sending around the JSON, or you can share them by publishing them to the LM Studio Hub.
You can also import Presets from other users by URL. See the [Import](/docs/app/presets/import) and [Publish](/docs/app/presets/publish) sections for more details.

## Example: Build your own Prompt Library

You can create your own prompt library by using Presets.

<video autoplay loop muted playsinline style="width:60vh;" data-caption="Save collections of parameters as a Preset for easy reuse." class="border border-border">
  <source src="https://files.lmstudio.ai/presets.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

In addition to system prompts, every parameter under the Advanced Configuration sidebar can be recorded in a named Preset.

For example, you might want to always use a certain Temperature, Top P, or Max Tokens for a particular use case. You can save these settings as a Preset (with or without a system prompt) and easily switch between them.

#### The Use Case for Presets

- Save your system prompts, inference parameters as a named `Preset`.
- Easily switch between different use cases, such as reasoning, creative writing, multi-turn conversations, or brainstorming.

## Where Presets are stored

Presets are stored in the following directory:

#### macOS or Linux

```xml
~/.lmstudio/config-presets
```

#### Windows

```xml
%USERPROFILE%\.lmstudio\config-presets
```

### Migration from LM Studio 0.2.\* Presets

- Presets you've saved in LM Studio 0.2.\* are automatically readable in 0.3.3 with no migration step needed.
- If you save **new changes** in a **legacy preset**, it'll be **copied** to a new format upon save.
  - The old files are NOT deleted.
- Notable difference: Load parameters are not included in the new preset format.
  - Favor editing the model's default config in My Models. See [how to do it here](/docs/configuration/per-model).

<hr>

### Community

Chat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).


--- 0_app/5_advanced/_branching.md ---
---
title: Branch conversation
description: Fork, clone, and branch a conversation in LM Studio
---

### Page

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam auctor, nunc nec suscipit ultricies, nunc nunc ultricies nunc, nec suscipit nunc nunc nec. Nullam.

--- 0_app/1_basics/chat.md ---
---
title: Manage chats
description: Manage conversation threads with LLMs
index: 2
---

LM Studio has a ChatGPT-like interface for chatting with local LLMs. You can create many different conversation threads and manage them in folders.

<hr>

### Create a new chat

You can create a new chat by clicking the "+" button or by using a keyboard shortcut: `⌘` + `N` on Mac, or `ctrl` + `N` on Windows / Linux.

### Create a folder

Create a new folder by clicking the new folder button or by pressing: `⌘` + `shift` + `N` on Mac, or `ctrl` + `shift` + `N` on Windows / Linux.

### Drag and drop

You can drag and drop chats in and out of folders, and even drag folders into folders!

### Duplicate chats

You can duplicate a whole chat conversation by clicking the `•••` menu and selecting "Duplicate". If the chat has any files in it, they will be duplicated too.

## FAQ

#### Where are chats stored in the file system?

Right-click on a chat and choose "Reveal in Finder" / "Show in File Explorer".
Conversations are stored in JSON format. It is NOT recommended to edit them manually, nor to rely on their structure.

#### Does the model learn from chats?

The model doesn't 'learn' from chats. The model only 'knows' the content that is present in the chat or is provided to it via configuration options such as the "system prompt".

## Conversations folder filesystem path

Mac / Linux:

```shell
~/.lmstudio/conversations/
```

Windows:

```ps
%USERPROFILE%\.lmstudio\conversations
```

<hr>

### Community

Chat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).


--- 0_app/1_basics/_connect-apps.md ---
---
title: Connect apps to LM Studio
description: Getting started with connecting applications to LM Studio
---

LM Studio comes with a few built-in themes for app-wide color palettes.

<hr>

### Selecting a Theme

You can choose a theme in the Settings tab.

Choosing the "Auto" option will automatically switch between Light and Dark themes based on your system settings.

```lms_protip
You can jump to Settings from anywhere in the app by pressing `cmd` + `,` on macOS or `ctrl` + `,` on Windows/Linux.
```
###### To get to the Settings page, you need to be on [Power User mode](/docs/modes) or higher.

<hr>

### Community
Chat with other LM Studio users, discuss LLMs, hardware, and more on the [LM Studio Discord server](https://discord.gg/aPQfnNkxGC).


--- 0_app/5_advanced/_context.md ---
---
title: Context construction
description: Inserting Assistant and User messages to construct a conversation context for a given purpose
---

### Page

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam auctor, nunc nec suscipit ultricies, nunc nunc ultricies nunc, nec suscipit nunc nunc nec. Nullam.

--- 0_app/2_mcp/deeplink.md ---
---
title: "`Add to LM Studio` Button"
description: Add MCP servers to LM Studio using a deeplink
index: 2
---

You can install MCP servers in LM Studio with one click using a deeplink.

Starting with version 0.3.17 (10), LM Studio can act as an MCP host. Learn more about it [here](../mcp).

---

# Generate your own MCP install link

Enter your MCP JSON entry to generate a deeplink for the `Add to LM Studio` button.

```lms_mcp_deep_link_generator

```

## Try an example

Try to copy and paste the following into the link generator above.

```json
{
  "hf-mcp-server": {
    "url": "https://huggingface.co/mcp",
    "headers": {
      "Authorization": "Bearer <YOUR_HF_TOKEN>"
    }
  }
}
```

### Deeplink format

```bash
lmstudio://add_mcp?name=hf-mcp-server&config=eyJ1cmwiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvL21jcCIsImhlYWRlcnMiOnsiQXV0aG9yaXphdGlvbiI6IkJlYXJlciA8WU9VUl9IRl9UT0tFTj4ifX0%3D
```

#### Parameters

```lms_params
- name: "lmstudio://"
  type: "protocol"
  description: "The protocol scheme to open LM Studio"
- name: "add_mcp"
  type: "path"
  description: "The action to install an MCP server"
- name: "name"
  type: "query parameter"
  description: "The name of the MCP server to install"
- name: "config"
  type: "query parameter"
  description: "Base64 encoded JSON configuration for the MCP server"
```


--- 1_developer/index.md ---
---
title: LM Studio Developer Docs
sidebar_title: Introduction
description: Build with LM Studio's local APIs and SDKs — TypeScript, Python, REST, and OpenAI‑compatible endpoints.
index: 1
---

```lms_hstack
## Get to know the stack

- TypeScript SDK: [lmstudio-js](/docs/typescript)
- Python SDK: [lmstudio-python](/docs/python)
- OpenAI‑compatible: [Chat, Responses, Embeddings](/docs/developer/openai-compat)
- LM Studio CLI: [`lms`](/docs/cli)

:::split:::

## What you can build

- Chat and text generation with streaming
- Structured output (JSON schema)
- Tool calling and local agents
- Embeddings and tokenization
- Model management (JIT load, TTL, auto‑evict)
```

## Super quick start

### TypeScript (`lmstudio-js`)

```bash
npm install @lmstudio/sdk
```

```ts
import { LMStudioClient } from "@lmstudio/sdk";

const client = new LMStudioClient();
const model = await client.llm.model("openai/gpt-oss-20b");
const result = await model.respond("Who are you, and what can you do?");

console.info(result.content);
```

Full docs: [lmstudio-js](/docs/typescript), Source: [GitHub](https://github.com/lmstudio-ai/lmstudio-js)

### Python (`lmstudio-python`)

```bash
pip install lmstudio
```

```python
import lmstudio as lms

with lms.Client() as client:
    model = client.llm.model("openai/gpt-oss-20b")
    result = model.respond("Who are you, and what can you do?")
    print(result)
```

Full docs: [lmstudio-python](/docs/python), Source: [GitHub](https://github.com/lmstudio-ai/lmstudio-python)

### Try a minimal HTTP request (OpenAI‑compatible)

```bash
lms server start --port 1234
```

```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-20b",
    "messages": [{"role": "user", "content": "Who are you, and what can you do?"}]
  }'
```

Full docs: [OpenAI‑compatible endpoints](/docs/developer/openai-compat)

## Helpful links

- API Changelog: [/docs/developer/api-changelog](/docs/developer/api-changelog)
- Local server basics: [/docs/developer/core](/docs/developer/core)
- CLI reference: [/docs/cli](/docs/cli)
- Community: [Discord](https://discord.gg/lmstudio)


--- 1_developer/_2_rest/index.md ---
---
title: LM Studio API
sidebar_title: Overview
description: Get started with LM Studio's REST API for local model management and inference.
fullPage: false
index: 1
---

LM Studio offers a powerful REST API with first-class support for local model management and inference. In addition to our native API, we provide full OpenAI compatibility mode ([learn more](/docs/developer/openai-compat)).

Our REST API handles local LLM workflows with model downloading, loading, configuration, and inference. Get performance stats like tokens per second, model status, context length, quantization info, and more. Configure loading parameters to customize how models initialize.

### Supported endpoints

| Endpoint                         | Method                          | Docs                                             |
| -------------------------------- | ------------------------------- | ------------------------------------------------ |
| `/api/v1/chat`                   | <apimethod method="POST" />     | [Chat](/docs/developer/rest/chat)               |
| `/api/v1/models`                 | <apimethod method="GET" />      | [List Models](/docs/developer/rest/list)        |
| `/api/v1/models/load`            | <apimethod method="POST" />     | [Load](/docs/developer/rest/load)               |
| `/api/v1/models/download`        | <apimethod method="POST" />     | [Download](/docs/developer/rest/download)       |
| `/api/v1/models/download/status` | <apimethod method="GET" />      | [Download Status](/docs/developer/rest/download-status) |

---

Please report bugs by opening an issue on [Github](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues).


--- 1_developer/0_core/index.md ---
---
title: LM Studio as a Local LLM API Server
sidebar_title: Local Server
description: Run an LLM API server on localhost with LM Studio
fullPage: false
---

You can serve local LLMs from LM Studio's Developer tab, either on localhost or on the network.

LM Studio's APIs can be used through [REST API](/docs/developer/rest), client libraries like [lmstudio-js](/docs/typescript) and [lmstudio-python](/docs/python), and [OpenAI compatibility endpoints](/docs/developer/openai-compat)

### API options

- [LM Studio REST API](/docs/developer/rest)
- [TypeScript SDK](/docs/typescript) - `lmstudio-js`
- [Python SDK](/docs/python) - `lmstudio-python`
- [OpenAI compatibility endpoints](/docs/developer/openai-compat)

<img src="/assets/docs/server.png" style="" data-caption="Load and serve LLMs from LM Studio" />


--- 1_developer/3_openai-compat/index.md ---
---
title: OpenAI Compatibility Endpoints
sidebar_title: Overview
description: Send requests to Responses, Chat Completions (text and images), Completions, and Embeddings endpoints.
index: 1
---

### Supported endpoints

| Endpoint               | Method                      | Docs                                                               |
| ---------------------- | --------------------------- | ------------------------------------------------------------------ |
| `/v1/models`           | <apimethod method="GET" />  | [Models](/docs/developer/openai-compat/models)                     |
| `/v1/responses`        | <apimethod method="POST" /> | [Responses](/docs/developer/openai-compat/responses)               |
| `/v1/chat/completions` | <apimethod method="POST" /> | [Chat Completions](/docs/developer/openai-compat/chat-completions) |
| `/v1/embeddings`       | <apimethod method="POST" /> | [Embeddings](/docs/developer/openai-compat/embeddings)             |
| `/v1/completions`      | <apimethod method="POST" /> | [Completions](/docs/developer/openai-compat/completions)           |

<hr>

## Set the `base url` to point to LM Studio

You can reuse existing OpenAI clients (in Python, JS, C#, etc) by switching up the "base URL" property to point to your LM Studio instead of OpenAI's servers.

Note: The following examples assume the server port is `1234`

### Python Example

```diff
from openai import OpenAI

client = OpenAI(
+    base_url="http://localhost:1234/v1"
)

# ... the rest of your code ...
```

### Typescript Example

```diff
import OpenAI from 'openai';

const client = new OpenAI({
+  baseUrl: "http://localhost:1234/v1"
});

// ... the rest of your code ...
```

### cURL Example

```diff
- curl https://api.openai.com/v1/chat/completions \
+ curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
-     "model": "gpt-4o-mini",
+     "model": "use the model identifier from LM Studio here",
     "messages": [{"role": "user", "content": "Say this is a test!"}],
     "temperature": 0.7
   }'
```

---

Other OpenAI client libraries should have similar options to set the base URL.

If you're running into trouble, hop onto our [Discord](https://discord.gg/lmstudio) and enter the `#🔨-developers` channel.


--- 1_developer/_embeddings.md ---
---
title: Embeddings
description: Use embedding models in LM Studio
---

### Page

lorem ipsum dolor sit amet, consectetur adipiscing elit. nullam auctor, nunc nec suscipit ultricies, nunc nunc ultricies nunc, nec suscipit nunc nunc nec. nullam.

--- 1_developer/0_core/_authentication.md ---
---
title: Authentication
sidebar_title: Authentication
description: Placeholder for authentication page
index: 2
---

This is a placeholder authentication page. Content coming soon.

--- 1_developer/_2_rest/chat.md ---
---
title: "Chat with a model"
description: "Send a message to a model and receive a response. Supports MCP integration."
fullPage: true
index: 3
api_info:
  method: POST
---

````lms_hstack
`POST /api/v1/chat`

**Request body**
```lms_params
- name: model
  type: string
  optional: false
  description: Unique identifier for the model to use.
- name: input
  type: string
  optional: false
  description: Message to send to the model.
- name: system_prompt
  type: string
  optional: true
  description: System message that sets model behavior or instructions.
- name: remote_mcp_servers
  type: array<object>
  optional: true
  description: List of remote MCP servers that the model can call.
  children:
    - name: server_label
      type: string
      description: Label to identify the MCP server.
    - name: server_url
      type: string
      description: URL of the remote MCP server.
    - name: allowed_tools
      type: array<string>
      optional: true
      description: List of tool names the model can call from this server.
    - name: headers
      type: object
      optional: true
      description: Custom HTTP headers to send with requests to the server.
- name: plugins
  type: array<string | object>
  optional: true
  description: Plugins to use tools from (e.g., `mcp.json` defined MCP servers like `mcp/playwright`). Each item can be a plugin identifier string, or an object plugin specification.
  children:
    - name: Plugin identifier string
      type: string
      description: Identifier of the plugin.
    - name: Plugin specification
      type: object
      description: Object form to specify allowed tools.
      children:
        - name: id
          type: string
          optional: true
          description: Identifier of the plugin.
        - name: allowed_tools
          type: array<string> | null
          optional: true
          description: Restrict which tools from the plugin can be used.
- name: stream
  type: boolean
  optional: true
  description: Whether to stream partial outputs via SSE. Default `false`. See [streaming events](/docs/developer/rest/streaming-events) for more information.
- name: temperature
  type: number
  optional: true
  description: Randomness in token selection. 0 is deterministic, higher values increase creativity [0,1].
- name: top_p
  type: number
  optional: true
  description: Minimum cumulative probability for the possible next tokens [0,1].
- name: top_k
  type: integer
  optional: true
  description: Limits next token selection to top-k most probable tokens.
- name: min_p
  type: number
  optional: true
  description: Minimum base probability for a token to be selected for output [0,1].
- name: repeat_penalty
  type: number
  optional: true
  description: Penalty for repeating token sequences. 1 is no penalty, higher values discourage repetition.
- name: max_output_tokens
  type: integer
  optional: true
  description: Maximum number of tokens to generate.
- name: reasoning
  type: '"off" | "low" | "medium" | "high" | "on"'
  optional: true
  description: Reasoning setting. Will error if the model being used does not support the reasoning setting using. Defaults to the automatically chosen setting for the model.
- name: context_length
  type: integer
  optional: true
  description: Number of tokens to consider as context. Higher values recommended for MCP usage.
- name: store
  type: boolean
  optional: true
  description: Whether to store the chat/thread. If set, response will return a `"thread_id"` field. Default `true`.
- name: thread_id
  type: string
  optional: true
  description: Identifier of existing thread to append to. Must start with `"thread_"`.
```
:::split:::
```lms_code_snippet
title: Example Request
variants:
  curl:
    language: bash
    code: |
      curl http://127.0.0.1:1234/api/v1/chat \
        -H "Content-Type: application/json" \
        -d '{    
          "model": "openai/gpt-oss-20b",
          "input": "What is the top trending model on huggingface?",
          "remote_mcp_servers": [
            {                   
              "server_label": "huggingface", 
              "server_url": "https://huggingface.co/mcp",
              "allowed_tools": ["model_search"]
            }                    
          ]
        }'
```
````

---

````lms_hstack
**Response fields**
```lms_params
- name: model_instance_id
  type: string
  description: Unique identifier for the loaded model instance that generated the response.
- name: output
  type: array<object>
  description: Array of output items generated. Each item can be one of three types.
  children:
    - name: Message
      type: object
      description: A text message from the model.
      children:
        - name: type
          type: '"message"'
          description: Type of output item.
        - name: content
          type: string
          description: Text content of the message.
    - name: Tool call
      type: object
      description: A tool call made by the model.
      children:
        - name: type
          type: '"tool_call"'
          description: Type of output item.
        - name: tool
          type: string
          description: Name of the tool called.
        - name: arguments
          type: object
          description: Arguments passed to the tool. Can have any keys/values depending on the tool definition.
        - name: output
          type: string
          description: Result returned from the tool.
        - name: provider_info
          type: object
          description: Information about the tool provider.
          children:
            - name: type
              type: '"plugin" | "remote_mcp"'
              description: Provider type.
            - name: plugin_id
              type: string
              optional: true
              description: Identifier of the plugin (when `type` is `"plugin"`).
            - name: server_label
              type: string
              optional: true
              description: Label of the MCP server (when `type` is `"remote_mcp"`).
    - name: Reasoning
      type: object
      description: Reasoning content from the model.
      children:
        - name: type
          type: '"reasoning"'
          description: Type of output item.
        - name: content
          type: string
          description: Text content of the reasoning.
- name: stats
  type: object
  description: Token usage and performance metrics.
  children:
    - name: input_tokens
      type: number
      description: Number of input tokens. Includes formatting, tool definitions, and prior messages in the thread.
    - name: total_output_tokens
      type: number
      description: Total number of output tokens generated.
    - name: reasoning_output_tokens
      type: number
      description: Number of tokens used for reasoning.
    - name: tokens_per_second
      type: number
      description: Generation speed in tokens per second.
    - name: time_to_first_token_seconds
      type: number
      description: Time in seconds to generate the first token.
- name: thread_id
  type: string
  optional: true
  description: Identifier of the thread for subsequent requests. Starts with `"thread_"`. Present when `store` is `true`.
```
:::split:::
```lms_code_snippet
title: Response
variants:
  json:
    language: json
    code: |
      {
        "model_instance_id": "openai/gpt-oss-20b",
        "output": [
          {
            "type": "reasoning",
            "content": "Need to call function."
          },
          {
            "type": "tool_call",
            "tool": "model_search",
            "arguments": {
              "sort": "trendingScore",
              "limit": 1
            },
            "output": "[{\"type\":\"text\",\"text\":\"Showing first 1 models...\"}]",
            "provider_info": {
              "type": "remote_mcp"
              "server_label": "huggingface",
            }
          },
          {
            "type": "message",
            "content": "The current top‑trending model is..."
          }
        ],
        "stats": {
          "input_tokens": 329,
          "total_output_tokens": 268,
          "reasoning_output_tokens": 5,
          "tokens_per_second": 43.73263766917279,
          "time_to_first_token_seconds": 0.781
        },
        "thread_id": "thread_02b2017dbc06c12bfc353a2ed6c2b802f8cc682884bb5716"
      }
```
````


--- 1_developer/3_openai-compat/chat-completions.md ---
---
title: Chat Completions
description: Send a chat history and get the assistant's response.
index: 4
api_info:
  method: POST
---

- Method: `POST`
- Prompt template is applied automatically for chat‑tuned models
- Provide inference parameters (temperature, top_p, etc.) in the payload
- See OpenAI docs: https://platform.openai.com/docs/api-reference/chat
- Tip: keep a terminal open with [`lms log stream`](/docs/cli/log-stream) to inspect model input

##### Python example

```python
from openai import OpenAI
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

completion = client.chat.completions.create(
  model="model-identifier",
  messages=[
    {"role": "system", "content": "Always answer in rhymes."},
    {"role": "user", "content": "Introduce yourself."}
  ],
  temperature=0.7,
)

print(completion.choices[0].message)
```

### Supported payload parameters

See https://platform.openai.com/docs/api-reference/chat/create for parameter semantics.

```py
model
top_p
top_k
messages
temperature
max_tokens
stream
stop
presence_penalty
frequency_penalty
logit_bias
repeat_penalty
seed
```


--- 1_developer/3_openai-compat/completions.md ---
---
title: Completions (Legacy)
description: Text completion for base models (legacy OpenAI endpoint).
index: 6
api_info:
  method: POST
---

```lms_warning
This endpoint is no longer supported by OpenAI. LM Studio continues to support it.

Using this endpoint with chat‑tuned models may produce unexpected tokens. Prefer base models.
```

- Method: `POST`
- Prompt template is not applied
- See OpenAI docs: https://platform.openai.com/docs/api-reference/completions


--- 1_developer/_2_rest/download.md ---
---
title: "Download a model"
description: "Download models from LM Studio Hub or Hugging Face"
fullPage: true
index: 5
api_info:
  method: POST
---

````lms_hstack
`POST /api/v1/models/download`

**Request body**
```lms_params
- name: model
  type: string
  optional: false
  description: The model to download.
- name: quantization
  type: string
  optional: true
  description: Specifies the quantization level for the hugging face model download. Example `Q4_K_M`.
```
:::split:::
```lms_code_snippet
title: Example Request
variants:
  curl:
    language: bash
    code: |
      curl http://127.0.0.1:1234/api/v1/models/download \
        -H "Content-Type: application/json" \
        -d '{
          "model": "qwen/qwen3-4b-2507"
        }'
```
````

````lms_hstack
**Response fields**

Returns a download job status object. The response varies based on the download status.

```lms_params
- name: job_id
  type: string
  optional: true
  description: Unique identifier for the download job. Absent when `status` is `already_downloaded`.
- name: status
  type: '"downloading" | "paused" | "completed" | "failed" | "already_downloaded"'
  description: Current status of the download.
- name: bytes_per_second
  type: number
  optional: true
  description: Current download speed in bytes per second. Present when `status` is `downloading`.
- name: estimated_completion
  type: string
  optional: true
  description: Estimated completion time in ISO 8601 format. Present when `status` is `downloading`.
- name: completed_at
  type: string
  optional: true
  description: Download completion time in ISO 8601 format. Present when `status` is `completed`.
- name: total_size_bytes
  type: number
  optional: true
  description: Total size of the download in bytes. Absent when `status` is `already_downloaded`.
- name: downloaded_bytes
  type: number
  optional: true
  description: Number of bytes downloaded so far. Absent when `status` is `already_downloaded`.
- name: items
  type: array
  optional: true
  description: Array of items being downloaded. Absent when `status` is `already_downloaded`.
  children:
    - name: type
      type: '"model" | "model_yaml"'
      description: Type of item being downloaded.
    - name: id
      type: string
      description: Unique identifier for the item.
    - name: size_bytes
      type: number
      description: Size of the item in bytes.
    - name: publisher
      type: string
      optional: true
      description: Model owner or repo owner for HF. Present when `type` is `model`.
    - name: display_name
      type: string
      optional: true
      description: Model display name. Present when `type` is `model`.
    - name: url
      type: string
      optional: true
      description: URL to the model page. Present when `type` is `model`.
    - name: quantization
      type: object | null
      optional: true
      description: Quantization information object. Present when `type` is `model`.
      children:
        - name: name
          type: string | null
          description: The quantization level (e.g., `4BIT`).
    - name: format
      type: string | null
      optional: true
      description: Model format (e.g., `mlx`). Present when `type` is `model`.
- name: started_at
  type: string
  optional: true
  description: Download start time in ISO 8601 format. Absent when `status` is `already_downloaded`.
```
:::split:::
```lms_code_snippet
title: Response
variants:
  json:
    language: json
    code: |
      {
        "job_id": "job_493c7c9ded",
        "status": "downloading",
        "total_size_bytes": 2279145003,
        "downloaded_bytes": 948,
        "items": [
          {
            "type": "model_yaml",
            "size_bytes": 171008,
            "id": "qwen/qwen3-4b-2507/model.yaml"
          },
          {
            "type": "model",
            "publisher": "qwen",
            "id": "qwen/qwen3-4b-2507",
            "display_name": "Qwen3 4B Instruct 2507",
            "url": "https://lmstudio.ai/models/qwen/qwen3-4b-2507",
            "size_bytes": 2279145003,
            "quantization": {
              "name": "4BIT"
            },
            "format": "mlx"
          }
        ],
        "bytes_per_second": 7834.710743801653,
        "estimated_completion": "2025-10-07T00:21:47.030Z",
        "started_at": "2025-10-03T15:33:23.496Z"
      }
```
````


--- 1_python/index.md ---
---
title: "`lmstudio-python` (Python SDK)"
sidebar_title: "Introduction"
description: "Getting started with LM Studio's Python SDK"
---

`lmstudio-python` provides you a set APIs to interact with LLMs, embeddings models, and agentic flows.

## Installing the SDK

`lmstudio-python` is available as a PyPI package. You can install it using pip.

```lms_code_snippet
  variants:
    pip:
      language: bash
      code: |
        pip install lmstudio
```

For the source code and open source contribution, visit [lmstudio-python](https://github.com/lmstudio-ai/lmstudio-python) on GitHub.

## Features

- Use LLMs to [respond in chats](./python/llm-prediction/chat-completion) or predict [text completions](./python/llm-prediction/completion)
- Define functions as tools, and turn LLMs into [autonomous agents](./python/agent) that run completely locally
- [Load](./python/manage-models/loading), [configure](./python/llm-prediction/parameters), and [unload](./python/manage-models/loading) models from memory
- Generate embeddings for text, and more!

## Quick Example: Chat with a Llama Model

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        model = lms.llm("qwen/qwen3-4b-2507")
        result = model.respond("What is the meaning of life?")

        print(result)

    "Python (scoped resource API)":
      language: python
      code: |
        import lmstudio as lms

        with lms.Client() as client:
            model = client.llm.model("qwen/qwen3-4b-2507")
            result = model.respond("What is the meaning of life?")

            print(result)

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms

        async with lms.AsyncClient() as client:
            model = await client.llm.model("qwen/qwen3-4b-2507")
            result = await model.respond("What is the meaning of life?")

            print(result)
```

### Getting Local Models

The above code requires the [qwen3-4b-2507](https://lmstudio.ai/models/qwen/qwen3-4b-2507) model.
If you don't have the model, run the following command in the terminal to download it.

```bash
lms get qwen/qwen3-4b-2507
```

Read more about `lms get` in LM Studio's CLI [here](./cli/get).

# Interactive Convenience, Deterministic Resource Management, or Structured Concurrency?

As shown in the example above, there are three distinct approaches for working
with the LM Studio Python SDK.

The first is the interactive convenience API (listed as "Python (convenience API)"
in examples), which focuses on the use of a default LM Studio client instance for
convenient interactions at a synchronous Python prompt, or when using Jupyter notebooks.

The second is a synchronous scoped resource API (listed as "Python (scoped resource API)"
in examples), which uses context managers to ensure that allocated resources
(such as network connections) are released deterministically, rather than
potentially remaining open until the entire process is terminated.

The last is an asynchronous structured concurrency API (listed as "Python (asynchronous API)" in
examples), which is designed for use in asynchronous programs that follow the design principles of
["structured concurrency"](https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/)
in order to ensure the background tasks handling the SDK's connections to the API server host
are managed correctly. Asynchronous applications which do not adhere to those design principles
will need to rely on threaded access to the synchronous scoped resource API rather than attempting
to use the SDK's native asynchronous API. Python SDK version 1.5.0 is the first version to fully
support the asynchronous API.

Some examples are common between the interactive convenience API and the synchronous scoped
resource API. These examples are listed as "Python (synchronous API)".

## Timeouts in the synchronous API

_Required Python SDK version_: **1.5.0**

Starting in Python SDK version 1.5.0, the synchronous API defaults to timing out after 60 seconds
with no activity when waiting for a response or streaming event notification from the API server.

The number of seconds to wait for responses and event notifications can be adjusted using the
`lmstudio.set_sync_api_timeout()` function. Setting the timeout to `None` disables the timeout
entirely (restoring the behaviour of previous SDK versions).

The current synchronous API timeout can be queried using the `lmstudio.get_sync_api_timeout()`
function.

## Timeouts in the asynchronous API

_Required Python SDK version_: **1.5.0**

As asynchronous coroutines support cancellation, there is no specific timeout support implemented
in the asynchronous API. Instead, general purpose async timeout mechanisms, such as
[`asyncio.wait_for()`](https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for) or
[`anyio.move_on_after()`](https://anyio.readthedocs.io/en/stable/cancellation.html#timeouts),
should be used.


--- 1_python/3_embedding/index.md ---
---
title: Embedding
sidebar_title: Generating embedding vectors
description: Generate text embeddings from input text
---

Generate embeddings for input text. Embeddings are vector representations of text that capture semantic meaning. Embeddings are a building block for RAG (Retrieval-Augmented Generation) and other similarity-based tasks.

### Prerequisite: Get an Embedding Model

If you don't yet have an embedding model, you can download a model like `nomic-ai/nomic-embed-text-v1.5` using the following command:

```bash
lms get nomic-ai/nomic-embed-text-v1.5
```

## Create Embeddings

To convert a string to a vector representation, pass it to the `embed` method on the corresponding embedding model handle.

```lms_code_snippet
  title: "example.py"
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        model = lms.embedding_model("nomic-embed-text-v1.5")

        embedding = model.embed("Hello, world!")

```


--- 1_python/4_tokenization/index.md ---
---
title: Tokenization
sidebar_title: Tokenizing text
description: Tokenize text using a model's tokenizer
---

Models use a tokenizer to internally convert text into "tokens" they can deal with more easily. LM Studio exposes this tokenizer for utility.

## Tokenize

You can tokenize a string with a loaded LLM or embedding model using the SDK.
In the below examples, the LLM reference can be replaced with an
embedding model reference without requiring any other changes.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        model = lms.llm()

        tokens = model.tokenize("Hello, world!")

        print(tokens) # Array of token IDs.
```

## Count tokens

If you only care about the number of tokens, simply check the length of the resulting array.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        token_count = len(model.tokenize("Hello, world!"))
        print("Token count:", token_count)
```

### Example: count context

You can determine if a given conversation fits into a model's context by doing the following:

1. Convert the conversation to a string using the prompt template.
2. Count the number of tokens in the string.
3. Compare the token count to the model's context length.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        def does_chat_fit_in_context(model: lms.LLM, chat: lms.Chat) -> bool:
            # Convert the conversation to a string using the prompt template.
            formatted = model.apply_prompt_template(chat)
            # Count the number of tokens in the string.
            token_count = len(model.tokenize(formatted))
            # Get the current loaded context length of the model
            context_length = model.get_context_length()
            return token_count < context_length

        model = lms.llm()

        chat = lms.Chat.from_history({
            "messages": [
                { "role": "user", "content": "What is the meaning of life." },
                { "role": "assistant", "content": "The meaning of life is..." },
                # ... More messages
            ]
        })

        print("Fits in context:", does_chat_fit_in_context(model, chat))

```


--- 1_python/2_agent/act.md ---
---
title: The `.act()` call
description: How to use the `.act()` call to turn LLMs into autonomous agents that can perform tasks on your local machine.
index: 1
---

## Automatic tool calling

We introduce the concept of execution "rounds" to describe the combined process of running a tool, providing its output to the LLM, and then waiting for the LLM to decide what to do next.

**Execution Round**

```
 • run a tool ->
 ↑   • provide the result to the LLM ->
 │       • wait for the LLM to generate a response
 │
 └────────────────────────────────────────┘ └➔ (return)
```

A model might choose to run tools multiple times before returning a final result. For example, if the LLM is writing code, it might choose to compile or run the program, fix errors, and then run it again, rinse and repeat until it gets the desired result.

With this in mind, we say that the `.act()` API is an automatic "multi-round" tool calling API.

### Quick Example

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        def multiply(a: float, b: float) -> float:
            """Given two numbers a and b. Returns the product of them."""
            return a * b

        model = lms.llm("qwen2.5-7b-instruct")
        model.act(
          "What is the result of 12345 multiplied by 54321?",
          [multiply],
          on_message=print,
        )
```

### What does it mean for an LLM to "use a tool"?

LLMs are largely text-in, text-out programs. So, you may ask "how can an LLM use a tool?". The answer is that some LLMs are trained to ask the human to call the tool for them, and expect the tool output to to be provided back in some format.

Imagine you're giving computer support to someone over the phone. You might say things like "run this command for me ... OK what did it output? ... OK now click there and tell me what it says ...". In this case you're the LLM! And you're "calling tools" vicariously through the person on the other side of the line.

### Running multiple tool calls in parallel

By default, version 1.4.0 and later of the Python SDK will only run a single tool call request at a time,
even if the model requests multiple tool calls in a single response message. This ensures the requests will
be processed correctly even if the tool implementations do not support multiple concurrent calls.

When the tool implementations are known to be thread-safe, and are both slow and frequent enough to be worth
running in parallel, the `max_parallel_tool_calls` option specifies the maximum number of tool call requests
that will be processed in parallel from a single model response. This value defaults to 1 (waiting for each
tool call to complete before starting the next one). Setting this value to `None` will automatically scale
the maximum number of parallel tool calls to a multiple of the number of CPU cores available to the process.

### Important: Model Selection

The model selected for tool use will greatly impact performance.

Some general guidance when selecting a model:

- Not all models are capable of intelligent tool use
- Bigger is better (i.e., a 7B parameter model will generally perform better than a 3B parameter model)
- We've observed [Qwen2.5-7B-Instruct](https://model.lmstudio.ai/download/lmstudio-community/Qwen2.5-7B-Instruct-GGUF) to perform well in a wide variety of cases
- This guidance may change

### Example: Multiple Tools

The following code demonstrates how to provide multiple tools in a single `.act()` call.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import math
        import lmstudio as lms

        def add(a: int, b: int) -> int:
            """Given two numbers a and b, returns the sum of them."""
            return a + b

        def is_prime(n: int) -> bool:
            """Given a number n, returns True if n is a prime number."""
            if n < 2:
                return False
            sqrt = int(math.sqrt(n))
            for i in range(2, sqrt):
                if n % i == 0:
                    return False
            return True

        model = lms.llm("qwen2.5-7b-instruct")
        model.act(
          "Is the result of 12345 + 45668 a prime? Think step by step.",
          [add, is_prime],
          on_message=print,
        )
```

### Example: Chat Loop with Create File Tool

The following code creates a conversation loop with an LLM agent that can create files.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import readline # Enables input line editing
        from pathlib import Path

        import lmstudio as lms

        def create_file(name: str, content: str):
            """Create a file with the given name and content."""
            dest_path = Path(name)
            if dest_path.exists():
                return "Error: File already exists."
            try:
                dest_path.write_text(content, encoding="utf-8")
            except Exception as exc:
                return "Error: {exc!r}"
            return "File created."

        def print_fragment(fragment, round_index=0):
            # .act() supplies the round index as the second parameter
            # Setting a default value means the callback is also
            # compatible with .complete() and .respond().
            print(fragment.content, end="", flush=True)

        model = lms.llm()
        chat = lms.Chat("You are a task focused AI assistant")

        while True:
            try:
                user_input = input("You (leave blank to exit): ")
            except EOFError:
                print()
                break
            if not user_input:
                break
            chat.add_user_message(user_input)
            print("Bot: ", end="", flush=True)
            model.act(
                chat,
                [create_file],
                on_message=chat.append,
                on_prediction_fragment=print_fragment,
            )
            print()

```

### Progress Callbacks

Complex interactions with a tool using agent may take some time to process.

The regular progress callbacks for any prediction request are available,
but the expected capabilities differ from those for single round predictions.

* `on_prompt_processing_progress`: called during prompt processing for each
  prediction round. Receives the progress ratio (as a float) and the round
  index as positional arguments.
* `on_first_token`: called after prompt processing is complete for each prediction round.
  Receives the round index as its sole argument.
* `on_prediction_fragment`: called for each prediction fragment received by the client.
  Receives the prediction fragment and the round index as positional arguments.
* `on_message`: called with an assistant response message when each prediction round is
  complete, and with tool result messages as each tool call request is completed.
  Intended for appending received messages to a chat history instance, and hence
  does *not* receive the round index as an argument.

The following additional callbacks are available to monitor the prediction rounds:

* `on_round_start`: called before submitting the prediction request for each round.
  Receives the round index as its sole argument.
* `on_prediction_completed`: called after the prediction for the round has been completed,
  but before any requested tool calls have been initiated. Receives the round's prediction
  result as its sole argument. A round prediction result is a regular prediction result
  with an additional `round_index` attribute.
* `on_round_end`: called after any tool call requests for the round have been resolved.

Finally, applications may request notifications when agents emit invalid tool requests:

* `handle_invalid_tool_request`: called when a tool request was unable to be processed.
  Receives the exception that is about to be reported, as well as the original tool
  request that resulted in the problem. When no tool request is given, this is
  purely a notification of an unrecoverable error before the agent interaction raises
  the given exception (allowing the application to raise its own exception instead).
  When a tool request is given, it indicates that rather than being raised locally,
  the text description of the exception is going to be passed back to the agent
  as the result of that failed tool request. In these cases, the callback may either
  return `None` to indicate that the error description should be sent to the agent,
  raise the given exception (or a different exception) locally, or return a text
  string that should be sent to the agent instead of the error description.

For additional details on defining tools, and an example of overriding the invalid
tool request handling to raise all exceptions locally instead of passing them to
back the agent, refer to [Tool Definition](./tools.md).


--- 1_python/_more/_apply-prompt-template.md ---
---
title: Apply Prompt Template
description: Apply a model's prompt template to a conversation
---

## Overview

LLMs (Large Language Models) operate on a text-in, text-out basis. Before processing conversations through these models, the input must be converted into a properly formatted string using a prompt template. If you need to inspect or work with this formatted string directly, the LM Studio SDK provides a streamlined way to apply a model's prompt template to your conversations.

```lms_info
You do not need to use this method when using `.respond`. It will automatically apply the prompt template for you.
```

## Usage with a Chat

You can apply a prompt template to a `Chat` by using the `applyPromptTemplate` method. This method takes a `Chat` object as input and returns a formatted string.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import { Chat, LMStudioClient } from "@lmstudio/sdk";

        client = new LMStudioClient()
        model = client.llm.model() # Use any loaded LLM

        chat = Chat.createEmpty()
        chat.append("system", "You are a helpful assistant.")
        chat.append("user", "What is LM Studio?")
        
        formatted = model.applyPromptTemplate(chat)
        print(formatted)
```

## Usage with an Array of Messages

The same method can also be used with any object that can be converted to a `Chat`, for example, an array of messages.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import { LMStudioClient } from "@lmstudio/sdk";

        client = new LMStudioClient()
        model = client.llm.model() # Use any loaded LLM

        formatted = model.applyPromptTemplate([
          { role: "system", content: "You are a helpful assistant." },
          { role: "user", content: "What is LM Studio?" },
        ])
        print(formatted)
```


--- 1_python/1_llm-prediction/cancelling-predictions.md ---
---
title: Cancelling Predictions
description: Stop an ongoing prediction in `lmstudio-python`
index: 4
---

One benefit of using the streaming API is the ability to cancel the
prediction request based on criteria that can't be represented using
the `stopStrings` or `maxPredictedTokens` configuration settings.

The following snippet illustrates cancelling the request in response
to an application specification cancellation condition (such as polling
an event set by another thread).

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms
        model = lms.llm()

        prediction_stream = model.respond_stream("What is the meaning of life?")
        cancelled = False
        for fragment in prediction_stream:
            if ...: # Cancellation condition will be app specific
                cancelled = True
                prediction_stream.cancel()
                # Note: it is recommended to let the iteration complete,
                # as doing so allows the partial result to be recorded.
                # Breaking the loop *is* permitted, but means the partial result
                # and final prediction stats won't be available to the client
        # The stream allows the prediction result to be retrieved after iteration
        if not cancelled:
            print(prediction_stream.result())

    "Python (scoped resource API)":
      language: python
      code: |
        import lmstudio as lms

        with lms.Client() as client:
            model = client.llm.model()

            prediction_stream = model.respond_stream("What is the meaning of life?")
            cancelled = False
            for fragment in prediction_stream:
                if ...: # Cancellation condition will be app specific
                    cancelled = True
                    prediction_stream.cancel()
                    # Note: it is recommended to let the iteration complete,
                    # as doing so allows the partial result to be recorded.
                    # Breaking the loop *is* permitted, but means the partial result
                    # and final prediction stats won't be available to the client
            # The stream allows the prediction result to be retrieved after iteration
            if not cancelled:
                print(prediction_stream.result())

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms

        async with lms.AsyncClient() as client:
            model = await client.llm.model()

            prediction_stream = await model.respond_stream("What is the meaning of life?")
            cancelled = False
            async for fragment in prediction_stream:
                if ...: # Cancellation condition will be app specific
                    cancelled = True
                    await prediction_stream.cancel()
                    # Note: it is recommended to let the iteration complete,
                    # as doing so allows the partial result to be recorded.
                    # Breaking the loop *is* permitted, but means the partial result
                    # and final prediction stats won't be available to the client
            # The stream allows the prediction result to be retrieved after iteration
            if not cancelled:
                print(prediction_stream.result())

```


--- 1_python/1_llm-prediction/chat-completion.md ---
---
title: Chat Completions
sidebar_title: Chat
description: APIs for a multi-turn chat conversations with an LLM
index: 2
---

Use `llm.respond(...)` to generate completions for a chat conversation.

## Quick Example: Generate a Chat Response

The following snippet shows how to obtain the AI's response to a quick chat prompt.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        model = lms.llm()
        print(model.respond("What is the meaning of life?"))

    "Python (scoped resource API)":
      language: python
      code: |
        import lmstudio as lms

        with lms.Client() as client:
            model = client.llm.model()
            print(model.respond("What is the meaning of life?"))

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms

        async with lms.AsyncClient() as client:
            model = await client.llm.model()
            print(await model.respond("What is the meaning of life?"))

```

## Streaming a Chat Response

The following snippet shows how to stream the AI's response to a chat prompt,
displaying text fragments as they are received (rather than waiting for the
entire response to be generated before displaying anything).

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms
        model = lms.llm()

        for fragment in model.respond_stream("What is the meaning of life?"):
            print(fragment.content, end="", flush=True)
        print() # Advance to a new line at the end of the response

    "Python (scoped resource API)":
      language: python
      code: |
        import lmstudio as lms

        with lms.Client() as client:
            model = client.llm.model()

            for fragment in model.respond_stream("What is the meaning of life?"):
                print(fragment.content, end="", flush=True)
            print() # Advance to a new line at the end of the response

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms

        async with lms.AsyncClient() as client:
            model = await client.llm.model()

            async for fragment in model.respond_stream("What is the meaning of life?"):
                print(fragment.content, end="", flush=True)
            print() # Advance to a new line at the end of the response

```

## Cancelling a Chat Response

See the [Cancelling a Prediction](./cancelling-predictions) section for how to cancel a prediction in progress.

## Obtain a Model

First, you need to get a model handle.
This can be done using the top-level `llm` convenience API,
or the `model` method in the `llm` namespace when using the scoped resource API.
For example, here is how to use Qwen2.5 7B Instruct.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        model = lms.llm("qwen2.5-7b-instruct")

    "Python (scoped resource API)":
      language: python
      code: |
        import lmstudio as lms

        with lms.Client() as client:
            model = client.llm.model("qwen2.5-7b-instruct")

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms

        async with lms.AsyncClient() as client:
            model = await client.llm.model("qwen2.5-7b-instruct")

```

There are other ways to get a model handle. See [Managing Models in Memory](./../manage-models/loading) for more info.

## Manage Chat Context

The input to the model is referred to as the "context".
Conceptually, the model receives a multi-turn conversation as input,
and it is asked to predict the assistant's response in that conversation.

```lms_code_snippet
  variants:
    "Constructing a Chat object":
      language: python
      code: |
        import lmstudio as lms

        # Create a chat with an initial system prompt.
        chat = lms.Chat("You are a resident AI philosopher.")

        # Build the chat context by adding messages of relevant types.
        chat.add_user_message("What is the meaning of life?")
        # ... continued in next example

  "From chat history data":
      language: python
      code: |
        import lmstudio as lms

        # Create a chat object from a chat history dict
        chat = lms.Chat.from_history({
            "messages": [
                { "role": "system", "content": "You are a resident AI philosopher." },
                { "role": "user", "content": "What is the meaning of life?" },
            ]
        })
        # ... continued in next example

```

See [Working with Chats](./working-with-chats) for more information on managing chat context.

<!-- , and [`Chat`](./../api-reference/chat) for API reference for the `Chat` class. -->

## Generate a response

You can ask the LLM to predict the next response in the chat context using the `respond()` method.

```lms_code_snippet
  variants:
    "Non-streaming (synchronous API)":
      language: python
      code: |
        # The `chat` object is created in the previous step.
        result = model.respond(chat)

        print(result)

    "Streaming (synchronous API)":
      language: python
      code: |
        # The `chat` object is created in the previous step.
        prediction_stream = model.respond_stream(chat)

        for fragment in prediction_stream:
            print(fragment.content, end="", flush=True)
        print() # Advance to a new line at the end of the response

    "Non-streaming (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        # The `chat` object is created in the previous step.
        result = await model.respond(chat)

        print(result)

    "Streaming (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        # The `chat` object is created in the previous step.
        prediction_stream = await model.respond_stream(chat)

        async for fragment in prediction_stream:
            print(fragment.content, end="", flush=True)
        print() # Advance to a new line at the end of the response

```

## Customize Inferencing Parameters

You can pass in inferencing parameters via the `config` keyword parameter on `.respond()`.

```lms_code_snippet
  variants:
    "Non-streaming (synchronous API)":
      language: python
      code: |
        result = model.respond(chat, config={
            "temperature": 0.6,
            "maxTokens": 50,
        })

    "Streaming (synchronous API)":
      language: python
      code: |
        prediction_stream = model.respond_stream(chat, config={
            "temperature": 0.6,
            "maxTokens": 50,
        })

    "Non-streaming (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        result = await model.respond(chat, config={
            "temperature": 0.6,
            "maxTokens": 50,
        })

    "Streaming (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        prediction_stream = await model.respond_stream(chat, config={
            "temperature": 0.6,
            "maxTokens": 50,
        })

```

See [Configuring the Model](./parameters) for more information on what can be configured.

## Print prediction stats

You can also print prediction metadata, such as the model used for generation, number of generated
tokens, time to first token, and stop reason.

```lms_code_snippet
  variants:
    "Non-streaming":
      language: python
      code: |
        # `result` is the response from the model.
        print("Model used:", result.model_info.display_name)
        print("Predicted tokens:", result.stats.predicted_tokens_count)
        print("Time to first token (seconds):", result.stats.time_to_first_token_sec)
        print("Stop reason:", result.stats.stop_reason)

    "Streaming":
      language: python
      code: |
        # After iterating through the prediction fragments,
        # the overall prediction result may be obtained from the stream
        result = prediction_stream.result()

        print("Model used:", result.model_info.display_name)
        print("Predicted tokens:", result.stats.predicted_tokens_count)
        print("Time to first token (seconds):", result.stats.time_to_first_token_sec)
        print("Stop reason:", result.stats.stop_reason)

```

Both the non-streaming and streaming result access is consistent across the synchronous and
asynchronous APIs, as `prediction_stream.result()` is a non-blocking API that raises an exception
if no result is available (either because the prediction is still running, or because the
prediction request failed). Prediction streams also offer a blocking (synchronous API) or
awaitable (asynchronous API) `prediction_stream.wait_for_result()` method that internally handles
iterating the stream to completion before returning the result.

## Example: Multi-turn Chat

```lms_code_snippet
  title: "chatbot.py"
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        model = lms.llm()
        chat = lms.Chat("You are a task focused AI assistant")

        while True:
            try:
                user_input = input("You (leave blank to exit): ")
            except EOFError:
                print()
                break
            if not user_input:
                break
            chat.add_user_message(user_input)
            prediction_stream = model.respond_stream(
                chat,
                on_message=chat.append,
            )
            print("Bot: ", end="", flush=True)
            for fragment in prediction_stream:
                print(fragment.content, end="", flush=True)
            print()

```

### Progress Callbacks

Long prompts will often take a long time to first token, i.e. it takes the model a long time to process your prompt.
If you want to get updates on the progress of this process, you can provide a float callback to `respond`
that receives a float from 0.0-1.0 representing prompt processing progress.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        llm = lms.llm()

        response = llm.respond(
            "What is LM Studio?",
            on_prompt_processing_progress = (lambda progress: print(f"{progress*100}% complete")),
        )

    "Python (scoped resource API)":
      language: python
      code: |
        import lmstudio as lms

        with lms.Client() as client:
            llm = client.llm.model()

            response = llm.respond(
                "What is LM Studio?",
                on_prompt_processing_progress = (lambda progress: print(f"{progress*100}% complete")),
            )

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms

        async with lms.AsyncClient() as client:
            llm = await client.llm.model()

            response = await llm.respond(
                "What is LM Studio?",
                on_prompt_processing_progress = (lambda progress: print(f"{progress*100}% complete")),
            )


```

In addition to `on_prompt_processing_progress`, the other available progress callbacks are:

- `on_first_token`: called after prompt processing is complete and the first token is being emitted.
  Does not receive any arguments (use the streaming iteration API or `on_prediction_fragment`
  to process tokens as they are emitted).
- `on_prediction_fragment`: called for each prediction fragment received by the client.
  Receives the same prediction fragments as iterating over the stream iteration API.
- `on_message`: called with an assistant response message when the prediction is complete.
  Intended for appending received messages to a chat history instance.


--- 1_python/1_llm-prediction/completion.md ---
---
title: Text Completions
description: "Provide a string input for the model to complete"
---

Use `llm.complete(...)` to generate text completions from a loaded language model.
Text completions mean sending a non-formatted string to the model with the expectation that the model will complete the text.

This is different from multi-turn chat conversations. For more information on chat completions, see [Chat Completions](./chat-completion).

## 1. Instantiate a Model

First, you need to load a model to generate completions from.
This can be done using the top-level `llm` convenience API,
or the `model` method in the `llm` namespace when using the scoped resource API.
For example, here is how to use Qwen2.5 7B Instruct.


```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        model = lms.llm("qwen2.5-7b-instruct")

    "Python (scoped resource API)":
      language: python
      code: |
        import lmstudio as lms

        with lms.Client() as client:
            model = client.llm.model("qwen2.5-7b-instruct")

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms

        async with lms.AsyncClient() as client:
            model = await client.llm.model("qwen2.5-7b-instruct")

```

## 2. Generate a Completion

Once you have a loaded model, you can generate completions by passing a string to the `complete` method on the `llm` handle.

```lms_code_snippet
  variants:
    "Non-streaming (synchronous API)":
      language: python
      code: |
        # The `chat` object is created in the previous step.
        result = model.complete("My name is", config={"maxTokens": 100})

        print(result)

    "Streaming (synchronous API)":
      language: python
      code: |
        # The `chat` object is created in the previous step.
        prediction_stream = model.complete_stream("My name is", config={"maxTokens": 100})

        for fragment in prediction_stream:
            print(fragment.content, end="", flush=True)
        print() # Advance to a new line at the end of the response

    "Non-streaming (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        # The `chat` object is created in the previous step.
        result = await model.complete("My name is", config={"maxTokens": 100})

        print(result)

    "Streaming (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        # The `chat` object is created in the previous step.
        prediction_stream = await model.complete_stream("My name is", config={"maxTokens": 100})

        async for fragment in prediction_stream:
            print(fragment.content, end="", flush=True)
        print() # Advance to a new line at the end of the response

```

## 3. Print Prediction Stats

You can also print prediction metadata, such as the model used for generation, number of generated tokens, time to first token, and stop reason.

```lms_code_snippet
  variants:
    "Non-streaming":
      language: python
      code: |
        # `result` is the response from the model.
        print("Model used:", result.model_info.display_name)
        print("Predicted tokens:", result.stats.predicted_tokens_count)
        print("Time to first token (seconds):", result.stats.time_to_first_token_sec)
        print("Stop reason:", result.stats.stop_reason)

    "Streaming":
      language: python
      code: |
        # After iterating through the prediction fragments,
        # the overall prediction result may be obtained from the stream
        result = prediction_stream.result()

        print("Model used:", result.model_info.display_name)
        print("Predicted tokens:", result.stats.predicted_tokens_count)
        print("Time to first token (seconds):", result.stats.time_to_first_token_sec)
        print("Stop reason:", result.stats.stop_reason)

```

Both the non-streaming and streaming result access is consistent across the synchronous and
asynchronous APIs, as `prediction_stream.result()` is a non-blocking API that raises an exception
if no result is available (either because the prediction is still running, or because the
prediction request failed). Prediction streams also offer a blocking (synchronous API) or
awaitable (asynchronous API) `prediction_stream.wait_for_result()` method that internally handles
iterating the stream to completion before returning the result.

## Example: Get an LLM to Simulate a Terminal

Here's an example of how you might use the `complete` method to simulate a terminal.

```lms_code_snippet
  title: "terminal-sim.py"
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        model = lms.llm()
        console_history = []

        while True:
            try:
                user_command = input("$ ")
            except EOFError:
                print()
                break
            if user_command.strip() == "exit":
                break
            console_history.append(f"$ {user_command}")
            history_prompt = "\n".join(console_history)
            prediction_stream = model.complete_stream(
                history_prompt,
                config={ "stopStrings": ["$"] },
            )
            for fragment in prediction_stream:
                print(fragment.content, end="", flush=True)
            print()
            console_history.append(prediction_stream.result().content)

```

## Customize Inferencing Parameters

You can pass in inferencing parameters via the `config` keyword parameter on `.complete()`.

```lms_code_snippet
  variants:
    "Non-streaming (synchronous API)":
      language: python
      code: |
        result = model.complete(initial_text, config={
            "temperature": 0.6,
            "maxTokens": 50,
        })

    "Streaming (synchronous API)":
      language: python
      code: |
        prediction_stream = model.complete_stream(initial_text, config={
            "temperature": 0.6,
            "maxTokens": 50,
        })

    "Non-streaming (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        result = await model.complete(initial_text, config={
            "temperature": 0.6,
            "maxTokens": 50,
        })

    "Streaming (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        prediction_stream = await model.complete_stream(initial_text, config={
            "temperature": 0.6,
            "maxTokens": 50,
        })

```

See [Configuring the Model](./parameters) for more information on what can be configured.

### Progress Callbacks

Long prompts will often take a long time to first token, i.e. it takes the model a long time to process your prompt.
If you want to get updates on the progress of this process, you can provide a float callback to `complete`
that receives a float from 0.0-1.0 representing prompt processing progress.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        llm = lms.llm()

        completion = llm.complete(
            "My name is",
            on_prompt_processing_progress = (lambda progress: print(f"{progress*100}% complete")),
        )

    "Python (scoped resource API)":
      language: python
      code: |
        import lmstudio as lms

        with lms.Client() as client:
            llm = client.llm.model()

            completion = llm.complete(
                "My name is",
                on_prompt_processing_progress = (lambda progress: print(f"{progress*100}% processed")),
            )

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms

        async with lms.AsyncClient() as client:
            llm = await client.llm.model()

            completion = await llm.complete(
                "My name is",
                on_prompt_processing_progress = (lambda progress: print(f"{progress*100}% processed")),
            )

```

In addition to `on_prompt_processing_progress`, the other available progress callbacks are:

* `on_first_token`: called after prompt processing is complete and the first token is being emitted.
  Does not receive any arguments (use the streaming iteration API or `on_prediction_fragment`
  to process tokens as they are emitted).
* `on_prediction_fragment`: called for each prediction fragment received by the client.
  Receives the same prediction fragments as iterating over the stream iteration API.
* `on_message`: called with an assistant response message when the prediction is complete.
  Intended for appending received messages to a chat history instance.


--- 1_python/5_manage-models/_download-models.md ---
---
title: Download Models
description: Download models to the machine running the LM Studio server
---

TODO: model downloading is available, but the current API is a bit awkward, so hold
      off on documenting it until the interface is nicer to use

## Overview

You can browse and download models using the LM Studio SDK just like you would
in the Discover tab of the app itself. Once a model is downloaded, you can
[load it](/docs/api/sdk/load-and-access-models) for inference.

### Usage

Downloading models consists of three steps:

1. Search for the model you want;
2. Find the download option you want (e.g. quantization) and
3. Download the model!


TODO: Actually translate this example code from TS to Python

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import { LMStudioClient } from "@lmstudio/sdk";

        const client = new LMStudioClient()

        # 1. Search for the model you want
        # Specify any/all of searchTerm, limit, compatibilityTypes
        const searchResults = client.repository.searchModels({
          searchTerm: "llama 3.2 1b",    # Search for Llama 3.2 1B
          limit: 5,                      # Get top 5 results
          compatibilityTypes: ["gguf"],  # Only download GGUFs
        })

        # 2. Find download options
        const bestResult = searchResults[0];
        const downloadOptions = bestResult.getDownloadOptions()

        # Let's download Q4_K_M, a good middle ground quantization
        const desiredModel = downloadOptions.find(option => option.quantization === 'Q4_K_M')

        # 3. Download it!
        const modelKey = desiredModel.download()

        # This returns a path you can use to load the model
        const loadedModel = client.llm.model(modelKey)
```

## Advanced Usage

### Progress callbacks

TODO: TS/python differ in callback names

Model downloading can take a very long time, depending on your local network speed.
If you want to get updates on the progress of this process, you can provide callbacks to `download`:
one for progress updates and/or one when the download is being finalized
(validating checksums, etc.)

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import { LMStudioClient, type DownloadProgressUpdate } from "@lmstudio/sdk";

        function printProgressUpdate(update: DownloadProgressUpdate) {
          process.stdout.write(`Downloaded ${update.downloadedBytes} bytes of ${update.totalBytes} total \
                                at ${update.speed_bytes_per_second} bytes/sec`)
        }

        const client = new LMStudioClient()

        # ... Same code as before ...

        modelKey = desiredModel.download({
          onProgress: printProgressUpdate,
          onStartFinalizing: () => console.log("Finalizing..."),
        })

        const loadedModel = client.llm.model(modelKey)

    "Python (scoped resource API)":
      language: python
      code: |
        import lmstudio as lms

        def print_progress_update(update: lmstudio.DownloadProgressUpdate) -> None:
            print(f"Downloaded {update.downloaded_bytes} bytes of {update.total_bytes} total \
                    at {update.speed_bytes_per_second} bytes/sec")

        with lms.Client() as client:
            # ... Same code as before ...

            model_key = desired_model.download(
                on_progress=print_progress_update,
                on_finalize: lambda: print("Finalizing download...")
            )

    "Python (asynchronous API)":
      language: python
      code: |
        # Note: assumes use of an async function or the "python -m asyncio" asynchronous REPL
        # Requires Python SDK version 1.5.0 or later
        import lmstudio as lms

```


--- 1_python/6_model-info/get-context-length.md ---
---
title: Get Context Length
description: API to get the maximum context length of a model.
---

LLMs and embedding models, due to their fundamental architecture, have a property called `context length`, and more specifically a **maximum** context length. Loosely speaking, this is how many tokens the models can "keep in memory" when generating text or embeddings. Exceeding this limit will result in the model behaving erratically.

## Use the `get_context_length()` function on the model object

It's useful to be able to check the context length of a model, especially as an extra check before providing potentially long input to the model.

```lms_code_snippet
  title: "example.py"
  variants:
    "Python (convenience API)":
      language: python
      code: |
        context_length = model.get_context_length()
```

The `model` in the above code snippet is an instance of a loaded model you get from the `llm.model` method. See [Manage Models in Memory](../manage-models/loading) for more information.

### Example: Check if the input will fit in the model's context window

You can determine if a given conversation fits into a model's context by doing the following:

1. Convert the conversation to a string using the prompt template.
2. Count the number of tokens in the string.
3. Compare the token count to the model's context length.

```lms_code_snippet
  variants:
    "Python (convenience API)":
      language: python
      code: |
        import lmstudio as lms

        def does_chat_fit_in_context(model: lms.LLM, chat: lms.Chat) -> bool:
            # Convert the conversation to a string using the prompt template.
            formatted = model.apply_prompt_template(chat)
            # Count the number of tokens in the string.
            token_count = len(model.tokenize(formatted))
            # Get the current loaded context length of the model
            context_length = model.get_context_length()
            return token_count < context_length

        model = lms.llm()

        chat = lms.Chat.from_history({
            "messages": [
                { "role": "user", "content": "What is the meaning of life." },
                { "role": "assistant", "content": "The meaning of life is..." },
                # ... More messages
            ]
        })

        print("Fits in context:", does_chat_fit_in_context(model, chat))

```


--- 2_typescript/index.md ---
---
title: "`lmstudio-js` (TypeScript SDK)"
sidebar_title: "Introduction"
description: "Getting started with LM Studio's Typescript / JavaScript SDK"
---

The SDK provides you a set of programmatic tools to interact with LLMs, embeddings models, and agentic flows.

## Installing the SDK

`lmstudio-js` is available as an npm package. You can install it using npm, yarn, or pnpm.

```lms_code_snippet
  variants:
    npm:
      language: bash
      code: |
        npm install @lmstudio/sdk --save
    yarn:
      language: bash
      code: |
        yarn add @lmstudio/sdk
    pnpm:
      language: bash
      code: |
        pnpm add @lmstudio/sdk
```

For the source code and open source contribution, visit [lmstudio-js](https://github.com/lmstudio-ai/lmstudio-js) on GitHub.

## Features

- Use LLMs to [respond in chats](./typescript/llm-prediction/chat-completion) or predict [text completions](./typescript/llm-prediction/completion)
- Define functions as tools, and turn LLMs into [autonomous agents](./typescript/agent/act) that run completely locally
- [Load](./typescript/manage-models/loading), [configure](./typescript/llm-prediction/parameters), and [unload](./typescript/manage-models/loading) models from memory
- Supports for both browser and any Node-compatible environments
- Generate embeddings for text, and more!

## Quick Example: Chat with a Llama Model

```lms_code_snippet
  title: "index.ts"
  variants:
    TypeScript:
      language: typescript
      code: |
        import { LMStudioClient } from "@lmstudio/sdk";
        const client = new LMStudioClient();

        const model = await client.llm.model("qwen/qwen3-4b-2507");
        const result = await model.respond("What is the meaning of life?");

        console.info(result.content);
```

### Getting Local Models

The above code requires the [qwen3-4b-2507](https://lmstudio.ai/models/qwen/qwen3-4b-2507). If you don't have the model, run the following command in the terminal to download it.

```bash
lms get qwen/qwen3-4b-2507
```

Read more about `lms get` in LM Studio's CLI [here](./cli/get).


--- 2_typescript/3_plugins/index.md ---
---
title: "Introduction to Plugins"
description: "A brief introduction to making plugins for LM Studio using TypeScript."
index: 1
---

Plugins extend LM Studio's functionality by providing "hook functions" that execute at specific points during operation.

Plugins are currently written in JavaScript/TypeScript and run on Node.js v20.18.0. Python support is in development.

## Getting Started

LM Studio includes Node.js, so no separate installation is required.

### Create a new plugin

To create a new plugin, navigate to LM Studio... [TO BE CONTINUED]

### Run a plugin in development mode

Once you've created a plugin, run this command in the plugin directory to start development mode:

```bash
lms dev
```

Your plugin will appear in LM Studio's plugin list. Development mode automatically rebuilds and reloads your plugin when you make code changes.

You only need `lms dev` during development. When the plugin is installed, LM Studio automatically runs them as needed. Learn more about distributing and installing plugins in the [Sharing Plugins](./plugins/sharing) section.

## Next Steps

- [Tools Providers](./plugins/tools-provider)

  Give models extra capabilities by creating tools they can use during generation, like accessing external APIs or performing calculations.

- [Prompt Preprocessors](./plugins/prompt-preprocessor)

  Modify user input before it reaches the model - handle file uploads, inject context, or transform queries.

- [Generators](./plugins/generator)

  Create custom text generation sources that replace the local model, perfect for online model adapters.

- [Custom Configurations](./plugins/custom-configuration)

  Add configuration UIs so users can customize your plugin's behavior.

- [Third-Party Dependencies](./plugins/dependencies)

  Use npm packages to leverage existing libraries in your plugins.

- [Sharing Plugins](./plugins/publish-plugins)

  Package and share your plugins with the community.


--- 2_typescript/4_embedding/index.md ---
---
title: Embedding
sidebar_title: Generating embedding vectors
description: Generate text embeddings from input text
---

Generate embeddings for input text. Embeddings are vector representations of text that capture semantic meaning. Embeddings are a building block for RAG (Retrieval-Augmented Generation) and other similarity-based tasks.

### Prerequisite: Get an Embedding Model

If you don't yet have an embedding model, you can download a model like `nomic-ai/nomic-embed-text-v1.5` using the following command:

```bash
lms get nomic-ai/nomic-embed-text-v1.5
```

## Create Embeddings

To convert a string to a vector representation, pass it to the `embed` method on the corresponding embedding model handle.

```lms_code_snippet
  title: "index.ts"
  variants:
    TypeScript:
      language: typescript
      code: |
        import { LMStudioClient } from "@lmstudio/sdk";
        const client = new LMStudioClient();

        const model = await client.embedding.model("nomic-embed-text-v1.5");

        const { embedding } = await model.embed("Hello, world!");
```


--- 2_typescript/5_tokenization/index.md ---
---
title: Tokenization
sidebar_title: Tokenizing text
description: Tokenize text using a model's tokenizer
---

Models use a tokenizer to internally convert text into "tokens" they can deal with more easily. LM Studio exposes this tokenizer for utility.

## Tokenize

You can tokenize a string with a loaded LLM or embedding model using the SDK. In the below examples, `llm` can be replaced with an embedding model `emb`.

```lms_code_snippet
  variants:
    TypeScript:
      language: typescript
      code: |
        import { LMStudioClient } from "@lmstudio/sdk";

        const client = new LMStudioClient();
        const model = await client.llm.model();

        const tokens = await model.tokenize("Hello, world!");

        console.info(tokens); // Array of token IDs.
```

## Count tokens

If you only care about the number of tokens, you can use the `.countTokens` method instead.

```lms_code_snippet
  variants:
    TypeScript:
      language: typescript
      code: |
        const tokenCount = await model.countTokens("Hello, world!");
        console.info("Token count:", tokenCount);
```

### Example: Count Context

You can determine if a given conversation fits into a model's context by doing the following:

1. Convert the conversation to a string using the prompt template.
2. Count the number of tokens in the string.
3. Compare the token count to the model's context length.

```lms_code_snippet
  variants:
    TypeScript:
      language: typescript
      code: |
        import { Chat, type LLM, LMStudioClient } from "@lmstudio/sdk";

        async function doesChatFitInContext(model: LLM, chat: Chat) {
          // Convert the conversation to a string using the prompt template.
          const formatted = await model.applyPromptTemplate(chat);
          // Count the number of tokens in the string.
          const tokenCount = await model.countTokens(formatted);
          // Get the current loaded context length of the model
          const contextLength = await model.getContextLength();
          return tokenCount < contextLength;
        }

        const client = new LMStudioClient();
        const model = await client.llm.model();

        const chat = Chat.from([
          { role: "user", content: "What is the meaning of life." },
          { role: "assistant", content: "The meaning of life is..." },
          // ... More messages
        ]);

        console.info("Fits in context:", await doesChatFitInContext(model, chat));
```

<!-- ### Context length comparisons

The below examples check whether a conversation is over a LLM's context length
(replace `llm` with `emb` to check for an embedding model).

```lms_code_snippet
  variants:
    TypeScript:
      language: typescript
      code: |
        import { LMStudioClient, Chat } from "@lmstudio/sdk";

        const client = new LMStudioClient();
        const llm = await client.llm.model();

        // To check for a string, simply tokenize
        var tokens = await llm.tokenize("Hello, world!");

        // To check for a Chat, apply the prompt template first
        const chat = Chat.createEmpty().withAppended("user", "Hello, world!");
        const templatedChat = await llm.applyPromptTemplate(chat);
        tokens = await llm.tokenize(templatedChat);

        // If the prompt's length in tokens is less than the context length, you're good!
        const contextLength = await llm.getContextLength()
        const isOkay = (tokens.length < contextLength)
``` -->


--- 2_typescript/3_plugins/1_tools-provider/index.md ---
---
title: "Introduction to Tools Provider"
description: "Writing tools providers for LM Studio plugins using TypeScript"
index: 1
---

Tools provider is a function that returns an array of tools that the model can use during generation.

## Examples

The following are some plugins that make use of tools providers:

- [lmstudio/wikipedia](https://lmstudio.ai/lmstudio/wikipedia)

  Gives the LLM tools to search and read Wikipedia articles.

- [lmstudio/js-code-sandbox](https://lmstudio.ai/lmstudio/js-code-sandbox)

  Gives the LLM tools to run JavaScript/TypeScript code in a sandbox environment using [deno](https://deno.com/).

- [lmstudio/dice](https://lmstudio.ai/lmstudio/dice)

  Allows the LLM to generate random numbers using "dice".


--- 2_typescript/3_plugins/2_prompt-preprocessor/index.md ---
---
title: "Introduction"
description: "Writing prompt preprocessors for LM Studio plugins using TypeScript"
index: 1
---

Prompt Preprocessor is a function that is called upon the user hitting the "Send" button. It receives the user input and can modify it before it reaches the model. If multiple prompt preprocessors are registered, they will be chained together, with each one receiving the output of the previous one.

The modified result will be saved in the chat history, meaning that even if your plugin is disabled afterwards, the modified input will still be used.

Prompt preprocessors will only be triggered for the current user input. It will not be triggered for previous messages in the chat history even if they were not preprocessed.

Prompt preprocessors takes in a `ctl` object for controlling the preprocessing and a `userMessage` it needs to preprocess. It returns either a string or a message object which will replace the user message.

### Examples

The following are some plugins that make use of prompt preprocessors:

- [lmstudio/rag-v1](https://lmstudio.ai/lmstudio/rag-v1)

  Retrieval Augmented Generation (RAG) for LM Studio. This is the plugin that gives document handling capabilities to LM Studio.


--- 2_typescript/3_plugins/3_generator/index.md ---
---
title: "Introduction"
description: "Writing generators for LM Studio plugins using TypeScript"
index: 1
---

Generators are replacement for local LLMs. They act like a token source. When a plugin with a generator is used, LM Studio will no longer use the local model to generate text. The generator will be used instead.

Generators are useful for implementing adapters for external models, such as using a remote LM Studio instance or other online models.

If a plugin contains a generator, it will no longer show up in the plugins list. Instead, it will show up in the model dropdown and act as a model. If your plugins contains [Tools Provider](./tools-providers.md) or [Prompt Preprocessor](./prompt-preprocessors.md), they will be used when your generator is being selected.

## Examples

The following are some plugins that make use of generators:

- [lmstudio/remote-lmstudio](https://lmstudio.ai/lmstudio/remote-lmstudio)

  Basic support for using a remote LM Studio instance to generate text.

- [lmstudio/openai-compat-endpoint](https://lmstudio.ai/lmstudio/openai-compat-endpoint)

  Use any OpenAI-compatible API in LM Studio.


--- 2_typescript/3_plugins/4_custom-configuration/index.md ---
---
title: "Introduction"
description: "Add custom configurations to LM Studio plugins using TypeScript"
index: 1
---

LM Studio plugins support custom configurations. That is, you can define a configuration schema and LM Studio will present a UI to the user so they can configure your plugin without having to edit any code.

There are two types of configurations:

- **Per-chat configuration**: tied to a specific chat. Different chats can have different configurations. Most configurations that affects the behavior of the plugin should be of this type.
- **Global configuration**: apply to _all_ chats and are shared across the application. This is useful for global settings such as API keys.

## Types of Configurations

You can define configurations in TypeScript using the `createConfigSchematics` function from the `@lmstudio/sdk` package. This function allows you to define fields with various types and options.

Supported types include:

- `string`: A text input field.
- `numeric`: A number input field with optional validation and slider UI.
- `boolean`: A checkbox or toggle input field.
- `stringArray`: An array of string values with configurable constraints.
- `select`: A dropdown selection field with predefined options.

See the [Defining New Fields](./custom-configuration/defining-new-fields) section for more details on how to define these fields.

## Examples

The following are some plugins that make use of custom configurations

- [lmstudio/wikipedia](https://lmstudio.ai/lmstudio/wikipedia)

  Gives the LLM tools to search and read Wikipedia articles.

- [lmstudio/openai-compat-endpoint](https://lmstudio.ai/lmstudio/openai-compat-endpoint)

  Use any OpenAI-compatible API in LM Studio.


--- 2_typescript/3_plugins/5_publish-plugins/index.md ---
---
title: "Sharing Plugins"
description: "How to publish your LM Studio plugins so they can be used by others"
index: 7
---

To share publish your LM Studio plugin, open the plugin directory in a terminal and run:

```bash
lms push
```

This command will package your plugin and upload it to the LM Studio Hub. You can use this command to create new plugins or update existing ones.

### Changing Plugin Names

If you wish to change the name of the plugin, you can do so by editing the `manifest.json` file in the root of your plugin directory. Look for the `name` field and update it to your desired plugin name. Note the `name` must be kebab-case.

When you `lms push` the plugin, it will be treated as a new plugin if the name has changed. You can delete the old plugin from the LM Studio Hub if you no longer need it.

### Publishing Plugins to an Organization

If you are in an organization and wish to publish the plugin to the organization, you can do so by editing the `manifest.json` file in the root of your plugin directory. Look for the `owner` field and set it to the name of your organization. When you run `lms push`, the plugin will be published to the organization instead of your personal account.

### Private Plugins

If your account supports private plugins, you can publish your plugins privately by using the `--private` flag when running `lms push`:

```bash
lms push --private
```

Private artifact is in test. Get in touch if you are interested.


--- 2_typescript/project-setup.md ---
---
title: "Project Setup"
sidebar_title: "Project Setup"
description: "Set up your `lmstudio-js` app or script."
index: 2
---

`@lmstudio/sdk` is a library published on npm that allows you to use `lmstudio-js` in your own projects. It is open source and it's developed on GitHub. You can find the source code [here](https://github.com/lmstudio-ai/lmstudio-js).

## Creating a New `node` Project

Use the following command to start an interactive project setup:

```lms_code_snippet
  variants:
    TypeScript (Recommended):
      language: bash
      code: |
        lms create node-typescript
    Javascript:
      language: bash
      code: |
        lms create node-javascript
```

## Add `lmstudio-js` to an Exiting Project

If you have already created a project and would like to use `lmstudio-js` in it, you can install it using npm, yarn, or pnpm.

```lms_code_snippet
  variants:
    npm:
      language: bash
      code: |
        npm install @lmstudio/sdk --save
    yarn:
      language: bash
      code: |
        yarn add @lmstudio/sdk
    pnpm:
      language: bash
      code: |
        pnpm add @lmstudio/sdk
```


--- 3_cli/index.md ---
---
title: "`lms` — LM Studio's CLI"
sidebar_title: "Introduction"
description: Get starting with the `lms` command line utility.
index: 1
---

LM Studio ships with `lms`, a command line tool for scripting and automating your local LLM workflows.

`lms` is **MIT Licensed** and is developed in this repository on GitHub: https://github.com/lmstudio-ai/lms

<hr>

```lms_info
👉 You need to run LM Studio _at least once_ before you can use `lms`.
```

### Install `lms`

`lms` ships with LM Studio and can be found under `/bin` in the LM Studio's working directory.

Use the following commands to add `lms` to your system path.

#### Bootstrap `lms` on macOS or Linux

Run the following command in your terminal:

```bash
~/.lmstudio/bin/lms bootstrap
```

#### Bootstrap `lms` on Windows

Run the following command in **PowerShell**:

```shell
cmd /c %USERPROFILE%/.lmstudio/bin/lms.exe bootstrap
```

#### Verify the installation

Open a **new terminal window** and run `lms`.

This is the current output you will get:

```bash
$ lms
lms - LM Studio CLI - v0.2.22
GitHub: https://github.com/lmstudio-ai/lmstudio-cli

Usage
lms <subcommand>

where <subcommand> can be one of:

- status - Prints the status of LM Studio
- server - Commands for managing the local server
- ls - List all downloaded models
- ps - List all loaded models
- load - Load a model
- unload - Unload a model
- create - Create a new project with scaffolding
- log - Log operations. Currently only supports streaming logs from LM Studio via `lms log stream`
- version - Prints the version of the CLI
- bootstrap - Bootstrap the CLI

For more help, try running `lms <subcommand> --help`
```

### Use `lms` to automate and debug your workflows

### Start and stop the local server

```bash
lms server start
lms server stop
```

### List the local models on the machine

```bash
lms ls
```

This will reflect the current LM Studio models directory, which you set in **📂 My Models** tab in the app.

### List the currently loaded models

```bash
lms ps
```

### Load a model (with options)

```bash
lms load [--gpu=max|auto|0.0-1.0] [--context-length=1-N]
```

`--gpu=1.0` means 'attempt to offload 100% of the computation to the GPU'.

- Optionally, assign an identifier to your local LLM:

```bash
lms load TheBloke/phi-2-GGUF --identifier="gpt-4-turbo"
```

This is useful if you want to keep the model identifier consistent.

### Unload models

```
lms unload [--all]
```


--- 3_cli/get.md ---
---
title: "`lms get`"
sidebar_title: "`lms get`"
description: Search and download models from the command line.
index: 4
---

The `lms get` command allows you to search and download models from online repositories. If no model is specified, it shows staff-picked recommendations.

Models you download via `lms get` will be stored in your LM Studio model directory. 

### Parameters
```lms_params
- name: "[search term]"
  type: "string"
  optional: true
  description: "The model to download. For specific quantizations, append '@' (e.g., 'llama-3.1-8b@q4_k_m')"
- name: "--mlx"
  type: "flag"
  optional: true
  description: "Include MLX models in search results"
- name: "--gguf"
  type: "flag"
  optional: true
  description: "Include GGUF models in search results"
- name: "--limit"
  type: "number"
  optional: true
  description: "Limit the number of model options shown"
- name: "--always-show-all-results"
  type: "flag"
  optional: true
  description: "Always show search results, even with exact matches"
- name: "--always-show-download-options"
  type: "flag"
  optional: true
  description: "Always show quantization options, even with exact matches"
- name: "--yes"
  type: "flag"
  optional: true
  description: "Skip all confirmations. Uses first match and recommended quantization"
```

## Download a model

Download a model by name:

```shell
lms get llama-3.1-8b
```

### Specify quantization

Download a specific model quantization:

```shell
lms get llama-3.1-8b@q4_k_m
```

### Filter by format

Show only MLX or GGUF models:

```shell
lms get --mlx
lms get --gguf
```

### Control search results

Limit the number of results:

```shell
lms get --limit 5
```

Always show all options:

```shell
lms get --always-show-all-results
lms get --always-show-download-options
```

### Automated downloads

For scripting, skip all prompts:

```shell
lms get llama-3.1-8b --yes
```

This will automatically select the first matching model and recommended quantization for your hardware.

--- 3_cli/_lms-load.md ---
---
title: "`lms load`"
description: Use the lms CLI to load or unload models
---

### `lms load`

```bash
lms load --model <model-name> --path <path-to-model>
```

--- 3_cli/load.md ---
---
title: "`lms load`"
sidebar_title: "`lms load`"
description: Load a model into memory, set context length, GPU offload, TTL, or estimate memory usage without loading.
index: 2
---

The `lms load` command loads a model into memory. You can optionally set parameters such as context length, GPU offload, and TTL.

### Parameters 
```lms_params
- name: "[path]"
  type: "string"
  optional: true
  description: "The path of the model to load. If not provided, you will be prompted to select one"
- name: "--ttl"
  type: "number"
  optional: true
  description: "If provided, when the model is not used for this number of seconds, it will be unloaded"
- name: "--gpu"
  type: "string"
  optional: true
  description: "How much to offload to the GPU. Values: 0-1, off, max"
- name: "--context-length"
  type: "number"
  optional: true
  description: "The number of tokens to consider as context when generating text"
- name: "--identifier"
  type: "string"
  optional: true
  description: "The identifier to assign to the loaded model for API reference"
- name: "--estimate-only"
  type: "boolean"
  optional: true
  description: "Print a resource (memory) estimate and exit without loading the model"
```

## Load a model

Load a model into memory by running the following command:

```shell
lms load <model_key>
```

You can find the `model_key` by first running [`lms ls`](/docs/cli/ls) to list your locally downloaded models.

### Set a custom identifier

Optionally, you can assign a custom identifier to the loaded model for API reference:

```shell
lms load <model_key> --identifier "my-custom-identifier"
```

You will then be able to refer to this model by the identifier `my_model` in subsequent commands and API calls (`model` parameter).

### Set context length

You can set the context length when loading a model using the `--context-length` flag:

```shell
lms load <model_key> --context-length 4096
```

This determines how many tokens the model will consider as context when generating text.

### Set GPU offload

Control GPU memory usage with the `--gpu` flag:

```shell
lms load <model_key> --gpu 0.5    # Offload 50% of layers to GPU
lms load <model_key> --gpu max    # Offload all layers to GPU
lms load <model_key> --gpu off    # Disable GPU offloading
```

If not specified, LM Studio will automatically determine optimal GPU usage.

### Set TTL

Set an auto-unload timer with the `--ttl` flag (in seconds):

```shell
lms load <model_key> --ttl 3600   # Unload after 1 hour of inactivity
```

### Estimate resources without loading

Preview memory requirements before loading a model using `--estimate-only`:

```shell
lms load --estimate-only <model_key>
```

Optional flags such as `--context-length` and `--gpu` are honored and reflected in the estimate. The estimator accounts for factors like context length, flash attention, and whether the model is vision‑enabled.

Example:

```bash
$ lms load --estimate-only gpt-oss-120b
Model: openai/gpt-oss-120b
Estimated GPU Memory:   65.68 GB
Estimated Total Memory: 65.68 GB

Estimate: This model may be loaded based on your resource guardrails settings.
```

## Operate on a remote LM Studio instance

`lms load` supports the `--host` flag to connect to a remote LM Studio instance. 

```shell
lms load <model_key> --host <host>
```

For this to work, the remote LM Studio instance must be running and accessible from your local machine, e.g. be accessible on the same subnet.


--- 3_cli/log-stream.md ---
---
title: "`lms log stream`"
sidebar_title: "`lms log stream`"
description: Stream logs from LM Studio. Useful for debugging prompts sent to the model.
index: -1
---

`lms log stream` lets you inspect the exact strings LM Studio sends to and receives from models, and (new in 0.3.26) stream server logs. This is useful for debugging prompt templates, model IO, and server operations.

<hr>

```lms_protip
If you haven't already, bootstrap `lms` on your machine by following the instructions [here](/docs/cli).
```

### Quick start (model input)

By default, `lms log stream` shows the formatted user message that is sent to the model:

```shell
lms log stream
```

Send a message in Chat or call the local HTTP API to see logs.

### Choose a source

Use `--source` to select which logs to stream:

- `--source model` (default) — model IO
- `--source server` — HTTP API server logs (startup, endpoints, status)

Example (server logs):

```shell
lms log stream --source server
```

### Filter model logs

When streaming `--source model`, filter by direction:

- `--filter input` — formatted user message sent to the model
- `--filter output` — model output (printed after completion)
- `--filter input,output` — both user input and model output

Examples:

```shell
# Only the formatted user input
lms log stream --source model --filter input

# Only the model output (emitted once the message completes)
lms log stream --source model --filter output

# Both directions
lms log stream --source model --filter input,output
```

Note: model output is queued and printed once the message completes.

### JSON output and stats

- Append `--json` to emit machine‑readable JSON logs:

```shell
lms log stream --source model --filter input,output --json
```

- Append `--stats` (model source) to include tokens/sec and related metrics:

```shell
lms log stream --source model --filter output --stats
```

### Example (model input and output)

```bash
$ lms log stream --source model --filter input,output
Streaming logs from LM Studio

timestamp: 9/15/2025, 3:16:39 PM
type: llm.prediction.input
modelIdentifier: gpt-oss-20b-mlx
modelPath: lmstudio-community/gpt-oss-20b-mlx-8bit
input:
<|start|>system<|message|>...<|end|><|start|>user<|message|>hello<|end|><|start|>assistant

timestamp: 9/15/2025, 3:16:40 PM
type: llm.prediction.output
modelIdentifier: gpt-oss-20b-mlx
output:
Hello! 👋 How can I assist you today?
```


--- 3_cli/ls.md ---
---
title: "`lms ls`"
sidebar_title: "`lms ls`"
description: List all downloaded models in your LM Studio installation.
index: 8
---

The `lms ls` command displays a list of all models downloaded to your machine, including their size, architecture, and parameters.

### Parameters

```lms_params
- name: "--llm"
  type: "flag"
  optional: true
  description: "Show only LLMs. When not set, all models are shown"
- name: "--embedding"
  type: "flag"
  optional: true
  description: "Show only embedding models"
- name: "--json"
  type: "flag"
  optional: true
  description: "Output the list in JSON format"
- name: "--detailed"
  type: "flag"
  optional: true
  description: "Show detailed information about each model"
```

## List all models

Show all downloaded models:

```shell
lms ls
```

Example output:

```
You have 47 models, taking up 160.78 GB of disk space.

LLMs (Large Language Models)                       PARAMS      ARCHITECTURE           SIZE
lmstudio-community/meta-llama-3.1-8b-instruct          8B         Llama            4.92 GB
hugging-quants/llama-3.2-1b-instruct                   1B         Llama            1.32 GB
mistral-7b-instruct-v0.3                                         Mistral           4.08 GB
zeta                                                   7B         Qwen2            4.09 GB

... (abbreviated in this example) ...

Embedding Models                                   PARAMS      ARCHITECTURE           SIZE
text-embedding-nomic-embed-text-v1.5@q4_k_m                     Nomic BERT        84.11 MB
text-embedding-bge-small-en-v1.5                     33M           BERT           24.81 MB
```

### Filter by model type

List only LLM models:

```shell
lms ls --llm
```

List only embedding models:

```shell
lms ls --embedding
```

### Additional output formats

Get detailed information about models:

```shell
lms ls --detailed
```

Output in JSON format:

```shell
lms ls --json
```

## Operate on a remote LM Studio instance

`lms ls` supports the `--host` flag to connect to a remote LM Studio instance:

```shell
lms ls --host <host>
```

For this to work, the remote LM Studio instance must be running and accessible from your local machine, e.g. be accessible on the same subnet.


--- 3_cli/ps.md ---
---
title: "`lms ps`"
sidebar_title: "`lms ps`"
description: Show information about currently loaded models from the command line.
---

The `lms ps` command displays information about all models currently loaded in memory.

## List loaded models

Show all currently loaded models:

```shell
lms ps
```

Example output:
```
   LOADED MODELS

Identifier: unsloth/deepseek-r1-distill-qwen-1.5b
  • Type:  LLM
  • Path: unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf
  • Size: 1.12 GB
  • Architecture: Qwen2
```

### JSON output

Get the list in machine-readable format:
```shell
lms ps --json
```

## Operate on a remote LM Studio instance

`lms ps` supports the `--host` flag to connect to a remote LM Studio instance:

```shell
lms ps --host <host>
```

For this to work, the remote LM Studio instance must be running and accessible from your local machine, e.g. be accessible on the same subnet.

--- 3_cli/push.md ---
---
title: "`lms push`"
sidebar_title: "`lms push`"
description: Upload a plugin, preset, or `model.yaml` to the LM Studio Hub.
index: 9
---

The `lms push` command packages the contents of the current directory and uploads
it to the LM Studio Hub. You can use it to share presets, plugins, or
[`model.yaml`](http://modelyaml.org) files.

### Parameters
```lms_params
- name: "--overrides"
  type: "string"
  optional: true
  description: "A JSON string of values to override in the manifest or metadata"
- name: "--write-revision"
  type: "flag"
  optional: true
  description: "Write the returned revision number to `manifest.json`"
```

## Upload a Plugin, Preset, or `model.yaml`

Run `lms push` inside the directory that contains your plugin, preset, or `model.yaml` file:

1. Navigate to the directory of your plugin, preset, or `model.yaml` file:
```shell
cd path/to/your/directory
```
2. Run the command:
```shell
lms push
```

The command uploads the artifact and prints the revision number. When used with
`--write-revision`, the revision number is also written to the `manifest.json`
file so you can track revisions in version control.

This command works for [presets](/docs/app/presets),
[plugins](/docs/typescript/plugins), and `model.yaml` files.

### Example Usage with `--overrides`
You can use the `--overrides` parameter to modify the metadata before pushing:

```shell
lms push --overrides '{"description": "new-description"}'
```



--- 3_cli/server-start.md ---
---
title: "`lms server start`"
sidebar_title: "`lms server start`"
description: Start the LM Studio local server with customizable port and logging options.
index: 5
---

The `lms server start` command launches the LM Studio local server, allowing you to interact with loaded models via HTTP API calls.

### Parameters
```lms_params
- name: "--port"
  type: "number"
  optional: true
  description: "Port to run the server on. If not provided, uses the last used port"
- name: "--cors"
  type: "flag"
  optional: true
  description: "Enable CORS support for web application development. When not set, CORS is disabled"
```

## Start the server

Start the server with default settings:

```shell
lms server start
```

### Specify a custom port

Run the server on a specific port:

```shell
lms server start --port 3000
```

### Enable CORS support

For usage with web applications or some VS Code extensions, you may need to enable CORS support:

```shell
lms server start --cors
```

Note that enabling CORS may expose your server to security risks, so use it only when necessary.

### Check the server status

See [`lms server status`](/docs/cli/server-status) for more information on checking the status of the server.

--- 3_cli/server-status.md ---
---
title: "`lms server status`"
sidebar_title: "`lms server status`"
description: Check the status of your running LM Studio server instance.
index: 5
---

The `lms server status` command displays the current status of the LM Studio local server, including whether it's running and its configuration.

### Parameters
```lms_params
- name: "--json"
  type: "flag"
  optional: true
  description: "Output the status in JSON format"
- name: "--verbose"
  type: "flag"
  optional: true
  description: "Enable detailed logging output"
- name: "--quiet"
  type: "flag"
  optional: true
  description: "Suppress all logging output"
- name: "--log-level"
  type: "string"
  optional: true
  description: "The level of logging to use. Defaults to 'info'"
```

## Check server status

Get the basic status of the server:

```shell
lms server status
```

Example output:
```
The server is running on port 1234.
```

### Example usage

```console
➜  ~ lms server start
Starting server...
Waking up LM Studio service...
Success! Server is now running on port 1234

➜  ~ lms server status
The server is running on port 1234.
```

### JSON output

Get the status in machine-readable JSON format:

```shell
lms server status --json --quiet
```

Example output:
```json
{"running":true,"port":1234}
```

### Control logging output

Adjust logging verbosity:

```shell
lms server status --verbose
lms server status --quiet
lms server status --log-level debug
```

You can only use one logging control flag at a time (`--verbose`, `--quiet`, or `--log-level`).


--- artifacts/fastapi/fastapi/fastapi-llms.txt ---
# Fastapi

> To create a high-performance API that is easy to develop, document, and maintain using Python, leveraging FastAPI's capabilities for data validation, async support, and interactive documentation.

**Remember:**
- Data validation using Pydantic models
- Automatic interactive API documentation (Swagger UI / ReDoc)
- Async support with `async def`
- Dependency injection system
- Integration with Starlette for ASGI
- Type hints for automatic type checking and completion

## Docs
- [Test Multipart Installation](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_multipart_installation.py): install & quickstart.
- [How To](https://raw.githubusercontent.com/fastapi/fastapi/master/docs/de/docs/how-to/index.md): worked example.
- [How To](https://raw.githubusercontent.com/fastapi/fastapi/master/docs/en/docs/how-to/index.md): worked example.
- [How To](https://raw.githubusercontent.com/fastapi/fastapi/master/docs/es/docs/how-to/index.md): worked example.
- [How To](https://raw.githubusercontent.com/fastapi/fastapi/master/docs/pt/docs/how-to/index.md): worked example.
- [How To](https://raw.githubusercontent.com/fastapi/fastapi/master/docs/ru/docs/how-to/index.md): worked example.
- [How To](https://raw.githubusercontent.com/fastapi/fastapi/master/docs/tr/docs/how-to/index.md): worked example.
- [How To](https://raw.githubusercontent.com/fastapi/fastapi/master/docs/zh-hant/docs/how-to/index.md): worked example.
- [How To](https://raw.githubusercontent.com/fastapi/fastapi/master/docs/zh/docs/how-to/index.md): worked example.
- [Tutorial](https://raw.githubusercontent.com/fastapi/fastapi/master/docs/de/docs/tutorial/index.md): worked example.

## Tutorials
- [Test Openapi Examples](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_openapi_examples.py): worked example.
- [Test Schema Extra Examples](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_schema_extra_examples.py): worked example.
- [Init](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_tutorial/__init__.py): worked example.
- [Init](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_tutorial/test_additional_responses/__init__.py): worked example.
- [Init](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_tutorial/test_additional_status_codes/__init__.py): worked example.
- [Init](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_tutorial/test_advanced_middleware/__init__.py): worked example.
- [Init](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_tutorial/test_async_tests/__init__.py): worked example.
- [Init](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_tutorial/test_background_tasks/__init__.py): worked example.
- [Init](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_tutorial/test_behind_a_proxy/__init__.py): worked example.
- [Init](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_tutorial/test_bigger_applications/__init__.py): worked example.

## API
- [Applications](https://raw.githubusercontent.com/fastapi/fastapi/master/fastapi/applications.py): docs page.
- [Background](https://raw.githubusercontent.com/fastapi/fastapi/master/fastapi/background.py): docs page.
- [Cli](https://raw.githubusercontent.com/fastapi/fastapi/master/fastapi/cli.py): docs page.
- [Concurrency](https://raw.githubusercontent.com/fastapi/fastapi/master/fastapi/concurrency.py): docs page.
- [Datastructures](https://raw.githubusercontent.com/fastapi/fastapi/master/fastapi/datastructures.py): docs page.
- [Encoders](https://raw.githubusercontent.com/fastapi/fastapi/master/fastapi/encoders.py): docs page.
- [Exception Handlers](https://raw.githubusercontent.com/fastapi/fastapi/master/fastapi/exception_handlers.py): docs page.
- [Exceptions](https://raw.githubusercontent.com/fastapi/fastapi/master/fastapi/exceptions.py): docs page.
- [Init](https://raw.githubusercontent.com/fastapi/fastapi/master/fastapi/__init__.py): docs page.
- [Logger](https://raw.githubusercontent.com/fastapi/fastapi/master/fastapi/logger.py): docs page.

## Optional
- [Contributing](https://raw.githubusercontent.com/fastapi/fastapi/master/CONTRIBUTING.md): docs page.
- [Security](https://raw.githubusercontent.com/fastapi/fastapi/master/SECURITY.md): security policy.
- [Test Dependency Security Overrides](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_dependency_security_overrides.py): security policy.
- [Test Security Http Base](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_security_http_base.py): security policy.
- [Test Security Http Base Description](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_security_http_base_description.py): security policy.
- [Test Security Http Base Optional](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_security_http_base_optional.py): security policy.
- [Test Security Http Basic Optional](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_security_http_basic_optional.py): security policy.
- [Test Security Http Basic Realm](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_security_http_basic_realm.py): security policy.
- [Test Security Http Basic Realm Description](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_security_http_basic_realm_description.py): security policy.
- [Test Security Http Bearer](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_security_http_bearer.py): security policy.
- [README](https://raw.githubusercontent.com/fastapi/fastapi/master/README.md): docs page.
- [Pdm Build](https://raw.githubusercontent.com/fastapi/fastapi/master/pdm_build.py): docs page.
- [Requirements Github Actions](https://raw.githubusercontent.com/fastapi/fastapi/master/requirements-github-actions.txt): docs page.
- [Requirements Tests](https://raw.githubusercontent.com/fastapi/fastapi/master/requirements-tests.txt): docs page.
- [Requirements Translations](https://raw.githubusercontent.com/fastapi/fastapi/master/requirements-translations.txt): docs page.
- [Requirements](https://raw.githubusercontent.com/fastapi/fastapi/master/requirements.txt): docs page.

## Scripts
- [Contributors](https://raw.githubusercontent.com/fastapi/fastapi/master/scripts/contributors.py): docs page.
- [Label Approved](https://raw.githubusercontent.com/fastapi/fastapi/master/scripts/label_approved.py): docs page.
- [Notify Translations](https://raw.githubusercontent.com/fastapi/fastapi/master/scripts/notify_translations.py): docs page.
- [People](https://raw.githubusercontent.com/fastapi/fastapi/master/scripts/people.py): docs page.
- [Sponsors](https://raw.githubusercontent.com/fastapi/fastapi/master/scripts/sponsors.py): docs page.
- [Topic Repos](https://raw.githubusercontent.com/fastapi/fastapi/master/scripts/topic_repos.py): docs page.
- [Translate](https://raw.githubusercontent.com/fastapi/fastapi/master/scripts/translate.py): docs page.
- [Image01](https://raw.githubusercontent.com/fastapi/fastapi/master/scripts/playwright/cookie_param_models/image01.py): docs page.
- [Image01](https://raw.githubusercontent.com/fastapi/fastapi/master/scripts/playwright/header_param_models/image01.py): docs page.
- [Image01](https://raw.githubusercontent.com/fastapi/fastapi/master/scripts/playwright/query_param_models/image01.py): docs page.

## Tests
- [Init](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/__init__.py): docs page.
- [Main](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/main.py): docs page.
- [Test Additional Properties](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_additional_properties.py): docs page.
- [Test Additional Properties Bool](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_additional_properties_bool.py): docs page.
- [Test Additional Response Extra](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_additional_response_extra.py): docs page.
- [Test Additional Responses Bad](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_additional_responses_bad.py): docs page.
- [Test Additional Responses Custom Model In Callback](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_additional_responses_custom_model_in_callback.py): docs page.
- [Test Additional Responses Custom Validationerror](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_additional_responses_custom_validationerror.py): docs page.
- [Test Additional Responses Default Validationerror](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_additional_responses_default_validationerror.py): docs page.
- [Test Additional Responses Router](https://raw.githubusercontent.com/fastapi/fastapi/master/tests/test_additional_responses_router.py): docs page.


--- artifacts/fastapi/fastapi/fastapi-llms-full.txt ---
# llms-full (private-aware)
> Built by GitHub fetches (raw or authenticated). Large files may be truncated.

--- tests/test_multipart_installation.py ---
import warnings

import pytest
from fastapi import FastAPI, File, Form, UploadFile
from fastapi.dependencies.utils import (
    multipart_incorrect_install_error,
    multipart_not_installed_error,
)


def test_incorrect_multipart_installed_form(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        monkeypatch.delattr("multipart.multipart.parse_options_header", raising=False)
    with pytest.raises(RuntimeError, match=multipart_incorrect_install_error):
        app = FastAPI()

        @app.post("/")
        async def root(username: str = Form()):
            return username  # pragma: nocover


def test_incorrect_multipart_installed_file_upload(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        monkeypatch.delattr("multipart.multipart.parse_options_header", raising=False)
    with pytest.raises(RuntimeError, match=multipart_incorrect_install_error):
        app = FastAPI()

        @app.post("/")
        async def root(f: UploadFile = File()):
            return f  # pragma: nocover


def test_incorrect_multipart_installed_file_bytes(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        monkeypatch.delattr("multipart.multipart.parse_options_header", raising=False)
    with pytest.raises(RuntimeError, match=multipart_incorrect_install_error):
        app = FastAPI()

        @app.post("/")
        async def root(f: bytes = File()):
            return f  # pragma: nocover


def test_incorrect_multipart_installed_multi_form(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        monkeypatch.delattr("multipart.multipart.parse_options_header", raising=False)
    with pytest.raises(RuntimeError, match=multipart_incorrect_install_error):
        app = FastAPI()

        @app.post("/")
        async def root(username: str = Form(), password: str = Form()):
            return username  # pragma: nocover


def test_incorrect_multipart_installed_form_file(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        monkeypatch.delattr("multipart.multipart.parse_options_header", raising=False)
    with pytest.raises(RuntimeError, match=multipart_incorrect_install_error):
        app = FastAPI()

        @app.post("/")
        async def root(username: str = Form(), f: UploadFile = File()):
            return username  # pragma: nocover


def test_no_multipart_installed(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        monkeypatch.delattr("multipart.__version__", raising=False)
        with pytest.raises(RuntimeError, match=multipart_not_installed_error):
            app = FastAPI()

            @app.post("/")
            async def root(username: str = Form()):
                return username  # pragma: nocover


def test_no_multipart_installed_file(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        monkeypatch.delattr("multipart.__version__", raising=False)
        with pytest.raises(RuntimeError, match=multipart_not_installed_error):
            app = FastAPI()

            @app.post("/")
            async def root(f: UploadFile = File()):
                return f  # pragma: nocover


def test_no_multipart_installed_file_bytes(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        monkeypatch.delattr("multipart.__version__", raising=False)
        with pytest.raises(RuntimeError, match=multipart_not_installed_error):
            app = FastAPI()

            @app.post("/")
            async def root(f: bytes = File()):
                return f  # pragma: nocover


def test_no_multipart_installed_multi_form(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        monkeypatch.delattr("multipart.__version__", raising=False)
        with pytest.raises(RuntimeError, match=multipart_not_installed_error):
            app = FastAPI()

            @app.post("/")
            async def root(username: str = Form(), password: str = Form()):
                return username  # pragma: nocover


def test_no_multipart_installed_form_file(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        monkeypatch.delattr("multipart.__version__", raising=False)
        with pytest.raises(RuntimeError, match=multipart_not_installed_error):
            app = FastAPI()

            @app.post("/")
            async def root(username: str = Form(), f: UploadFile = File()):
                return username  # pragma: nocover


def test_old_multipart_installed(monkeypatch):
    monkeypatch.setattr("python_multipart.__version__", "0.0.12")
    with warnings.catch_warnings(record=True):
        warnings.simplefilter("always")
        app = FastAPI()

        @app.post("/")
        async def root(username: str = Form()):
            return username  # pragma: nocover


--- docs/de/docs/how-to/index.md ---
# How-To – Rezepte { #how-to-recipes }

Hier finden Sie verschiedene Rezepte und „How-To“-Anleitungen zu **verschiedenen Themen**.

Die meisten dieser Ideen sind mehr oder weniger **unabhängig**, und in den meisten Fällen müssen Sie diese nur studieren, wenn sie direkt auf **Ihr Projekt** anwendbar sind.

Wenn etwas für Ihr Projekt interessant und nützlich erscheint, lesen Sie es, andernfalls überspringen Sie es einfach.

/// tip | Tipp

Wenn Sie strukturiert **FastAPI lernen** möchten (empfohlen), lesen Sie stattdessen Kapitel für Kapitel das [Tutorial – Benutzerhandbuch](../tutorial/index.md){.internal-link target=_blank}.

///


--- docs/en/docs/how-to/index.md ---
# How To - Recipes { #how-to-recipes }

Here you will see different recipes or "how to" guides for **several topics**.

Most of these ideas would be more or less **independent**, and in most cases you should only need to study them if they apply directly to **your project**.

If something seems interesting and useful to your project, go ahead and check it, but otherwise, you might probably just skip them.

/// tip

If you want to **learn FastAPI** in a structured way (recommended), go and read the [Tutorial - User Guide](../tutorial/index.md){.internal-link target=_blank} chapter by chapter instead.

///


--- docs/es/docs/how-to/index.md ---
# How To - Recetas

Aquí verás diferentes recetas o guías de "cómo hacer" para **varios temas**.

La mayoría de estas ideas serían más o menos **independientes**, y en la mayoría de los casos solo deberías estudiarlas si aplican directamente a **tu proyecto**.

Si algo parece interesante y útil para tu proyecto, adelante y revísalo, pero de lo contrario, probablemente puedas simplemente omitirlas.

/// tip | Consejo

Si quieres **aprender FastAPI** de una manera estructurada (recomendado), ve y lee el [Tutorial - User Guide](../tutorial/index.md){.internal-link target=_blank} capítulo por capítulo.

///


--- docs/pt/docs/how-to/index.md ---
# Como Fazer - Exemplos Práticos

Aqui você encontrará diferentes exemplos práticos ou tutoriais de "como fazer" para vários tópicos.

A maioria dessas ideias será mais ou menos **independente**, e na maioria dos casos você só precisará estudá-las se elas se aplicarem diretamente ao **seu projeto**.

Se algo parecer interessante e útil para o seu projeto, vá em frente e dê uma olhada. Caso contrário, você pode simplesmente ignorá-lo.

/// tip

Se você deseja **aprender FastAPI** de forma estruturada (recomendado), leia capítulo por capítulo [Tutorial - Guia de Usuário](../tutorial/index.md){.internal-link target=_blank} em vez disso.

///


--- docs/ru/docs/how-to/index.md ---
# Как сделать — Рецепты { #how-to-recipes }

Здесь вы найдете разные рецепты и руководства «как сделать» по различным темам.

Большинство из этих идей более-менее независимы, и в большинстве случаев вам стоит изучать их только если они напрямую относятся к вашему проекту.

Если что-то кажется интересным и полезным для вашего проекта, смело изучайте; в противном случае, вероятно, можно просто пропустить.

/// tip | Совет

Если вы хотите изучить FastAPI структурированно (рекомендуется), вместо этого читайте [Учебник — Руководство пользователя](../tutorial/index.md){.internal-link target=_blank} по главам.

///


--- docs/tr/docs/how-to/index.md ---
# Nasıl Yapılır - Tarifler

Burada çeşitli konular hakkında farklı tarifler veya "nasıl yapılır" kılavuzları yer alıyor.

Bu fikirlerin büyük bir kısmı aşağı yukarı **bağımsız** olacaktır, çoğu durumda bunları sadece **projenize** hitap ediyorsa incelemelisiniz.

Projeniz için ilginç ve yararlı görünen bir şey varsa devam edin ve inceleyin, aksi halde bunları atlayabilirsiniz.

/// tip | İpucu

**FastAPI**'ı düzgün (ve önerilen) şekilde öğrenmek istiyorsanız [Öğretici - Kullanıcı Rehberi](../tutorial/index.md){.internal-link target=_blank}'ni bölüm bölüm okuyun.

///


--- docs/zh-hant/docs/how-to/index.md ---
# 使用指南 - 範例集

在這裡，你將會看到**不同主題**的範例或「如何使用」的指南。

大多數這些想法都是**獨立**的，在大多數情況下，你只需要研究那些直接適用於**你的專案**的東西。

如果有些東西看起來很有趣且對你的專案很有用的話再去讀它，否則你可能可以跳過它們。

/// tip

如果你想要以結構化的方式**學習 FastAPI**（推薦），請前往[教學 - 使用者指南](../tutorial/index.md){.internal-link target=_blank}逐章閱讀。

///


--- docs/zh/docs/how-to/index.md ---
# 如何操作 - 诀窍

在这里，你将看到关于**多个主题**的不同诀窍或“如何操作”指南。

这些方法多数是**相互独立**的，在大多数情况下，你只需在这些内容适用于**你的项目**时才需要学习它们。

如果某些内容看起来对你的项目有用，请继续查阅，否则请直接跳过它们。

/// tip | 小技巧

如果你想以系统的方式**学习 FastAPI**（推荐），请阅读 [教程 - 用户指南](../tutorial/index.md){.internal-link target=_blank} 的每一章节。

///


--- docs/de/docs/tutorial/index.md ---
# Tutorial – Benutzerhandbuch { #tutorial-user-guide }

Dieses Tutorial zeigt Ihnen Schritt für Schritt, wie Sie **FastAPI** mit den meisten seiner Funktionen verwenden können.

Jeder Abschnitt baut schrittweise auf den vorhergehenden auf, ist jedoch in einzelne Themen gegliedert, sodass Sie direkt zu einem bestimmten Thema übergehen können, um Ihre spezifischen API-Anforderungen zu lösen.

Es ist auch so gestaltet, dass es als zukünftige Referenz dient, sodass Sie jederzeit zurückkommen und genau das sehen, was Sie benötigen.

## Den Code ausführen { #run-the-code }

Alle Codeblöcke können kopiert und direkt verwendet werden (es sind tatsächlich getestete Python-Dateien).

Um eines der Beispiele auszuführen, kopieren Sie den Code in eine Datei `main.py` und starten Sie `fastapi dev` mit:

<div class="termy">

```console
$ <font color="#4E9A06">fastapi</font> dev <u style="text-decoration-style:solid">main.py</u>

  <span style="background-color:#009485"><font color="#D3D7CF"> FastAPI </font></span>  Starting development server 🚀

             Searching for package file structure from directories
             with <font color="#3465A4">__init__.py</font> files
             Importing from <font color="#75507B">/home/user/code/</font><font color="#AD7FA8">awesomeapp</font>

   <span style="background-color:#007166"><font color="#D3D7CF"> module </font></span>  🐍 main.py

     <span style="background-color:#007166"><font color="#D3D7CF"> code </font></span>  Importing the FastAPI app object from the module with
             the following code:

             <u style="text-decoration-style:solid">from </u><u style="text-decoration-style:solid"><b>main</b></u><u style="text-decoration-style:solid"> import </u><u style="text-decoration-style:solid"><b>app</b></u>

      <span style="background-color:#007166"><font color="#D3D7CF"> app </font></span>  Using import string: <font color="#3465A4">main:app</font>

   <span style="background-color:#007166"><font color="#D3D7CF"> server </font></span>  Server started at <font color="#729FCF"><u style="text-decoration-style:solid">http://127.0.0.1:8000</u></font>
   <span style="background-color:#007166"><font color="#D3D7CF"> server </font></span>  Documentation at <font color="#729FCF"><u style="text-decoration-style:solid">http://127.0.0.1:8000/docs</u></font>

      <span style="background-color:#007166"><font color="#D3D7CF"> tip </font></span>  Running in development mode, for production use:
             <b>fastapi run</b>

             Logs:

     <span style="background-color:#007166"><font color="#D3D7CF"> INFO </font></span>  Will watch for changes in these directories:
             <b>[</b><font color="#4E9A06">&apos;/home/user/code/awesomeapp&apos;</font><b>]</b>
     <span style="background-color:#007166"><font color="#D3D7CF"> INFO </font></span>  Uvicorn running on <font color="#729FCF"><u style="text-decoration-style:solid">http://127.0.0.1:8000</u></font> <b>(</b>Press CTRL+C
             to quit<b>)</b>
     <span style="background-color:#007166"><font color="#D3D7CF"> INFO </font></span>  Started reloader process <b>[</b><font color="#34E2E2"><b>383138</b></font><b>]</b> using WatchFiles
     <span style="background-color:#007166"><font color="#D3D7CF"> INFO </font></span>  Started server process <b>[</b><font color="#34E2E2"><b>383153</b></font><b>]</b>
     <span style="background-color:#007166"><font color="#D3D7CF"> INFO </font></span>  Waiting for application startup.
     <span style="background-color:#007166"><font color="#D3D7CF"> INFO </font></span>  Application startup complete.
```

</div>

Es wird **dringend empfohlen**, den Code zu schreiben oder zu kopieren, ihn zu bearbeiten und lokal auszuführen.

Die Verwendung in Ihrem eigenen Editor zeigt Ihnen die Vorteile von FastAPI am besten, wenn Sie sehen, wie wenig Code Sie schreiben müssen, all die Typprüfungen, die automatische Vervollständigung usw.

---

## FastAPI installieren { #install-fastapi }

Der erste Schritt besteht darin, FastAPI zu installieren.

Stellen Sie sicher, dass Sie eine [virtuelle Umgebung](../virtual-environments.md){.internal-link target=_blank} erstellen, sie aktivieren und dann **FastAPI installieren**:

<div class="termy">

```console
$ pip install "fastapi[standard]"

---> 100%
```

</div>

/// note | Hinweis

Wenn Sie mit `pip install "fastapi[standard]"` installieren, werden einige optionale Standard-Abhängigkeiten mit installiert, einschließlich `fastapi-cloud-cli`, welches Ihnen das Deployment in der <a href="https://fastapicloud.com" class="external-link" target="_blank">FastAPI Cloud</a> ermöglicht.

Wenn Sie diese optionalen Abhängigkeiten nicht haben möchten, können Sie stattdessen `pip install fastapi` installieren.

Wenn Sie die Standard-Abhängigkeiten, aber ohne das `fastapi-cloud-cli` installieren möchten, können Sie mit `pip install "fastapi[standard-no-fastapi-cloud-cli]"` installieren.

///

## Handbuch für fortgeschrittene Benutzer { #advanced-user-guide }

Es gibt auch ein **Handbuch für fortgeschrittene Benutzer**, das Sie nach diesem **Tutorial – Benutzerhandbuch** lesen können.

Das **Handbuch für fortgeschrittene Benutzer** baut hierauf auf, verwendet dieselben Konzepte und bringt Ihnen einige zusätzliche Funktionen bei.

Sie sollten jedoch zuerst das **Tutorial – Benutzerhandbuch** lesen (was Sie gerade tun).

Es ist so konzipiert, dass Sie mit dem **Tutorial – Benutzerhandbuch** eine vollständige Anwendung erstellen können und diese dann je nach Bedarf mit einigen der zusätzlichen Ideen aus dem **Handbuch für fortgeschrittene Benutzer** erweitern können.


--- tests/test_openapi_examples.py ---
from typing import Union

from dirty_equals import IsDict
from fastapi import Body, Cookie, FastAPI, Header, Path, Query
from fastapi.testclient import TestClient
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    data: str


@app.post("/examples/")
def examples(
    item: Item = Body(
        examples=[
            {"data": "Data in Body examples, example1"},
        ],
        openapi_examples={
            "Example One": {
                "summary": "Example One Summary",
                "description": "Example One Description",
                "value": {"data": "Data in Body examples, example1"},
            },
            "Example Two": {
                "value": {"data": "Data in Body examples, example2"},
            },
        },
    ),
):
    return item


@app.get("/path_examples/{item_id}")
def path_examples(
    item_id: str = Path(
        examples=[
            "json_schema_item_1",
            "json_schema_item_2",
        ],
        openapi_examples={
            "Path One": {
                "summary": "Path One Summary",
                "description": "Path One Description",
                "value": "item_1",
            },
            "Path Two": {
                "value": "item_2",
            },
        },
    ),
):
    return item_id


@app.get("/query_examples/")
def query_examples(
    data: Union[str, None] = Query(
        default=None,
        examples=[
            "json_schema_query1",
            "json_schema_query2",
        ],
        openapi_examples={
            "Query One": {
                "summary": "Query One Summary",
                "description": "Query One Description",
                "value": "query1",
            },
            "Query Two": {
                "value": "query2",
            },
        },
    ),
):
    return data


@app.get("/header_examples/")
def header_examples(
    data: Union[str, None] = Header(
        default=None,
        examples=[
            "json_schema_header1",
            "json_schema_header2",
        ],
        openapi_examples={
            "Header One": {
                "summary": "Header One Summary",
                "description": "Header One Description",
                "value": "header1",
            },
            "Header Two": {
                "value": "header2",
            },
        },
    ),
):
    return data


@app.get("/cookie_examples/")
def cookie_examples(
    data: Union[str, None] = Cookie(
        default=None,
        examples=["json_schema_cookie1", "json_schema_cookie2"],
        openapi_examples={
            "Cookie One": {
                "summary": "Cookie One Summary",
                "description": "Cookie One Description",
                "value": "cookie1",
            },
            "Cookie Two": {
                "value": "cookie2",
            },
        },
    ),
):
    return data


client = TestClient(app)


def test_call_api():
    response = client.post("/examples/", json={"data": "example1"})
    assert response.status_code == 200, response.text

    response = client.get("/path_examples/foo")
    assert response.status_code == 200, response.text

    response = client.get("/query_examples/")
    assert response.status_code == 200, response.text

    response = client.get("/header_examples/")
    assert response.status_code == 200, response.text

    response = client.get("/cookie_examples/")
    assert response.status_code == 200, response.text


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/examples/": {
                "post": {
                    "summary": "Examples",
                    "operationId": "examples_examples__post",
                    "requestBody": {
                        "content": {
                            "application/json": {
                                "schema": IsDict(
                                    {
                                        "$ref": "#/components/schemas/Item",
                                        "examples": [
                                            {"data": "Data in Body examples, example1"}
                                        ],
                                    }
                                )
                                | IsDict(
                                    {
                                        # TODO: remove when deprecating Pydantic v1
                                        "allOf": [
                                            {"$ref": "#/components/schemas/Item"}
                                        ],
                                        "title": "Item",
                                        "examples": [
                                            {"data": "Data in Body examples, example1"}
                                        ],
                                    }
                                ),
                                "examples": {
                                    "Example One": {
                                        "summary": "Example One Summary",
                                        "description": "Example One Description",
                                        "value": {
                                            "data": "Data in Body examples, example1"
                                        },
                                    },
                                    "Example Two": {
                                        "value": {
                                            "data": "Data in Body examples, example2"
                                        }
                                    },
                                },
                            }
                        },
                        "required": True,
                    },
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/path_examples/{item_id}": {
                "get": {
                    "summary": "Path Examples",
                    "operationId": "path_examples_path_examples__item_id__get",
                    "parameters": [
                        {
                            "name": "item_id",
                            "in": "path",
                            "required": True,
                            "schema": {
                                "type": "string",
                                "examples": [
                                    "json_schema_item_1",
                                    "json_schema_item_2",
                                ],
                                "title": "Item Id",
                            },
                            "examples": {
                                "Path One": {
                                    "summary": "Path One Summary",
                                    "description": "Path One Description",
                                    "value": "item_1",
                                },
                                "Path Two": {"value": "item_2"},
                            },
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/query_examples/": {
                "get": {
                    "summary": "Query Examples",
                    "operationId": "query_examples_query_examples__get",
                    "parameters": [
                        {
                            "name": "data",
                            "in": "query",
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "examples": [
                                        "json_schema_query1",
                                        "json_schema_query2",
                                    ],
                                    "title": "Data",
                                }
                            )
                            | IsDict(
                                # TODO: remove when deprecating Pydantic v1
                                {
                                    "examples": [
                                        "json_schema_query1",
                                        "json_schema_query2",
                                    ],
                                    "type": "string",
                                    "title": "Data",
                                }
                            ),
                            "examples": {
                                "Query One": {
                                    "summary": "Query One Summary",
                                    "description": "Query One Description",
                                    "value": "query1",
                                },
                                "Query Two": {"value": "query2"},
                            },
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/header_examples/": {
                "get": {
                    "summary": "Header Examples",
                    "operationId": "header_examples_header_examples__get",
                    "parameters": [
                        {
                            "name": "data",
                            "in": "header",
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "examples": [
                                        "json_schema_header1",
                                        "json_schema_header2",
                                    ],
                                    "title": "Data",
                                }
                            )
                            | IsDict(
                                # TODO: remove when deprecating Pydantic v1
                                {
                                    "type": "string",
                                    "examples": [
                                        "json_schema_header1",
                                        "json_schema_header2",
                                    ],
                                    "title": "Data",
                                }
                            ),
                            "examples": {
                                "Header One": {
                                    "summary": "Header One Summary",
                                    "description": "Header One Description",
                                    "value": "header1",
                                },
                                "Header Two": {"value": "header2"},
                            },
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/cookie_examples/": {
                "get": {
                    "summary": "Cookie Examples",
                    "operationId": "cookie_examples_cookie_examples__get",
                    "parameters": [
                        {
                            "name": "data",
                            "in": "cookie",
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "examples": [
                                        "json_schema_cookie1",
                                        "json_schema_cookie2",
                                    ],
                                    "title": "Data",
                                }
                            )
                            | IsDict(
                                # TODO: remove when deprecating Pydantic v1
                                {
                                    "type": "string",
                                    "examples": [
                                        "json_schema_cookie1",
                                        "json_schema_cookie2",
                                    ],
                                    "title": "Data",
                                }
                            ),
                            "examples": {
                                "Cookie One": {
                                    "summary": "Cookie One Summary",
                                    "description": "Cookie One Description",
                                    "value": "cookie1",
                                },
                                "Cookie Two": {"value": "cookie2"},
                            },
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
        },
        "components": {
            "schemas": {
                "HTTPValidationError": {
                    "properties": {
                        "detail": {
                            "items": {"$ref": "#/components/schemas/ValidationError"},
                            "type": "array",
                            "title": "Detail",
                        }
                    },
                    "type": "object",
                    "title": "HTTPValidationError",
                },
                "Item": {
                    "properties": {"data": {"type": "string", "title": "Data"}},
                    "type": "object",
                    "required": ["data"],
                    "title": "Item",
                },
                "ValidationError": {
                    "properties": {
                        "loc": {
                            "items": {
                                "anyOf": [{"type": "string"}, {"type": "integer"}]
                            },
                            "type": "array",
                            "title": "Location",
                        },
                        "msg": {"type": "string", "title": "Message"},
                        "type": {"type": "string", "title": "Error Type"},
                    },
                    "type": "object",
                    "required": ["loc", "msg", "type"],
                    "title": "ValidationError",
                },
            }
        },
    }


--- tests/test_schema_extra_examples.py ---
from typing import Union

import pytest
from dirty_equals import IsDict
from fastapi import Body, Cookie, FastAPI, Header, Path, Query
from fastapi._compat import PYDANTIC_V2
from fastapi.testclient import TestClient
from pydantic import BaseModel, ConfigDict


def create_app():
    app = FastAPI()

    class Item(BaseModel):
        data: str

        if PYDANTIC_V2:
            model_config = ConfigDict(
                json_schema_extra={"example": {"data": "Data in schema_extra"}}
            )
        else:

            class Config:
                schema_extra = {"example": {"data": "Data in schema_extra"}}

    @app.post("/schema_extra/")
    def schema_extra(item: Item):
        return item

    with pytest.warns(DeprecationWarning):

        @app.post("/example/")
        def example(item: Item = Body(example={"data": "Data in Body example"})):
            return item

    @app.post("/examples/")
    def examples(
        item: Item = Body(
            examples=[
                {"data": "Data in Body examples, example1"},
                {"data": "Data in Body examples, example2"},
            ],
        ),
    ):
        return item

    with pytest.warns(DeprecationWarning):

        @app.post("/example_examples/")
        def example_examples(
            item: Item = Body(
                example={"data": "Overridden example"},
                examples=[
                    {"data": "examples example_examples 1"},
                    {"data": "examples example_examples 2"},
                ],
            ),
        ):
            return item

    # TODO: enable these tests once/if Form(embed=False) is supported
    # TODO: In that case, define if File() should support example/examples too
    # @app.post("/form_example")
    # def form_example(firstname: str = Form(example="John")):
    #     return firstname

    # @app.post("/form_examples")
    # def form_examples(
    #     lastname: str = Form(
    #         ...,
    #         examples={
    #             "example1": {"summary": "last name summary", "value": "Doe"},
    #             "example2": {"value": "Doesn't"},
    #         },
    #     ),
    # ):
    #     return lastname

    # @app.post("/form_example_examples")
    # def form_example_examples(
    #     lastname: str = Form(
    #         ...,
    #         example="Doe overridden",
    #         examples={
    #             "example1": {"summary": "last name summary", "value": "Doe"},
    #             "example2": {"value": "Doesn't"},
    #         },
    #     ),
    # ):
    #     return lastname

    with pytest.warns(DeprecationWarning):

        @app.get("/path_example/{item_id}")
        def path_example(
            item_id: str = Path(
                example="item_1",
            ),
        ):
            return item_id

    @app.get("/path_examples/{item_id}")
    def path_examples(
        item_id: str = Path(
            examples=["item_1", "item_2"],
        ),
    ):
        return item_id

    with pytest.warns(DeprecationWarning):

        @app.get("/path_example_examples/{item_id}")
        def path_example_examples(
            item_id: str = Path(
                example="item_overridden",
                examples=["item_1", "item_2"],
            ),
        ):
            return item_id

    with pytest.warns(DeprecationWarning):

        @app.get("/query_example/")
        def query_example(
            data: Union[str, None] = Query(
                default=None,
                example="query1",
            ),
        ):
            return data

    @app.get("/query_examples/")
    def query_examples(
        data: Union[str, None] = Query(
            default=None,
            examples=["query1", "query2"],
        ),
    ):
        return data

    with pytest.warns(DeprecationWarning):

        @app.get("/query_example_examples/")
        def query_example_examples(
            data: Union[str, None] = Query(
                default=None,
                example="query_overridden",
                examples=["query1", "query2"],
            ),
        ):
            return data

    with pytest.warns(DeprecationWarning):

        @app.get("/header_example/")
        def header_example(
            data: Union[str, None] = Header(
                default=None,
                example="header1",
            ),
        ):
            return data

    @app.get("/header_examples/")
    def header_examples(
        data: Union[str, None] = Header(
            default=None,
            examples=[
                "header1",
                "header2",
            ],
        ),
    ):
        return data

    with pytest.warns(DeprecationWarning):

        @app.get("/header_example_examples/")
        def header_example_examples(
            data: Union[str, None] = Header(
                default=None,
                example="header_overridden",
                examples=["header1", "header2"],
            ),
        ):
            return data

    with pytest.warns(DeprecationWarning):

        @app.get("/cookie_example/")
        def cookie_example(
            data: Union[str, None] = Cookie(
                default=None,
                example="cookie1",
            ),
        ):
            return data

    @app.get("/cookie_examples/")
    def cookie_examples(
        data: Union[str, None] = Cookie(
            default=None,
            examples=["cookie1", "cookie2"],
        ),
    ):
        return data

    with pytest.warns(DeprecationWarning):

        @app.get("/cookie_example_examples/")
        def cookie_example_examples(
            data: Union[str, None] = Cookie(
                default=None,
                example="cookie_overridden",
                examples=["cookie1", "cookie2"],
            ),
        ):
            return data

    return app


def test_call_api():
    app = create_app()
    client = TestClient(app)
    response = client.post("/schema_extra/", json={"data": "Foo"})
    assert response.status_code == 200, response.text
    response = client.post("/example/", json={"data": "Foo"})
    assert response.status_code == 200, response.text
    response = client.post("/examples/", json={"data": "Foo"})
    assert response.status_code == 200, response.text
    response = client.post("/example_examples/", json={"data": "Foo"})
    assert response.status_code == 200, response.text
    response = client.get("/path_example/foo")
    assert response.status_code == 200, response.text
    response = client.get("/path_examples/foo")
    assert response.status_code == 200, response.text
    response = client.get("/path_example_examples/foo")
    assert response.status_code == 200, response.text
    response = client.get("/query_example/")
    assert response.status_code == 200, response.text
    response = client.get("/query_examples/")
    assert response.status_code == 200, response.text
    response = client.get("/query_example_examples/")
    assert response.status_code == 200, response.text
    response = client.get("/header_example/")
    assert response.status_code == 200, response.text
    response = client.get("/header_examples/")
    assert response.status_code == 200, response.text
    response = client.get("/header_example_examples/")
    assert response.status_code == 200, response.text
    response = client.get("/cookie_example/")
    assert response.status_code == 200, response.text
    response = client.get("/cookie_examples/")
    assert response.status_code == 200, response.text
    response = client.get("/cookie_example_examples/")
    assert response.status_code == 200, response.text


def test_openapi_schema():
    """
    Test that example overrides work:

    * pydantic model schema_extra is included
    * Body(example={}) overrides schema_extra in pydantic model
    * Body(examples{}) overrides Body(example={}) and schema_extra in pydantic model
    """
    app = create_app()
    client = TestClient(app)
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/schema_extra/": {
                "post": {
                    "summary": "Schema Extra",
                    "operationId": "schema_extra_schema_extra__post",
                    "requestBody": {
                        "content": {
                            "application/json": {
                                "schema": {"$ref": "#/components/schemas/Item"}
                            }
                        },
                        "required": True,
                    },
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/example/": {
                "post": {
                    "summary": "Example",
                    "operationId": "example_example__post",
                    "requestBody": {
                        "content": {
                            "application/json": {
                                "schema": {"$ref": "#/components/schemas/Item"},
                                "example": {"data": "Data in Body example"},
                            }
                        },
                        "required": True,
                    },
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/examples/": {
                "post": {
                    "summary": "Examples",
                    "operationId": "examples_examples__post",
                    "requestBody": {
                        "content": {
                            "application/json": {
                                "schema": IsDict(
                                    {
                                        "$ref": "#/components/schemas/Item",
                                        "examples": [
                                            {"data": "Data in Body examples, example1"},
                                            {"data": "Data in Body examples, example2"},
                                        ],
                                    }
                                )
                                | IsDict(
                                    # TODO: remove this when deprecating Pydantic v1
                                    {
                                        "allOf": [
                                            {"$ref": "#/components/schemas/Item"}
                                        ],
                                        "title": "Item",
                                        "examples": [
                                            {"data": "Data in Body examples, example1"},
                                            {"data": "Data in Body examples, example2"},
                                        ],
                                    }
                                )
                            }
                        },
                        "required": True,
                    },
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/example_examples/": {
                "post": {
                    "summary": "Example Examples",
                    "operationId": "example_examples_example_examples__post",
                    "requestBody": {
                        "content": {
                            "application/json": {
                                "schema": IsDict(
                                    {
                                        "$ref": "#/components/schemas/Item",
                                        "examples": [
                                            {"data": "examples example_examples 1"},
                                            {"data": "examples example_examples 2"},
                                        ],
                                    }
                                )
                                | IsDict(
                                    # TODO: remove this when deprecating Pydantic v1
                                    {
                                        "allOf": [
                                            {"$ref": "#/components/schemas/Item"}
                                        ],
                                        "title": "Item",
                                        "examples": [
                                            {"data": "examples example_examples 1"},
                                            {"data": "examples example_examples 2"},
                                        ],
                                    },
                                ),
                                "example": {"data": "Overridden example"},
                            }
                        },
                        "required": True,
                    },
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/path_example/{item_id}": {
                "get": {
                    "summary": "Path Example",
                    "operationId": "path_example_path_example__item_id__get",
                    "parameters": [
                        {
                            "required": True,
                            "schema": {"title": "Item Id", "type": "string"},
                            "example": "item_1",
                            "name": "item_id",
                            "in": "path",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/path_examples/{item_id}": {
                "get": {
                    "summary": "Path Examples",
                    "operationId": "path_examples_path_examples__item_id__get",
                    "parameters": [
                        {
                            "required": True,
                            "schema": {
                                "title": "Item Id",
                                "type": "string",
                                "examples": ["item_1", "item_2"],
                            },
                            "name": "item_id",
                            "in": "path",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/path_example_examples/{item_id}": {
                "get": {
                    "summary": "Path Example Examples",
                    "operationId": "path_example_examples_path_example_examples__item_id__get",
                    "parameters": [
                        {
                            "required": True,
                            "schema": {
                                "title": "Item Id",
                                "type": "string",
                                "examples": ["item_1", "item_2"],
                            },
                            "example": "item_overridden",
                            "name": "item_id",
                            "in": "path",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/query_example/": {
                "get": {
                    "summary": "Query Example",
                    "operationId": "query_example_query_example__get",
                    "parameters": [
                        {
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "title": "Data",
                                }
                            )
                            | IsDict(
                                # TODO: Remove this when deprecating Pydantic v1
                                {"title": "Data", "type": "string"}
                            ),
                            "example": "query1",
                            "name": "data",
                            "in": "query",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/query_examples/": {
                "get": {
                    "summary": "Query Examples",
                    "operationId": "query_examples_query_examples__get",
                    "parameters": [
                        {
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "title": "Data",
                                    "examples": ["query1", "query2"],
                                }
                            )
                            | IsDict(
                                # TODO: Remove this when deprecating Pydantic v1
                                {
                                    "type": "string",
                                    "title": "Data",
                                    "examples": ["query1", "query2"],
                                }
                            ),
                            "name": "data",
                            "in": "query",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/query_example_examples/": {
                "get": {
                    "summary": "Query Example Examples",
                    "operationId": "query_example_examples_query_example_examples__get",
                    "parameters": [
                        {
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "title": "Data",
                                    "examples": ["query1", "query2"],
                                }
                            )
                            | IsDict(
                                # TODO: Remove this when deprecating Pydantic v1
                                {
                                    "type": "string",
                                    "title": "Data",
                                    "examples": ["query1", "query2"],
                                }
                            ),
                            "example": "query_overridden",
                            "name": "data",
                            "in": "query",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/header_example/": {
                "get": {
                    "summary": "Header Example",
                    "operationId": "header_example_header_example__get",
                    "parameters": [
                        {
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "title": "Data",
                                }
                            )
                            | IsDict(
                                # TODO: Remove this when deprecating Pydantic v1
                                {"title": "Data", "type": "string"}
                            ),
                            "example": "header1",
                            "name": "data",
                            "in": "header",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/header_examples/": {
                "get": {
                    "summary": "Header Examples",
                    "operationId": "header_examples_header_examples__get",
                    "parameters": [
                        {
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "title": "Data",
                                    "examples": ["header1", "header2"],
                                }
                            )
                            | IsDict(
                                # TODO: Remove this when deprecating Pydantic v1
                                {
                                    "type": "string",
                                    "title": "Data",
                                    "examples": ["header1", "header2"],
                                }
                            ),
                            "name": "data",
                            "in": "header",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/header_example_examples/": {
                "get": {
                    "summary": "Header Example Examples",
                    "operationId": "header_example_examples_header_example_examples__get",
                    "parameters": [
                        {
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "title": "Data",
                                    "examples": ["header1", "header2"],
                                }
                            )
                            | IsDict(
                                # TODO: Remove this when deprecating Pydantic v1
                                {
                                    "title": "Data",
                                    "type": "string",
                                    "examples": ["header1", "header2"],
                                }
                            ),
                            "example": "header_overridden",
                            "name": "data",
                            "in": "header",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/cookie_example/": {
                "get": {
                    "summary": "Cookie Example",
                    "operationId": "cookie_example_cookie_example__get",
                    "parameters": [
                        {
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "title": "Data",
                                }
                            )
                            | IsDict(
                                # TODO: Remove this when deprecating Pydantic v1
                                {"title": "Data", "type": "string"}
                            ),
                            "example": "cookie1",
                            "name": "data",
                            "in": "cookie",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/cookie_examples/": {
                "get": {
                    "summary": "Cookie Examples",
                    "operationId": "cookie_examples_cookie_examples__get",
                    "parameters": [
                        {
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "title": "Data",
                                    "examples": ["cookie1", "cookie2"],
                                }
                            )
                            | IsDict(
                                # TODO: Remove this when deprecating Pydantic v1
                                {
                                    "title": "Data",
                                    "type": "string",
                                    "examples": ["cookie1", "cookie2"],
                                }
                            ),
                            "name": "data",
                            "in": "cookie",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
            "/cookie_example_examples/": {
                "get": {
                    "summary": "Cookie Example Examples",
                    "operationId": "cookie_example_examples_cookie_example_examples__get",
                    "parameters": [
                        {
                            "required": False,
                            "schema": IsDict(
                                {
                                    "anyOf": [{"type": "string"}, {"type": "null"}],
                                    "title": "Data",
                                    "examples": ["cookie1", "cookie2"],
                                }
                            )
                            | IsDict(
                                # TODO: Remove this when deprecating Pydantic v1
                                {
                                    "title": "Data",
                                    "type": "string",
                                    "examples": ["cookie1", "cookie2"],
                                }
                            ),
                            "example": "cookie_overridden",
                            "name": "data",
                            "in": "cookie",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            },
        },
        "components": {
            "schemas": {
                "HTTPValidationError": {
                    "title": "HTTPValidationError",
                    "type": "object",
                    "properties": {
                        "detail": {
                            "title": "Detail",
                            "type": "array",
                            "items": {"$ref": "#/components/schemas/ValidationError"},
                        }
                    },
                },
                "Item": {
                    "title": "Item",
                    "required": ["data"],
                    "type": "object",
                    "properties": {"data": {"title": "Data", "type": "string"}},
                    "example": {"data": "Data in schema_extra"},
                },
                "ValidationError": {
                    "title": "ValidationError",
                    "required": ["loc", "msg", "type"],
                    "type": "object",
                    "properties": {
                        "loc": {
                            "title": "Location",
                            "type": "array",
                            "items": {
                                "anyOf": [{"type": "string"}, {"type": "integer"}]
                            },
                        },
                        "msg": {"title": "Message", "type": "string"},
                        "type": {"title": "Error Type", "type": "string"},
                    },
                },
            }
        },
    }


--- tests/test_tutorial/__init__.py ---


--- tests/test_tutorial/test_additional_responses/__init__.py ---


--- tests/test_tutorial/test_additional_status_codes/__init__.py ---


--- tests/test_tutorial/test_advanced_middleware/__init__.py ---


--- tests/test_tutorial/test_async_tests/__init__.py ---


--- tests/test_tutorial/test_background_tasks/__init__.py ---


--- tests/test_tutorial/test_behind_a_proxy/__init__.py ---


--- tests/test_tutorial/test_bigger_applications/__init__.py ---


--- fastapi/applications.py ---
from enum import Enum
from typing import (
    Any,
    Awaitable,
    Callable,
    Coroutine,
    Dict,
    List,
    Optional,
    Sequence,
    Type,
    TypeVar,
    Union,
)

from fastapi import routing
from fastapi.datastructures import Default, DefaultPlaceholder
from fastapi.exception_handlers import (
    http_exception_handler,
    request_validation_exception_handler,
    websocket_request_validation_exception_handler,
)
from fastapi.exceptions import RequestValidationError, WebSocketRequestValidationError
from fastapi.logger import logger
from fastapi.middleware.asyncexitstack import AsyncExitStackMiddleware
from fastapi.openapi.docs import (
    get_redoc_html,
    get_swagger_ui_html,
    get_swagger_ui_oauth2_redirect_html,
)
from fastapi.openapi.utils import get_openapi
from fastapi.params import Depends
from fastapi.types import DecoratedCallable, IncEx
from fastapi.utils import generate_unique_id
from starlette.applications import Starlette
from starlette.datastructures import State
from starlette.exceptions import HTTPException
from starlette.middleware import Middleware
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.middleware.errors import ServerErrorMiddleware
from starlette.middleware.exceptions import ExceptionMiddleware
from starlette.requests import Request
from starlette.responses import HTMLResponse, JSONResponse, Response
from starlette.routing import BaseRoute
from starlette.types import ASGIApp, ExceptionHandler, Lifespan, Receive, Scope, Send
from typing_extensions import Annotated, Doc, deprecated

AppType = TypeVar("AppType", bound="FastAPI")


class FastAPI(Starlette):
    """
    `FastAPI` app class, the main entrypoint to use FastAPI.

    Read more in the
    [FastAPI docs for First Steps](https://fastapi.tiangolo.com/tutorial/first-steps/).

    ## Example

    ```python
    from fastapi import FastAPI

    app = FastAPI()
    ```
    """

    def __init__(
        self: AppType,
        *,
        debug: Annotated[
            bool,
            Doc(
                """
                Boolean indicating if debug tracebacks should be returned on server
                errors.

                Read more in the
                [Starlette docs for Applications](https://www.starlette.dev/applications/#instantiating-the-application).
                """
            ),
        ] = False,
        routes: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                **Note**: you probably shouldn't use this parameter, it is inherited
                from Starlette and supported for compatibility.

                ---

                A list of routes to serve incoming HTTP and WebSocket requests.
                """
            ),
            deprecated(
                """
                You normally wouldn't use this parameter with FastAPI, it is inherited
                from Starlette and supported for compatibility.

                In FastAPI, you normally would use the *path operation methods*,
                like `app.get()`, `app.post()`, etc.
                """
            ),
        ] = None,
        title: Annotated[
            str,
            Doc(
                """
                The title of the API.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more in the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(title="ChimichangApp")
                ```
                """
            ),
        ] = "FastAPI",
        summary: Annotated[
            Optional[str],
            Doc(
                """
                A short summary of the API.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more in the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(summary="Deadpond's favorite app. Nuff said.")
                ```
                """
            ),
        ] = None,
        description: Annotated[
            str,
            Doc(
                '''
                A description of the API. Supports Markdown (using
                [CommonMark syntax](https://commonmark.org/)).

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more in the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(
                    description="""
                                ChimichangApp API helps you do awesome stuff. 🚀

                                ## Items

                                You can **read items**.

                                ## Users

                                You will be able to:

                                * **Create users** (_not implemented_).
                                * **Read users** (_not implemented_).

                                """
                )
                ```
                '''
            ),
        ] = "",
        version: Annotated[
            str,
            Doc(
                """
                The version of the API.

                **Note** This is the version of your application, not the version of
                the OpenAPI specification nor the version of FastAPI being used.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more in the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(version="0.0.1")
                ```
                """
            ),
        ] = "0.1.0",
        openapi_url: Annotated[
            Optional[str],
            Doc(
                """
                The URL where the OpenAPI schema will be served from.

                If you set it to `None`, no OpenAPI schema will be served publicly, and
                the default automatic endpoints `/docs` and `/redoc` will also be
                disabled.

                Read more in the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#openapi-url).

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(openapi_url="/api/v1/openapi.json")
                ```
                """
            ),
        ] = "/openapi.json",
        openapi_tags: Annotated[
            Optional[List[Dict[str, Any]]],
            Doc(
                """
                A list of tags used by OpenAPI, these are the same `tags` you can set
                in the *path operations*, like:

                * `@app.get("/users/", tags=["users"])`
                * `@app.get("/items/", tags=["items"])`

                The order of the tags can be used to specify the order shown in
                tools like Swagger UI, used in the automatic path `/docs`.

                It's not required to specify all the tags used.

                The tags that are not declared MAY be organized randomly or based
                on the tools' logic. Each tag name in the list MUST be unique.

                The value of each item is a `dict` containing:

                * `name`: The name of the tag.
                * `description`: A short description of the tag.
                    [CommonMark syntax](https://commonmark.org/) MAY be used for rich
                    text representation.
                * `externalDocs`: Additional external documentation for this tag. If
                    provided, it would contain a `dict` with:
                    * `description`: A short description of the target documentation.
                        [CommonMark syntax](https://commonmark.org/) MAY be used for
                        rich text representation.
                    * `url`: The URL for the target documentation. Value MUST be in
                        the form of a URL.

                Read more in the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-tags).

                **Example**

                ```python
                from fastapi import FastAPI

                tags_metadata = [
                    {
                        "name": "users",
                        "description": "Operations with users. The **login** logic is also here.",
                    },
                    {
                        "name": "items",
                        "description": "Manage items. So _fancy_ they have their own docs.",
                        "externalDocs": {
                            "description": "Items external docs",
                            "url": "https://fastapi.tiangolo.com/",
                        },
                    },
                ]

                app = FastAPI(openapi_tags=tags_metadata)
                ```
                """
            ),
        ] = None,
        servers: Annotated[
            Optional[List[Dict[str, Union[str, Any]]]],
            Doc(
                """
                A `list` of `dict`s with connectivity information to a target server.

                You would use it, for example, if your application is served from
                different domains and you want to use the same Swagger UI in the
                browser to interact with each of them (instead of having multiple
                browser tabs open). Or if you want to leave fixed the possible URLs.

                If the servers `list` is not provided, or is an empty `list`, the
                default value would be a `dict` with a `url` value of `/`.

                Each item in the `list` is a `dict` containing:

                * `url`: A URL to the target host. This URL supports Server Variables
                and MAY be relative, to indicate that the host location is relative
                to the location where the OpenAPI document is being served. Variable
                substitutions will be made when a variable is named in `{`brackets`}`.
                * `description`: An optional string describing the host designated by
                the URL. [CommonMark syntax](https://commonmark.org/) MAY be used for
                rich text representation.
                * `variables`: A `dict` between a variable name and its value. The value
                    is used for substitution in the server's URL template.

                Read more in the
                [FastAPI docs for Behind a Proxy](https://fastapi.tiangolo.com/advanced/behind-a-proxy/#additional-servers).

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(
                    servers=[
                        {"url": "https://stag.example.com", "description": "Staging environment"},
                        {"url": "https://prod.example.com", "description": "Production environment"},
                    ]
                )
                ```
                """
            ),
        ] = None,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of global dependencies, they will be applied to each
                *path operation*, including in sub-routers.

                Read more about it in the
                [FastAPI docs for Global Dependencies](https://fastapi.tiangolo.com/tutorial/dependencies/global-dependencies/).

                **Example**

                ```python
                from fastapi import Depends, FastAPI

                from .dependencies import func_dep_1, func_dep_2

                app = FastAPI(dependencies=[Depends(func_dep_1), Depends(func_dep_2)])
                ```
                """
            ),
        ] = None,
        default_response_class: Annotated[
            Type[Response],
            Doc(
                """
                The default response class to be used.

                Read more in the
                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#default-response-class).

                **Example**

                ```python
                from fastapi import FastAPI
                from fastapi.responses import ORJSONResponse

                app = FastAPI(default_response_class=ORJSONResponse)
                ```
                """
            ),
        ] = Default(JSONResponse),
        redirect_slashes: Annotated[
            bool,
            Doc(
                """
                Whether to detect and redirect slashes in URLs when the client doesn't
                use the same format.

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(redirect_slashes=True)  # the default

                @app.get("/items/")
                async def read_items():
                    return [{"item_id": "Foo"}]
                ```

                With this app, if a client goes to `/items` (without a trailing slash),
                they will be automatically redirected with an HTTP status code of 307
                to `/items/`.
                """
            ),
        ] = True,
        docs_url: Annotated[
            Optional[str],
            Doc(
                """
                The path to the automatic interactive API documentation.
                It is handled in the browser by Swagger UI.

                The default URL is `/docs`. You can disable it by setting it to `None`.

                If `openapi_url` is set to `None`, this will be automatically disabled.

                Read more in the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#docs-urls).

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(docs_url="/documentation", redoc_url=None)
                ```
                """
            ),
        ] = "/docs",
        redoc_url: Annotated[
            Optional[str],
            Doc(
                """
                The path to the alternative automatic interactive API documentation
                provided by ReDoc.

                The default URL is `/redoc`. You can disable it by setting it to `None`.

                If `openapi_url` is set to `None`, this will be automatically disabled.

                Read more in the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#docs-urls).

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(docs_url="/documentation", redoc_url="redocumentation")
                ```
                """
            ),
        ] = "/redoc",
        swagger_ui_oauth2_redirect_url: Annotated[
            Optional[str],
            Doc(
                """
                The OAuth2 redirect endpoint for the Swagger UI.

                By default it is `/docs/oauth2-redirect`.

                This is only used if you use OAuth2 (with the "Authorize" button)
                with Swagger UI.
                """
            ),
        ] = "/docs/oauth2-redirect",
        swagger_ui_init_oauth: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                OAuth2 configuration for the Swagger UI, by default shown at `/docs`.

                Read more about the available configuration options in the
                [Swagger UI docs](https://swagger.io/docs/open-source-tools/swagger-ui/usage/oauth2/).
                """
            ),
        ] = None,
        middleware: Annotated[
            Optional[Sequence[Middleware]],
            Doc(
                """
                List of middleware to be added when creating the application.

                In FastAPI you would normally do this with `app.add_middleware()`
                instead.

                Read more in the
                [FastAPI docs for Middleware](https://fastapi.tiangolo.com/tutorial/middleware/).
                """
            ),
        ] = None,
        exception_handlers: Annotated[
            Optional[
                Dict[
                    Union[int, Type[Exception]],
                    Callable[[Request, Any], Coroutine[Any, Any, Response]],
                ]
            ],
            Doc(
                """
                A dictionary with handlers for exceptions.

                In FastAPI, you would normally use the decorator
                `@app.exception_handler()`.

                Read more in the
                [FastAPI docs for Handling Errors](https://fastapi.tiangolo.com/tutorial/handling-errors/).
                """
            ),
        ] = None,
        on_startup: Annotated[
            Optional[Sequence[Callable[[], Any]]],
            Doc(
                """
                A list of startup event handler functions.

                You should instead use the `lifespan` handlers.

                Read more in the [FastAPI docs for `lifespan`](https://fastapi.tiangolo.com/advanced/events/).
                """
            ),
        ] = None,
        on_shutdown: Annotated[
            Optional[Sequence[Callable[[], Any]]],
            Doc(
                """
                A list of shutdown event handler functions.

                You should instead use the `lifespan` handlers.

                Read more in the
                [FastAPI docs for `lifespan`](https://fastapi.tiangolo.com/advanced/events/).
                """
            ),
        ] = None,
        lifespan: Annotated[
            Optional[Lifespan[AppType]],
            Doc(
                """
                A `Lifespan` context manager handler. This replaces `startup` and
                `shutdown` functions with a single context manager.

                Read more in the
                [FastAPI docs for `lifespan`](https://fastapi.tiangolo.com/advanced/events/).
                """
            ),
        ] = None,
        terms_of_service: Annotated[
            Optional[str],
            Doc(
                """
                A URL to the Terms of Service for your API.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more at the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).

                **Example**

                ```python
                app = FastAPI(terms_of_service="http://example.com/terms/")
                ```
                """
            ),
        ] = None,
        contact: Annotated[
            Optional[Dict[str, Union[str, Any]]],
            Doc(
                """
                A dictionary with the contact information for the exposed API.

                It can contain several fields.

                * `name`: (`str`) The name of the contact person/organization.
                * `url`: (`str`) A URL pointing to the contact information. MUST be in
                    the format of a URL.
                * `email`: (`str`) The email address of the contact person/organization.
                    MUST be in the format of an email address.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more at the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).

                **Example**

                ```python
                app = FastAPI(
                    contact={
                        "name": "Deadpoolio the Amazing",
                        "url": "http://x-force.example.com/contact/",
                        "email": "dp@x-force.example.com",
                    }
                )
                ```
                """
            ),
        ] = None,
        license_info: Annotated[
            Optional[Dict[str, Union[str, Any]]],
            Doc(
                """
                A dictionary with the license information for the exposed API.

                It can contain several fields.

                * `name`: (`str`) **REQUIRED** (if a `license_info` is set). The
                    license name used for the API.
                * `identifier`: (`str`) An [SPDX](https://spdx.dev/) license expression
                    for the API. The `identifier` field is mutually exclusive of the `url`
                    field. Available since OpenAPI 3.1.0, FastAPI 0.99.0.
                * `url`: (`str`) A URL to the license used for the API. This MUST be
                    the format of a URL.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more at the
                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).

                **Example**

                ```python
                app = FastAPI(
                    license_info={
                        "name": "Apache 2.0",
                        "url": "https://www.apache.org/licenses/LICENSE-2.0.html",
                    }
                )
                ```
                """
            ),
        ] = None,
        openapi_prefix: Annotated[
            str,
            Doc(
                """
                A URL prefix for the OpenAPI URL.
                """
            ),
            deprecated(
                """
                "openapi_prefix" has been deprecated in favor of "root_path", which
                follows more closely the ASGI standard, is simpler, and more
                automatic.
                """
            ),
        ] = "",
        root_path: Annotated[
            str,
            Doc(
                """
                A path prefix handled by a proxy that is not seen by the application
                but is seen by external clients, which affects things like Swagger UI.

                Read more about it at the
                [FastAPI docs for Behind a Proxy](https://fastapi.tiangolo.com/advanced/behind-a-proxy/).

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(root_path="/api/v1")
                ```
                """
            ),
        ] = "",
        root_path_in_servers: Annotated[
            bool,
            Doc(
                """
                To disable automatically generating the URLs in the `servers` field
                in the autogenerated OpenAPI using the `root_path`.

                Read more about it in the
                [FastAPI docs for Behind a Proxy](https://fastapi.tiangolo.com/advanced/behind-a-proxy/#disable-automatic-server-from-root_path).

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI(root_path_in_servers=False)
                ```
                """
            ),
        ] = True,
        responses: Annotated[
            Optional[Dict[Union[int, str], Dict[str, Any]]],
            Doc(
                """
                Additional responses to be shown in OpenAPI.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Additional Responses in OpenAPI](https://fastapi.tiangolo.com/advanced/additional-responses/).

                And in the
                [FastAPI docs for Bigger Applications](https://fastapi.tiangolo.com/tutorial/bigger-applications/#include-an-apirouter-with-a-custom-prefix-tags-responses-and-dependencies).
                """
            ),
        ] = None,
        callbacks: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                OpenAPI callbacks that should apply to all *path operations*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).
                """
            ),
        ] = None,
        webhooks: Annotated[
            Optional[routing.APIRouter],
            Doc(
                """
                Add OpenAPI webhooks. This is similar to `callbacks` but it doesn't
                depend on specific *path operations*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                **Note**: This is available since OpenAPI 3.1.0, FastAPI 0.99.0.

                Read more about it in the
                [FastAPI docs for OpenAPI Webhooks](https://fastapi.tiangolo.com/advanced/openapi-webhooks/).
                """
            ),
        ] = None,
        deprecated: Annotated[
            Optional[bool],
            Doc(
                """
                Mark all *path operations* as deprecated. You probably don't need it,
                but it's available.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        include_in_schema: Annotated[
            bool,
            Doc(
                """
                To include (or not) all the *path operations* in the generated OpenAPI.
                You probably don't need it, but it's available.

                This affects the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).
                """
            ),
        ] = True,
        swagger_ui_parameters: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                Parameters to configure Swagger UI, the autogenerated interactive API
                documentation (by default at `/docs`).

                Read more about it in the
                [FastAPI docs about how to Configure Swagger UI](https://fastapi.tiangolo.com/how-to/configure-swagger-ui/).
                """
            ),
        ] = None,
        generate_unique_id_function: Annotated[
            Callable[[routing.APIRoute], str],
            Doc(
                """
                Customize the function used to generate unique IDs for the *path
                operations* shown in the generated OpenAPI.

                This is particularly useful when automatically generating clients or
                SDKs for your API.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = Default(generate_unique_id),
        separate_input_output_schemas: Annotated[
            bool,
            Doc(
                """
                Whether to generate separate OpenAPI schemas for request body and
                response body when the results would be more precise.

                This is particularly useful when automatically generating clients.

                For example, if you have a model like:

                ```python
                from pydantic import BaseModel

                class Item(BaseModel):
                    name: str
                    tags: list[str] = []
                ```

                When `Item` is used for input, a request body, `tags` is not required,
                the client doesn't have to provide it.

                But when using `Item` for output, for a response body, `tags` is always
                available because it has a default value, even if it's just an empty
                list. So, the client should be able to always expect it.

                In this case, there would be two different schemas, one for input and
                another one for output.
                """
            ),
        ] = True,
        openapi_external_docs: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                This field allows you to provide additional external documentation links.
                If provided, it must be a dictionary containing:

                * `description`: A brief description of the external documentation.
                * `url`: The URL pointing to the external documentation. The value **MUST**
                be a valid URL format.

                **Example**:

                ```python
                from fastapi import FastAPI

                external_docs = {
                    "description": "Detailed API Reference",
                    "url": "https://example.com/api-docs",
                }

                app = FastAPI(openapi_external_docs=external_docs)
                ```
                """
            ),
        ] = None,
        **extra: Annotated[
            Any,
            Doc(
                """
                Extra keyword arguments to be stored in the app, not used by FastAPI
                anywhere.
                """
            ),
        ],
    ) -> None:
        self.debug = debug
        self.title = title
        self.summary = summary
        self.description = description
        self.version = version
        self.terms_of_service = terms_of_service
        self.contact = contact
        self.license_info = license_info
        self.openapi_url = openapi_url
        self.openapi_tags = openapi_tags
        self.root_path_in_servers = root_path_in_servers
        self.docs_url = docs_url
        self.redoc_url = redoc_url
        self.swagger_ui_oauth2_redirect_url = swagger_ui_oauth2_redirect_url
        self.swagger_ui_init_oauth = swagger_ui_init_oauth
        self.swagger_ui_parameters = swagger_ui_parameters
        self.servers = servers or []
        self.separate_input_output_schemas = separate_input_output_schemas
        self.openapi_external_docs = openapi_external_docs
        self.extra = extra
        self.openapi_version: Annotated[
            str,
            Doc(
                """
                The version string of OpenAPI.

                FastAPI will generate OpenAPI version 3.1.0, and will output that as
                the OpenAPI version. But some tools, even though they might be
                compatible with OpenAPI 3.1.0, might not recognize it as a valid.

                So you could override this value to trick those tools into using
                the generated OpenAPI. Have in mind that this is a hack. But if you
                avoid using features added in OpenAPI 3.1.0, it might work for your
                use case.

                This is not passed as a parameter to the `FastAPI` class to avoid
                giving the false idea that FastAPI would generate a different OpenAPI
                schema. It is only available as an attribute.

                **Example**

                ```python
                from fastapi import FastAPI

                app = FastAPI()

                app.openapi_version = "3.0.2"
                ```
                """
            ),
        ] = "3.1.0"
        self.openapi_schema: Optional[Dict[str, Any]] = None
        if self.openapi_url:
            assert self.title, "A title must be provided for OpenAPI, e.g.: 'My API'"
            assert self.version, "A version must be provided for OpenAPI, e.g.: '2.1.0'"
        # TODO: remove when discarding the openapi_prefix parameter
        if openapi_prefix:
            logger.warning(
                '"openapi_prefix" has been deprecated in favor of "root_path", which '
                "follows more closely the ASGI standard, is simpler, and more "
                "automatic. Check the docs at "
                "https://fastapi.tiangolo.com/advanced/sub-applications/"
            )
        self.webhooks: Annotated[
            routing.APIRouter,
            Doc(
                """
                The `app.webhooks` attribute is an `APIRouter` with the *path
                operations* that will be used just for documentation of webhooks.

                Read more about it in the
                [FastAPI docs for OpenAPI Webhooks](https://fastapi.tiangolo.com/advanced/openapi-webhooks/).
                """
            ),
        ] = webhooks or routing.APIRouter()
        self.root_path = root_path or openapi_prefix
        self.state: Annotated[
            State,
            Doc(
                """
                A state object for the application. This is the same object for the
                entire application, it doesn't change from request to request.

                You normally wouldn't use this in FastAPI, for most of the cases you
                would instead use FastAPI dependencies.

                This is simply inherited from Starlette.

                Read more about it in the
                [Starlette docs for Applications](https://www.starlette.dev/applications/#storing-state-on-the-app-instance).
                """
            ),
        ] = State()
        self.dependency_overrides: Annotated[
            Dict[Callable[..., Any], Callable[..., Any]],
            Doc(
                """
                A dictionary with overrides for the dependencies.

                Each key is the original dependency callable, and the value is the
                actual dependency that should be called.

                This is for testing, to replace expensive dependencies with testing
                versions.

                Read more about it in the
                [FastAPI docs for Testing Dependencies with Overrides](https://fastapi.tiangolo.com/advanced/testing-dependencies/).
                """
            ),
        ] = {}
        self.router: routing.APIRouter = routing.APIRouter(
            routes=routes,
            redirect_slashes=redirect_slashes,
            dependency_overrides_provider=self,
            on_startup=on_startup,
            on_shutdown=on_shutdown,
            lifespan=lifespan,
            default_response_class=default_response_class,
            dependencies=dependencies,
            callbacks=callbacks,
            deprecated=deprecated,
            include_in_schema=include_in_schema,
            responses=responses,
            generate_unique_id_function=generate_unique_id_function,
        )
        self.exception_handlers: Dict[
            Any, Callable[[Request, Any], Union[Response, Awaitable[Response]]]
        ] = {} if exception_handlers is None else dict(exception_handlers)
        self.exception_handlers.setdefault(HTTPException, http_exception_handler)
        self.exception_handlers.setdefault(
            RequestValidationError, request_validation_exception_handler
        )
        self.exception_handlers.setdefault(
            WebSocketRequestValidationError,
            # Starlette still has incorrect type specification for the handlers
            websocket_request_validation_exception_handler,  # type: ignore
        )

        self.user_middleware: List[Middleware] = (
            [] if middleware is None else list(middleware)
        )
        self.middleware_stack: Union[ASGIApp, None] = None
        self.setup()

    def build_middleware_stack(self) -> ASGIApp:
        # Duplicate/override from Starlette to add AsyncExitStackMiddleware
        # inside of ExceptionMiddleware, inside of custom user middlewares
        debug = self.debug
        error_handler = None
        exception_handlers: dict[Any, ExceptionHandler] = {}

        for key, value in self.exception_handlers.items():
            if key in (500, Exception):
                error_handler = value
            else:
                exception_handlers[key] = value

        middleware = (
            [Middleware(ServerErrorMiddleware, handler=error_handler, debug=debug)]
            + self.user_middleware
            + [
                Middleware(
                    ExceptionMiddleware, handlers=exception_handlers, debug=debug
                ),
                # Add FastAPI-specific AsyncExitStackMiddleware for closing files.
                # Before this was also used for closing dependencies with yield but
                # those now have their own AsyncExitStack, to properly support
                # streaming responses while keeping compatibility with the previous
                # versions (as of writing 0.117.1) that allowed doing
                # except HTTPException inside a dependency with yield.
                # This needs to happen after user middlewares because those create a
                # new contextvars context copy by using a new AnyIO task group.
                # This AsyncExitStack preserves the context for contextvars, not
                # strictly necessary for closing files but it was one of the original
                # intentions.
                # If the AsyncExitStack lived outside of the custom middlewares and
                # contextvars were set, for example in a dependency with 'yield'
                # in that internal contextvars context, the values would not be
                # available in the outer context of the AsyncExitStack.
                # By placing the middleware and the AsyncExitStack here, inside all
                # user middlewares, the same context is used.
                # This is currently not needed, only for closing files, but used to be
                # important when dependencies with yield were closed here.
                Middleware(AsyncExitStackMiddleware),
            ]
        )

        app = self.router
        for cls, args, kwargs in reversed(middleware):
            app = cls(app, *args, **kwargs)
        return app

    def openapi(self) -> Dict[str, Any]:
        """
        Generate the OpenAPI schema of the application. This is called by FastAPI
        internally.

        The first time it is called it stores the result in the attribute
        `app.openapi_schema`, and next times it is called, it just returns that same
        result. To avoid the cost of generating the schema every time.

        If you need to modify the generated OpenAPI schema, you could modify it.

        Read more in the
        [FastAPI docs for OpenAPI](https://fastapi.tiangolo.com/how-to/extending-openapi/).
        """
        if not self.openapi_schema:
            self.openapi_schema = get_openapi(
                title=self.title,
                version=self.version,
                openapi_version=self.openapi_version,
                summary=self.summary,
                description=self.description,
                terms_of_service=self.terms_of_service,
                contact=self.contact,
                license_info=self.license_info,
                routes=self.routes,
                webhooks=self.webhooks.routes,
                tags=self.openapi_tags,
                servers=self.servers,
                separate_input_output_schemas=self.separate_input_output_schemas,
                external_docs=self.openapi_external_docs,
            )
        return self.openapi_schema

    def setup(self) -> None:
        if self.openapi_url:
            urls = (server_data.get("url") for server_data in self.servers)
            server_urls = {url for url in urls if url}

            async def openapi(req: Request) -> JSONResponse:
                root_path = req.scope.get("root_path", "").rstrip("/")
                if root_path not in server_urls:
                    if root_path and self.root_path_in_servers:
                        self.servers.insert(0, {"url": root_path})
                        server_urls.add(root_path)
                return JSONResponse(self.openapi())

            self.add_route(self.openapi_url, openapi, include_in_schema=False)
        if self.openapi_url and self.docs_url:

            async def swagger_ui_html(req: Request) -> HTMLResponse:
                root_path = req.scope.get("root_path", "").rstrip("/")
                openapi_url = root_path + self.openapi_url
                oauth2_redirect_url = self.swagger_ui_oauth2_redirect_url
                if oauth2_redirect_url:
                    oauth2_redirect_url = root_path + oauth2_redirect_url
                return get_swagger_ui_html(
                    openapi_url=openapi_url,
                    title=f"{self.title} - Swagger UI",
                    oauth2_redirect_url=oauth2_redirect_url,
                    init_oauth=self.swagger_ui_init_oauth,
                    swagger_ui_parameters=self.swagger_ui_parameters,
                )

            self.add_route(self.docs_url, swagger_ui_html, include_in_schema=False)

            if self.swagger_ui_oauth2_redirect_url:

                async def swagger_ui_redirect(req: Request) -> HTMLResponse:
                    return get_swagger_ui_oauth2_redirect_html()

                self.add_route(
                    self.swagger_ui_oauth2_redirect_url,
                    swagger_ui_redirect,
                    include_in_schema=False,
                )
        if self.openapi_url and self.redoc_url:

            async def redoc_html(req: Request) -> HTMLResponse:
                root_path = req.scope.get("root_path", "").rstrip("/")
                openapi_url = root_path + self.openapi_url
                return get_redoc_html(
                    openapi_url=openapi_url, title=f"{self.title} - ReDoc"
                )

            self.add_route(self.redoc_url, redoc_html, include_in_schema=False)

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if self.root_path:
            scope["root_path"] = self.root_path
        await super().__call__(scope, receive, send)

    def add_api_route(
        self,
        path: str,
        endpoint: Callable[..., Any],
        *,
        response_model: Any = Default(None),
        status_code: Optional[int] = None,
        tags: Optional[List[Union[str, Enum]]] = None,
        dependencies: Optional[Sequence[Depends]] = None,
        summary: Optional[str] = None,
        description: Optional[str] = None,
        response_description: str = "Successful Response",
        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,
        deprecated: Optional[bool] = None,
        methods: Optional[List[str]] = None,
        operation_id: Optional[str] = None,
        response_model_include: Optional[IncEx] = None,
        response_model_exclude: Optional[IncEx] = None,
        response_model_by_alias: bool = True,
        response_model_exclude_unset: bool = False,
        response_model_exclude_defaults: bool = False,
        response_model_exclude_none: bool = False,
        include_in_schema: bool = True,
        response_class: Union[Type[Response], DefaultPlaceholder] = Default(
            JSONResponse
        ),
        name: Optional[str] = None,
        openapi_extra: Optional[Dict[str, Any]] = None,
        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(
            generate_unique_id
        ),
    ) -> None:
        self.router.add_api_route(
            path,
            endpoint=endpoint,
            response_model=response_model,
            status_code=status_code,
            tags=tags,
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses,
            deprecated=deprecated,
            methods=methods,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_exclude_unset=response_model_exclude_unset,
            response_model_exclude_defaults=response_model_exclude_defaults,
            response_model_exclude_none=response_model_exclude_none,
            include_in_schema=include_in_schema,
            response_class=response_class,
            name=name,
            openapi_extra=openapi_extra,
            generate_unique_id_function=generate_unique_id_function,
        )

    def api_route(
        self,
        path: str,
        *,
        response_model: Any = Default(None),
        status_code: Optional[int] = None,
        tags: Optional[List[Union[str, Enum]]] = None,
        dependencies: Optional[Sequence[Depends]] = None,
        summary: Optional[str] = None,
        description: Optional[str] = None,
        response_description: str = "Successful Response",
        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,
        deprecated: Optional[bool] = None,
        methods: Optional[List[str]] = None,
        operation_id: Optional[str] = None,
        response_model_include: Optional[IncEx] = None,
        response_model_exclude: Optional[IncEx] = None,
        response_model_by_alias: bool = True,
        response_model_exclude_unset: bool = False,
        response_model_exclude_defaults: bool = False,
        response_model_exclude_none: bool = False,
        include_in_schema: bool = True,
        response_class: Type[Response] = Default(JSONResponse),
        name: Optional[str] = None,
        openapi_extra: Optional[Dict[str, Any]] = None,
        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(
            generate_unique_id
        ),
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        def decorator(func: DecoratedCallable) -> DecoratedCallable:
            self.router.add_api_route(
                path,
                func,
                response_model=response_model,
                status_code=status_code,
                tags=tags,
                dependencies=dependencies,
                summary=summary,
                description=description,
                response_description=response_description,
                responses=responses,
                deprecated=deprecated,
                methods=methods,
                operation_id=operation_id,
                response_model_include=response_model_include,
                response_model_exclude=response_model_exclude,
                response_model_by_alias=response_model_by_alias,
                response_model_exclude_unset=response_model_exclude_unset,
                response_model_exclude_defaults=response_model_exclude_defaults,
                response_model_exclude_none=response_model_exclude_none,
                include_in_schema=include_in_schema,
                response_class=response_class,
                name=name,
                openapi_extra=openapi_extra,
                generate_unique_id_function=generate_unique_id_function,
            )
            return func

        return decorator

    def add_api_websocket_route(
        self,
        path: str,
        endpoint: Callable[..., Any],
        name: Optional[str] = None,
        *,
        dependencies: Optional[Sequence[Depends]] = None,
    ) -> None:
        self.router.add_api_websocket_route(
            path,
            endpoint,
            name=name,
            dependencies=dependencies,
        )

    def websocket(
        self,
        path: Annotated[
            str,
            Doc(
                """
                WebSocket path.
                """
            ),
        ],
        name: Annotated[
            Optional[str],
            Doc(
                """
                A name for the WebSocket. Only used internally.
                """
            ),
        ] = None,
        *,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of dependencies (using `Depends()`) to be used for this
                WebSocket.

                Read more about it in the
                [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).
                """
            ),
        ] = None,
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Decorate a WebSocket function.

        Read more about it in the
        [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).

        **Example**

        ```python
        from fastapi import FastAPI, WebSocket

        app = FastAPI()

        @app.websocket("/ws")
        async def websocket_endpoint(websocket: WebSocket):
            await websocket.accept()
            while True:
                data = await websocket.receive_text()
                await websocket.send_text(f"Message text was: {data}")
        ```
        """

        def decorator(func: DecoratedCallable) -> DecoratedCallable:
            self.add_api_websocket_route(
                path,
                func,
                name=name,
                dependencies=dependencies,
            )
            return func

        return decorator

    def include_router(
        self,
        router: Annotated[routing.APIRouter, Doc("The `APIRouter` to include.")],
        *,
        prefix: Annotated[str, Doc("An optional path prefix for the router.")] = "",
        tags: Annotated[
            Optional[List[Union[str, Enum]]],
            Doc(
                """
                A list of tags to be applied to all the *path operations* in this
                router.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of dependencies (using `Depends()`) to be applied to all the
                *path operations* in this router.

                Read more about it in the
                [FastAPI docs for Bigger Applications - Multiple Files](https://fastapi.tiangolo.com/tutorial/bigger-applications/#include-an-apirouter-with-a-custom-prefix-tags-responses-and-dependencies).

                **Example**

                ```python
                from fastapi import Depends, FastAPI

                from .dependencies import get_token_header
                from .internal import admin

                app = FastAPI()

                app.include_router(
                    admin.router,
                    dependencies=[Depends(get_token_header)],
                )
                ```
                """
            ),
        ] = None,
        responses: Annotated[
            Optional[Dict[Union[int, str], Dict[str, Any]]],
            Doc(
                """
                Additional responses to be shown in OpenAPI.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Additional Responses in OpenAPI](https://fastapi.tiangolo.com/advanced/additional-responses/).

                And in the
                [FastAPI docs for Bigger Applications](https://fastapi.tiangolo.com/tutorial/bigger-applications/#include-an-apirouter-with-a-custom-prefix-tags-responses-and-dependencies).
                """
            ),
        ] = None,
        deprecated: Annotated[
            Optional[bool],
            Doc(
                """
                Mark all the *path operations* in this router as deprecated.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                **Example**

                ```python
                from fastapi import FastAPI

                from .internal import old_api

                app = FastAPI()

                app.include_router(
                    old_api.router,
                    deprecated=True,
                )
                ```
                """
            ),
        ] = None,
        include_in_schema: Annotated[
            bool,
            Doc(
                """
                Include (or not) all the *path operations* in this router in the
                generated OpenAPI schema.

                This affects the generated OpenAPI (e.g. visible at `/docs`).

                **Example**

                ```python
                from fastapi import FastAPI

                from .internal import old_api

                app = FastAPI()

                app.include_router(
                    old_api.router,
                    include_in_schema=False,
                )
                ```
                """
            ),
        ] = True,
        default_response_class: Annotated[
            Type[Response],
            Doc(
                """
                Default response class to be used for the *path operations* in this
                router.

                Read more in the
                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#default-response-class).

                **Example**

                ```python
                from fastapi import FastAPI
                from fastapi.responses import ORJSONResponse

                from .internal import old_api

                app = FastAPI()

                app.include_router(
                    old_api.router,
                    default_response_class=ORJSONResponse,
                )
                ```
                """
            ),
        ] = Default(JSONResponse),
        callbacks: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                List of *path operations* that will be used as OpenAPI callbacks.

                This is only for OpenAPI documentation, the callbacks won't be used
                directly.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).
                """
            ),
        ] = None,
        generate_unique_id_function: Annotated[
            Callable[[routing.APIRoute], str],
            Doc(
                """
                Customize the function used to generate unique IDs for the *path
                operations* shown in the generated OpenAPI.

                This is particularly useful when automatically generating clients or
                SDKs for your API.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = Default(generate_unique_id),
    ) -> None:
        """
        Include an `APIRouter` in the same app.

        Read more about it in the
        [FastAPI docs for Bigger Applications](https://fastapi.tiangolo.com/tutorial/bigger-applications/).

        ## Example

        ```python
        from fastapi import FastAPI

        from .users import users_router

        app = FastAPI()

        app.include_router(users_router)
        ```
        """
        self.router.include_router(
            router,
            prefix=prefix,
            tags=tags,
            dependencies=dependencies,
            responses=responses,
            deprecated=deprecated,
            include_in_schema=include_in_schema,
            default_response_class=default_response_class,
            callbacks=callbacks,
            generate_unique_id_function=generate_unique_id_function,
        )

    def get(
        self,
        path: Annotated[
            str,
            Doc(
                """
                The URL path to be used for this *path operation*.

                For example, in `http://example.com/items`, the path is `/items`.
                """
            ),
        ],
        *,
        response_model: Annotated[
            Any,
            Doc(
                """
                The type to use for the response.

                It could be any valid Pydantic *field* type. So, it doesn't have to
                be a Pydantic model, it could be other things, like a `list`, `dict`,
                etc.

                It will be used for:

                * Documentation: the generated OpenAPI (and the UI at `/docs`) will
                    show it as the response (JSON Schema).
                * Serialization: you could return an arbitrary object and the
                    `response_model` would be used to serialize that object into the
                    corresponding JSON.
                * Filtering: the JSON sent to the client will only contain the data
                    (fields) defined in the `response_model`. If you returned an object
                    that contains an attribute `password` but the `response_model` does
                    not include that field, the JSON sent to the client would not have
                    that `password`.
                * Validation: whatever you return will be serialized with the
                    `response_model`, converting any data as necessary to generate the
                    corresponding JSON. But if the data in the object returned is not
                    valid, that would mean a violation of the contract with the client,
                    so it's an error from the API developer. So, FastAPI will raise an
                    error and return a 500 error code (Internal Server Error).

                Read more about it in the
                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).
                """
            ),
        ] = Default(None),
        status_code: Annotated[
            Optional[int],
            Doc(
                """
                The default status code to be used for the response.

                You could override the status code by returning a response directly.

                Read more about it in the
                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).
                """
            ),
        ] = None,
        tags: Annotated[
            Optional[List[Union[str, Enum]]],
            Doc(
                """
                A list of tags to be applied to the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).
                """
            ),
        ] = None,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of dependencies (using `Depends()`) to be applied to the
                *path operation*.

                Read more about it in the
                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).
                """
            ),
        ] = None,
        summary: Annotated[
            Optional[str],
            Doc(
                """
                A summary for the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        description: Annotated[
            Optional[str],
            Doc(
                """
                A description for the *path operation*.

                If not provided, it will be extracted automatically from the docstring
                of the *path operation function*.

                It can contain Markdown.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        response_description: Annotated[
            str,
            Doc(
                """
                The description for the default response.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = "Successful Response",
        responses: Annotated[
            Optional[Dict[Union[int, str], Dict[str, Any]]],
            Doc(
                """
                Additional responses that could be returned by this *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        deprecated: Annotated[
            Optional[bool],
            Doc(
                """
                Mark this *path operation* as deprecated.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        operation_id: Annotated[
            Optional[str],
            Doc(
                """
                Custom operation ID to be used by this *path operation*.

                By default, it is generated automatically.

                If you provide a custom operation ID, you need to make sure it is
                unique for the whole API.

                You can customize the
                operation ID generation with the parameter
                `generate_unique_id_function` in the `FastAPI` class.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = None,
        response_model_include: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to include only certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_exclude: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to exclude certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_by_alias: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response model
                should be serialized by alias when an alias is used.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = True,
        response_model_exclude_unset: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that were not set and
                have their default values. This is different from
                `response_model_exclude_defaults` in that if the fields are set,
                they will be included in the response, even if the value is the same
                as the default.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_defaults: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that have the same value
                as the default. This is different from `response_model_exclude_unset`
                in that if the fields are set but contain the same default values,
                they will be excluded from the response.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_none: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data should
                exclude fields set to `None`.

                This is much simpler (less smart) than `response_model_exclude_unset`
                and `response_model_exclude_defaults`. You probably want to use one of
                those two instead of this one, as those allow returning `None` values
                when it makes sense.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).
                """
            ),
        ] = False,
        include_in_schema: Annotated[
            bool,
            Doc(
                """
                Include this *path operation* in the generated OpenAPI schema.

                This affects the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).
                """
            ),
        ] = True,
        response_class: Annotated[
            Type[Response],
            Doc(
                """
                Response class to be used for this *path operation*.

                This will not be used if you return a response directly.

                Read more about it in the
                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).
                """
            ),
        ] = Default(JSONResponse),
        name: Annotated[
            Optional[str],
            Doc(
                """
                Name for this *path operation*. Only used internally.
                """
            ),
        ] = None,
        callbacks: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                List of *path operations* that will be used as OpenAPI callbacks.

                This is only for OpenAPI documentation, the callbacks won't be used
                directly.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).
                """
            ),
        ] = None,
        openapi_extra: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                Extra metadata to be included in the OpenAPI schema for this *path
                operation*.

                Read more about it in the
                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).
                """
            ),
        ] = None,
        generate_unique_id_function: Annotated[
            Callable[[routing.APIRoute], str],
            Doc(
                """
                Customize the function used to generate unique IDs for the *path
                operations* shown in the generated OpenAPI.

                This is particularly useful when automatically generating clients or
                SDKs for your API.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = Default(generate_unique_id),
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add a *path operation* using an HTTP GET operation.

        ## Example

        ```python
        from fastapi import FastAPI

        app = FastAPI()

        @app.get("/items/")
        def read_items():
            return [{"name": "Empanada"}, {"name": "Arepa"}]
        ```
        """
        return self.router.get(
            path,
            response_model=response_model,
            status_code=status_code,
            tags=tags,
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses,
            deprecated=deprecated,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_exclude_unset=response_model_exclude_unset,
            response_model_exclude_defaults=response_model_exclude_defaults,
            response_model_exclude_none=response_model_exclude_none,
            include_in_schema=include_in_schema,
            response_class=response_class,
            name=name,
            callbacks=callbacks,
            openapi_extra=openapi_extra,
            generate_unique_id_function=generate_unique_id_function,
        )

    def put(
        self,
        path: Annotated[
            str,
            Doc(
                """
                The URL path to be used for this *path operation*.

                For example, in `http://example.com/items`, the path is `/items`.
                """
            ),
        ],
        *,
        response_model: Annotated[
            Any,
            Doc(
                """
                The type to use for the response.

                It could be any valid Pydantic *field* type. So, it doesn't have to
                be a Pydantic model, it could be other things, like a `list`, `dict`,
                etc.

                It will be used for:

                * Documentation: the generated OpenAPI (and the UI at `/docs`) will
                    show it as the response (JSON Schema).
                * Serialization: you could return an arbitrary object and the
                    `response_model` would be used to serialize that object into the
                    corresponding JSON.
                * Filtering: the JSON sent to the client will only contain the data
                    (fields) defined in the `response_model`. If you returned an object
                    that contains an attribute `password` but the `response_model` does
                    not include that field, the JSON sent to the client would not have
                    that `password`.
                * Validation: whatever you return will be serialized with the
                    `response_model`, converting any data as necessary to generate the
                    corresponding JSON. But if the data in the object returned is not
                    valid, that would mean a violation of the contract with the client,
                    so it's an error from the API developer. So, FastAPI will raise an
                    error and return a 500 error code (Internal Server Error).

                Read more about it in the
                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).
                """
            ),
        ] = Default(None),
        status_code: Annotated[
            Optional[int],
            Doc(
                """
                The default status code to be used for the response.

                You could override the status code by returning a response directly.

                Read more about it in the
                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).
                """
            ),
        ] = None,
        tags: Annotated[
            Optional[List[Union[str, Enum]]],
            Doc(
                """
                A list of tags to be applied to the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).
                """
            ),
        ] = None,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of dependencies (using `Depends()`) to be applied to the
                *path operation*.

                Read more about it in the
                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).
                """
            ),
        ] = None,
        summary: Annotated[
            Optional[str],
            Doc(
                """
                A summary for the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        description: Annotated[
            Optional[str],
            Doc(
                """
                A description for the *path operation*.

                If not provided, it will be extracted automatically from the docstring
                of the *path operation function*.

                It can contain Markdown.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        response_description: Annotated[
            str,
            Doc(
                """
                The description for the default response.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = "Successful Response",
        responses: Annotated[
            Optional[Dict[Union[int, str], Dict[str, Any]]],
            Doc(
                """
                Additional responses that could be returned by this *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        deprecated: Annotated[
            Optional[bool],
            Doc(
                """
                Mark this *path operation* as deprecated.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        operation_id: Annotated[
            Optional[str],
            Doc(
                """
                Custom operation ID to be used by this *path operation*.

                By default, it is generated automatically.

                If you provide a custom operation ID, you need to make sure it is
                unique for the whole API.

                You can customize the
                operation ID generation with the parameter
                `generate_unique_id_function` in the `FastAPI` class.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = None,
        response_model_include: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to include only certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_exclude: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to exclude certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_by_alias: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response model
                should be serialized by alias when an alias is used.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = True,
        response_model_exclude_unset: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that were not set and
                have their default values. This is different from
                `response_model_exclude_defaults` in that if the fields are set,
                they will be included in the response, even if the value is the same
                as the default.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_defaults: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that have the same value
                as the default. This is different from `response_model_exclude_unset`
                in that if the fields are set but contain the same default values,
                they will be excluded from the response.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_none: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data should
                exclude fields set to `None`.

                This is much simpler (less smart) than `response_model_exclude_unset`
                and `response_model_exclude_defaults`. You probably want to use one of
                those two instead of this one, as those allow returning `None` values
                when it makes sense.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).
                """
            ),
        ] = False,
        include_in_schema: Annotated[
            bool,
            Doc(
                """
                Include this *path operation* in the generated OpenAPI schema.

                This affects the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).
                """
            ),
        ] = True,
        response_class: Annotated[
            Type[Response],
            Doc(
                """
                Response class to be used for this *path operation*.

                This will not be used if you return a response directly.

                Read more about it in the
                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).
                """
            ),
        ] = Default(JSONResponse),
        name: Annotated[
            Optional[str],
            Doc(
                """
                Name for this *path operation*. Only used internally.
                """
            ),
        ] = None,
        callbacks: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                List of *path operations* that will be used as OpenAPI callbacks.

                This is only for OpenAPI documentation, the callbacks won't be used
                directly.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).
                """
            ),
        ] = None,
        openapi_extra: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                Extra metadata to be included in the OpenAPI schema for this *path
                operation*.

                Read more about it in the
                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).
                """
            ),
        ] = None,
        generate_unique_id_function: Annotated[
            Callable[[routing.APIRoute], str],
            Doc(
                """
                Customize the function used to generate unique IDs for the *path
                operations* shown in the generated OpenAPI.

                This is particularly useful when automatically generating clients or
                SDKs for your API.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = Default(generate_unique_id),
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add a *path operation* using an HTTP PUT operation.

        ## Example

        ```python
        from fastapi import FastAPI
        from pydantic import BaseModel

        class Item(BaseModel):
            name: str
            description: str | None = None

        app = FastAPI()

        @app.put("/items/{item_id}")
        def replace_item(item_id: str, item: Item):
            return {"message": "Item replaced", "id": item_id}
        ```
        """
        return self.router.put(
            path,
            response_model=response_model,
            status_code=status_code,
            tags=tags,
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses,
            deprecated=deprecated,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_exclude_unset=response_model_exclude_unset,
            response_model_exclude_defaults=response_model_exclude_defaults,
            response_model_exclude_none=response_model_exclude_none,
            include_in_schema=include_in_schema,
            response_class=response_class,
            name=name,
            callbacks=callbacks,
            openapi_extra=openapi_extra,
            generate_unique_id_function=generate_unique_id_function,
        )

    def post(
        self,
        path: Annotated[
            str,
            Doc(
                """
                The URL path to be used for this *path operation*.

                For example, in `http://example.com/items`, the path is `/items`.
                """
            ),
        ],
        *,
        response_model: Annotated[
            Any,
            Doc(
                """
                The type to use for the response.

                It could be any valid Pydantic *field* type. So, it doesn't have to
                be a Pydantic model, it could be other things, like a `list`, `dict`,
                etc.

                It will be used for:

                * Documentation: the generated OpenAPI (and the UI at `/docs`) will
                    show it as the response (JSON Schema).
                * Serialization: you could return an arbitrary object and the
                    `response_model` would be used to serialize that object into the
                    corresponding JSON.
                * Filtering: the JSON sent to the client will only contain the data
                    (fields) defined in the `response_model`. If you returned an object
                    that contains an attribute `password` but the `response_model` does
                    not include that field, the JSON sent to the client would not have
                    that `password`.
                * Validation: whatever you return will be serialized with the
                    `response_model`, converting any data as necessary to generate the
                    corresponding JSON. But if the data in the object returned is not
                    valid, that would mean a violation of the contract with the client,
                    so it's an error from the API developer. So, FastAPI will raise an
                    error and return a 500 error code (Internal Server Error).

                Read more about it in the
                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).
                """
            ),
        ] = Default(None),
        status_code: Annotated[
            Optional[int],
            Doc(
                """
                The default status code to be used for the response.

                You could override the status code by returning a response directly.

                Read more about it in the
                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).
                """
            ),
        ] = None,
        tags: Annotated[
            Optional[List[Union[str, Enum]]],
            Doc(
                """
                A list of tags to be applied to the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).
                """
            ),
        ] = None,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of dependencies (using `Depends()`) to be applied to the
                *path operation*.

                Read more about it in the
                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).
                """
            ),
        ] = None,
        summary: Annotated[
            Optional[str],
            Doc(
                """
                A summary for the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        description: Annotated[
            Optional[str],
            Doc(
                """
                A description for the *path operation*.

                If not provided, it will be extracted automatically from the docstring
                of the *path operation function*.

                It can contain Markdown.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        response_description: Annotated[
            str,
            Doc(
                """
                The description for the default response.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = "Successful Response",
        responses: Annotated[
            Optional[Dict[Union[int, str], Dict[str, Any]]],
            Doc(
                """
                Additional responses that could be returned by this *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        deprecated: Annotated[
            Optional[bool],
            Doc(
                """
                Mark this *path operation* as deprecated.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        operation_id: Annotated[
            Optional[str],
            Doc(
                """
                Custom operation ID to be used by this *path operation*.

                By default, it is generated automatically.

                If you provide a custom operation ID, you need to make sure it is
                unique for the whole API.

                You can customize the
                operation ID generation with the parameter
                `generate_unique_id_function` in the `FastAPI` class.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = None,
        response_model_include: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to include only certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_exclude: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to exclude certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_by_alias: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response model
                should be serialized by alias when an alias is used.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = True,
        response_model_exclude_unset: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that were not set and
                have their default values. This is different from
                `response_model_exclude_defaults` in that if the fields are set,
                they will be included in the response, even if the value is the same
                as the default.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_defaults: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that have the same value
                as the default. This is different from `response_model_exclude_unset`
                in that if the fields are set but contain the same default values,
                they will be excluded from the response.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_none: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data should
                exclude fields set to `None`.

                This is much simpler (less smart) than `response_model_exclude_unset`
                and `response_model_exclude_defaults`. You probably want to use one of
                those two instead of this one, as those allow returning `None` values
                when it makes sense.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).
                """
            ),
        ] = False,
        include_in_schema: Annotated[
            bool,
            Doc(
                """
                Include this *path operation* in the generated OpenAPI schema.

                This affects the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).
                """
            ),
        ] = True,
        response_class: Annotated[
            Type[Response],
            Doc(
                """
                Response class to be used for this *path operation*.

                This will not be used if you return a response directly.

                Read more about it in the
                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).
                """
            ),
        ] = Default(JSONResponse),
        name: Annotated[
            Optional[str],
            Doc(
                """
                Name for this *path operation*. Only used internally.
                """
            ),
        ] = None,
        callbacks: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                List of *path operations* that will be used as OpenAPI callbacks.

                This is only for OpenAPI documentation, the callbacks won't be used
                directly.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).
                """
            ),
        ] = None,
        openapi_extra: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                Extra metadata to be included in the OpenAPI schema for this *path
                operation*.

                Read more about it in the
                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).
                """
            ),
        ] = None,
        generate_unique_id_function: Annotated[
            Callable[[routing.APIRoute], str],
            Doc(
                """
                Customize the function used to generate unique IDs for the *path
                operations* shown in the generated OpenAPI.

                This is particularly useful when automatically generating clients or
                SDKs for your API.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = Default(generate_unique_id),
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add a *path operation* using an HTTP POST operation.

        ## Example

        ```python
        from fastapi import FastAPI
        from pydantic import BaseModel

        class Item(BaseModel):
            name: str
            description: str | None = None

        app = FastAPI()

        @app.post("/items/")
        def create_item(item: Item):
            return {"message": "Item created"}
        ```
        """
        return self.router.post(
            path,
            response_model=response_model,
            status_code=status_code,
            tags=tags,
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses,
            deprecated=deprecated,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_exclude_unset=response_model_exclude_unset,
            response_model_exclude_defaults=response_model_exclude_defaults,
            response_model_exclude_none=response_model_exclude_none,
            include_in_schema=include_in_schema,
            response_class=response_class,
            name=name,
            callbacks=callbacks,
            openapi_extra=openapi_extra,
            generate_unique_id_function=generate_unique_id_function,
        )

    def delete(
        self,
        path: Annotated[
            str,
            Doc(
                """
                The URL path to be used for this *path operation*.

                For example, in `http://example.com/items`, the path is `/items`.
                """
            ),
        ],
        *,
        response_model: Annotated[
            Any,
            Doc(
                """
                The type to use for the response.

                It could be any valid Pydantic *field* type. So, it doesn't have to
                be a Pydantic model, it could be other things, like a `list`, `dict`,
                etc.

                It will be used for:

                * Documentation: the generated OpenAPI (and the UI at `/docs`) will
                    show it as the response (JSON Schema).
                * Serialization: you could return an arbitrary object and the
                    `response_model` would be used to serialize that object into the
                    corresponding JSON.
                * Filtering: the JSON sent to the client will only contain the data
                    (fields) defined in the `response_model`. If you returned an object
                    that contains an attribute `password` but the `response_model` does
                    not include that field, the JSON sent to the client would not have
                    that `password`.
                * Validation: whatever you return will be serialized with the
                    `response_model`, converting any data as necessary to generate the
                    corresponding JSON. But if the data in the object returned is not
                    valid, that would mean a violation of the contract with the client,
                    so it's an error from the API developer. So, FastAPI will raise an
                    error and return a 500 error code (Internal Server Error).

                Read more about it in the
                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).
                """
            ),
        ] = Default(None),
        status_code: Annotated[
            Optional[int],
            Doc(
                """
                The default status code to be used for the response.

                You could override the status code by returning a response directly.

                Read more about it in the
                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).
                """
            ),
        ] = None,
        tags: Annotated[
            Optional[List[Union[str, Enum]]],
            Doc(
                """
                A list of tags to be applied to the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).
                """
            ),
        ] = None,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of dependencies (using `Depends()`) to be applied to the
                *path operation*.

                Read more about it in the
                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).
                """
            ),
        ] = None,
        summary: Annotated[
            Optional[str],
            Doc(
                """
                A summary for the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        description: Annotated[
            Optional[str],
            Doc(
                """
                A description for the *path operation*.

                If not provided, it will be extracted automatically from the docstring
                of the *path operation function*.

                It can contain Markdown.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        response_description: Annotated[
            str,
            Doc(
                """
                The description for the default response.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = "Successful Response",
        responses: Annotated[
            Optional[Dict[Union[int, str], Dict[str, Any]]],
            Doc(
                """
                Additional responses that could be returned by this *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        deprecated: Annotated[
            Optional[bool],
            Doc(
                """
                Mark this *path operation* as deprecated.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        operation_id: Annotated[
            Optional[str],
            Doc(
                """
                Custom operation ID to be used by this *path operation*.

                By default, it is generated automatically.

                If you provide a custom operation ID, you need to make sure it is
                unique for the whole API.

                You can customize the
                operation ID generation with the parameter
                `generate_unique_id_function` in the `FastAPI` class.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = None,
        response_model_include: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to include only certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_exclude: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to exclude certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_by_alias: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response model
                should be serialized by alias when an alias is used.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = True,
        response_model_exclude_unset: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that were not set and
                have their default values. This is different from
                `response_model_exclude_defaults` in that if the fields are set,
                they will be included in the response, even if the value is the same
                as the default.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_defaults: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that have the same value
                as the default. This is different from `response_model_exclude_unset`
                in that if the fields are set but contain the same default values,
                they will be excluded from the response.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_none: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data should
                exclude fields set to `None`.

                This is much simpler (less smart) than `response_model_exclude_unset`
                and `response_model_exclude_defaults`. You probably want to use one of
                those two instead of this one, as those allow returning `None` values
                when it makes sense.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).
                """
            ),
        ] = False,
        include_in_schema: Annotated[
            bool,
            Doc(
                """
                Include this *path operation* in the generated OpenAPI schema.

                This affects the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).
                """
            ),
        ] = True,
        response_class: Annotated[
            Type[Response],
            Doc(
                """
                Response class to be used for this *path operation*.

                This will not be used if you return a response directly.

                Read more about it in the
                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).
                """
            ),
        ] = Default(JSONResponse),
        name: Annotated[
            Optional[str],
            Doc(
                """
                Name for this *path operation*. Only used internally.
                """
            ),
        ] = None,
        callbacks: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                List of *path operations* that will be used as OpenAPI callbacks.

                This is only for OpenAPI documentation, the callbacks won't be used
                directly.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).
                """
            ),
        ] = None,
        openapi_extra: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                Extra metadata to be included in the OpenAPI schema for this *path
                operation*.

                Read more about it in the
                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).
                """
            ),
        ] = None,
        generate_unique_id_function: Annotated[
            Callable[[routing.APIRoute], str],
            Doc(
                """
                Customize the function used to generate unique IDs for the *path
                operations* shown in the generated OpenAPI.

                This is particularly useful when automatically generating clients or
                SDKs for your API.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = Default(generate_unique_id),
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add a *path operation* using an HTTP DELETE operation.

        ## Example

        ```python
        from fastapi import FastAPI

        app = FastAPI()

        @app.delete("/items/{item_id}")
        def delete_item(item_id: str):
            return {"message": "Item deleted"}
        ```
        """
        return self.router.delete(
            path,
            response_model=response_model,
            status_code=status_code,
            tags=tags,
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses,
            deprecated=deprecated,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_exclude_unset=response_model_exclude_unset,
            response_model_exclude_defaults=response_model_exclude_defaults,
            response_model_exclude_none=response_model_exclude_none,
            include_in_schema=include_in_schema,
            response_class=response_class,
            name=name,
            callbacks=callbacks,
            openapi_extra=openapi_extra,
            generate_unique_id_function=generate_unique_id_function,
        )

    def options(
        self,
        path: Annotated[
            str,
            Doc(
                """
                The URL path to be used for this *path operation*.

                For example, in `http://example.com/items`, the path is `/items`.
                """
            ),
        ],
        *,
        response_model: Annotated[
            Any,
            Doc(
                """
                The type to use for the response.

                It could be any valid Pydantic *field* type. So, it doesn't have to
                be a Pydantic model, it could be other things, like a `list`, `dict`,
                etc.

                It will be used for:

                * Documentation: the generated OpenAPI (and the UI at `/docs`) will
                    show it as the response (JSON Schema).
                * Serialization: you could return an arbitrary object and the
                    `response_model` would be used to serialize that object into the
                    corresponding JSON.
                * Filtering: the JSON sent to the client will only contain the data
                    (fields) defined in the `response_model`. If you returned an object
                    that contains an attribute `password` but the `response_model` does
                    not include that field, the JSON sent to the client would not have
                    that `password`.
                * Validation: whatever you return will be serialized with the
                    `response_model`, converting any data as necessary to generate the
                    corresponding JSON. But if the data in the object returned is not
                    valid, that would mean a violation of the contract with the client,
                    so it's an error from the API developer. So, FastAPI will raise an
                    error and return a 500 error code (Internal Server Error).

                Read more about it in the
                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).
                """
            ),
        ] = Default(None),
        status_code: Annotated[
            Optional[int],
            Doc(
                """
                The default status code to be used for the response.

                You could override the status code by returning a response directly.

                Read more about it in the
                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).
                """
            ),
        ] = None,
        tags: Annotated[
            Optional[List[Union[str, Enum]]],
            Doc(
                """
                A list of tags to be applied to the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).
                """
            ),
        ] = None,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of dependencies (using `Depends()`) to be applied to the
                *path operation*.

                Read more about it in the
                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).
                """
            ),
        ] = None,
        summary: Annotated[
            Optional[str],
            Doc(
                """
                A summary for the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        description: Annotated[
            Optional[str],
            Doc(
                """
                A description for the *path operation*.

                If not provided, it will be extracted automatically from the docstring
                of the *path operation function*.

                It can contain Markdown.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        response_description: Annotated[
            str,
            Doc(
                """
                The description for the default response.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = "Successful Response",
        responses: Annotated[
            Optional[Dict[Union[int, str], Dict[str, Any]]],
            Doc(
                """
                Additional responses that could be returned by this *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        deprecated: Annotated[
            Optional[bool],
            Doc(
                """
                Mark this *path operation* as deprecated.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        operation_id: Annotated[
            Optional[str],
            Doc(
                """
                Custom operation ID to be used by this *path operation*.

                By default, it is generated automatically.

                If you provide a custom operation ID, you need to make sure it is
                unique for the whole API.

                You can customize the
                operation ID generation with the parameter
                `generate_unique_id_function` in the `FastAPI` class.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = None,
        response_model_include: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to include only certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_exclude: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to exclude certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_by_alias: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response model
                should be serialized by alias when an alias is used.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = True,
        response_model_exclude_unset: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that were not set and
                have their default values. This is different from
                `response_model_exclude_defaults` in that if the fields are set,
                they will be included in the response, even if the value is the same
                as the default.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_defaults: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that have the same value
                as the default. This is different from `response_model_exclude_unset`
                in that if the fields are set but contain the same default values,
                they will be excluded from the response.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_none: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data should
                exclude fields set to `None`.

                This is much simpler (less smart) than `response_model_exclude_unset`
                and `response_model_exclude_defaults`. You probably want to use one of
                those two instead of this one, as those allow returning `None` values
                when it makes sense.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).
                """
            ),
        ] = False,
        include_in_schema: Annotated[
            bool,
            Doc(
                """
                Include this *path operation* in the generated OpenAPI schema.

                This affects the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).
                """
            ),
        ] = True,
        response_class: Annotated[
            Type[Response],
            Doc(
                """
                Response class to be used for this *path operation*.

                This will not be used if you return a response directly.

                Read more about it in the
                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).
                """
            ),
        ] = Default(JSONResponse),
        name: Annotated[
            Optional[str],
            Doc(
                """
                Name for this *path operation*. Only used internally.
                """
            ),
        ] = None,
        callbacks: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                List of *path operations* that will be used as OpenAPI callbacks.

                This is only for OpenAPI documentation, the callbacks won't be used
                directly.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).
                """
            ),
        ] = None,
        openapi_extra: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                Extra metadata to be included in the OpenAPI schema for this *path
                operation*.

                Read more about it in the
                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).
                """
            ),
        ] = None,
        generate_unique_id_function: Annotated[
            Callable[[routing.APIRoute], str],
            Doc(
                """
                Customize the function used to generate unique IDs for the *path
                operations* shown in the generated OpenAPI.

                This is particularly useful when automatically generating clients or
                SDKs for your API.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = Default(generate_unique_id),
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add a *path operation* using an HTTP OPTIONS operation.

        ## Example

        ```python
        from fastapi import FastAPI

        app = FastAPI()

        @app.options("/items/")
        def get_item_options():
            return {"additions": ["Aji", "Guacamole"]}
        ```
        """
        return self.router.options(
            path,
            response_model=response_model,
            status_code=status_code,
            tags=tags,
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses,
            deprecated=deprecated,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_exclude_unset=response_model_exclude_unset,
            response_model_exclude_defaults=response_model_exclude_defaults,
            response_model_exclude_none=response_model_exclude_none,
            include_in_schema=include_in_schema,
            response_class=response_class,
            name=name,
            callbacks=callbacks,
            openapi_extra=openapi_extra,
            generate_unique_id_function=generate_unique_id_function,
        )

    def head(
        self,
        path: Annotated[
            str,
            Doc(
                """
                The URL path to be used for this *path operation*.

                For example, in `http://example.com/items`, the path is `/items`.
                """
            ),
        ],
        *,
        response_model: Annotated[
            Any,
            Doc(
                """
                The type to use for the response.

                It could be any valid Pydantic *field* type. So, it doesn't have to
                be a Pydantic model, it could be other things, like a `list`, `dict`,
                etc.

                It will be used for:

                * Documentation: the generated OpenAPI (and the UI at `/docs`) will
                    show it as the response (JSON Schema).
                * Serialization: you could return an arbitrary object and the
                    `response_model` would be used to serialize that object into the
                    corresponding JSON.
                * Filtering: the JSON sent to the client will only contain the data
                    (fields) defined in the `response_model`. If you returned an object
                    that contains an attribute `password` but the `response_model` does
                    not include that field, the JSON sent to the client would not have
                    that `password`.
                * Validation: whatever you return will be serialized with the
                    `response_model`, converting any data as necessary to generate the
                    corresponding JSON. But if the data in the object returned is not
                    valid, that would mean a violation of the contract with the client,
                    so it's an error from the API developer. So, FastAPI will raise an
                    error and return a 500 error code (Internal Server Error).

                Read more about it in the
                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).
                """
            ),
        ] = Default(None),
        status_code: Annotated[
            Optional[int],
            Doc(
                """
                The default status code to be used for the response.

                You could override the status code by returning a response directly.

                Read more about it in the
                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).
                """
            ),
        ] = None,
        tags: Annotated[
            Optional[List[Union[str, Enum]]],
            Doc(
                """
                A list of tags to be applied to the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).
                """
            ),
        ] = None,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of dependencies (using `Depends()`) to be applied to the
                *path operation*.

                Read more about it in the
                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).
                """
            ),
        ] = None,
        summary: Annotated[
            Optional[str],
            Doc(
                """
                A summary for the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        description: Annotated[
            Optional[str],
            Doc(
                """
                A description for the *path operation*.

                If not provided, it will be extracted automatically from the docstring
                of the *path operation function*.

                It can contain Markdown.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        response_description: Annotated[
            str,
            Doc(
                """
                The description for the default response.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = "Successful Response",
        responses: Annotated[
            Optional[Dict[Union[int, str], Dict[str, Any]]],
            Doc(
                """
                Additional responses that could be returned by this *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        deprecated: Annotated[
            Optional[bool],
            Doc(
                """
                Mark this *path operation* as deprecated.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        operation_id: Annotated[
            Optional[str],
            Doc(
                """
                Custom operation ID to be used by this *path operation*.

                By default, it is generated automatically.

                If you provide a custom operation ID, you need to make sure it is
                unique for the whole API.

                You can customize the
                operation ID generation with the parameter
                `generate_unique_id_function` in the `FastAPI` class.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = None,
        response_model_include: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to include only certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_exclude: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to exclude certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_by_alias: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response model
                should be serialized by alias when an alias is used.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = True,
        response_model_exclude_unset: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that were not set and
                have their default values. This is different from
                `response_model_exclude_defaults` in that if the fields are set,
                they will be included in the response, even if the value is the same
                as the default.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_defaults: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that have the same value
                as the default. This is different from `response_model_exclude_unset`
                in that if the fields are set but contain the same default values,
                they will be excluded from the response.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_none: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data should
                exclude fields set to `None`.

                This is much simpler (less smart) than `response_model_exclude_unset`
                and `response_model_exclude_defaults`. You probably want to use one of
                those two instead of this one, as those allow returning `None` values
                when it makes sense.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).
                """
            ),
        ] = False,
        include_in_schema: Annotated[
            bool,
            Doc(
                """
                Include this *path operation* in the generated OpenAPI schema.

                This affects the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).
                """
            ),
        ] = True,
        response_class: Annotated[
            Type[Response],
            Doc(
                """
                Response class to be used for this *path operation*.

                This will not be used if you return a response directly.

                Read more about it in the
                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).
                """
            ),
        ] = Default(JSONResponse),
        name: Annotated[
            Optional[str],
            Doc(
                """
                Name for this *path operation*. Only used internally.
                """
            ),
        ] = None,
        callbacks: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                List of *path operations* that will be used as OpenAPI callbacks.

                This is only for OpenAPI documentation, the callbacks won't be used
                directly.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).
                """
            ),
        ] = None,
        openapi_extra: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                Extra metadata to be included in the OpenAPI schema for this *path
                operation*.

                Read more about it in the
                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).
                """
            ),
        ] = None,
        generate_unique_id_function: Annotated[
            Callable[[routing.APIRoute], str],
            Doc(
                """
                Customize the function used to generate unique IDs for the *path
                operations* shown in the generated OpenAPI.

                This is particularly useful when automatically generating clients or
                SDKs for your API.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = Default(generate_unique_id),
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add a *path operation* using an HTTP HEAD operation.

        ## Example

        ```python
        from fastapi import FastAPI, Response

        app = FastAPI()

        @app.head("/items/", status_code=204)
        def get_items_headers(response: Response):
            response.headers["X-Cat-Dog"] = "Alone in the world"
        ```
        """
        return self.router.head(
            path,
            response_model=response_model,
            status_code=status_code,
            tags=tags,
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses,
            deprecated=deprecated,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_exclude_unset=response_model_exclude_unset,
            response_model_exclude_defaults=response_model_exclude_defaults,
            response_model_exclude_none=response_model_exclude_none,
            include_in_schema=include_in_schema,
            response_class=response_class,
            name=name,
            callbacks=callbacks,
            openapi_extra=openapi_extra,
            generate_unique_id_function=generate_unique_id_function,
        )

    def patch(
        self,
        path: Annotated[
            str,
            Doc(
                """
                The URL path to be used for this *path operation*.

                For example, in `http://example.com/items`, the path is `/items`.
                """
            ),
        ],
        *,
        response_model: Annotated[
            Any,
            Doc(
                """
                The type to use for the response.

                It could be any valid Pydantic *field* type. So, it doesn't have to
                be a Pydantic model, it could be other things, like a `list`, `dict`,
                etc.

                It will be used for:

                * Documentation: the generated OpenAPI (and the UI at `/docs`) will
                    show it as the response (JSON Schema).
                * Serialization: you could return an arbitrary object and the
                    `response_model` would be used to serialize that object into the
                    corresponding JSON.
                * Filtering: the JSON sent to the client will only contain the data
                    (fields) defined in the `response_model`. If you returned an object
                    that contains an attribute `password` but the `response_model` does
                    not include that field, the JSON sent to the client would not have
                    that `password`.
                * Validation: whatever you return will be serialized with the
                    `response_model`, converting any data as necessary to generate the
                    corresponding JSON. But if the data in the object returned is not
                    valid, that would mean a violation of the contract with the client,
                    so it's an error from the API developer. So, FastAPI will raise an
                    error and return a 500 error code (Internal Server Error).

                Read more about it in the
                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).
                """
            ),
        ] = Default(None),
        status_code: Annotated[
            Optional[int],
            Doc(
                """
                The default status code to be used for the response.

                You could override the status code by returning a response directly.

                Read more about it in the
                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).
                """
            ),
        ] = None,
        tags: Annotated[
            Optional[List[Union[str, Enum]]],
            Doc(
                """
                A list of tags to be applied to the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).
                """
            ),
        ] = None,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of dependencies (using `Depends()`) to be applied to the
                *path operation*.

                Read more about it in the
                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).
                """
            ),
        ] = None,
        summary: Annotated[
            Optional[str],
            Doc(
                """
                A summary for the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        description: Annotated[
            Optional[str],
            Doc(
                """
                A description for the *path operation*.

                If not provided, it will be extracted automatically from the docstring
                of the *path operation function*.

                It can contain Markdown.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        response_description: Annotated[
            str,
            Doc(
                """
                The description for the default response.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = "Successful Response",
        responses: Annotated[
            Optional[Dict[Union[int, str], Dict[str, Any]]],
            Doc(
                """
                Additional responses that could be returned by this *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        deprecated: Annotated[
            Optional[bool],
            Doc(
                """
                Mark this *path operation* as deprecated.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        operation_id: Annotated[
            Optional[str],
            Doc(
                """
                Custom operation ID to be used by this *path operation*.

                By default, it is generated automatically.

                If you provide a custom operation ID, you need to make sure it is
                unique for the whole API.

                You can customize the
                operation ID generation with the parameter
                `generate_unique_id_function` in the `FastAPI` class.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = None,
        response_model_include: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to include only certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_exclude: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to exclude certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_by_alias: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response model
                should be serialized by alias when an alias is used.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = True,
        response_model_exclude_unset: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that were not set and
                have their default values. This is different from
                `response_model_exclude_defaults` in that if the fields are set,
                they will be included in the response, even if the value is the same
                as the default.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_defaults: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that have the same value
                as the default. This is different from `response_model_exclude_unset`
                in that if the fields are set but contain the same default values,
                they will be excluded from the response.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_none: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data should
                exclude fields set to `None`.

                This is much simpler (less smart) than `response_model_exclude_unset`
                and `response_model_exclude_defaults`. You probably want to use one of
                those two instead of this one, as those allow returning `None` values
                when it makes sense.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).
                """
            ),
        ] = False,
        include_in_schema: Annotated[
            bool,
            Doc(
                """
                Include this *path operation* in the generated OpenAPI schema.

                This affects the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).
                """
            ),
        ] = True,
        response_class: Annotated[
            Type[Response],
            Doc(
                """
                Response class to be used for this *path operation*.

                This will not be used if you return a response directly.

                Read more about it in the
                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).
                """
            ),
        ] = Default(JSONResponse),
        name: Annotated[
            Optional[str],
            Doc(
                """
                Name for this *path operation*. Only used internally.
                """
            ),
        ] = None,
        callbacks: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                List of *path operations* that will be used as OpenAPI callbacks.

                This is only for OpenAPI documentation, the callbacks won't be used
                directly.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).
                """
            ),
        ] = None,
        openapi_extra: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                Extra metadata to be included in the OpenAPI schema for this *path
                operation*.

                Read more about it in the
                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).
                """
            ),
        ] = None,
        generate_unique_id_function: Annotated[
            Callable[[routing.APIRoute], str],
            Doc(
                """
                Customize the function used to generate unique IDs for the *path
                operations* shown in the generated OpenAPI.

                This is particularly useful when automatically generating clients or
                SDKs for your API.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = Default(generate_unique_id),
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add a *path operation* using an HTTP PATCH operation.

        ## Example

        ```python
        from fastapi import FastAPI
        from pydantic import BaseModel

        class Item(BaseModel):
            name: str
            description: str | None = None

        app = FastAPI()

        @app.patch("/items/")
        def update_item(item: Item):
            return {"message": "Item updated in place"}
        ```
        """
        return self.router.patch(
            path,
            response_model=response_model,
            status_code=status_code,
            tags=tags,
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses,
            deprecated=deprecated,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_exclude_unset=response_model_exclude_unset,
            response_model_exclude_defaults=response_model_exclude_defaults,
            response_model_exclude_none=response_model_exclude_none,
            include_in_schema=include_in_schema,
            response_class=response_class,
            name=name,
            callbacks=callbacks,
            openapi_extra=openapi_extra,
            generate_unique_id_function=generate_unique_id_function,
        )

    def trace(
        self,
        path: Annotated[
            str,
            Doc(
                """
                The URL path to be used for this *path operation*.

                For example, in `http://example.com/items`, the path is `/items`.
                """
            ),
        ],
        *,
        response_model: Annotated[
            Any,
            Doc(
                """
                The type to use for the response.

                It could be any valid Pydantic *field* type. So, it doesn't have to
                be a Pydantic model, it could be other things, like a `list`, `dict`,
                etc.

                It will be used for:

                * Documentation: the generated OpenAPI (and the UI at `/docs`) will
                    show it as the response (JSON Schema).
                * Serialization: you could return an arbitrary object and the
                    `response_model` would be used to serialize that object into the
                    corresponding JSON.
                * Filtering: the JSON sent to the client will only contain the data
                    (fields) defined in the `response_model`. If you returned an object
                    that contains an attribute `password` but the `response_model` does
                    not include that field, the JSON sent to the client would not have
                    that `password`.
                * Validation: whatever you return will be serialized with the
                    `response_model`, converting any data as necessary to generate the
                    corresponding JSON. But if the data in the object returned is not
                    valid, that would mean a violation of the contract with the client,
                    so it's an error from the API developer. So, FastAPI will raise an
                    error and return a 500 error code (Internal Server Error).

                Read more about it in the
                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).
                """
            ),
        ] = Default(None),
        status_code: Annotated[
            Optional[int],
            Doc(
                """
                The default status code to be used for the response.

                You could override the status code by returning a response directly.

                Read more about it in the
                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).
                """
            ),
        ] = None,
        tags: Annotated[
            Optional[List[Union[str, Enum]]],
            Doc(
                """
                A list of tags to be applied to the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).
                """
            ),
        ] = None,
        dependencies: Annotated[
            Optional[Sequence[Depends]],
            Doc(
                """
                A list of dependencies (using `Depends()`) to be applied to the
                *path operation*.

                Read more about it in the
                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).
                """
            ),
        ] = None,
        summary: Annotated[
            Optional[str],
            Doc(
                """
                A summary for the *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        description: Annotated[
            Optional[str],
            Doc(
                """
                A description for the *path operation*.

                If not provided, it will be extracted automatically from the docstring
                of the *path operation function*.

                It can contain Markdown.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).
                """
            ),
        ] = None,
        response_description: Annotated[
            str,
            Doc(
                """
                The description for the default response.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = "Successful Response",
        responses: Annotated[
            Optional[Dict[Union[int, str], Dict[str, Any]]],
            Doc(
                """
                Additional responses that could be returned by this *path operation*.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        deprecated: Annotated[
            Optional[bool],
            Doc(
                """
                Mark this *path operation* as deprecated.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).
                """
            ),
        ] = None,
        operation_id: Annotated[
            Optional[str],
            Doc(
                """
                Custom operation ID to be used by this *path operation*.

                By default, it is generated automatically.

                If you provide a custom operation ID, you need to make sure it is
                unique for the whole API.

                You can customize the
                operation ID generation with the parameter
                `generate_unique_id_function` in the `FastAPI` class.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = None,
        response_model_include: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to include only certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_exclude: Annotated[
            Optional[IncEx],
            Doc(
                """
                Configuration passed to Pydantic to exclude certain fields in the
                response data.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = None,
        response_model_by_alias: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response model
                should be serialized by alias when an alias is used.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).
                """
            ),
        ] = True,
        response_model_exclude_unset: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that were not set and
                have their default values. This is different from
                `response_model_exclude_defaults` in that if the fields are set,
                they will be included in the response, even if the value is the same
                as the default.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_defaults: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data
                should have all the fields, including the ones that have the same value
                as the default. This is different from `response_model_exclude_unset`
                in that if the fields are set but contain the same default values,
                they will be excluded from the response.

                When `True`, default values are omitted from the response.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).
                """
            ),
        ] = False,
        response_model_exclude_none: Annotated[
            bool,
            Doc(
                """
                Configuration passed to Pydantic to define if the response data should
                exclude fields set to `None`.

                This is much simpler (less smart) than `response_model_exclude_unset`
                and `response_model_exclude_defaults`. You probably want to use one of
                those two instead of this one, as those allow returning `None` values
                when it makes sense.

                Read more about it in the
                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).
                """
            ),
        ] = False,
        include_in_schema: Annotated[
            bool,
            Doc(
                """
                Include this *path operation* in the generated OpenAPI schema.

                This affects the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).
                """
            ),
        ] = True,
        response_class: Annotated[
            Type[Response],
            Doc(
                """
                Response class to be used for this *path operation*.

                This will not be used if you return a response directly.

                Read more about it in the
                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).
                """
            ),
        ] = Default(JSONResponse),
        name: Annotated[
            Optional[str],
            Doc(
                """
                Name for this *path operation*. Only used internally.
                """
            ),
        ] = None,
        callbacks: Annotated[
            Optional[List[BaseRoute]],
            Doc(
                """
                List of *path operations* that will be used as OpenAPI callbacks.

                This is only for OpenAPI documentation, the callbacks won't be used
                directly.

                It will be added to the generated OpenAPI (e.g. visible at `/docs`).

                Read more about it in the
                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).
                """
            ),
        ] = None,
        openapi_extra: Annotated[
            Optional[Dict[str, Any]],
            Doc(
                """
                Extra metadata to be included in the OpenAPI schema for this *path
                operation*.

                Read more about it in the
                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).
                """
            ),
        ] = None,
        generate_unique_id_function: Annotated[
            Callable[[routing.APIRoute], str],
            Doc(
                """
                Customize the function used to generate unique IDs for the *path
                operations* shown in the generated OpenAPI.

                This is particularly useful when automatically generating clients or
                SDKs for your API.

                Read more about it in the
                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).
                """
            ),
        ] = Default(generate_unique_id),
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add a *path operation* using an HTTP TRACE operation.

        ## Example

        ```python
        from fastapi import FastAPI

        app = FastAPI()

        @app.trace("/items/{item_id}")
        def trace_item(item_id: str):
            return None
        ```
        """
        return self.router.trace(
            path,
            response_model=response_model,
            status_code=status_code,
            tags=tags,
            dependencies=dependencies,
            summary=summary,
            description=description,
            response_description=response_description,
            responses=responses,
            deprecated=deprecated,
            operation_id=operation_id,
            response_model_include=response_model_include,
            response_model_exclude=response_model_exclude,
            response_model_by_alias=response_model_by_alias,
            response_model_exclude_unset=response_model_exclude_unset,
            response_model_exclude_defaults=response_model_exclude_defaults,
            response_model_exclude_none=response_model_exclude_none,
            include_in_schema=include_in_schema,
            response_class=response_class,
            name=name,
            callbacks=callbacks,
            openapi_extra=openapi_extra,
            generate_unique_id_function=generate_unique_id_function,
        )

    def websocket_route(
        self, path: str, name: Union[str, None] = None
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        def decorator(func: DecoratedCallable) -> DecoratedCallable:
            self.router.add_websocket_route(path, func, name=name)
            return func

        return decorator

    @deprecated(
        """
        on_event is deprecated, use lifespan event handlers instead.

        Read more about it in the
        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
        """
    )
    def on_event(
        self,
        event_type: Annotated[
            str,
            Doc(
                """
                The type of event. `startup` or `shutdown`.
                """
            ),
        ],
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add an event handler for the application.

        `on_event` is deprecated, use `lifespan` event handlers instead.

        Read more about it in the
        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/#alternative-events-deprecated).
        """
        return self.router.on_event(event_type)

    def middleware(
        self,
        middleware_type: Annotated[
            str,
            Doc(
                """
                The type of middleware. Currently only supports `http`.
                """
            ),
        ],
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add a middleware to the application.

        Read more about it in the
        [FastAPI docs for Middleware](https://fastapi.tiangolo.com/tutorial/middleware/).

        ## Example

        ```python
        import time
        from typing import Awaitable, Callable

        from fastapi import FastAPI, Request, Response

        app = FastAPI()


        @app.middleware("http")
        async def add_process_time_header(
            request: Request, call_next: Callable[[Request], Awaitable[Response]]
        ) -> Response:
            start_time = time.time()
            response = await call_next(request)
            process_time = time.time() - start_time
            response.headers["X-Process-Time"] = str(process_time)
            return response
        ```
        """

        def decorator(func: DecoratedCallable) -> DecoratedCallable:
            self.add_middleware(BaseHTTPMiddleware, dispatch=func)
            return func

        return decorator

    def exception_handler(
        self,
        exc_class_or_status_code: Annotated[
            Union[int, Type[Exception]],
            Doc(
                """
                The Exception class this would handle, or a status code.
                """
            ),
        ],
    ) -> Callable[[DecoratedCallable], DecoratedCallable]:
        """
        Add an exception handler to the app.

        Read more about it in the
        [FastAPI docs for Handling Errors](https://fastapi.tiangolo.com/tutorial/handling-errors/).

        ## Example

        ```python
        from fastapi import FastAPI, Request
        from fastapi.responses import JSONResponse


        class UnicornException(Exception):
            def __init__(self, name: str):
                self.name = name


        app = FastAPI()


        @app.exception_handler(UnicornException)
        async def unicorn_exception_handler(request: Request, exc: UnicornException):
            return JSONResponse(
                status_code=418,
                content={"message": f"Oops! {exc.name} did something. There goes a rainbow..."},
            )
        ```
        """

        def decorator(func: DecoratedCallable) -> DecoratedCallable:
            self.add_exception_handler(exc_class_or_status_code, func)
            return func

        return decorator


--- fastapi/background.py ---
from typing import Any, Callable

from starlette.background import BackgroundTasks as StarletteBackgroundTasks
from typing_extensions import Annotated, Doc, ParamSpec

P = ParamSpec("P")


class BackgroundTasks(StarletteBackgroundTasks):
    """
    A collection of background tasks that will be called after a response has been
    sent to the client.

    Read more about it in the
    [FastAPI docs for Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/).

    ## Example

    ```python
    from fastapi import BackgroundTasks, FastAPI

    app = FastAPI()


    def write_notification(email: str, message=""):
        with open("log.txt", mode="w") as email_file:
            content = f"notification for {email}: {message}"
            email_file.write(content)


    @app.post("/send-notification/{email}")
    async def send_notification(email: str, background_tasks: BackgroundTasks):
        background_tasks.add_task(write_notification, email, message="some notification")
        return {"message": "Notification sent in the background"}
    ```
    """

    def add_task(
        self,
        func: Annotated[
            Callable[P, Any],
            Doc(
                """
                The function to call after the response is sent.

                It can be a regular `def` function or an `async def` function.
                """
            ),
        ],
        *args: P.args,
        **kwargs: P.kwargs,
    ) -> None:
        """
        Add a function to be called in the background after the response is sent.

        Read more about it in the
        [FastAPI docs for Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/).
        """
        return super().add_task(func, *args, **kwargs)


--- fastapi/cli.py ---
try:
    from fastapi_cli.cli import main as cli_main

except ImportError:  # pragma: no cover
    cli_main = None  # type: ignore


def main() -> None:
    if not cli_main:  # type: ignore[truthy-function]
        message = 'To use the fastapi command, please install "fastapi[standard]":\n\n\tpip install "fastapi[standard]"\n'
        print(message)
        raise RuntimeError(message)  # noqa: B904
    cli_main()


--- fastapi/concurrency.py ---
from contextlib import asynccontextmanager as asynccontextmanager
from typing import AsyncGenerator, ContextManager, TypeVar

import anyio.to_thread
from anyio import CapacityLimiter
from starlette.concurrency import iterate_in_threadpool as iterate_in_threadpool  # noqa
from starlette.concurrency import run_in_threadpool as run_in_threadpool  # noqa
from starlette.concurrency import (  # noqa
    run_until_first_complete as run_until_first_complete,
)

_T = TypeVar("_T")


@asynccontextmanager
async def contextmanager_in_threadpool(
    cm: ContextManager[_T],
) -> AsyncGenerator[_T, None]:
    # blocking __exit__ from running waiting on a free thread
    # can create race conditions/deadlocks if the context manager itself
    # has its own internal pool (e.g. a database connection pool)
    # to avoid this we let __exit__ run without a capacity limit
    # since we're creating a new limiter for each call, any non-zero limit
    # works (1 is arbitrary)
    exit_limiter = CapacityLimiter(1)
    try:
        yield await run_in_threadpool(cm.__enter__)
    except Exception as e:
        ok = bool(
            await anyio.to_thread.run_sync(
                cm.__exit__, type(e), e, e.__traceback__, limiter=exit_limiter
            )
        )
        if not ok:
            raise e
    else:
        await anyio.to_thread.run_sync(
            cm.__exit__, None, None, None, limiter=exit_limiter
        )


--- fastapi/datastructures.py ---
from typing import (
    Any,
    BinaryIO,
    Callable,
    Dict,
    Iterable,
    Optional,
    Type,
    TypeVar,
    cast,
)

from fastapi._compat import (
    CoreSchema,
    GetJsonSchemaHandler,
    JsonSchemaValue,
)
from starlette.datastructures import URL as URL  # noqa: F401
from starlette.datastructures import Address as Address  # noqa: F401
from starlette.datastructures import FormData as FormData  # noqa: F401
from starlette.datastructures import Headers as Headers  # noqa: F401
from starlette.datastructures import QueryParams as QueryParams  # noqa: F401
from starlette.datastructures import State as State  # noqa: F401
from starlette.datastructures import UploadFile as StarletteUploadFile
from typing_extensions import Annotated, Doc


class UploadFile(StarletteUploadFile):
    """
    A file uploaded in a request.

    Define it as a *path operation function* (or dependency) parameter.

    If you are using a regular `def` function, you can use the `upload_file.file`
    attribute to access the raw standard Python file (blocking, not async), useful and
    needed for non-async code.

    Read more about it in the
    [FastAPI docs for Request Files](https://fastapi.tiangolo.com/tutorial/request-files/).

    ## Example

    ```python
    from typing import Annotated

    from fastapi import FastAPI, File, UploadFile

    app = FastAPI()


    @app.post("/files/")
    async def create_file(file: Annotated[bytes, File()]):
        return {"file_size": len(file)}


    @app.post("/uploadfile/")
    async def create_upload_file(file: UploadFile):
        return {"filename": file.filename}
    ```
    """

    file: Annotated[
        BinaryIO,
        Doc("The standard Python file object (non-async)."),
    ]
    filename: Annotated[Optional[str], Doc("The original file name.")]
    size: Annotated[Optional[int], Doc("The size of the file in bytes.")]
    headers: Annotated[Headers, Doc("The headers of the request.")]
    content_type: Annotated[
        Optional[str], Doc("The content type of the request, from the headers.")
    ]

    async def write(
        self,
        data: Annotated[
            bytes,
            Doc(
                """
                The bytes to write to the file.
                """
            ),
        ],
    ) -> None:
        """
        Write some bytes to the file.

        You normally wouldn't use this from a file you read in a request.

        To be awaitable, compatible with async, this is run in threadpool.
        """
        return await super().write(data)

    async def read(
        self,
        size: Annotated[
            int,
            Doc(
                """
                The number of bytes to read from the file.
                """
            ),
        ] = -1,
    ) -> bytes:
        """
        Read some bytes from the file.

        To be awaitable, compatible with async, this is run in threadpool.
        """
        return await super().read(size)

    async def seek(
        self,
        offset: Annotated[
            int,
            Doc(
                """
                The position in bytes to seek to in the file.
                """
            ),
        ],
    ) -> None:
        """
        Move to a position in the file.

        Any next read or write will be done from that position.

        To be awaitable, compatible with async, this is run in threadpool.
        """
        return await super().seek(offset)

    async def close(self) -> None:
        """
        Close the file.

        To be awaitable, compatible with async, this is run in threadpool.
        """
        return await super().close()

    @classmethod
    def __get_validators__(cls: Type["UploadFile"]) -> Iterable[Callable[..., Any]]:
        yield cls.validate

    @classmethod
    def validate(cls: Type["UploadFile"], v: Any) -> Any:
        if not isinstance(v, StarletteUploadFile):
            raise ValueError(f"Expected UploadFile, received: {type(v)}")
        return v

    @classmethod
    def _validate(cls, __input_value: Any, _: Any) -> "UploadFile":
        if not isinstance(__input_value, StarletteUploadFile):
            raise ValueError(f"Expected UploadFile, received: {type(__input_value)}")
        return cast(UploadFile, __input_value)

    # TODO: remove when deprecating Pydantic v1
    @classmethod
    def __modify_schema__(cls, field_schema: Dict[str, Any]) -> None:
        field_schema.update({"type": "string", "format": "binary"})

    @classmethod
    def __get_pydantic_json_schema__(
        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler
    ) -> JsonSchemaValue:
        return {"type": "string", "format": "binary"}

    @classmethod
    def __get_pydantic_core_schema__(
        cls, source: Type[Any], handler: Callable[[Any], CoreSchema]
    ) -> CoreSchema:
        from ._compat.v2 import with_info_plain_validator_function

        return with_info_plain_validator_function(cls._validate)


class DefaultPlaceholder:
    """
    You shouldn't use this class directly.

    It's used internally to recognize when a default value has been overwritten, even
    if the overridden default value was truthy.
    """

    def __init__(self, value: Any):
        self.value = value

    def __bool__(self) -> bool:
        return bool(self.value)

    def __eq__(self, o: object) -> bool:
        return isinstance(o, DefaultPlaceholder) and o.value == self.value


DefaultType = TypeVar("DefaultType")


def Default(value: DefaultType) -> DefaultType:
    """
    You shouldn't use this function directly.

    It's used internally to recognize when a default value has been overwritten, even
    if the overridden default value was truthy.
    """
    return DefaultPlaceholder(value)  # type: ignore


--- fastapi/encoders.py ---
import dataclasses
import datetime
from collections import defaultdict, deque
from decimal import Decimal
from enum import Enum
from ipaddress import (
    IPv4Address,
    IPv4Interface,
    IPv4Network,
    IPv6Address,
    IPv6Interface,
    IPv6Network,
)
from pathlib import Path, PurePath
from re import Pattern
from types import GeneratorType
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
from uuid import UUID

from fastapi._compat import v1
from fastapi.types import IncEx
from pydantic import BaseModel
from pydantic.color import Color
from pydantic.networks import AnyUrl, NameEmail
from pydantic.types import SecretBytes, SecretStr
from typing_extensions import Annotated, Doc

from ._compat import Url, _is_undefined, _model_dump


# Taken from Pydantic v1 as is
def isoformat(o: Union[datetime.date, datetime.time]) -> str:
    return o.isoformat()


# Taken from Pydantic v1 as is
# TODO: pv2 should this return strings instead?
def decimal_encoder(dec_value: Decimal) -> Union[int, float]:
    """
    Encodes a Decimal as int of there's no exponent, otherwise float

    This is useful when we use ConstrainedDecimal to represent Numeric(x,0)
    where a integer (but not int typed) is used. Encoding this as a float
    results in failed round-tripping between encode and parse.
    Our Id type is a prime example of this.

    >>> decimal_encoder(Decimal("1.0"))
    1.0

    >>> decimal_encoder(Decimal("1"))
    1
    """
    if dec_value.as_tuple().exponent >= 0:  # type: ignore[operator]
        return int(dec_value)
    else:
        return float(dec_value)


ENCODERS_BY_TYPE: Dict[Type[Any], Callable[[Any], Any]] = {
    bytes: lambda o: o.decode(),
    Color: str,
    v1.Color: str,
    datetime.date: isoformat,
    datetime.datetime: isoformat,
    datetime.time: isoformat,
    datetime.timedelta: lambda td: td.total_seconds(),
    Decimal: decimal_encoder,
    Enum: lambda o: o.value,
    frozenset: list,
    deque: list,
    GeneratorType: list,
    IPv4Address: str,
    IPv4Interface: str,
    IPv4Network: str,
    IPv6Address: str,
    IPv6Interface: str,
    IPv6Network: str,
    NameEmail: str,
    v1.NameEmail: str,
    Path: str,
    Pattern: lambda o: o.pattern,
    SecretBytes: str,
    v1.SecretBytes: str,
    SecretStr: str,
    v1.SecretStr: str,
    set: list,
    UUID: str,
    Url: str,
    v1.Url: str,
    AnyUrl: str,
    v1.AnyUrl: str,
}


def generate_encoders_by_class_tuples(
    type_encoder_map: Dict[Any, Callable[[Any], Any]],
) -> Dict[Callable[[Any], Any], Tuple[Any, ...]]:
    encoders_by_class_tuples: Dict[Callable[[Any], Any], Tuple[Any, ...]] = defaultdict(
        tuple
    )
    for type_, encoder in type_encoder_map.items():
        encoders_by_class_tuples[encoder] += (type_,)
    return encoders_by_class_tuples


encoders_by_class_tuples = generate_encoders_by_class_tuples(ENCODERS_BY_TYPE)


def jsonable_encoder(
    obj: Annotated[
        Any,
        Doc(
            """
            The input object to convert to JSON.
            """
        ),
    ],
    include: Annotated[
        Optional[IncEx],
        Doc(
            """
            Pydantic's `include` parameter, passed to Pydantic models to set the
            fields to include.
            """
        ),
    ] = None,
    exclude: Annotated[
        Optional[IncEx],
        Doc(
            """
            Pydantic's `exclude` parameter, passed to Pydantic models to set the
            fields to exclude.
            """
        ),
    ] = None,
    by_alias: Annotated[
        bool,
        Doc(
            """
            Pydantic's `by_alias` parameter, passed to Pydantic models to define if
            the output should use the alias names (when provided) or the Python
            attribute names. In an API, if you set an alias, it's probably because you
            want to use it in the result, so you probably want to leave this set to
            `True`.
            """
        ),
    ] = True,
    exclude_unset: Annotated[
        bool,
        Doc(
            """
            Pydantic's `exclude_unset` parameter, passed to Pydantic models to define
            if it should exclude from the output the fields that were not explicitly
            set (and that only had their default values).
            """
        ),
    ] = False,
    exclude_defaults: Annotated[
        bool,
        Doc(
            """
            Pydantic's `exclude_defaults` parameter, passed to Pydantic models to define
            if it should exclude from the output the fields that had the same default
            value, even when they were explicitly set.
            """
        ),
    ] = False,
    exclude_none: Annotated[
        bool,
        Doc(
            """
            Pydantic's `exclude_none` parameter, passed to Pydantic models to define
            if it should exclude from the output any fields that have a `None` value.
            """
        ),
    ] = False,
    custom_encoder: Annotated[
        Optional[Dict[Any, Callable[[Any], Any]]],
        Doc(
            """
            Pydantic's `custom_encoder` parameter, passed to Pydantic models to define
            a custom encoder.
            """
        ),
    ] = None,
    sqlalchemy_safe: Annotated[
        bool,
        Doc(
            """
            Exclude from the output any fields that start with the name `_sa`.

            This is mainly a hack for compatibility with SQLAlchemy objects, they
            store internal SQLAlchemy-specific state in attributes named with `_sa`,
            and those objects can't (and shouldn't be) serialized to JSON.
            """
        ),
    ] = True,
) -> Any:
    """
    Convert any object to something that can be encoded in JSON.

    This is used internally by FastAPI to make sure anything you return can be
    encoded as JSON before it is sent to the client.

    You can also use it yourself, for example to convert objects before saving them
    in a database that supports only JSON.

    Read more about it in the
    [FastAPI docs for JSON Compatible Encoder](https://fastapi.tiangolo.com/tutorial/encoder/).
    """
    custom_encoder = custom_encoder or {}
    if custom_encoder:
        if type(obj) in custom_encoder:
            return custom_encoder[type(obj)](obj)
        else:
            for encoder_type, encoder_instance in custom_encoder.items():
                if isinstance(obj, encoder_type):
                    return encoder_instance(obj)
    if include is not None and not isinstance(include, (set, dict)):
        include = set(include)
    if exclude is not None and not isinstance(exclude, (set, dict)):
        exclude = set(exclude)
    if isinstance(obj, (BaseModel, v1.BaseModel)):
        # TODO: remove when deprecating Pydantic v1
        encoders: Dict[Any, Any] = {}
        if isinstance(obj, v1.BaseModel):
            encoders = getattr(obj.__config__, "json_encoders", {})  # type: ignore[attr-defined]
            if custom_encoder:
                encoders = {**encoders, **custom_encoder}
        obj_dict = _model_dump(
            obj,
            mode="json",
            include=include,
            exclude=exclude,
            by_alias=by_alias,
            exclude_unset=exclude_unset,
            exclude_none=exclude_none,
            exclude_defaults=exclude_defaults,
        )
        if "__root__" in obj_dict:
            obj_dict = obj_dict["__root__"]
        return jsonable_encoder(
            obj_dict,
            exclude_none=exclude_none,
            exclude_defaults=exclude_defaults,
            # TODO: remove when deprecating Pydantic v1
            custom_encoder=encoders,
            sqlalchemy_safe=sqlalchemy_safe,
        )
    if dataclasses.is_dataclass(obj):
        assert not isinstance(obj, type)
        obj_dict = dataclasses.asdict(obj)
        return jsonable_encoder(
            obj_dict,
            include=include,
            exclude=exclude,
            by_alias=by_alias,
            exclude_unset=exclude_unset,
            exclude_defaults=exclude_defaults,
            exclude_none=exclude_none,
            custom_encoder=custom_encoder,
            sqlalchemy_safe=sqlalchemy_safe,
        )
    if isinstance(obj, Enum):
        return obj.value
    if isinstance(obj, PurePath):
        return str(obj)
    if isinstance(obj, (str, int, float, type(None))):
        return obj
    if _is_undefined(obj):
        return None
    if isinstance(obj, dict):
        encoded_dict = {}
        allowed_keys = set(obj.keys())
        if include is not None:
            allowed_keys &= set(include)
        if exclude is not None:
            allowed_keys -= set(exclude)
        for key, value in obj.items():
            if (
                (
                    not sqlalchemy_safe
                    or (not isinstance(key, str))
                    or (not key.startswith("_sa"))
                )
                and (value is not None or not exclude_none)
                and key in allowed_keys
            ):
                encoded_key = jsonable_encoder(
                    key,
                    by_alias=by_alias,
                    exclude_unset=exclude_unset,
                    exclude_none=exclude_none,
                    custom_encoder=custom_encoder,
                    sqlalchemy_safe=sqlalchemy_safe,
                )
                encoded_value = jsonable_encoder(
                    value,
                    by_alias=by_alias,
                    exclude_unset=exclude_unset,
                    exclude_none=exclude_none,
                    custom_encoder=custom_encoder,
                    sqlalchemy_safe=sqlalchemy_safe,
                )
                encoded_dict[encoded_key] = encoded_value
        return encoded_dict
    if isinstance(obj, (list, set, frozenset, GeneratorType, tuple, deque)):
        encoded_list = []
        for item in obj:
            encoded_list.append(
                jsonable_encoder(
                    item,
                    include=include,
                    exclude=exclude,
                    by_alias=by_alias,
                    exclude_unset=exclude_unset,
                    exclude_defaults=exclude_defaults,
                    exclude_none=exclude_none,
                    custom_encoder=custom_encoder,
                    sqlalchemy_safe=sqlalchemy_safe,
                )
            )
        return encoded_list

    if type(obj) in ENCODERS_BY_TYPE:
        return ENCODERS_BY_TYPE[type(obj)](obj)
    for encoder, classes_tuple in encoders_by_class_tuples.items():
        if isinstance(obj, classes_tuple):
            return encoder(obj)

    try:
        data = dict(obj)
    except Exception as e:
        errors: List[Exception] = []
        errors.append(e)
        try:
            data = vars(obj)
        except Exception as e:
            errors.append(e)
            raise ValueError(errors) from e
    return jsonable_encoder(
        data,
        include=include,
        exclude=exclude,
        by_alias=by_alias,
        exclude_unset=exclude_unset,
        exclude_defaults=exclude_defaults,
        exclude_none=exclude_none,
        custom_encoder=custom_encoder,
        sqlalchemy_safe=sqlalchemy_safe,
    )


--- fastapi/exception_handlers.py ---
from fastapi.encoders import jsonable_encoder
from fastapi.exceptions import RequestValidationError, WebSocketRequestValidationError
from fastapi.utils import is_body_allowed_for_status_code
from fastapi.websockets import WebSocket
from starlette.exceptions import HTTPException
from starlette.requests import Request
from starlette.responses import JSONResponse, Response
from starlette.status import WS_1008_POLICY_VIOLATION


async def http_exception_handler(request: Request, exc: HTTPException) -> Response:
    headers = getattr(exc, "headers", None)
    if not is_body_allowed_for_status_code(exc.status_code):
        return Response(status_code=exc.status_code, headers=headers)
    return JSONResponse(
        {"detail": exc.detail}, status_code=exc.status_code, headers=headers
    )


async def request_validation_exception_handler(
    request: Request, exc: RequestValidationError
) -> JSONResponse:
    return JSONResponse(
        status_code=422,
        content={"detail": jsonable_encoder(exc.errors())},
    )


async def websocket_request_validation_exception_handler(
    websocket: WebSocket, exc: WebSocketRequestValidationError
) -> None:
    await websocket.close(
        code=WS_1008_POLICY_VIOLATION, reason=jsonable_encoder(exc.errors())
    )


--- fastapi/exceptions.py ---
from typing import Any, Dict, Optional, Sequence, Type, Union

from pydantic import BaseModel, create_model
from starlette.exceptions import HTTPException as StarletteHTTPException
from starlette.exceptions import WebSocketException as StarletteWebSocketException
from typing_extensions import Annotated, Doc


class HTTPException(StarletteHTTPException):
    """
    An HTTP exception you can raise in your own code to show errors to the client.

    This is for client errors, invalid authentication, invalid data, etc. Not for server
    errors in your code.

    Read more about it in the
    [FastAPI docs for Handling Errors](https://fastapi.tiangolo.com/tutorial/handling-errors/).

    ## Example

    ```python
    from fastapi import FastAPI, HTTPException

    app = FastAPI()

    items = {"foo": "The Foo Wrestlers"}


    @app.get("/items/{item_id}")
    async def read_item(item_id: str):
        if item_id not in items:
            raise HTTPException(status_code=404, detail="Item not found")
        return {"item": items[item_id]}
    ```
    """

    def __init__(
        self,
        status_code: Annotated[
            int,
            Doc(
                """
                HTTP status code to send to the client.
                """
            ),
        ],
        detail: Annotated[
            Any,
            Doc(
                """
                Any data to be sent to the client in the `detail` key of the JSON
                response.
                """
            ),
        ] = None,
        headers: Annotated[
            Optional[Dict[str, str]],
            Doc(
                """
                Any headers to send to the client in the response.
                """
            ),
        ] = None,
    ) -> None:
        super().__init__(status_code=status_code, detail=detail, headers=headers)


class WebSocketException(StarletteWebSocketException):
    """
    A WebSocket exception you can raise in your own code to show errors to the client.

    This is for client errors, invalid authentication, invalid data, etc. Not for server
    errors in your code.

    Read more about it in the
    [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).

    ## Example

    ```python
    from typing import Annotated

    from fastapi import (
        Cookie,
        FastAPI,
        WebSocket,
        WebSocketException,
        status,
    )

    app = FastAPI()

    @app.websocket("/items/{item_id}/ws")
    async def websocket_endpoint(
        *,
        websocket: WebSocket,
        session: Annotated[str | None, Cookie()] = None,
        item_id: str,
    ):
        if session is None:
            raise WebSocketException(code=status.WS_1008_POLICY_VIOLATION)
        await websocket.accept()
        while True:
            data = await websocket.receive_text()
            await websocket.send_text(f"Session cookie is: {session}")
            await websocket.send_text(f"Message text was: {data}, for item ID: {item_id}")
    ```
    """

    def __init__(
        self,
        code: Annotated[
            int,
            Doc(
                """
                A closing code from the
                [valid codes defined in the specification](https://datatracker.ietf.org/doc/html/rfc6455#section-7.4.1).
                """
            ),
        ],
        reason: Annotated[
            Union[str, None],
            Doc(
                """
                The reason to close the WebSocket connection.

                It is UTF-8-encoded data. The interpretation of the reason is up to the
                application, it is not specified by the WebSocket specification.

                It could contain text that could be human-readable or interpretable
                by the client code, etc.
                """
            ),
        ] = None,
    ) -> None:
        super().__init__(code=code, reason=reason)


RequestErrorModel: Type[BaseModel] = create_model("Request")
WebSocketErrorModel: Type[BaseModel] = create_model("WebSocket")


class FastAPIError(RuntimeError):
    """
    A generic, FastAPI-specific error.
    """


class ValidationException(Exception):
    def __init__(self, errors: Sequence[Any]) -> None:
        self._errors = errors

    def errors(self) -> Sequence[Any]:
        return self._errors


class RequestValidationError(ValidationException):
    def __init__(self, errors: Sequence[Any], *, body: Any = None) -> None:
        super().__init__(errors)
        self.body = body


class WebSocketRequestValidationError(ValidationException):
    pass


class ResponseValidationError(ValidationException):
    def __init__(self, errors: Sequence[Any], *, body: Any = None) -> None:
        super().__init__(errors)
        self.body = body

    def __str__(self) -> str:
        message = f"{len(self._errors)} validation errors:\n"
        for err in self._errors:
            message += f"  {err}\n"
        return message


--- fastapi/__init__.py ---
"""FastAPI framework, high performance, easy to learn, fast to code, ready for production"""

__version__ = "0.119.0"

from starlette import status as status

from .applications import FastAPI as FastAPI
from .background import BackgroundTasks as BackgroundTasks
from .datastructures import UploadFile as UploadFile
from .exceptions import HTTPException as HTTPException
from .exceptions import WebSocketException as WebSocketException
from .param_functions import Body as Body
from .param_functions import Cookie as Cookie
from .param_functions import Depends as Depends
from .param_functions import File as File
from .param_functions import Form as Form
from .param_functions import Header as Header
from .param_functions import Path as Path
from .param_functions import Query as Query
from .param_functions import Security as Security
from .requests import Request as Request
from .responses import Response as Response
from .routing import APIRouter as APIRouter
from .websockets import WebSocket as WebSocket
from .websockets import WebSocketDisconnect as WebSocketDisconnect


--- fastapi/logger.py ---
import logging

logger = logging.getLogger("fastapi")


--- CONTRIBUTING.md ---
Please read the [Development - Contributing](https://fastapi.tiangolo.com/contributing/) guidelines in the documentation site.


--- SECURITY.md ---
# Security Policy

Security is very important for FastAPI and its community. 🔒

Learn more about it below. 👇

## Versions

The latest version of FastAPI is supported.

You are encouraged to [write tests](https://fastapi.tiangolo.com/tutorial/testing/) for your application and update your FastAPI version frequently after ensuring that your tests are passing. This way you will benefit from the latest features, bug fixes, and **security fixes**.

You can learn more about [FastAPI versions and how to pin and upgrade them](https://fastapi.tiangolo.com/deployment/versions/) for your project in the docs.

## Reporting a Vulnerability

If you think you found a vulnerability, and even if you are not sure about it, please report it right away by sending an email to: security@tiangolo.com. Please try to be as explicit as possible, describing all the steps and example code to reproduce the security issue.

I (the author, [@tiangolo](https://x.com/tiangolo)) will review it thoroughly and get back to you.

## Public Discussions

Please restrain from publicly discussing a potential security vulnerability. 🙊

It's better to discuss privately and try to find a solution first, to limit the potential impact as much as possible.

---

Thanks for your help!

The FastAPI community and I thank you for that. 🙇


--- tests/test_dependency_security_overrides.py ---
from typing import List, Tuple

from fastapi import Depends, FastAPI, Security
from fastapi.security import SecurityScopes
from fastapi.testclient import TestClient

app = FastAPI()


def get_user(required_scopes: SecurityScopes):
    return "john", required_scopes.scopes


def get_user_override(required_scopes: SecurityScopes):
    return "alice", required_scopes.scopes


def get_data():
    return [1, 2, 3]


def get_data_override():
    return [3, 4, 5]


@app.get("/user")
def read_user(
    user_data: Tuple[str, List[str]] = Security(get_user, scopes=["foo", "bar"]),
    data: List[int] = Depends(get_data),
):
    return {"user": user_data[0], "scopes": user_data[1], "data": data}


client = TestClient(app)


def test_normal():
    response = client.get("/user")
    assert response.json() == {
        "user": "john",
        "scopes": ["foo", "bar"],
        "data": [1, 2, 3],
    }


def test_override_data():
    app.dependency_overrides[get_data] = get_data_override
    response = client.get("/user")
    assert response.json() == {
        "user": "john",
        "scopes": ["foo", "bar"],
        "data": [3, 4, 5],
    }
    app.dependency_overrides = {}


def test_override_security():
    app.dependency_overrides[get_user] = get_user_override
    response = client.get("/user")
    assert response.json() == {
        "user": "alice",
        "scopes": ["foo", "bar"],
        "data": [1, 2, 3],
    }
    app.dependency_overrides = {}


--- tests/test_security_http_base.py ---
from fastapi import FastAPI, Security
from fastapi.security.http import HTTPAuthorizationCredentials, HTTPBase
from fastapi.testclient import TestClient

app = FastAPI()

security = HTTPBase(scheme="Other")


@app.get("/users/me")
def read_current_user(credentials: HTTPAuthorizationCredentials = Security(security)):
    return {"scheme": credentials.scheme, "credentials": credentials.credentials}


client = TestClient(app)


def test_security_http_base():
    response = client.get("/users/me", headers={"Authorization": "Other foobar"})
    assert response.status_code == 200, response.text
    assert response.json() == {"scheme": "Other", "credentials": "foobar"}


def test_security_http_base_no_credentials():
    response = client.get("/users/me")
    assert response.status_code == 403, response.text
    assert response.json() == {"detail": "Not authenticated"}


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/users/me": {
                "get": {
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        }
                    },
                    "summary": "Read Current User",
                    "operationId": "read_current_user_users_me_get",
                    "security": [{"HTTPBase": []}],
                }
            }
        },
        "components": {
            "securitySchemes": {"HTTPBase": {"type": "http", "scheme": "Other"}}
        },
    }


--- tests/test_security_http_base_description.py ---
from fastapi import FastAPI, Security
from fastapi.security.http import HTTPAuthorizationCredentials, HTTPBase
from fastapi.testclient import TestClient

app = FastAPI()

security = HTTPBase(scheme="Other", description="Other Security Scheme")


@app.get("/users/me")
def read_current_user(credentials: HTTPAuthorizationCredentials = Security(security)):
    return {"scheme": credentials.scheme, "credentials": credentials.credentials}


client = TestClient(app)


def test_security_http_base():
    response = client.get("/users/me", headers={"Authorization": "Other foobar"})
    assert response.status_code == 200, response.text
    assert response.json() == {"scheme": "Other", "credentials": "foobar"}


def test_security_http_base_no_credentials():
    response = client.get("/users/me")
    assert response.status_code == 403, response.text
    assert response.json() == {"detail": "Not authenticated"}


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/users/me": {
                "get": {
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        }
                    },
                    "summary": "Read Current User",
                    "operationId": "read_current_user_users_me_get",
                    "security": [{"HTTPBase": []}],
                }
            }
        },
        "components": {
            "securitySchemes": {
                "HTTPBase": {
                    "type": "http",
                    "scheme": "Other",
                    "description": "Other Security Scheme",
                }
            }
        },
    }


--- tests/test_security_http_base_optional.py ---
from typing import Optional

from fastapi import FastAPI, Security
from fastapi.security.http import HTTPAuthorizationCredentials, HTTPBase
from fastapi.testclient import TestClient

app = FastAPI()

security = HTTPBase(scheme="Other", auto_error=False)


@app.get("/users/me")
def read_current_user(
    credentials: Optional[HTTPAuthorizationCredentials] = Security(security),
):
    if credentials is None:
        return {"msg": "Create an account first"}
    return {"scheme": credentials.scheme, "credentials": credentials.credentials}


client = TestClient(app)


def test_security_http_base():
    response = client.get("/users/me", headers={"Authorization": "Other foobar"})
    assert response.status_code == 200, response.text
    assert response.json() == {"scheme": "Other", "credentials": "foobar"}


def test_security_http_base_no_credentials():
    response = client.get("/users/me")
    assert response.status_code == 200, response.text
    assert response.json() == {"msg": "Create an account first"}


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/users/me": {
                "get": {
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        }
                    },
                    "summary": "Read Current User",
                    "operationId": "read_current_user_users_me_get",
                    "security": [{"HTTPBase": []}],
                }
            }
        },
        "components": {
            "securitySchemes": {"HTTPBase": {"type": "http", "scheme": "Other"}}
        },
    }


--- tests/test_security_http_basic_optional.py ---
from base64 import b64encode
from typing import Optional

from fastapi import FastAPI, Security
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from fastapi.testclient import TestClient

app = FastAPI()

security = HTTPBasic(auto_error=False)


@app.get("/users/me")
def read_current_user(credentials: Optional[HTTPBasicCredentials] = Security(security)):
    if credentials is None:
        return {"msg": "Create an account first"}
    return {"username": credentials.username, "password": credentials.password}


client = TestClient(app)


def test_security_http_basic():
    response = client.get("/users/me", auth=("john", "secret"))
    assert response.status_code == 200, response.text
    assert response.json() == {"username": "john", "password": "secret"}


def test_security_http_basic_no_credentials():
    response = client.get("/users/me")
    assert response.status_code == 200, response.text
    assert response.json() == {"msg": "Create an account first"}


def test_security_http_basic_invalid_credentials():
    response = client.get(
        "/users/me", headers={"Authorization": "Basic notabase64token"}
    )
    assert response.status_code == 401, response.text
    assert response.headers["WWW-Authenticate"] == "Basic"
    assert response.json() == {"detail": "Invalid authentication credentials"}


def test_security_http_basic_non_basic_credentials():
    payload = b64encode(b"johnsecret").decode("ascii")
    auth_header = f"Basic {payload}"
    response = client.get("/users/me", headers={"Authorization": auth_header})
    assert response.status_code == 401, response.text
    assert response.headers["WWW-Authenticate"] == "Basic"
    assert response.json() == {"detail": "Invalid authentication credentials"}


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/users/me": {
                "get": {
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        }
                    },
                    "summary": "Read Current User",
                    "operationId": "read_current_user_users_me_get",
                    "security": [{"HTTPBasic": []}],
                }
            }
        },
        "components": {
            "securitySchemes": {"HTTPBasic": {"type": "http", "scheme": "basic"}}
        },
    }


--- tests/test_security_http_basic_realm.py ---
from base64 import b64encode

from fastapi import FastAPI, Security
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from fastapi.testclient import TestClient

app = FastAPI()

security = HTTPBasic(realm="simple")


@app.get("/users/me")
def read_current_user(credentials: HTTPBasicCredentials = Security(security)):
    return {"username": credentials.username, "password": credentials.password}


client = TestClient(app)


def test_security_http_basic():
    response = client.get("/users/me", auth=("john", "secret"))
    assert response.status_code == 200, response.text
    assert response.json() == {"username": "john", "password": "secret"}


def test_security_http_basic_no_credentials():
    response = client.get("/users/me")
    assert response.json() == {"detail": "Not authenticated"}
    assert response.status_code == 401, response.text
    assert response.headers["WWW-Authenticate"] == 'Basic realm="simple"'


def test_security_http_basic_invalid_credentials():
    response = client.get(
        "/users/me", headers={"Authorization": "Basic notabase64token"}
    )
    assert response.status_code == 401, response.text
    assert response.headers["WWW-Authenticate"] == 'Basic realm="simple"'
    assert response.json() == {"detail": "Invalid authentication credentials"}


def test_security_http_basic_non_basic_credentials():
    payload = b64encode(b"johnsecret").decode("ascii")
    auth_header = f"Basic {payload}"
    response = client.get("/users/me", headers={"Authorization": auth_header})
    assert response.status_code == 401, response.text
    assert response.headers["WWW-Authenticate"] == 'Basic realm="simple"'
    assert response.json() == {"detail": "Invalid authentication credentials"}


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/users/me": {
                "get": {
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        }
                    },
                    "summary": "Read Current User",
                    "operationId": "read_current_user_users_me_get",
                    "security": [{"HTTPBasic": []}],
                }
            }
        },
        "components": {
            "securitySchemes": {"HTTPBasic": {"type": "http", "scheme": "basic"}}
        },
    }


--- tests/test_security_http_basic_realm_description.py ---
from base64 import b64encode

from fastapi import FastAPI, Security
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from fastapi.testclient import TestClient

app = FastAPI()

security = HTTPBasic(realm="simple", description="HTTPBasic scheme")


@app.get("/users/me")
def read_current_user(credentials: HTTPBasicCredentials = Security(security)):
    return {"username": credentials.username, "password": credentials.password}


client = TestClient(app)


def test_security_http_basic():
    response = client.get("/users/me", auth=("john", "secret"))
    assert response.status_code == 200, response.text
    assert response.json() == {"username": "john", "password": "secret"}


def test_security_http_basic_no_credentials():
    response = client.get("/users/me")
    assert response.json() == {"detail": "Not authenticated"}
    assert response.status_code == 401, response.text
    assert response.headers["WWW-Authenticate"] == 'Basic realm="simple"'


def test_security_http_basic_invalid_credentials():
    response = client.get(
        "/users/me", headers={"Authorization": "Basic notabase64token"}
    )
    assert response.status_code == 401, response.text
    assert response.headers["WWW-Authenticate"] == 'Basic realm="simple"'
    assert response.json() == {"detail": "Invalid authentication credentials"}


def test_security_http_basic_non_basic_credentials():
    payload = b64encode(b"johnsecret").decode("ascii")
    auth_header = f"Basic {payload}"
    response = client.get("/users/me", headers={"Authorization": auth_header})
    assert response.status_code == 401, response.text
    assert response.headers["WWW-Authenticate"] == 'Basic realm="simple"'
    assert response.json() == {"detail": "Invalid authentication credentials"}


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/users/me": {
                "get": {
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        }
                    },
                    "summary": "Read Current User",
                    "operationId": "read_current_user_users_me_get",
                    "security": [{"HTTPBasic": []}],
                }
            }
        },
        "components": {
            "securitySchemes": {
                "HTTPBasic": {
                    "type": "http",
                    "scheme": "basic",
                    "description": "HTTPBasic scheme",
                }
            }
        },
    }


--- tests/test_security_http_bearer.py ---
from fastapi import FastAPI, Security
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from fastapi.testclient import TestClient

app = FastAPI()

security = HTTPBearer()


@app.get("/users/me")
def read_current_user(credentials: HTTPAuthorizationCredentials = Security(security)):
    return {"scheme": credentials.scheme, "credentials": credentials.credentials}


client = TestClient(app)


def test_security_http_bearer():
    response = client.get("/users/me", headers={"Authorization": "Bearer foobar"})
    assert response.status_code == 200, response.text
    assert response.json() == {"scheme": "Bearer", "credentials": "foobar"}


def test_security_http_bearer_no_credentials():
    response = client.get("/users/me")
    assert response.status_code == 403, response.text
    assert response.json() == {"detail": "Not authenticated"}


def test_security_http_bearer_incorrect_scheme_credentials():
    response = client.get("/users/me", headers={"Authorization": "Basic notreally"})
    assert response.status_code == 403, response.text
    assert response.json() == {"detail": "Invalid authentication credentials"}


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/users/me": {
                "get": {
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        }
                    },
                    "summary": "Read Current User",
                    "operationId": "read_current_user_users_me_get",
                    "security": [{"HTTPBearer": []}],
                }
            }
        },
        "components": {
            "securitySchemes": {"HTTPBearer": {"type": "http", "scheme": "bearer"}}
        },
    }


--- README.md ---
<p align="center">
  <a href="https://fastapi.tiangolo.com"><img src="https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png" alt="FastAPI"></a>
</p>
<p align="center">
    <em>FastAPI framework, high performance, easy to learn, fast to code, ready for production</em>
</p>
<p align="center">
<a href="https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster" target="_blank">
    <img src="https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&branch=master" alt="Test">
</a>
<a href="https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi" target="_blank">
    <img src="https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg" alt="Coverage">
</a>
<a href="https://pypi.org/project/fastapi" target="_blank">
    <img src="https://img.shields.io/pypi/v/fastapi?color=%2334D058&label=pypi%20package" alt="Package version">
</a>
<a href="https://pypi.org/project/fastapi" target="_blank">
    <img src="https://img.shields.io/pypi/pyversions/fastapi.svg?color=%2334D058" alt="Supported Python versions">
</a>
</p>

---

**Documentation**: <a href="https://fastapi.tiangolo.com" target="_blank">https://fastapi.tiangolo.com</a>

**Source Code**: <a href="https://github.com/fastapi/fastapi" target="_blank">https://github.com/fastapi/fastapi</a>

---

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.

The key features are:

* **Fast**: Very high performance, on par with **NodeJS** and **Go** (thanks to Starlette and Pydantic). [One of the fastest Python frameworks available](#performance).
* **Fast to code**: Increase the speed to develop features by about 200% to 300%. *
* **Fewer bugs**: Reduce about 40% of human (developer) induced errors. *
* **Intuitive**: Great editor support. <abbr title="also known as auto-complete, autocompletion, IntelliSense">Completion</abbr> everywhere. Less time debugging.
* **Easy**: Designed to be easy to use and learn. Less time reading docs.
* **Short**: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.
* **Robust**: Get production-ready code. With automatic interactive documentation.
* **Standards-based**: Based on (and fully compatible with) the open standards for APIs: <a href="https://github.com/OAI/OpenAPI-Specification" class="external-link" target="_blank">OpenAPI</a> (previously known as Swagger) and <a href="https://json-schema.org/" class="external-link" target="_blank">JSON Schema</a>.

<small>* estimation based on tests on an internal development team, building production applications.</small>

## Sponsors

<!-- sponsors -->

<a href="https://blockbee.io?ref=fastapi" target="_blank" title="BlockBee Cryptocurrency Payment Gateway"><img src="https://fastapi.tiangolo.com/img/sponsors/blockbee.png"></a>
<a href="https://github.com/scalar/scalar/?utm_source=fastapi&utm_medium=website&utm_campaign=main-badge" target="_blank" title="Scalar: Beautiful Open-Source API References from Swagger/OpenAPI files"><img src="https://fastapi.tiangolo.com/img/sponsors/scalar.svg"></a>
<a href="https://www.propelauth.com/?utm_source=fastapi&utm_campaign=1223&utm_medium=mainbadge" target="_blank" title="Auth, user management and more for your B2B product"><img src="https://fastapi.tiangolo.com/img/sponsors/propelauth.png"></a>
<a href="https://zuplo.link/fastapi-gh" target="_blank" title="Zuplo: Deploy, Secure, Document, and Monetize your FastAPI"><img src="https://fastapi.tiangolo.com/img/sponsors/zuplo.png"></a>
<a href="https://liblab.com?utm_source=fastapi" target="_blank" title="liblab - Generate SDKs from FastAPI"><img src="https://fastapi.tiangolo.com/img/sponsors/liblab.png"></a>
<a href="https://docs.render.com/deploy-fastapi?utm_source=deploydoc&utm_medium=referral&utm_campaign=fastapi" target="_blank" title="Deploy & scale any full-stack web app on Render. Focus on building apps, not infra."><img src="https://fastapi.tiangolo.com/img/sponsors/render.svg"></a>
<a href="https://www.coderabbit.ai/?utm_source=fastapi&utm_medium=badge&utm_campaign=fastapi" target="_blank" title="Cut Code Review Time & Bugs in Half with CodeRabbit"><img src="https://fastapi.tiangolo.com/img/sponsors/coderabbit.png"></a>
<a href="https://subtotal.com/?utm_source=fastapi&utm_medium=sponsorship&utm_campaign=open-source" target="_blank" title="The Gold Standard in Retail Account Linking"><img src="https://fastapi.tiangolo.com/img/sponsors/subtotal.svg"></a>
<a href="https://docs.railway.com/guides/fastapi?utm_medium=integration&utm_source=docs&utm_campaign=fastapi" target="_blank" title="Deploy enterprise applications at startup speed"><img src="https://fastapi.tiangolo.com/img/sponsors/railway.png"></a>
<a href="https://databento.com/?utm_source=fastapi&utm_medium=sponsor&utm_content=display" target="_blank" title="Pay as you go for market data"><img src="https://fastapi.tiangolo.com/img/sponsors/databento.svg"></a>
<a href="https://speakeasy.com/editor?utm_source=fastapi+repo&utm_medium=github+sponsorship" target="_blank" title="SDKs for your API | Speakeasy"><img src="https://fastapi.tiangolo.com/img/sponsors/speakeasy.png"></a>
<a href="https://www.svix.com/" target="_blank" title="Svix - Webhooks as a service"><img src="https://fastapi.tiangolo.com/img/sponsors/svix.svg"></a>
<a href="https://www.stainlessapi.com/?utm_source=fastapi&utm_medium=referral" target="_blank" title="Stainless | Generate best-in-class SDKs"><img src="https://fastapi.tiangolo.com/img/sponsors/stainless.png"></a>
<a href="https://www.permit.io/blog/implement-authorization-in-fastapi?utm_source=github&utm_medium=referral&utm_campaign=fastapi" target="_blank" title="Fine-Grained Authorization for FastAPI"><img src="https://fastapi.tiangolo.com/img/sponsors/permit.png"></a>
<a href="https://www.interviewpal.com/?utm_source=fastapi&utm_medium=open-source&utm_campaign=dev-hiring" target="_blank" title="InterviewPal - AI Interview Coach for Engineers and Devs"><img src="https://fastapi.tiangolo.com/img/sponsors/interviewpal.png"></a>
<a href="https://dribia.com/en/" target="_blank" title="Dribia - Data Science within your reach"><img src="https://fastapi.tiangolo.com/img/sponsors/dribia.png"></a>

<!-- /sponsors -->

<a href="https://fastapi.tiangolo.com/fastapi-people/#sponsors" class="external-link" target="_blank">Other sponsors</a>

## Opinions

"_[...] I'm using **FastAPI** a ton these days. [...] I'm actually planning to use it for all of my team's **ML services at Microsoft**. Some of them are getting integrated into the core **Windows** product and some **Office** products._"

<div style="text-align: right; margin-right: 10%;">Kabir Khan - <strong>Microsoft</strong> <a href="https://github.com/fastapi/fastapi/pull/26" target="_blank"><small>(ref)</small></a></div>

---

"_We adopted the **FastAPI** library to spawn a **REST** server that can be queried to obtain **predictions**. [for Ludwig]_"

<div style="text-align: right; margin-right: 10%;">Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - <strong>Uber</strong> <a href="https://eng.uber.com/ludwig-v0-2/" target="_blank"><small>(ref)</small></a></div>

---

"_**Netflix** is pleased to announce the open-source release of our **crisis management** orchestration framework: **Dispatch**! [built with **FastAPI**]_"

<div style="text-align: right; margin-right: 10%;">Kevin Glisson, Marc Vilanova, Forest Monsen - <strong>Netflix</strong> <a href="https://netflixtechblog.com/introducing-dispatch-da4b8a2a8072" target="_blank"><small>(ref)</small></a></div>

---

"_I’m over the moon excited about **FastAPI**. It’s so fun!_"

<div style="text-align: right; margin-right: 10%;">Brian Okken - <strong><a href="https://pythonbytes.fm/episodes/show/123/time-to-right-the-py-wrongs?time_in_sec=855" target="_blank">Python Bytes</a> podcast host</strong> <a href="https://x.com/brianokken/status/1112220079972728832" target="_blank"><small>(ref)</small></a></div>

---

"_Honestly, what you've built looks super solid and polished. In many ways, it's what I wanted **Hug** to be - it's really inspiring to see someone build that._"

<div style="text-align: right; margin-right: 10%;">Timothy Crosley - <strong><a href="https://github.com/hugapi/hug" target="_blank">Hug</a> creator</strong> <a href="https://news.ycombinator.com/item?id=19455465" target="_blank"><small>(ref)</small></a></div>

---

"_If you're looking to learn one **modern framework** for building REST APIs, check out **FastAPI** [...] It's fast, easy to use and easy to learn [...]_"

"_We've switched over to **FastAPI** for our **APIs** [...] I think you'll like it [...]_"

<div style="text-align: right; margin-right: 10%;">Ines Montani - Matthew Honnibal - <strong><a href="https://explosion.ai" target="_blank">Explosion AI</a> founders - <a href="https://spacy.io" target="_blank">spaCy</a> creators</strong> <a href="https://x.com/_inesmontani/status/1144173225322143744" target="_blank"><small>(ref)</small></a> - <a href="https://x.com/honnibal/status/1144031421859655680" target="_blank"><small>(ref)</small></a></div>

---

"_If anyone is looking to build a production Python API, I would highly recommend **FastAPI**. It is **beautifully designed**, **simple to use** and **highly scalable**, it has become a **key component** in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._"

<div style="text-align: right; margin-right: 10%;">Deon Pillsbury - <strong>Cisco</strong> <a href="https://www.linkedin.com/posts/deonpillsbury_cisco-cx-python-activity-6963242628536487936-trAp/" target="_blank"><small>(ref)</small></a></div>

---

## **Typer**, the FastAPI of CLIs

<a href="https://typer.tiangolo.com" target="_blank"><img src="https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg" style="width: 20%;"></a>

If you are building a <abbr title="Command Line Interface">CLI</abbr> app to be used in the terminal instead of a web API, check out <a href="https://typer.tiangolo.com/" class="external-link" target="_blank">**Typer**</a>.

**Typer** is FastAPI's little sibling. And it's intended to be the **FastAPI of CLIs**. ⌨️ 🚀

## Requirements

FastAPI stands on the shoulders of giants:

* <a href="https://www.starlette.dev/" class="external-link" target="_blank">Starlette</a> for the web parts.
* <a href="https://docs.pydantic.dev/" class="external-link" target="_blank">Pydantic</a> for the data parts.

## Installation

Create and activate a <a href="https://fastapi.tiangolo.com/virtual-environments/" class="external-link" target="_blank">virtual environment</a> and then install FastAPI:

<div class="termy">

```console
$ pip install "fastapi[standard]"

---> 100%
```

</div>

**Note**: Make sure you put `"fastapi[standard]"` in quotes to ensure it works in all terminals.

## Example

### Create it

Create a file `main.py` with:

```Python
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get("/")
def read_root():
    return {"Hello": "World"}


@app.get("/items/{item_id}")
def read_item(item_id: int, q: Union[str, None] = None):
    return {"item_id": item_id, "q": q}
```

<details markdown="1">
<summary>Or use <code>async def</code>...</summary>

If your code uses `async` / `await`, use `async def`:

```Python hl_lines="9  14"
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get("/")
async def read_root():
    return {"Hello": "World"}


@app.get("/items/{item_id}")
async def read_item(item_id: int, q: Union[str, None] = None):
    return {"item_id": item_id, "q": q}
```

**Note**:

If you don't know, check the _"In a hurry?"_ section about <a href="https://fastapi.tiangolo.com/async/#in-a-hurry" target="_blank">`async` and `await` in the docs</a>.

</details>

### Run it

Run the server with:

<div class="termy">

```console
$ fastapi dev main.py

 ╭────────── FastAPI CLI - Development mode ───────────╮
 │                                                     │
 │  Serving at: http://127.0.0.1:8000                  │
 │                                                     │
 │  API docs: http://127.0.0.1:8000/docs               │
 │                                                     │
 │  Running in development mode, for production use:   │
 │                                                     │
 │  fastapi run                                        │
 │                                                     │
 ╰─────────────────────────────────────────────────────╯

INFO:     Will watch for changes in these directories: ['/home/user/code/awesomeapp']
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [2248755] using WatchFiles
INFO:     Started server process [2248757]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

</div>

<details markdown="1">
<summary>About the command <code>fastapi dev main.py</code>...</summary>

The command `fastapi dev` reads your `main.py` file, detects the **FastAPI** app in it, and starts a server using <a href="https://www.uvicorn.dev" class="external-link" target="_blank">Uvicorn</a>.

By default, `fastapi dev` will start with auto-reload enabled for local development.

You can read more about it in the <a href="https://fastapi.tiangolo.com/fastapi-cli/" target="_blank">FastAPI CLI docs</a>.

</details>

### Check it

Open your browser at <a href="http://127.0.0.1:8000/items/5?q=somequery" class="external-link" target="_blank">http://127.0.0.1:8000/items/5?q=somequery</a>.

You will see the JSON response as:

```JSON
{"item_id": 5, "q": "somequery"}
```

You already created an API that:

* Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.
* Both _paths_ take `GET` <em>operations</em> (also known as HTTP _methods_).
* The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.
* The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.

### Interactive API docs

Now go to <a href="http://127.0.0.1:8000/docs" class="external-link" target="_blank">http://127.0.0.1:8000/docs</a>.

You will see the automatic interactive API documentation (provided by <a href="https://github.com/swagger-api/swagger-ui" class="external-link" target="_blank">Swagger UI</a>):

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png)

### Alternative API docs

And now, go to <a href="http://127.0.0.1:8000/redoc" class="external-link" target="_blank">http://127.0.0.1:8000/redoc</a>.

You will see the alternative automatic documentation (provided by <a href="https://github.com/Rebilly/ReDoc" class="external-link" target="_blank">ReDoc</a>):

![ReDoc](https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png)

## Example upgrade

Now modify the file `main.py` to receive a body from a `PUT` request.

Declare the body using standard Python types, thanks to Pydantic.

```Python hl_lines="4  9-12  25-27"
from typing import Union

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    name: str
    price: float
    is_offer: Union[bool, None] = None


@app.get("/")
def read_root():
    return {"Hello": "World"}


@app.get("/items/{item_id}")
def read_item(item_id: int, q: Union[str, None] = None):
    return {"item_id": item_id, "q": q}


@app.put("/items/{item_id}")
def update_item(item_id: int, item: Item):
    return {"item_name": item.name, "item_id": item_id}
```

The `fastapi dev` server should reload automatically.

### Interactive API docs upgrade

Now go to <a href="http://127.0.0.1:8000/docs" class="external-link" target="_blank">http://127.0.0.1:8000/docs</a>.

* The interactive API documentation will be automatically updated, including the new body:

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png)

* Click on the button "Try it out", it allows you to fill the parameters and directly interact with the API:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png)

* Then click on the "Execute" button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png)

### Alternative API docs upgrade

And now, go to <a href="http://127.0.0.1:8000/redoc" class="external-link" target="_blank">http://127.0.0.1:8000/redoc</a>.

* The alternative documentation will also reflect the new query parameter and body:

![ReDoc](https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png)

### Recap

In summary, you declare **once** the types of parameters, body, etc. as function parameters.

You do that with standard modern Python types.

You don't have to learn a new syntax, the methods or classes of a specific library, etc.

Just standard **Python**.

For example, for an `int`:

```Python
item_id: int
```

or for a more complex `Item` model:

```Python
item: Item
```

...and with that single declaration you get:

* Editor support, including:
    * Completion.
    * Type checks.
* Validation of data:
    * Automatic and clear errors when the data is invalid.
    * Validation even for deeply nested JSON objects.
* <abbr title="also known as: serialization, parsing, marshalling">Conversion</abbr> of input data: coming from the network to Python data and types. Reading from:
    * JSON.
    * Path parameters.
    * Query parameters.
    * Cookies.
    * Headers.
    * Forms.
    * Files.
* <abbr title="also known as: serialization, parsing, marshalling">Conversion</abbr> of output data: converting from Python data and types to network data (as JSON):
    * Convert Python types (`str`, `int`, `float`, `bool`, `list`, etc).
    * `datetime` objects.
    * `UUID` objects.
    * Database models.
    * ...and many more.
* Automatic interactive API documentation, including 2 alternative user interfaces:
    * Swagger UI.
    * ReDoc.

---

Coming back to the previous code example, **FastAPI** will:

* Validate that there is an `item_id` in the path for `GET` and `PUT` requests.
* Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.
    * If it is not, the client will see a useful, clear error.
* Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.
    * As the `q` parameter is declared with `= None`, it is optional.
    * Without the `None` it would be required (as is the body in the case with `PUT`).
* For `PUT` requests to `/items/{item_id}`, read the body as JSON:
    * Check that it has a required attribute `name` that should be a `str`.
    * Check that it has a required attribute `price` that has to be a `float`.
    * Check that it has an optional attribute `is_offer`, that should be a `bool`, if present.
    * All this would also work for deeply nested JSON objects.
* Convert from and to JSON automatically.
* Document everything with OpenAPI, that can be used by:
    * Interactive documentation systems.
    * Automatic client code generation systems, for many languages.
* Provide 2 interactive documentation web interfaces directly.

---

We just scratched the surface, but you already get the idea of how it all works.

Try changing the line with:

```Python
    return {"item_name": item.name, "item_id": item_id}
```

...from:

```Python
        ... "item_name": item.name ...
```

...to:

```Python
        ... "item_price": item.price ...
```

...and see how your editor will auto-complete the attributes and know their types:

![editor support](https://fastapi.tiangolo.com/img/vscode-completion.png)

For a more complete example including more features, see the <a href="https://fastapi.tiangolo.com/tutorial/">Tutorial - User Guide</a>.

**Spoiler alert**: the tutorial - user guide includes:

* Declaration of **parameters** from other different places as: **headers**, **cookies**, **form fields** and **files**.
* How to set **validation constraints** as `maximum_length` or `regex`.
* A very powerful and easy to use **<abbr title="also known as components, resources, providers, services, injectables">Dependency Injection</abbr>** system.
* Security and authentication, including support for **OAuth2** with **JWT tokens** and **HTTP Basic** auth.
* More advanced (but equally easy) techniques for declaring **deeply nested JSON models** (thanks to Pydantic).
* **GraphQL** integration with <a href="https://strawberry.rocks" class="external-link" target="_blank">Strawberry</a> and other libraries.
* Many extra features (thanks to Starlette) as:
    * **WebSockets**
    * extremely easy tests based on HTTPX and `pytest`
    * **CORS**
    * **Cookie Sessions**
    * ...and more.

## Performance

Independent TechEmpower benchmarks show **FastAPI** applications running under Uvicorn as <a href="https://www.techempower.com/benchmarks/#section=test&runid=7464e520-0dc2-473d-bd34-dbdfd7e85911&hw=ph&test=query&l=zijzen-7" class="external-link" target="_blank">one of the fastest Python frameworks available</a>, only below Starlette and Uvicorn themselves (used internally by FastAPI). (*)

To understand more about it, see the section <a href="https://fastapi.tiangolo.com/benchmarks/" class="internal-link" target="_blank">Benchmarks</a>.

## Dependencies

FastAPI depends on Pydantic and Starlette.

### `standard` Dependencies

When you install FastAPI with `pip install "fastapi[standard]"` it comes with the `standard` group of optional dependencies:

Used by Pydantic:

* <a href="https://github.com/JoshData/python-email-validator" target="_blank"><code>email-validator</code></a> - for email validation.

Used by Starlette:

* <a href="https://www.python-httpx.org" target="_blank"><code>httpx</code></a> - Required if you want to use the `TestClient`.
* <a href="https://jinja.palletsprojects.com" target="_blank"><code>jinja2</code></a> - Required if you want to use the default template configuration.
* <a href="https://github.com/Kludex/python-multipart" target="_blank"><code>python-multipart</code></a> - Required if you want to support form <abbr title="converting the string that comes from an HTTP request into Python data">"parsing"</abbr>, with `request.form()`.

Used by FastAPI:

* <a href="https://www.uvicorn.dev" target="_blank"><code>uvicorn</code></a> - for the server that loads and serves your application. This includes `uvicorn[standard]`, which includes some dependencies (e.g. `uvloop`) needed for high performance serving.
* `fastapi-cli[standard]` - to provide the `fastapi` command.
    * This includes `fastapi-cloud-cli`, which allows you to deploy your FastAPI application to <a href="https://fastapicloud.com" class="external-link" target="_blank">FastAPI Cloud</a>.

### Without `standard` Dependencies

If you don't want to include the `standard` optional dependencies, you can install with `pip install fastapi` instead of `pip install "fastapi[standard]"`.

### Without `fastapi-cloud-cli`

If you want to install FastAPI with the standard dependencies but without the `fastapi-cloud-cli`, you can install with `pip install "fastapi[standard-no-fastapi-cloud-cli]"`.

### Additional Optional Dependencies

There are some additional dependencies you might want to install.

Additional optional Pydantic dependencies:

* <a href="https://docs.pydantic.dev/latest/usage/pydantic_settings/" target="_blank"><code>pydantic-settings</code></a> - for settings management.
* <a href="https://docs.pydantic.dev/latest/usage/types/extra_types/extra_types/" target="_blank"><code>pydantic-extra-types</code></a> - for extra types to be used with Pydantic.

Additional optional FastAPI dependencies:

* <a href="https://github.com/ijl/orjson" target="_blank"><code>orjson</code></a> - Required if you want to use `ORJSONResponse`.
* <a href="https://github.com/esnme/ultrajson" target="_blank"><code>ujson</code></a> - Required if you want to use `UJSONResponse`.

## License

This project is licensed under the terms of the MIT license.


--- pdm_build.py ---
import os
from typing import Any, Dict

from pdm.backend.hooks import Context

TIANGOLO_BUILD_PACKAGE = os.getenv("TIANGOLO_BUILD_PACKAGE", "fastapi")


def pdm_build_initialize(context: Context) -> None:
    metadata = context.config.metadata
    # Get custom config for the current package, from the env var
    config: Dict[str, Any] = context.config.data["tool"]["tiangolo"][
        "_internal-slim-build"
    ]["packages"].get(TIANGOLO_BUILD_PACKAGE)
    if not config:
        return
    project_config: Dict[str, Any] = config["project"]
    # Override main [project] configs with custom configs for this package
    for key, value in project_config.items():
        metadata[key] = value


--- requirements-github-actions.txt ---
PyGithub>=2.3.0,<3.0.0
pydantic>=2.5.3,<3.0.0
pydantic-settings>=2.1.0,<3.0.0
httpx>=0.27.0,<1.0.0
pyyaml >=5.3.1,<7.0.0
smokeshow


--- requirements-tests.txt ---
-e .[all]
-r requirements-docs-tests.txt
pytest >=7.1.3,<9.0.0
coverage[toml] >= 6.5.0,< 8.0
mypy ==1.14.1
dirty-equals ==0.9.0
sqlmodel==0.0.27
flask >=1.1.2,<4.0.0
anyio[trio] >=3.2.1,<5.0.0
PyJWT==2.9.0
pyyaml >=5.3.1,<7.0.0
pwdlib[argon2] >=0.2.1
inline-snapshot>=0.21.1
# types
types-ujson ==5.10.0.20240515
types-orjson ==3.6.2


--- requirements-translations.txt ---
pydantic-ai==0.4.10
GitPython==3.1.45


--- requirements.txt ---
-e .[all]
-r requirements-tests.txt
-r requirements-docs.txt
pre-commit >=2.17.0,<5.0.0
# For generating screenshots
playwright


--- scripts/contributors.py ---
import logging
import secrets
import subprocess
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Any

import httpx
import yaml
from github import Github
from pydantic import BaseModel, SecretStr
from pydantic_settings import BaseSettings

github_graphql_url = "https://api.github.com/graphql"


prs_query = """
query Q($after: String) {
  repository(name: "fastapi", owner: "fastapi") {
    pullRequests(first: 100, after: $after) {
      edges {
        cursor
        node {
          number
          labels(first: 100) {
            nodes {
              name
            }
          }
          author {
            login
            avatarUrl
            url
          }
          title
          createdAt
          lastEditedAt
          updatedAt
          state
          reviews(first:100) {
            nodes {
              author {
                login
                avatarUrl
                url
              }
              state
            }
          }
        }
      }
    }
  }
}
"""


class Author(BaseModel):
    login: str
    avatarUrl: str
    url: str


class LabelNode(BaseModel):
    name: str


class Labels(BaseModel):
    nodes: list[LabelNode]


class ReviewNode(BaseModel):
    author: Author | None = None
    state: str


class Reviews(BaseModel):
    nodes: list[ReviewNode]


class PullRequestNode(BaseModel):
    number: int
    labels: Labels
    author: Author | None = None
    title: str
    createdAt: datetime
    lastEditedAt: datetime | None = None
    updatedAt: datetime | None = None
    state: str
    reviews: Reviews


class PullRequestEdge(BaseModel):
    cursor: str
    node: PullRequestNode


class PullRequests(BaseModel):
    edges: list[PullRequestEdge]


class PRsRepository(BaseModel):
    pullRequests: PullRequests


class PRsResponseData(BaseModel):
    repository: PRsRepository


class PRsResponse(BaseModel):
    data: PRsResponseData


class Settings(BaseSettings):
    github_token: SecretStr
    github_repository: str
    httpx_timeout: int = 30


def get_graphql_response(
    *,
    settings: Settings,
    query: str,
    after: str | None = None,
) -> dict[str, Any]:
    headers = {"Authorization": f"token {settings.github_token.get_secret_value()}"}
    variables = {"after": after}
    response = httpx.post(
        github_graphql_url,
        headers=headers,
        timeout=settings.httpx_timeout,
        json={"query": query, "variables": variables, "operationName": "Q"},
    )
    if response.status_code != 200:
        logging.error(f"Response was not 200, after: {after}")
        logging.error(response.text)
        raise RuntimeError(response.text)
    data = response.json()
    if "errors" in data:
        logging.error(f"Errors in response, after: {after}")
        logging.error(data["errors"])
        logging.error(response.text)
        raise RuntimeError(response.text)
    return data


def get_graphql_pr_edges(
    *, settings: Settings, after: str | None = None
) -> list[PullRequestEdge]:
    data = get_graphql_response(settings=settings, query=prs_query, after=after)
    graphql_response = PRsResponse.model_validate(data)
    return graphql_response.data.repository.pullRequests.edges


def get_pr_nodes(settings: Settings) -> list[PullRequestNode]:
    pr_nodes: list[PullRequestNode] = []
    pr_edges = get_graphql_pr_edges(settings=settings)

    while pr_edges:
        for edge in pr_edges:
            pr_nodes.append(edge.node)
        last_edge = pr_edges[-1]
        pr_edges = get_graphql_pr_edges(settings=settings, after=last_edge.cursor)
    return pr_nodes


class ContributorsResults(BaseModel):
    contributors: Counter[str]
    translation_reviewers: Counter[str]
    translators: Counter[str]
    authors: dict[str, Author]


def get_contributors(pr_nodes: list[PullRequestNode]) -> ContributorsResults:
    contributors = Counter[str]()
    translation_reviewers = Counter[str]()
    translators = Counter[str]()
    authors: dict[str, Author] = {}

    for pr in pr_nodes:
        if pr.author:
            authors[pr.author.login] = pr.author
        is_lang = False
        for label in pr.labels.nodes:
            if label.name == "lang-all":
                is_lang = True
                break
        for review in pr.reviews.nodes:
            if review.author:
                authors[review.author.login] = review.author
                if is_lang:
                    translation_reviewers[review.author.login] += 1
        if pr.state == "MERGED" and pr.author:
            if is_lang:
                translators[pr.author.login] += 1
            else:
                contributors[pr.author.login] += 1
    return ContributorsResults(
        contributors=contributors,
        translation_reviewers=translation_reviewers,
        translators=translators,
        authors=authors,
    )


def get_users_to_write(
    *,
    counter: Counter[str],
    authors: dict[str, Author],
    min_count: int = 2,
) -> dict[str, Any]:
    users: dict[str, Any] = {}
    for user, count in counter.most_common():
        if count >= min_count:
            author = authors[user]
            users[user] = {
                "login": user,
                "count": count,
                "avatarUrl": author.avatarUrl,
                "url": author.url,
            }
    return users


def update_content(*, content_path: Path, new_content: Any) -> bool:
    old_content = content_path.read_text(encoding="utf-8")

    new_content = yaml.dump(new_content, sort_keys=False, width=200, allow_unicode=True)
    if old_content == new_content:
        logging.info(f"The content hasn't changed for {content_path}")
        return False
    content_path.write_text(new_content, encoding="utf-8")
    logging.info(f"Updated {content_path}")
    return True


def main() -> None:
    logging.basicConfig(level=logging.INFO)
    settings = Settings()
    logging.info(f"Using config: {settings.model_dump_json()}")
    g = Github(settings.github_token.get_secret_value())
    repo = g.get_repo(settings.github_repository)

    pr_nodes = get_pr_nodes(settings=settings)
    contributors_results = get_contributors(pr_nodes=pr_nodes)
    authors = contributors_results.authors

    top_contributors = get_users_to_write(
        counter=contributors_results.contributors,
        authors=authors,
    )

    top_translators = get_users_to_write(
        counter=contributors_results.translators,
        authors=authors,
    )
    top_translations_reviewers = get_users_to_write(
        counter=contributors_results.translation_reviewers,
        authors=authors,
    )

    # For local development
    # contributors_path = Path("../docs/en/data/contributors.yml")
    contributors_path = Path("./docs/en/data/contributors.yml")
    # translators_path = Path("../docs/en/data/translators.yml")
    translators_path = Path("./docs/en/data/translators.yml")
    # translation_reviewers_path = Path("../docs/en/data/translation_reviewers.yml")
    translation_reviewers_path = Path("./docs/en/data/translation_reviewers.yml")

    updated = [
        update_content(content_path=contributors_path, new_content=top_contributors),
        update_content(content_path=translators_path, new_content=top_translators),
        update_content(
            content_path=translation_reviewers_path,
            new_content=top_translations_reviewers,
        ),
    ]

    if not any(updated):
        logging.info("The data hasn't changed, finishing.")
        return

    logging.info("Setting up GitHub Actions git user")
    subprocess.run(["git", "config", "user.name", "github-actions"], check=True)
    subprocess.run(
        ["git", "config", "user.email", "github-actions@github.com"], check=True
    )
    branch_name = f"fastapi-people-contributors-{secrets.token_hex(4)}"
    logging.info(f"Creating a new branch {branch_name}")
    subprocess.run(["git", "checkout", "-b", branch_name], check=True)
    logging.info("Adding updated file")
    subprocess.run(
        [
            "git",
            "add",
            str(contributors_path),
            str(translators_path),
            str(translation_reviewers_path),
        ],
        check=True,
    )
    logging.info("Committing updated file")
    message = "👥 Update FastAPI People - Contributors and Translators"
    subprocess.run(["git", "commit", "-m", message], check=True)
    logging.info("Pushing branch")
    subprocess.run(["git", "push", "origin", branch_name], check=True)
    logging.info("Creating PR")
    pr = repo.create_pull(title=message, body=message, base="master", head=branch_name)
    logging.info(f"Created PR: {pr.number}")
    logging.info("Finished")


if __name__ == "__main__":
    main()


--- scripts/label_approved.py ---
import logging
from typing import Literal

from github import Github
from github.PullRequestReview import PullRequestReview
from pydantic import BaseModel, SecretStr
from pydantic_settings import BaseSettings


class LabelSettings(BaseModel):
    await_label: str | None = None
    number: int


default_config = {"approved-2": LabelSettings(await_label="awaiting-review", number=2)}


class Settings(BaseSettings):
    github_repository: str
    token: SecretStr
    debug: bool | None = False
    config: dict[str, LabelSettings] | Literal[""] = default_config


settings = Settings()
if settings.debug:
    logging.basicConfig(level=logging.DEBUG)
else:
    logging.basicConfig(level=logging.INFO)
logging.debug(f"Using config: {settings.model_dump_json()}")
g = Github(settings.token.get_secret_value())
repo = g.get_repo(settings.github_repository)
for pr in repo.get_pulls(state="open"):
    logging.info(f"Checking PR: #{pr.number}")
    pr_labels = list(pr.get_labels())
    pr_label_by_name = {label.name: label for label in pr_labels}
    reviews = list(pr.get_reviews())
    review_by_user: dict[str, PullRequestReview] = {}
    for review in reviews:
        if review.user.login in review_by_user:
            stored_review = review_by_user[review.user.login]
            if review.submitted_at >= stored_review.submitted_at:
                review_by_user[review.user.login] = review
        else:
            review_by_user[review.user.login] = review
    approved_reviews = [
        review for review in review_by_user.values() if review.state == "APPROVED"
    ]
    config = settings.config or default_config
    for approved_label, conf in config.items():
        logging.debug(f"Processing config: {conf.model_dump_json()}")
        if conf.await_label is None or (conf.await_label in pr_label_by_name):
            logging.debug(f"Processable PR: {pr.number}")
            if len(approved_reviews) >= conf.number:
                logging.info(f"Adding label to PR: {pr.number}")
                pr.add_to_labels(approved_label)
                if conf.await_label:
                    logging.info(f"Removing label from PR: {pr.number}")
                    pr.remove_from_labels(conf.await_label)
logging.info("Finished")


--- scripts/notify_translations.py ---
import logging
import random
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Union, cast

import httpx
from github import Github
from pydantic import BaseModel, SecretStr
from pydantic_settings import BaseSettings

awaiting_label = "awaiting-review"
lang_all_label = "lang-all"
approved_label = "approved-1"


github_graphql_url = "https://api.github.com/graphql"
questions_translations_category_id = "DIC_kwDOCZduT84CT5P9"

all_discussions_query = """
query Q($category_id: ID) {
  repository(name: "fastapi", owner: "fastapi") {
    discussions(categoryId: $category_id, first: 100) {
      nodes {
        title
        id
        number
        labels(first: 10) {
          edges {
            node {
              id
              name
            }
          }
        }
      }
    }
  }
}
"""

translation_discussion_query = """
query Q($after: String, $discussion_number: Int!) {
  repository(name: "fastapi", owner: "fastapi") {
    discussion(number: $discussion_number) {
      comments(first: 100, after: $after) {
        edges {
          cursor
          node {
            id
            url
            body
          }
        }
      }
    }
  }
}
"""

add_comment_mutation = """
mutation Q($discussion_id: ID!, $body: String!) {
  addDiscussionComment(input: {discussionId: $discussion_id, body: $body}) {
    comment {
      id
      url
      body
    }
  }
}
"""

update_comment_mutation = """
mutation Q($comment_id: ID!, $body: String!) {
  updateDiscussionComment(input: {commentId: $comment_id, body: $body}) {
    comment {
      id
      url
      body
    }
  }
}
"""


class Comment(BaseModel):
    id: str
    url: str
    body: str


class UpdateDiscussionComment(BaseModel):
    comment: Comment


class UpdateCommentData(BaseModel):
    updateDiscussionComment: UpdateDiscussionComment


class UpdateCommentResponse(BaseModel):
    data: UpdateCommentData


class AddDiscussionComment(BaseModel):
    comment: Comment


class AddCommentData(BaseModel):
    addDiscussionComment: AddDiscussionComment


class AddCommentResponse(BaseModel):
    data: AddCommentData


class CommentsEdge(BaseModel):
    node: Comment
    cursor: str


class Comments(BaseModel):
    edges: List[CommentsEdge]


class CommentsDiscussion(BaseModel):
    comments: Comments


class CommentsRepository(BaseModel):
    discussion: CommentsDiscussion


class CommentsData(BaseModel):
    repository: CommentsRepository


class CommentsResponse(BaseModel):
    data: CommentsData


class AllDiscussionsLabelNode(BaseModel):
    id: str
    name: str


class AllDiscussionsLabelsEdge(BaseModel):
    node: AllDiscussionsLabelNode


class AllDiscussionsDiscussionLabels(BaseModel):
    edges: List[AllDiscussionsLabelsEdge]


class AllDiscussionsDiscussionNode(BaseModel):
    title: str
    id: str
    number: int
    labels: AllDiscussionsDiscussionLabels


class AllDiscussionsDiscussions(BaseModel):
    nodes: List[AllDiscussionsDiscussionNode]


class AllDiscussionsRepository(BaseModel):
    discussions: AllDiscussionsDiscussions


class AllDiscussionsData(BaseModel):
    repository: AllDiscussionsRepository


class AllDiscussionsResponse(BaseModel):
    data: AllDiscussionsData


class Settings(BaseSettings):
    model_config = {"env_ignore_empty": True}

    github_repository: str
    github_token: SecretStr
    github_event_path: Path
    github_event_name: Union[str, None] = None
    httpx_timeout: int = 30
    debug: Union[bool, None] = False
    number: int | None = None


class PartialGitHubEventIssue(BaseModel):
    number: int | None = None


class PartialGitHubEvent(BaseModel):
    pull_request: PartialGitHubEventIssue | None = None


def get_graphql_response(
    *,
    settings: Settings,
    query: str,
    after: Union[str, None] = None,
    category_id: Union[str, None] = None,
    discussion_number: Union[int, None] = None,
    discussion_id: Union[str, None] = None,
    comment_id: Union[str, None] = None,
    body: Union[str, None] = None,
) -> Dict[str, Any]:
    headers = {"Authorization": f"token {settings.github_token.get_secret_value()}"}
    variables = {
        "after": after,
        "category_id": category_id,
        "discussion_number": discussion_number,
        "discussion_id": discussion_id,
        "comment_id": comment_id,
        "body": body,
    }
    response = httpx.post(
        github_graphql_url,
        headers=headers,
        timeout=settings.httpx_timeout,
        json={"query": query, "variables": variables, "operationName": "Q"},
    )
    if response.status_code != 200:
        logging.error(
            f"Response was not 200, after: {after}, category_id: {category_id}"
        )
        logging.error(response.text)
        raise RuntimeError(response.text)
    data = response.json()
    if "errors" in data:
        logging.error(f"Errors in response, after: {after}, category_id: {category_id}")
        logging.error(data["errors"])
        logging.error(response.text)
        raise RuntimeError(response.text)
    return cast(Dict[str, Any], data)


def get_graphql_translation_discussions(
    *, settings: Settings
) -> List[AllDiscussionsDiscussionNode]:
    data = get_graphql_response(
        settings=settings,
        query=all_discussions_query,
        category_id=questions_translations_category_id,
    )
    graphql_response = AllDiscussionsResponse.model_validate(data)
    return graphql_response.data.repository.discussions.nodes


def get_graphql_translation_discussion_comments_edges(
    *, settings: Settings, discussion_number: int, after: Union[str, None] = None
) -> List[CommentsEdge]:
    data = get_graphql_response(
        settings=settings,
        query=translation_discussion_query,
        discussion_number=discussion_number,
        after=after,
    )
    graphql_response = CommentsResponse.model_validate(data)
    return graphql_response.data.repository.discussion.comments.edges


def get_graphql_translation_discussion_comments(
    *, settings: Settings, discussion_number: int
) -> list[Comment]:
    comment_nodes: List[Comment] = []
    discussion_edges = get_graphql_translation_discussion_comments_edges(
        settings=settings, discussion_number=discussion_number
    )

    while discussion_edges:
        for discussion_edge in discussion_edges:
            comment_nodes.append(discussion_edge.node)
        last_edge = discussion_edges[-1]
        discussion_edges = get_graphql_translation_discussion_comments_edges(
            settings=settings,
            discussion_number=discussion_number,
            after=last_edge.cursor,
        )
    return comment_nodes


def create_comment(*, settings: Settings, discussion_id: str, body: str) -> Comment:
    data = get_graphql_response(
        settings=settings,
        query=add_comment_mutation,
        discussion_id=discussion_id,
        body=body,
    )
    response = AddCommentResponse.model_validate(data)
    return response.data.addDiscussionComment.comment


def update_comment(*, settings: Settings, comment_id: str, body: str) -> Comment:
    data = get_graphql_response(
        settings=settings,
        query=update_comment_mutation,
        comment_id=comment_id,
        body=body,
    )
    response = UpdateCommentResponse.model_validate(data)
    return response.data.updateDiscussionComment.comment


def main() -> None:
    settings = Settings()
    if settings.debug:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    logging.debug(f"Using config: {settings.model_dump_json()}")
    g = Github(settings.github_token.get_secret_value())
    repo = g.get_repo(settings.github_repository)
    if not settings.github_event_path.is_file():
        raise RuntimeError(
            f"No github event file available at: {settings.github_event_path}"
        )
    contents = settings.github_event_path.read_text()
    github_event = PartialGitHubEvent.model_validate_json(contents)
    logging.info(f"Using GitHub event: {github_event}")
    number = (
        github_event.pull_request and github_event.pull_request.number
    ) or settings.number
    if number is None:
        raise RuntimeError("No PR number available")

    # Avoid race conditions with multiple labels
    sleep_time = random.random() * 10  # random number between 0 and 10 seconds
    logging.info(
        f"Sleeping for {sleep_time} seconds to avoid "
        "race conditions and multiple comments"
    )
    time.sleep(sleep_time)

    # Get PR
    logging.debug(f"Processing PR: #{number}")
    pr = repo.get_pull(number)
    label_strs = {label.name for label in pr.get_labels()}
    langs = []
    for label in label_strs:
        if label.startswith("lang-") and not label == lang_all_label:
            langs.append(label[5:])
    logging.info(f"PR #{pr.number} has labels: {label_strs}")
    if not langs or lang_all_label not in label_strs:
        logging.info(f"PR #{pr.number} doesn't seem to be a translation PR, skipping")
        sys.exit(0)

    # Generate translation map, lang ID to discussion
    discussions = get_graphql_translation_discussions(settings=settings)
    lang_to_discussion_map: Dict[str, AllDiscussionsDiscussionNode] = {}
    for discussion in discussions:
        for edge in discussion.labels.edges:
            label = edge.node.name
            if label.startswith("lang-") and not label == lang_all_label:
                lang = label[5:]
                lang_to_discussion_map[lang] = discussion
    logging.debug(f"Using translations map: {lang_to_discussion_map}")

    # Messages to create or check
    new_translation_message = f"Good news everyone! 😉 There's a new translation PR to be reviewed: #{pr.number} by @{pr.user.login}. 🎉 This requires 2 approvals from native speakers to be merged. 🤓"
    done_translation_message = f"~There's a new translation PR to be reviewed: #{pr.number} by @{pr.user.login}~ Good job! This is done. 🍰☕"

    # Normally only one language, but still
    for lang in langs:
        if lang not in lang_to_discussion_map:
            log_message = f"Could not find discussion for language: {lang}"
            logging.error(log_message)
            raise RuntimeError(log_message)
        discussion = lang_to_discussion_map[lang]
        logging.info(
            f"Found a translation discussion for language: {lang} in discussion: #{discussion.number}"
        )

        already_notified_comment: Union[Comment, None] = None
        already_done_comment: Union[Comment, None] = None

        logging.info(
            f"Checking current comments in discussion: #{discussion.number} to see if already notified about this PR: #{pr.number}"
        )
        comments = get_graphql_translation_discussion_comments(
            settings=settings, discussion_number=discussion.number
        )
        for comment in comments:
            if new_translation_message in comment.body:
                already_notified_comment = comment
            elif done_translation_message in comment.body:
                already_done_comment = comment
        logging.info(
            f"Already notified comment: {already_notified_comment}, already done comment: {already_done_comment}"
        )

        if pr.state == "open" and awaiting_label in label_strs:
            logging.info(
                f"This PR seems to be a language translation and awaiting reviews: #{pr.number}"
            )
            if already_notified_comment:
                logging.info(
                    f"This PR #{pr.number} was already notified in comment: {already_notified_comment.url}"
                )
            else:
                logging.info(
                    f"Writing notification comment about PR #{pr.number} in Discussion: #{discussion.number}"
                )
                comment = create_comment(
                    settings=settings,
                    discussion_id=discussion.id,
                    body=new_translation_message,
                )
                logging.info(f"Notified in comment: {comment.url}")
        elif pr.state == "closed" or approved_label in label_strs:
            logging.info(f"Already approved or closed PR #{pr.number}")
            if already_done_comment:
                logging.info(
                    f"This PR #{pr.number} was already marked as done in comment: {already_done_comment.url}"
                )
            elif already_notified_comment:
                updated_comment = update_comment(
                    settings=settings,
                    comment_id=already_notified_comment.id,
                    body=done_translation_message,
                )
                logging.info(f"Marked as done in comment: {updated_comment.url}")
        else:
            logging.info(
                f"There doesn't seem to be anything to be done about PR #{pr.number}"
            )
    logging.info("Finished")


if __name__ == "__main__":
    main()


--- scripts/people.py ---
import logging
import secrets
import subprocess
import time
from collections import Counter
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Container, Union

import httpx
import yaml
from github import Github
from pydantic import BaseModel, SecretStr
from pydantic_settings import BaseSettings

github_graphql_url = "https://api.github.com/graphql"
questions_category_id = "MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDAxNDM0"

discussions_query = """
query Q($after: String, $category_id: ID) {
  repository(name: "fastapi", owner: "fastapi") {
    discussions(first: 100, after: $after, categoryId: $category_id) {
      edges {
        cursor
        node {
          number
          author {
            login
            avatarUrl
            url
          }
          createdAt
          comments(first: 50) {
            totalCount
            nodes {
              createdAt
              author {
                login
                avatarUrl
                url
              }
              isAnswer
              replies(first: 10) {
                totalCount
                nodes {
                  createdAt
                  author {
                    login
                    avatarUrl
                    url
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
"""


class Author(BaseModel):
    login: str
    avatarUrl: str | None = None
    url: str | None = None


class CommentsNode(BaseModel):
    createdAt: datetime
    author: Union[Author, None] = None


class Replies(BaseModel):
    totalCount: int
    nodes: list[CommentsNode]


class DiscussionsCommentsNode(CommentsNode):
    replies: Replies


class DiscussionsComments(BaseModel):
    totalCount: int
    nodes: list[DiscussionsCommentsNode]


class DiscussionsNode(BaseModel):
    number: int
    author: Union[Author, None] = None
    title: str | None = None
    createdAt: datetime
    comments: DiscussionsComments


class DiscussionsEdge(BaseModel):
    cursor: str
    node: DiscussionsNode


class Discussions(BaseModel):
    edges: list[DiscussionsEdge]


class DiscussionsRepository(BaseModel):
    discussions: Discussions


class DiscussionsResponseData(BaseModel):
    repository: DiscussionsRepository


class DiscussionsResponse(BaseModel):
    data: DiscussionsResponseData


class Settings(BaseSettings):
    github_token: SecretStr
    github_repository: str
    httpx_timeout: int = 30
    sleep_interval: int = 5


def get_graphql_response(
    *,
    settings: Settings,
    query: str,
    after: Union[str, None] = None,
    category_id: Union[str, None] = None,
) -> dict[str, Any]:
    headers = {"Authorization": f"token {settings.github_token.get_secret_value()}"}
    variables = {"after": after, "category_id": category_id}
    response = httpx.post(
        github_graphql_url,
        headers=headers,
        timeout=settings.httpx_timeout,
        json={"query": query, "variables": variables, "operationName": "Q"},
    )
    if response.status_code != 200:
        logging.error(
            f"Response was not 200, after: {after}, category_id: {category_id}"
        )
        logging.error(response.text)
        raise RuntimeError(response.text)
    data = response.json()
    if "errors" in data:
        logging.error(f"Errors in response, after: {after}, category_id: {category_id}")
        logging.error(data["errors"])
        logging.error(response.text)
        raise RuntimeError(response.text)
    return data


def get_graphql_question_discussion_edges(
    *,
    settings: Settings,
    after: Union[str, None] = None,
) -> list[DiscussionsEdge]:
    data = get_graphql_response(
        settings=settings,
        query=discussions_query,
        after=after,
        category_id=questions_category_id,
    )
    graphql_response = DiscussionsResponse.model_validate(data)
    return graphql_response.data.repository.discussions.edges


class DiscussionExpertsResults(BaseModel):
    commenters: Counter[str]
    last_month_commenters: Counter[str]
    three_months_commenters: Counter[str]
    six_months_commenters: Counter[str]
    one_year_commenters: Counter[str]
    authors: dict[str, Author]


def get_discussion_nodes(settings: Settings) -> list[DiscussionsNode]:
    discussion_nodes: list[DiscussionsNode] = []
    discussion_edges = get_graphql_question_discussion_edges(settings=settings)

    while discussion_edges:
        for discussion_edge in discussion_edges:
            discussion_nodes.append(discussion_edge.node)
        last_edge = discussion_edges[-1]
        # Handle GitHub secondary rate limits, requests per minute
        time.sleep(settings.sleep_interval)
        discussion_edges = get_graphql_question_discussion_edges(
            settings=settings, after=last_edge.cursor
        )
    return discussion_nodes


def get_discussions_experts(
    discussion_nodes: list[DiscussionsNode],
) -> DiscussionExpertsResults:
    commenters = Counter[str]()
    last_month_commenters = Counter[str]()
    three_months_commenters = Counter[str]()
    six_months_commenters = Counter[str]()
    one_year_commenters = Counter[str]()
    authors: dict[str, Author] = {}

    now = datetime.now(tz=timezone.utc)
    one_month_ago = now - timedelta(days=30)
    three_months_ago = now - timedelta(days=90)
    six_months_ago = now - timedelta(days=180)
    one_year_ago = now - timedelta(days=365)

    for discussion in discussion_nodes:
        discussion_author_name = None
        if discussion.author:
            authors[discussion.author.login] = discussion.author
            discussion_author_name = discussion.author.login
        discussion_commentors: dict[str, datetime] = {}
        for comment in discussion.comments.nodes:
            if comment.author:
                authors[comment.author.login] = comment.author
                if comment.author.login != discussion_author_name:
                    author_time = discussion_commentors.get(
                        comment.author.login, comment.createdAt
                    )
                    discussion_commentors[comment.author.login] = max(
                        author_time, comment.createdAt
                    )
            for reply in comment.replies.nodes:
                if reply.author:
                    authors[reply.author.login] = reply.author
                    if reply.author.login != discussion_author_name:
                        author_time = discussion_commentors.get(
                            reply.author.login, reply.createdAt
                        )
                        discussion_commentors[reply.author.login] = max(
                            author_time, reply.createdAt
                        )
        for author_name, author_time in discussion_commentors.items():
            commenters[author_name] += 1
            if author_time > one_month_ago:
                last_month_commenters[author_name] += 1
            if author_time > three_months_ago:
                three_months_commenters[author_name] += 1
            if author_time > six_months_ago:
                six_months_commenters[author_name] += 1
            if author_time > one_year_ago:
                one_year_commenters[author_name] += 1
    discussion_experts_results = DiscussionExpertsResults(
        authors=authors,
        commenters=commenters,
        last_month_commenters=last_month_commenters,
        three_months_commenters=three_months_commenters,
        six_months_commenters=six_months_commenters,
        one_year_commenters=one_year_commenters,
    )
    return discussion_experts_results


def get_top_users(
    *,
    counter: Counter[str],
    authors: dict[str, Author],
    skip_users: Container[str],
    min_count: int = 2,
) -> list[dict[str, Any]]:
    users: list[dict[str, Any]] = []
    for commenter, count in counter.most_common(50):
        if commenter in skip_users:
            continue
        if count >= min_count:
            author = authors[commenter]
            users.append(
                {
                    "login": commenter,
                    "count": count,
                    "avatarUrl": author.avatarUrl,
                    "url": author.url,
                }
            )
    return users


def get_users_to_write(
    *,
    counter: Counter[str],
    authors: dict[str, Author],
    min_count: int = 2,
) -> list[dict[str, Any]]:
    users: dict[str, Any] = {}
    users_list: list[dict[str, Any]] = []
    for user, count in counter.most_common(60):
        if count >= min_count:
            author = authors[user]
            user_data = {
                "login": user,
                "count": count,
                "avatarUrl": author.avatarUrl,
                "url": author.url,
            }
            users[user] = user_data
            users_list.append(user_data)
    return users_list


def update_content(*, content_path: Path, new_content: Any) -> bool:
    old_content = content_path.read_text(encoding="utf-8")

    new_content = yaml.dump(new_content, sort_keys=False, width=200, allow_unicode=True)
    if old_content == new_content:
        logging.info(f"The content hasn't changed for {content_path}")
        return False
    content_path.write_text(new_content, encoding="utf-8")
    logging.info(f"Updated {content_path}")
    return True


def main() -> None:
    logging.basicConfig(level=logging.INFO)
    settings = Settings()
    logging.info(f"Using config: {settings.model_dump_json()}")
    g = Github(settings.github_token.get_secret_value())
    repo = g.get_repo(settings.github_repository)

    discussion_nodes = get_discussion_nodes(settings=settings)
    experts_results = get_discussions_experts(discussion_nodes=discussion_nodes)

    authors = experts_results.authors
    maintainers_logins = {"tiangolo"}
    maintainers = []
    for login in maintainers_logins:
        user = authors[login]
        maintainers.append(
            {
                "login": login,
                "answers": experts_results.commenters[login],
                "avatarUrl": user.avatarUrl,
                "url": user.url,
            }
        )

    experts = get_users_to_write(
        counter=experts_results.commenters,
        authors=authors,
    )
    last_month_experts = get_users_to_write(
        counter=experts_results.last_month_commenters,
        authors=authors,
    )
    three_months_experts = get_users_to_write(
        counter=experts_results.three_months_commenters,
        authors=authors,
    )
    six_months_experts = get_users_to_write(
        counter=experts_results.six_months_commenters,
        authors=authors,
    )
    one_year_experts = get_users_to_write(
        counter=experts_results.one_year_commenters,
        authors=authors,
    )

    people = {
        "maintainers": maintainers,
        "experts": experts,
        "last_month_experts": last_month_experts,
        "three_months_experts": three_months_experts,
        "six_months_experts": six_months_experts,
        "one_year_experts": one_year_experts,
    }

    # For local development
    # people_path = Path("../docs/en/data/people.yml")
    people_path = Path("./docs/en/data/people.yml")

    updated = update_content(content_path=people_path, new_content=people)

    if not updated:
        logging.info("The data hasn't changed, finishing.")
        return

    logging.info("Setting up GitHub Actions git user")
    subprocess.run(["git", "config", "user.name", "github-actions"], check=True)
    subprocess.run(
        ["git", "config", "user.email", "github-actions@github.com"], check=True
    )
    branch_name = f"fastapi-people-experts-{secrets.token_hex(4)}"
    logging.info(f"Creating a new branch {branch_name}")
    subprocess.run(["git", "checkout", "-b", branch_name], check=True)
    logging.info("Adding updated file")
    subprocess.run(["git", "add", str(people_path)], check=True)
    logging.info("Committing updated file")
    message = "👥 Update FastAPI People - Experts"
    subprocess.run(["git", "commit", "-m", message], check=True)
    logging.info("Pushing branch")
    subprocess.run(["git", "push", "origin", branch_name], check=True)
    logging.info("Creating PR")
    pr = repo.create_pull(title=message, body=message, base="master", head=branch_name)
    logging.info(f"Created PR: {pr.number}")
    logging.info("Finished")


if __name__ == "__main__":
    main()


--- scripts/sponsors.py ---
import logging
import secrets
import subprocess
from collections import defaultdict
from pathlib import Path
from typing import Any

import httpx
import yaml
from github import Github
from pydantic import BaseModel, SecretStr
from pydantic_settings import BaseSettings

github_graphql_url = "https://api.github.com/graphql"


sponsors_query = """
query Q($after: String) {
  user(login: "tiangolo") {
    sponsorshipsAsMaintainer(first: 100, after: $after) {
      edges {
        cursor
        node {
          sponsorEntity {
            ... on Organization {
              login
              avatarUrl
              url
            }
            ... on User {
              login
              avatarUrl
              url
            }
          }
          tier {
            name
            monthlyPriceInDollars
          }
        }
      }
    }
  }
}
"""


class SponsorEntity(BaseModel):
    login: str
    avatarUrl: str
    url: str


class Tier(BaseModel):
    name: str
    monthlyPriceInDollars: float


class SponsorshipAsMaintainerNode(BaseModel):
    sponsorEntity: SponsorEntity
    tier: Tier


class SponsorshipAsMaintainerEdge(BaseModel):
    cursor: str
    node: SponsorshipAsMaintainerNode


class SponsorshipAsMaintainer(BaseModel):
    edges: list[SponsorshipAsMaintainerEdge]


class SponsorsUser(BaseModel):
    sponsorshipsAsMaintainer: SponsorshipAsMaintainer


class SponsorsResponseData(BaseModel):
    user: SponsorsUser


class SponsorsResponse(BaseModel):
    data: SponsorsResponseData


class Settings(BaseSettings):
    sponsors_token: SecretStr
    pr_token: SecretStr
    github_repository: str
    httpx_timeout: int = 30


def get_graphql_response(
    *,
    settings: Settings,
    query: str,
    after: str | None = None,
) -> dict[str, Any]:
    headers = {"Authorization": f"token {settings.sponsors_token.get_secret_value()}"}
    variables = {"after": after}
    response = httpx.post(
        github_graphql_url,
        headers=headers,
        timeout=settings.httpx_timeout,
        json={"query": query, "variables": variables, "operationName": "Q"},
    )
    if response.status_code != 200:
        logging.error(f"Response was not 200, after: {after}")
        logging.error(response.text)
        raise RuntimeError(response.text)
    data = response.json()
    if "errors" in data:
        logging.error(f"Errors in response, after: {after}")
        logging.error(data["errors"])
        logging.error(response.text)
        raise RuntimeError(response.text)
    return data


def get_graphql_sponsor_edges(
    *, settings: Settings, after: str | None = None
) -> list[SponsorshipAsMaintainerEdge]:
    data = get_graphql_response(settings=settings, query=sponsors_query, after=after)
    graphql_response = SponsorsResponse.model_validate(data)
    return graphql_response.data.user.sponsorshipsAsMaintainer.edges


def get_individual_sponsors(
    settings: Settings,
) -> defaultdict[float, dict[str, SponsorEntity]]:
    nodes: list[SponsorshipAsMaintainerNode] = []
    edges = get_graphql_sponsor_edges(settings=settings)

    while edges:
        for edge in edges:
            nodes.append(edge.node)
        last_edge = edges[-1]
        edges = get_graphql_sponsor_edges(settings=settings, after=last_edge.cursor)

    tiers: defaultdict[float, dict[str, SponsorEntity]] = defaultdict(dict)
    for node in nodes:
        tiers[node.tier.monthlyPriceInDollars][node.sponsorEntity.login] = (
            node.sponsorEntity
        )
    return tiers


def update_content(*, content_path: Path, new_content: Any) -> bool:
    old_content = content_path.read_text(encoding="utf-8")

    new_content = yaml.dump(new_content, sort_keys=False, width=200, allow_unicode=True)
    if old_content == new_content:
        logging.info(f"The content hasn't changed for {content_path}")
        return False
    content_path.write_text(new_content, encoding="utf-8")
    logging.info(f"Updated {content_path}")
    return True


def main() -> None:
    logging.basicConfig(level=logging.INFO)
    settings = Settings()
    logging.info(f"Using config: {settings.model_dump_json()}")
    g = Github(settings.pr_token.get_secret_value())
    repo = g.get_repo(settings.github_repository)

    tiers = get_individual_sponsors(settings=settings)
    keys = list(tiers.keys())
    keys.sort(reverse=True)
    sponsors = []
    for key in keys:
        sponsor_group = []
        for login, sponsor in tiers[key].items():
            sponsor_group.append(
                {"login": login, "avatarUrl": sponsor.avatarUrl, "url": sponsor.url}
            )
        sponsors.append(sponsor_group)
    github_sponsors = {
        "sponsors": sponsors,
    }

    # For local development
    # github_sponsors_path = Path("../docs/en/data/github_sponsors.yml")
    github_sponsors_path = Path("./docs/en/data/github_sponsors.yml")
    updated = update_content(
        content_path=github_sponsors_path, new_content=github_sponsors
    )

    if not updated:
        logging.info("The data hasn't changed, finishing.")
        return

    logging.info("Setting up GitHub Actions git user")
    subprocess.run(["git", "config", "user.name", "github-actions"], check=True)
    subprocess.run(
        ["git", "config", "user.email", "github-actions@github.com"], check=True
    )
    branch_name = f"fastapi-people-sponsors-{secrets.token_hex(4)}"
    logging.info(f"Creating a new branch {branch_name}")
    subprocess.run(["git", "checkout", "-b", branch_name], check=True)
    logging.info("Adding updated file")
    subprocess.run(
        [
            "git",
            "add",
            str(github_sponsors_path),
        ],
        check=True,
    )
    logging.info("Committing updated file")
    message = "👥 Update FastAPI People - Sponsors"
    subprocess.run(["git", "commit", "-m", message], check=True)
    logging.info("Pushing branch")
    subprocess.run(["git", "push", "origin", branch_name], check=True)
    logging.info("Creating PR")
    pr = repo.create_pull(title=message, body=message, base="master", head=branch_name)
    logging.info(f"Created PR: {pr.number}")
    logging.info("Finished")


if __name__ == "__main__":
    main()


--- scripts/topic_repos.py ---
import logging
import secrets
import subprocess
from pathlib import Path

import yaml
from github import Github
from pydantic import BaseModel, SecretStr
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    github_repository: str
    github_token: SecretStr


class Repo(BaseModel):
    name: str
    html_url: str
    stars: int
    owner_login: str
    owner_html_url: str


def main() -> None:
    logging.basicConfig(level=logging.INFO)
    settings = Settings()

    logging.info(f"Using config: {settings.model_dump_json()}")
    g = Github(settings.github_token.get_secret_value(), per_page=100)
    r = g.get_repo(settings.github_repository)
    repos = g.search_repositories(query="topic:fastapi")
    repos_list = list(repos)
    final_repos: list[Repo] = []
    for repo in repos_list[:100]:
        if repo.full_name == settings.github_repository:
            continue
        final_repos.append(
            Repo(
                name=repo.name,
                html_url=repo.html_url,
                stars=repo.stargazers_count,
                owner_login=repo.owner.login,
                owner_html_url=repo.owner.html_url,
            )
        )
    data = [repo.model_dump() for repo in final_repos]

    # Local development
    # repos_path = Path("../docs/en/data/topic_repos.yml")
    repos_path = Path("./docs/en/data/topic_repos.yml")
    repos_old_content = repos_path.read_text(encoding="utf-8")
    new_repos_content = yaml.dump(data, sort_keys=False, width=200, allow_unicode=True)
    if repos_old_content == new_repos_content:
        logging.info("The data hasn't changed. Finishing.")
        return
    repos_path.write_text(new_repos_content, encoding="utf-8")
    logging.info("Setting up GitHub Actions git user")
    subprocess.run(["git", "config", "user.name", "github-actions"], check=True)
    subprocess.run(
        ["git", "config", "user.email", "github-actions@github.com"], check=True
    )
    branch_name = f"fastapi-topic-repos-{secrets.token_hex(4)}"
    logging.info(f"Creating a new branch {branch_name}")
    subprocess.run(["git", "checkout", "-b", branch_name], check=True)
    logging.info("Adding updated file")
    subprocess.run(["git", "add", str(repos_path)], check=True)
    logging.info("Committing updated file")
    message = "👥 Update FastAPI GitHub topic repositories"
    subprocess.run(["git", "commit", "-m", message], check=True)
    logging.info("Pushing branch")
    subprocess.run(["git", "push", "origin", branch_name], check=True)
    logging.info("Creating PR")
    pr = r.create_pull(title=message, body=message, base="master", head=branch_name)
    logging.info(f"Created PR: {pr.number}")
    logging.info("Finished")


if __name__ == "__main__":
    main()


--- scripts/translate.py ---
import secrets
import subprocess
from collections.abc import Iterable
from functools import lru_cache
from os import sep as pathsep
from pathlib import Path
from typing import Annotated

import git
import typer
import yaml
from github import Github
from pydantic_ai import Agent
from rich import print

non_translated_sections = (
    f"reference{pathsep}",
    "release-notes.md",
    "fastapi-people.md",
    "external-links.md",
    "newsletter.md",
    "management-tasks.md",
    "management.md",
    "contributing.md",
)


general_prompt = """
### About literal text in this prompt

1) In the following instructions (after I say: `The above rules are in effect now`) the two characters `«` and `»` will be used to surround LITERAL TEXT, which is text or characters you shall interpret literally. The `«` and the `»` are not part of the literal text, they are the meta characters denoting it.

2) Furthermore, text surrounded by `«««` and `»»»` is a BLOCK OF LITERAL TEXT which spans multiple lines. To get its content, dedent all lines of the block until the `«««` and `»»»` are at column zero, then remove the newline (`\n`) after the `«««` and the newline before the `»»»`. The `«««` and the `»»»` are not part of the literal text block, they are the meta characters denoting it.

3) If you see backticks or any other quotes inside literal text – inside `«` and `»` –  or inside blocks of literal text – inside `«««` and `»»»` – then interpret them as literal characters, do NOT interpret them as meta characters.

The above rules are in effect now.


### Definitions of terms used in this prompt

"backtick"

    The character «`»
    Unicode U+0060 (GRAVE ACCENT)

"single backtick"

    A single backtick – «`»

"triple backticks"

    Three backticks in a row – «```»

"neutral double quote"

    The character «"»
    Unicode U+0022 (QUOTATION MARK)

"neutral single quote"

    The character «'»
    Unicode U+0027 (APOSTROPHE)

"English double typographic quotes"

    The characters «“» and «”»
    Unicode U+201C (LEFT DOUBLE QUOTATION MARK) and Unicode U+201D (RIGHT DOUBLE QUOTATION MARK)

"English single typographic quotes"

    The characters «‘» and «’»
    Unicode U+2018 (LEFT SINGLE QUOTATION MARK) and Unicode U+2019 (RIGHT SINGLE QUOTATION MARK)

"code snippet"

    Also called "inline code". Text in a Markdown document which is surrounded by single backticks. A paragraph in a Markdown document can have a more than one code snippet.

    Example:

        «««
        `i am a code snippet`
        »»»

    Example:

        «««
        `first code snippet` `second code snippet` `third code snippet`
        »»»

"code block"

    Text in a Markdown document which is surrounded by triple backticks. Spreads multiple lines.

    Example:

        «««
        ```
        Hello
        World
        ```
        »»»

    Example:

        «««
        ```python
        print("hello World")
        ```
        »»»

"HTML element"

    a HTML opening tag – e.g. «<div>» – and a HTML closing tag – e.g. «</div>» – surrounding text or other HTML elements.


### Your task

Translate an English text – the original content – to a target language.

The original content is written in Markdown, write the translation in Markdown as well.

The original content will be surrounded by triple percentage signs («%%%»). Do not include the triple percentage signs in the translation.


### Technical terms in English

For technical terms in English that don't have a common translation term, use the original term in English.


### Content of code snippets

Do not translate the content of code snippets, keep the original in English. For example, «`list`», «`dict`», keep them as is.


### Content of code blocks

Do not translate the content of code blocks, except for comments in the language which the code block uses.

Examples:

    Source (English) – The code block is a bash code example with one comment:

        «««
        ```bash
        # Print greeting
        echo "Hello, World!"
        ```
        »»»

    Result (German):

        «««
        ```bash
        # Gruß ausgeben
        echo "Hello, World!"
        ```
        »»»

    Source (English) – The code block is a console example containing HTML tags. No comments, so nothing to change here:

        «««
        ```console
        $ <font color="#4E9A06">fastapi</font> run <u style="text-decoration-style:solid">main.py</u>
        <span style="background-color:#009485"><font color="#D3D7CF"> FastAPI </font></span>  Starting server
                Searching for package file structure
        ```
        »»»

    Result (German):

        «««
        ```console
        $ <font color="#4E9A06">fastapi</font> run <u style="text-decoration-style:solid">main.py</u>
        <span style="background-color:#009485"><font color="#D3D7CF"> FastAPI </font></span>  Starting server
                Searching for package file structure
        ```
        »»»

    Source (English) – The code block is a console example containing 5 comments:

        «««
        ```console
        // Go to the home directory
        $ cd
        // Create a directory for all your code projects
        $ mkdir code
        // Enter into that code directory
        $ cd code
        // Create a directory for this project
        $ mkdir awesome-project
        // Enter into that project directory
        $ cd awesome-project
        ```
        »»»

    Result (German):

        «««
        ```console
        // Gehe zum Home-Verzeichnis
        $ cd
        // Erstelle ein Verzeichnis für alle Ihre Code-Projekte
        $ mkdir code
        // Gehe in dieses Code-Verzeichnis
        $ cd code
        // Erstelle ein Verzeichnis für dieses Projekt
        $ mkdir awesome-project
        // Gehe in dieses Projektverzeichnis
        $ cd awesome-project
        ```
        »»»

If there is an existing translation and its Mermaid diagram is in sync with the Mermaid diagram in the English source, except a few translated words, then use the Mermaid diagram of the existing translation. The human editor of the translation translated these words in the Mermaid diagram. Keep these translations, do not revert them back to the English source.

Example:

    Source (English):

        «««
        ```mermaid
        flowchart LR
            subgraph global[global env]
                harry-1[harry v1]
            end
            subgraph stone-project[philosophers-stone project]
                stone(philosophers-stone) -->|requires| harry-1
            end
        ```
        »»»

    Existing translation (German) – has three translations:

        «««
        ```mermaid
        flowchart LR
            subgraph global[globale Umgebung]
                harry-1[harry v1]
            end
            subgraph stone-project[philosophers-stone-Projekt]
                stone(philosophers-stone) -->|benötigt| harry-1
            end
        ```
        »»»

    Result (German) – you change nothing:

        «««
        ```mermaid
        flowchart LR
            subgraph global[globale Umgebung]
                harry-1[harry v1]
            end
            subgraph stone-project[philosophers-stone-Projekt]
                stone(philosophers-stone) -->|benötigt| harry-1
            end
        ```
        »»»


### Special blocks

There are special blocks of notes, tips and others that look like:

    «««
    /// note
    »»»

To translate it, keep the same line and add the translation after a vertical bar.

For example, if you were translating to Spanish, you would write:

    «««
    /// note | Nota
    »»»

Some examples in Spanish:

    Source:

        «««
        /// tip
        »»»

    Result:

        «««
        /// tip | Consejo
        »»»

    Source:

        «««
        /// details | Preview
        »»»

    Result:

        «««
        /// details | Vista previa
        »»»


### Tab blocks

There are special blocks surrounded by four slashes («////»). They mark text, which will be rendered as part of a tab in the final document. The scheme is:

    //// tab | {tab title}
    {tab content, may span many lines}
    ////

Keep everything before the vertical bar («|») as is, including the vertical bar. Translate the tab title. Translate the tab content, applying the rules you know. Keep the four block closing slashes as is.

Examples:

    Source (English):

        «««
        //// tab | Python 3.8+ non-Annotated
        Hello
        ////
        »»»

    Result (German):

        «««
        //// tab | Python 3.8+ nicht annotiert
        Hallo
        ////
        »»»

    Source (English) – Here there is nothing to translate in the tab title:

        «««
        //// tab | Linux, macOS, Windows Bash
        Hello again
        ////
        »»»

    Result (German):

        «««
        //// tab | Linux, macOS, Windows Bash
        Hallo wieder
        ////
        »»»


### Headings

Every Markdown heading in the English text (all levels) ends with a part inside curly brackets. This part denotes the hash of this heading, which is used in links to this heading. In translations, translate the heading, but do not translate this hash part, so that links do not break.

Examples of how to translate a heading:

    Source (English):

        «««
        ## Alternative API docs { #alternative-api-docs }
        »»»

    Result (Spanish):

        «««
        ## Documentación de la API alternativa { #alternative-api-docs }
        »»»

    Source (English):

        «««
        ### Example { #example }
        »»»

    Result (German):

        «««
        ### Beispiel { #example }
        »»»


### Links

Use the following rules for links (apply both to Markdown-style links ([text](url)) and to HTML-style <a> tags):

1) For relative URLs, only translate link text. Do not translate the URL or its parts

Example:

    Source (English):

        «««
        [One of the fastest Python frameworks available](#performance)
        »»»

    Result (German):

        «««
        [Eines der schnellsten verfügbaren Python-Frameworks](#performance)
        »»»

2) For absolute URLs which DO NOT start EXACTLY with «https://fastapi.tiangolo.com», only translate link text and leave the URL unchanged.

Example:

    Source (English):

        «««
        <a href="https://sqlmodel.tiangolo.com/" class="external-link" target="_blank">SQLModel docs</a>
        »»»

    Result (German):

        «««
        <a href="https://sqlmodel.tiangolo.com/" class="external-link" target="_blank">SQLModel-Dokumentation</a>
        »»»

3) For absolute URLs which DO start EXACTLY with «https://fastapi.tiangolo.com», only translate link text and change the URL by adding language code («https://fastapi.tiangolo.com/{language_code}[rest part of the url]»).

Example:

    Source (English):

        «««
        <a href="https://fastapi.tiangolo.com/tutorial/path-params/#documentation" class="external-link" target="_blank">Documentation</a>
        »»»

    Result (Spanish):

        «««
        <a href="https://fastapi.tiangolo.com/es/tutorial/path-params/#documentation" class="external-link" target="_blank">Documentación</a>
        »»»

3.1) Do not add language codes for URLs that point to static assets (e.g., images, CSS, JavaScript).

Example:

    Source (English):

        «««
        <a href="https://fastapi.tiangolo.com/img/something.jpg" class="external-link" target="_blank">Something</a>
        »»»

    Result (Spanish):

        «««
        <a href="https://fastapi.tiangolo.com/img/something.jpg" class="external-link" target="_blank">Algo</a>
        »»»

4) For internal links, only translate link text.

Example:

    Source (English):

        «««
        [Create Pull Requests](help-fastapi.md#create-a-pull-request){.internal-link target=_blank}
        »»»

    Result (German):

        «««
        [Pull Requests erzeugen](help-fastapi.md#create-a-pull-request){.internal-link target=_blank}
        »»»

5) Do not translate anchor fragments in links (the part after «#»), as they must remain the same to work correctly.

5.1) If an existing translation has a link with an anchor fragment different to the anchor fragment in the English source, then this is an error. Fix this by using the anchor fragment of the English source.

Example:

    Source (English):

        «««
        [Body - Multiple Parameters: Singular values in body](body-multiple-params.md#singular-values-in-body){.internal-link target=_blank}
        »»»

    Existing wrong translation (German) – notice the wrongly translated anchor fragment:

        «««
        [Body – Mehrere Parameter: Einfache Werte im Body](body-multiple-params.md#einzelne-werte-im-body){.internal-link target=_blank}.
        »»»

    Result (German) – you fix the anchor fragment:

        «««
        [Body – Mehrere Parameter: Einfache Werte im Body](body-multiple-params.md#singular-values-in-body){.internal-link target=_blank}.
        »»»

5.2) Do not add anchor fragments at will, even if this makes sense. If the English source has no anchor, don't add one.

Example:

    Source (English):

        «««
        Create a [virtual environment](../virtual-environments.md){.internal-link target=_blank}
        »»»

    Wrong translation (German) – Anchor added to the URL.

        «««
        Erstelle eine [virtuelle Umgebung](../virtual-environments.md#create-a-virtual-environment){.internal-link target=_blank}
        »»»

    Good translation (German) – URL stays like in the English source.

        «««
        Erstelle eine [Virtuelle Umgebung](../virtual-environments.md){.internal-link target=_blank}
        »»»


### HTML abbr elements

Translate HTML abbr elements («<abbr title="description">text</abbr>») as follows:

1) If the text surrounded by the abbr element is an abbreviation (the text may be surrounded by further HTML or Markdown markup or quotes, for example «<code>text</code>» or «`text`» or «"text"», ignore that further markup when deciding if the text is an abbreviation), and if the description (the text inside the title attribute) contains the full phrase for this abbreviation, then append a dash («–») to the full phrase, followed by the translation of the full phrase.

Conversion scheme:

    Source (English):

        <abbr title="{full phrase}">{abbreviation}</abbr>

    Result:

        <abbr title="{full phrase} – {translation of full phrase}">{abbreviation}</abbr>

Examples:

    Source (English):

        «««
        <abbr title="Internet of Things">IoT</abbr>
        <abbr title="Central Processing Unit">CPU</abbr>
        <abbr title="too long; didn't read"><strong>TL;DR:</strong></abbr>
        »»»

    Result (German):

        «««
        <abbr title="Internet of Things – Internet der Dinge">IoT</abbr>
        <abbr title="Central Processing Unit – Zentrale Verarbeitungseinheit">CPU</abbr>
        <abbr title="too long; didn't read – zu lang; hab's nicht gelesen"><strong>TL;DR:</strong></abbr>
        »»»

1.1) If the language to which you translate mostly uses the letters of the ASCII char set (for example Spanish, French, German, but not Russian, Chinese) and if the translation of the full phrase is identical to, or starts with the same letters as the original full phrase, then only give the translation of the full phrase.

Conversion scheme:

    Source (English):

        <abbr title="{full phrase}">{abbreviation}</abbr>

    Result:

        <abbr title="{translation of full phrase}">{abbreviation}</abbr>

Examples:

    Source (English):

        «««
        <abbr title="JSON Web Tokens">JWT</abbr>
        <abbr title="Enumeration">Enum</abbr>
        <abbr title="Asynchronous Server Gateway Interface">ASGI</abbr>
        »»»

    Result (German):

        «««
        <abbr title="JSON Web Tokens">JWT</abbr>
        <abbr title="Enumeration">Enum</abbr>
        <abbr title="Asynchrones Server-Gateway-Interface">ASGI</abbr>
        »»»

2) If the description is not a full phrase for an abbreviation which the abbr element surrounds, but some other information, then just translate the description.

Conversion scheme:

    Source (English):

        <abbr title="{description}">{text}</abbr>

    Result:

        <abbr title="{translation of description}">{translation of text}</abbr>

Examples:

    Source (English):

        «««
        <abbr title="also known as: endpoints, routes">path</abbr>
        <abbr title="a program that checks for code errors">linter</abbr>
        <abbr title="converting the string that comes from an HTTP request into Python data">parsing</abbr>
        <abbr title="before 2023-03">0.95.0</abbr>
        <abbr title="2023-08-26">at the time of writing this</abbr>
        »»»

    Result (German):

        «««
        <abbr title="auch bekannt als: Endpunkte, Routen">Pfad</abbr>
        <abbr title="Programm das auf Fehler im Code prüft">Linter</abbr>
        <abbr title="Konvertieren des Strings eines HTTP-Requests in Python-Daten">Parsen</abbr>
        <abbr title="vor 2023-03">0.95.0</abbr>
        <abbr title="2023-08-26">zum Zeitpunkt als das hier geschrieben wurde</abbr>
        »»»


3) If the text surrounded by the abbr element is an abbreviation and the description contains both the full phrase for that abbreviation, and other information, separated by a colon («:»), then append a dash («–») and the translation of the full phrase to the original full phrase and translate the other information.

Conversion scheme:

    Source (English):

        <abbr title="{full phrase}: {other information}">{abbreviation}</abbr>

    Result:

        <abbr title="{full phrase} – {translation of full phrase}: {translation of other information}">{abbreviation}</abbr>

Examples:

    Source (English):

        «««
        <abbr title="Input/Output: disk reading or writing, network communication.">I/O</abbr>
        <abbr title="Content Delivery Network: service, that provides static files.">CDN</abbr>
        <abbr title="Integrated Development Environment: similar to a code editor">IDE</abbr>
        »»»

    Result (German):

        «««
        <abbr title="Input/Output – Eingabe/Ausgabe: Lesen oder Schreiben auf der Festplatte, Netzwerkkommunikation.">I/O</abbr>
        <abbr title="Content Delivery Network – Inhalte auslieferndes Netzwerk: Dienst, der statische Dateien bereitstellt.">CDN</abbr>
        <abbr title="Integrated Development Environment – Integrierte Entwicklungsumgebung: Ähnlich einem Code-Editor">IDE</abbr>
        »»»

3.1) Like in rule 2.1, you can leave the original full phrase away, if the translated full phrase is identical or starts with the same letters as the original full phrase.

Conversion scheme:

    Source (English):

        <abbr title="{full phrase}: {information}">{abbreviation}</abbr>

    Result:

        <abbr title="{translation of full phrase}: {translation of information}">{abbreviation}</abbr>

Example:

    Source (English):

        «««
        <abbr title="Object Relational Mapper: a fancy term for a library where some classes represent SQL tables and instances represent rows in those tables">ORM</abbr>
        »»»

    Result (German):

        «««
        <abbr title="Objektrelationaler Mapper: Ein Fachbegriff für eine Bibliothek, in der einige Klassen SQL-Tabellen und Instanzen Zeilen in diesen Tabellen darstellen">ORM</abbr>
        »»»

4) If there is an existing translation, and it has ADDITIONAL abbr elements in a sentence, and these additional abbr elements do not exist in the related sentence in the English text, then KEEP those additional abbr elements in the translation. Do not remove them. Except when you remove the whole sentence from the translation, because the whole sentence was removed from the English text, then also remove the abbr element. The reasoning for this rule is, that such additional abbr elements are manually added by the human editor of the translation, in order to translate or explain an English word to the human readers of the translation. These additional abbr elements would not make sense in the English text, but they do make sense in the translation. So keep them in the translation, even though they are not part of the English text. This rule only applies to abbr elements.

5) Apply above rules also when there is an existing translation! Make sure that all title attributes in abbr elements get properly translated or updated, using the schemes given above. However, leave the ADDITIONAL abbr's from rule 4 alone. Do not change their formatting or content.

"""

app = typer.Typer()


@lru_cache
def get_langs() -> dict[str, str]:
    return yaml.safe_load(Path("docs/language_names.yml").read_text(encoding="utf-8"))


def generate_lang_path(*, lang: str, path: Path) -> Path:
    en_docs_path = Path("docs/en/docs")
    assert str(path).startswith(str(en_docs_path)), (
        f"Path must be inside {en_docs_path}"
    )
    lang_docs_path = Path(f"docs/{lang}/docs")
    out_path = Path(str(path).replace(str(en_docs_path), str(lang_docs_path)))
    return out_path


def generate_en_path(*, lang: str, path: Path) -> Path:
    en_docs_path = Path("docs/en/docs")
    assert not str(path).startswith(str(en_docs_path)), (
        f"Path must not be inside {en_docs_path}"
    )
    lang_docs_path = Path(f"docs/{lang}/docs")
    out_path = Path(str(path).replace(str(lang_docs_path), str(en_docs_path)))
    return out_path


@app.command()
def translate_page(
    *,
    language: Annotated[str, typer.Option(envvar="LANGUAGE")],
    en_path: Annotated[Path, typer.Option(envvar="EN_PATH")],
) -> None:
    assert language != "en", (
        "`en` is the source language, choose another language as translation target"
    )
    langs = get_langs()
    language_name = langs[language]
    lang_path = Path(f"docs/{language}")
    lang_path.mkdir(exist_ok=True)
    lang_prompt_path = lang_path / "llm-prompt.md"
    assert lang_prompt_path.exists(), f"Prompt file not found: {lang_prompt_path}"
    lang_prompt_content = lang_prompt_path.read_text(encoding="utf-8")

    en_docs_path = Path("docs/en/docs")
    assert str(en_path).startswith(str(en_docs_path)), (
        f"Path must be inside {en_docs_path}"
    )
    out_path = generate_lang_path(lang=language, path=en_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    original_content = en_path.read_text(encoding="utf-8")
    old_translation: str | None = None
    if out_path.exists():
        print(f"Found existing translation: {out_path}")
        old_translation = out_path.read_text(encoding="utf-8")
    print(f"Translating {en_path} to {language} ({language_name})")
    agent = Agent("openai:gpt-5")

    prompt_segments = [
        general_prompt,
        lang_prompt_content,
    ]
    if old_translation:
        prompt_segments.extend(
            [
                "There is an existing previous translation for the original English content, that may be outdated.",
                "Update the translation only where necessary:",
                "- If the original English content has added parts, also add these parts to the translation.",
                "- If the original English content has removed parts, also remove them from the translation, unless you were instructed earlier to not do that in specific cases.",
                "- If parts of the original English content have changed, also change those parts in the translation.",
                "- If the previous translation violates current instructions, update it.",
                "- Otherwise, preserve the original translation LINE-BY-LINE, AS-IS.",
                "Do not:",
                "- rephrase or rewrite correct lines just to improve the style.",
                "- add or remove line breaks, unless the original English content changed.",
                "- change formatting or whitespace unless absolutely required.",
                "Only change what must be changed. The goal is to minimize diffs for easier human review.",
                "UNLESS you were instructed earlier to behave different, there MUST NOT be whole sentences or partial sentences in the updated translation, which are not in the original English content, and there MUST NOT be whole sentences or partial sentences in the original English content, which are not in the updated translation. Remember: the updated translation shall be IN SYNC with the original English content.",
                "Previous translation:",
                f"%%%\n{old_translation}%%%",
            ]
        )
    prompt_segments.extend(
        [
            f"Translate to {language} ({language_name}).",
            "Original content:",
            f"%%%\n{original_content}%%%",
        ]
    )
    prompt = "\n\n".join(prompt_segments)
    print(f"Running agent for {out_path}")
    result = agent.run_sync(prompt)
    out_content = f"{result.output.strip()}\n"
    print(f"Saving translation to {out_path}")
    out_path.write_text(out_content, encoding="utf-8", newline="\n")


def iter_all_en_paths() -> Iterable[Path]:
    """
    Iterate on the markdown files to translate in order of priority.
    """
    first_dirs = [
        Path("docs/en/docs/learn"),
        Path("docs/en/docs/tutorial"),
        Path("docs/en/docs/advanced"),
        Path("docs/en/docs/about"),
        Path("docs/en/docs/how-to"),
    ]
    first_parent = Path("docs/en/docs")
    yield from first_parent.glob("*.md")
    for dir_path in first_dirs:
        yield from dir_path.rglob("*.md")
    first_dirs_str = tuple(str(d) for d in first_dirs)
    for path in Path("docs/en/docs").rglob("*.md"):
        if str(path).startswith(first_dirs_str):
            continue
        if path.parent == first_parent:
            continue
        yield path


def iter_en_paths_to_translate() -> Iterable[Path]:
    en_docs_root = Path("docs/en/docs/")
    for path in iter_all_en_paths():
        relpath = path.relative_to(en_docs_root)
        if not str(relpath).startswith(non_translated_sections):
            yield path


@app.command()
def translate_lang(language: Annotated[str, typer.Option(envvar="LANGUAGE")]) -> None:
    paths_to_process = list(iter_en_paths_to_translate())
    print("Original paths:")
    for p in paths_to_process:
        print(f"  - {p}")
    print(f"Total original paths: {len(paths_to_process)}")
    missing_paths: list[Path] = []
    skipped_paths: list[Path] = []
    for p in paths_to_process:
        lang_path = generate_lang_path(lang=language, path=p)
        if lang_path.exists():
            skipped_paths.append(p)
            continue
        missing_paths.append(p)
    print("Paths to skip:")
    for p in skipped_paths:
        print(f"  - {p}")
    print(f"Total paths to skip: {len(skipped_paths)}")
    print("Paths to process:")
    for p in missing_paths:
        print(f"  - {p}")
    print(f"Total paths to process: {len(missing_paths)}")
    for p in missing_paths:
        print(f"Translating: {p}")
        translate_page(language="es", en_path=p)
        print(f"Done translating: {p}")


@app.command()
def list_removable(language: str) -> list[Path]:
    removable_paths: list[Path] = []
    lang_paths = Path(f"docs/{language}").rglob("*.md")
    for path in lang_paths:
        en_path = generate_en_path(lang=language, path=path)
        if not en_path.exists():
            removable_paths.append(path)
    print(removable_paths)
    return removable_paths


@app.command()
def list_all_removable() -> list[Path]:
    all_removable_paths: list[Path] = []
    langs = get_langs()
    for lang in langs:
        if lang == "en":
            continue
        removable_paths = list_removable(lang)
        all_removable_paths.extend(removable_paths)
    print(all_removable_paths)
    return all_removable_paths


@app.command()
def remove_removable(language: str) -> None:
    removable_paths = list_removable(language)
    for path in removable_paths:
        path.unlink()
        print(f"Removed: {path}")
    print("Done removing all removable paths")


@app.command()
def remove_all_removable() -> None:
    all_removable = list_all_removable()
    for removable_path in all_removable:
        removable_path.unlink()
        print(f"Removed: {removable_path}")
    print("Done removing all removable paths")


@app.command()
def list_missing(language: str) -> list[Path]:
    missing_paths: list[Path] = []
    en_lang_paths = list(iter_en_paths_to_translate())
    for path in en_lang_paths:
        lang_path = generate_lang_path(lang=language, path=path)
        if not lang_path.exists():
            missing_paths.append(path)
    print(missing_paths)
    return missing_paths


@app.command()
def list_outdated(language: str) -> list[Path]:
    dir_path = Path(__file__).absolute().parent.parent
    repo = git.Repo(dir_path)

    outdated_paths: list[Path] = []
    en_lang_paths = list(iter_en_paths_to_translate())
    for path in en_lang_paths:
        lang_path = generate_lang_path(lang=language, path=path)
        if not lang_path.exists():
            continue
        en_commit_datetime = list(repo.iter_commits(paths=path, max_count=1))[
            0
        ].committed_datetime
        lang_commit_datetime = list(repo.iter_commits(paths=lang_path, max_count=1))[
            0
        ].committed_datetime
        if lang_commit_datetime < en_commit_datetime:
            outdated_paths.append(path)
    print(outdated_paths)
    return outdated_paths


@app.command()
def update_outdated(language: Annotated[str, typer.Option(envvar="LANGUAGE")]) -> None:
    outdated_paths = list_outdated(language)
    for path in outdated_paths:
        print(f"Updating lang: {language} path: {path}")
        translate_page(language=language, en_path=path)
        print(f"Done updating: {path}")
    print("Done updating all outdated paths")


@app.command()
def add_missing(language: Annotated[str, typer.Option(envvar="LANGUAGE")]) -> None:
    missing_paths = list_missing(language)
    for path in missing_paths:
        print(f"Adding lang: {language} path: {path}")
        translate_page(language=language, en_path=path)
        print(f"Done adding: {path}")
    print("Done adding all missing paths")


@app.command()
def update_and_add(language: Annotated[str, typer.Option(envvar="LANGUAGE")]) -> None:
    print(f"Updating outdated translations for {language}")
    update_outdated(language=language)
    print(f"Adding missing translations for {language}")
    add_missing(language=language)
    print(f"Done updating and adding for {language}")


@app.command()
def make_pr(
    *,
    language: Annotated[str | None, typer.Option(envvar="LANGUAGE")] = None,
    github_token: Annotated[str, typer.Option(envvar="GITHUB_TOKEN")],
    github_repository: Annotated[str, typer.Option(envvar="GITHUB_REPOSITORY")],
) -> None:
    print("Setting up GitHub Actions git user")
    repo = git.Repo(Path(__file__).absolute().parent.parent)
    if not repo.is_dirty(untracked_files=True):
        print("Repository is clean, no changes to commit")
        return
    subprocess.run(["git", "config", "user.name", "github-actions"], check=True)
    subprocess.run(
        ["git", "config", "user.email", "github-actions@github.com"], check=True
    )
    branch_name = "translate"
    if language:
        branch_name += f"-{language}"
    branch_name += f"-{secrets.token_hex(4)}"
    print(f"Creating a new branch {branch_name}")
    subprocess.run(["git", "checkout", "-b", branch_name], check=True)
    print("Adding updated files")
    git_path = Path("docs")
    subprocess.run(["git", "add", str(git_path)], check=True)
    print("Committing updated file")
    message = "🌐 Update translations"
    if language:
        message += f" for {language}"
    subprocess.run(["git", "commit", "-m", message], check=True)
    print("Pushing branch")
    subprocess.run(["git", "push", "origin", branch_name], check=True)
    print("Creating PR")
    g = Github(github_token)
    gh_repo = g.get_repo(github_repository)
    pr = gh_repo.create_pull(
        title=message, body=message, base="master", head=branch_name
    )
    print(f"Created PR: {pr.number}")
    print("Finished")


if __name__ == "__main__":
    app()


--- scripts/playwright/cookie_param_models/image01.py ---
import subprocess
import time

import httpx
from playwright.sync_api import Playwright, sync_playwright


# Run playwright codegen to generate the code below, copy paste the sections in run()
def run(playwright: Playwright) -> None:
    browser = playwright.chromium.launch(headless=False)
    # Update the viewport manually
    context = browser.new_context(viewport={"width": 960, "height": 1080})
    browser = playwright.chromium.launch(headless=False)
    context = browser.new_context()
    page = context.new_page()
    page.goto("http://localhost:8000/docs")
    page.get_by_role("link", name="/items/").click()
    # Manually add the screenshot
    page.screenshot(path="docs/en/docs/img/tutorial/cookie-param-models/image01.png")

    # ---------------------
    context.close()
    browser.close()


process = subprocess.Popen(
    ["fastapi", "run", "docs_src/cookie_param_models/tutorial001.py"]
)
try:
    for _ in range(3):
        try:
            response = httpx.get("http://localhost:8000/docs")
        except httpx.ConnectError:
            time.sleep(1)
            break
    with sync_playwright() as playwright:
        run(playwright)
finally:
    process.terminate()


--- scripts/playwright/header_param_models/image01.py ---
import subprocess
import time

import httpx
from playwright.sync_api import Playwright, sync_playwright


# Run playwright codegen to generate the code below, copy paste the sections in run()
def run(playwright: Playwright) -> None:
    browser = playwright.chromium.launch(headless=False)
    # Update the viewport manually
    context = browser.new_context(viewport={"width": 960, "height": 1080})
    page = context.new_page()
    page.goto("http://localhost:8000/docs")
    page.get_by_role("button", name="GET /items/ Read Items").click()
    page.get_by_role("button", name="Try it out").click()
    # Manually add the screenshot
    page.screenshot(path="docs/en/docs/img/tutorial/header-param-models/image01.png")

    # ---------------------
    context.close()
    browser.close()


process = subprocess.Popen(
    ["fastapi", "run", "docs_src/header_param_models/tutorial001.py"]
)
try:
    for _ in range(3):
        try:
            response = httpx.get("http://localhost:8000/docs")
        except httpx.ConnectError:
            time.sleep(1)
            break
    with sync_playwright() as playwright:
        run(playwright)
finally:
    process.terminate()


--- scripts/playwright/query_param_models/image01.py ---
import subprocess
import time

import httpx
from playwright.sync_api import Playwright, sync_playwright


# Run playwright codegen to generate the code below, copy paste the sections in run()
def run(playwright: Playwright) -> None:
    browser = playwright.chromium.launch(headless=False)
    # Update the viewport manually
    context = browser.new_context(viewport={"width": 960, "height": 1080})
    browser = playwright.chromium.launch(headless=False)
    context = browser.new_context()
    page = context.new_page()
    page.goto("http://localhost:8000/docs")
    page.get_by_role("button", name="GET /items/ Read Items").click()
    page.get_by_role("button", name="Try it out").click()
    page.get_by_role("heading", name="Servers").click()
    # Manually add the screenshot
    page.screenshot(path="docs/en/docs/img/tutorial/query-param-models/image01.png")

    # ---------------------
    context.close()
    browser.close()


process = subprocess.Popen(
    ["fastapi", "run", "docs_src/query_param_models/tutorial001.py"]
)
try:
    for _ in range(3):
        try:
            response = httpx.get("http://localhost:8000/docs")
        except httpx.ConnectError:
            time.sleep(1)
            break
    with sync_playwright() as playwright:
        run(playwright)
finally:
    process.terminate()


--- tests/__init__.py ---


--- tests/main.py ---
import http
from typing import FrozenSet, List, Optional

from fastapi import FastAPI, Path, Query

external_docs = {
    "description": "External API documentation.",
    "url": "https://docs.example.com/api-general",
}

app = FastAPI(openapi_external_docs=external_docs)


@app.api_route("/api_route")
def non_operation():
    return {"message": "Hello World"}


def non_decorated_route():
    return {"message": "Hello World"}


app.add_api_route("/non_decorated_route", non_decorated_route)


@app.get("/text")
def get_text():
    return "Hello World"


@app.get("/path/{item_id}")
def get_id(item_id):
    return item_id


@app.get("/path/str/{item_id}")
def get_str_id(item_id: str):
    return item_id


@app.get("/path/int/{item_id}")
def get_int_id(item_id: int):
    return item_id


@app.get("/path/float/{item_id}")
def get_float_id(item_id: float):
    return item_id


@app.get("/path/bool/{item_id}")
def get_bool_id(item_id: bool):
    return item_id


@app.get("/path/param/{item_id}")
def get_path_param_id(item_id: Optional[str] = Path()):
    return item_id


@app.get("/path/param-minlength/{item_id}")
def get_path_param_min_length(item_id: str = Path(min_length=3)):
    return item_id


@app.get("/path/param-maxlength/{item_id}")
def get_path_param_max_length(item_id: str = Path(max_length=3)):
    return item_id


@app.get("/path/param-min_maxlength/{item_id}")
def get_path_param_min_max_length(item_id: str = Path(max_length=3, min_length=2)):
    return item_id


@app.get("/path/param-gt/{item_id}")
def get_path_param_gt(item_id: float = Path(gt=3)):
    return item_id


@app.get("/path/param-gt0/{item_id}")
def get_path_param_gt0(item_id: float = Path(gt=0)):
    return item_id


@app.get("/path/param-ge/{item_id}")
def get_path_param_ge(item_id: float = Path(ge=3)):
    return item_id


@app.get("/path/param-lt/{item_id}")
def get_path_param_lt(item_id: float = Path(lt=3)):
    return item_id


@app.get("/path/param-lt0/{item_id}")
def get_path_param_lt0(item_id: float = Path(lt=0)):
    return item_id


@app.get("/path/param-le/{item_id}")
def get_path_param_le(item_id: float = Path(le=3)):
    return item_id


@app.get("/path/param-lt-gt/{item_id}")
def get_path_param_lt_gt(item_id: float = Path(lt=3, gt=1)):
    return item_id


@app.get("/path/param-le-ge/{item_id}")
def get_path_param_le_ge(item_id: float = Path(le=3, ge=1)):
    return item_id


@app.get("/path/param-lt-int/{item_id}")
def get_path_param_lt_int(item_id: int = Path(lt=3)):
    return item_id


@app.get("/path/param-gt-int/{item_id}")
def get_path_param_gt_int(item_id: int = Path(gt=3)):
    return item_id


@app.get("/path/param-le-int/{item_id}")
def get_path_param_le_int(item_id: int = Path(le=3)):
    return item_id


@app.get("/path/param-ge-int/{item_id}")
def get_path_param_ge_int(item_id: int = Path(ge=3)):
    return item_id


@app.get("/path/param-lt-gt-int/{item_id}")
def get_path_param_lt_gt_int(item_id: int = Path(lt=3, gt=1)):
    return item_id


@app.get("/path/param-le-ge-int/{item_id}")
def get_path_param_le_ge_int(item_id: int = Path(le=3, ge=1)):
    return item_id


@app.get("/query")
def get_query(query):
    return f"foo bar {query}"


@app.get("/query/optional")
def get_query_optional(query=None):
    if query is None:
        return "foo bar"
    return f"foo bar {query}"


@app.get("/query/int")
def get_query_type(query: int):
    return f"foo bar {query}"


@app.get("/query/int/optional")
def get_query_type_optional(query: Optional[int] = None):
    if query is None:
        return "foo bar"
    return f"foo bar {query}"


@app.get("/query/int/default")
def get_query_type_int_default(query: int = 10):
    return f"foo bar {query}"


@app.get("/query/param")
def get_query_param(query=Query(default=None)):
    if query is None:
        return "foo bar"
    return f"foo bar {query}"


@app.get("/query/param-required")
def get_query_param_required(query=Query()):
    return f"foo bar {query}"


@app.get("/query/param-required/int")
def get_query_param_required_type(query: int = Query()):
    return f"foo bar {query}"


@app.get("/enum-status-code", status_code=http.HTTPStatus.CREATED)
def get_enum_status_code():
    return "foo bar"


@app.get("/query/frozenset")
def get_query_type_frozenset(query: FrozenSet[int] = Query(...)):
    return ",".join(map(str, sorted(query)))


@app.get("/query/list")
def get_query_list(device_ids: List[int] = Query()) -> List[int]:
    return device_ids


@app.get("/query/list-default")
def get_query_list_default(device_ids: List[int] = Query(default=[])) -> List[int]:
    return device_ids


--- tests/test_additional_properties.py ---
from typing import Dict

from fastapi import FastAPI
from fastapi.testclient import TestClient
from pydantic import BaseModel

app = FastAPI()


class Items(BaseModel):
    items: Dict[str, int]


@app.post("/foo")
def foo(items: Items):
    return items.items


client = TestClient(app)


def test_additional_properties_post():
    response = client.post("/foo", json={"items": {"foo": 1, "bar": 2}})
    assert response.status_code == 200, response.text
    assert response.json() == {"foo": 1, "bar": 2}


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/foo": {
                "post": {
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                    "summary": "Foo",
                    "operationId": "foo_foo_post",
                    "requestBody": {
                        "content": {
                            "application/json": {
                                "schema": {"$ref": "#/components/schemas/Items"}
                            }
                        },
                        "required": True,
                    },
                }
            }
        },
        "components": {
            "schemas": {
                "Items": {
                    "title": "Items",
                    "required": ["items"],
                    "type": "object",
                    "properties": {
                        "items": {
                            "title": "Items",
                            "type": "object",
                            "additionalProperties": {"type": "integer"},
                        }
                    },
                },
                "ValidationError": {
                    "title": "ValidationError",
                    "required": ["loc", "msg", "type"],
                    "type": "object",
                    "properties": {
                        "loc": {
                            "title": "Location",
                            "type": "array",
                            "items": {
                                "anyOf": [{"type": "string"}, {"type": "integer"}]
                            },
                        },
                        "msg": {"title": "Message", "type": "string"},
                        "type": {"title": "Error Type", "type": "string"},
                    },
                },
                "HTTPValidationError": {
                    "title": "HTTPValidationError",
                    "type": "object",
                    "properties": {
                        "detail": {
                            "title": "Detail",
                            "type": "array",
                            "items": {"$ref": "#/components/schemas/ValidationError"},
                        }
                    },
                },
            }
        },
    }


--- tests/test_additional_properties_bool.py ---
from typing import Union

from dirty_equals import IsDict
from fastapi import FastAPI
from fastapi._compat import PYDANTIC_V2
from fastapi.testclient import TestClient
from pydantic import BaseModel, ConfigDict


class FooBaseModel(BaseModel):
    if PYDANTIC_V2:
        model_config = ConfigDict(extra="forbid")
    else:

        class Config:
            extra = "forbid"


class Foo(FooBaseModel):
    pass


app = FastAPI()


@app.post("/")
async def post(
    foo: Union[Foo, None] = None,
):
    return foo


client = TestClient(app)


def test_call_invalid():
    response = client.post("/", json={"foo": {"bar": "baz"}})
    assert response.status_code == 422


def test_call_valid():
    response = client.post("/", json={})
    assert response.status_code == 200
    assert response.json() == {}


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/": {
                "post": {
                    "summary": "Post",
                    "operationId": "post__post",
                    "requestBody": {
                        "content": {
                            "application/json": {
                                "schema": IsDict(
                                    {
                                        "anyOf": [
                                            {"$ref": "#/components/schemas/Foo"},
                                            {"type": "null"},
                                        ],
                                        "title": "Foo",
                                    }
                                )
                                | IsDict(
                                    # TODO: remove when deprecating Pydantic v1
                                    {"$ref": "#/components/schemas/Foo"}
                                )
                            }
                        }
                    },
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                }
            }
        },
        "components": {
            "schemas": {
                "Foo": {
                    "properties": {},
                    "additionalProperties": False,
                    "type": "object",
                    "title": "Foo",
                },
                "HTTPValidationError": {
                    "properties": {
                        "detail": {
                            "items": {"$ref": "#/components/schemas/ValidationError"},
                            "type": "array",
                            "title": "Detail",
                        }
                    },
                    "type": "object",
                    "title": "HTTPValidationError",
                },
                "ValidationError": {
                    "properties": {
                        "loc": {
                            "items": {
                                "anyOf": [{"type": "string"}, {"type": "integer"}]
                            },
                            "type": "array",
                            "title": "Location",
                        },
                        "msg": {"type": "string", "title": "Message"},
                        "type": {"type": "string", "title": "Error Type"},
                    },
                    "type": "object",
                    "required": ["loc", "msg", "type"],
                    "title": "ValidationError",
                },
            }
        },
    }


--- tests/test_additional_response_extra.py ---
from fastapi import APIRouter, FastAPI
from fastapi.testclient import TestClient

router = APIRouter()

sub_router = APIRouter()

app = FastAPI()


@sub_router.get("/")
def read_item():
    return {"id": "foo"}


router.include_router(sub_router, prefix="/items")

app.include_router(router)

client = TestClient(app)


def test_path_operation():
    response = client.get("/items/")
    assert response.status_code == 200, response.text
    assert response.json() == {"id": "foo"}


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/items/": {
                "get": {
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        }
                    },
                    "summary": "Read Item",
                    "operationId": "read_item_items__get",
                }
            }
        },
    }


--- tests/test_additional_responses_bad.py ---
import pytest
from fastapi import FastAPI
from fastapi.testclient import TestClient

app = FastAPI()


@app.get("/a", responses={"hello": {"description": "Not a valid additional response"}})
async def a():
    pass  # pragma: no cover


openapi_schema = {
    "openapi": "3.1.0",
    "info": {"title": "FastAPI", "version": "0.1.0"},
    "paths": {
        "/a": {
            "get": {
                "responses": {
                    # this is how one would imagine the openapi schema to be
                    # but since the key is not valid, openapi.utils.get_openapi will raise ValueError
                    "hello": {"description": "Not a valid additional response"},
                    "200": {
                        "description": "Successful Response",
                        "content": {"application/json": {"schema": {}}},
                    },
                },
                "summary": "A",
                "operationId": "a_a_get",
            }
        }
    },
}

client = TestClient(app)


def test_openapi_schema():
    with pytest.raises(ValueError):
        client.get("/openapi.json")


--- tests/test_additional_responses_custom_model_in_callback.py ---
from dirty_equals import IsDict
from fastapi import APIRouter, FastAPI
from fastapi.testclient import TestClient
from pydantic import BaseModel, HttpUrl
from starlette.responses import JSONResponse


class CustomModel(BaseModel):
    a: int


app = FastAPI()

callback_router = APIRouter(default_response_class=JSONResponse)


@callback_router.get(
    "{$callback_url}/callback/", responses={400: {"model": CustomModel}}
)
def callback_route():
    pass  # pragma: no cover


@app.post("/", callbacks=callback_router.routes)
def main_route(callback_url: HttpUrl):
    pass  # pragma: no cover


client = TestClient(app)


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/": {
                "post": {
                    "summary": "Main Route",
                    "operationId": "main_route__post",
                    "parameters": [
                        {
                            "required": True,
                            "schema": IsDict(
                                {
                                    "title": "Callback Url",
                                    "minLength": 1,
                                    "type": "string",
                                    "format": "uri",
                                }
                            )
                            # TODO: remove when deprecating Pydantic v1
                            | IsDict(
                                {
                                    "title": "Callback Url",
                                    "maxLength": 2083,
                                    "minLength": 1,
                                    "type": "string",
                                    "format": "uri",
                                }
                            ),
                            "name": "callback_url",
                            "in": "query",
                        }
                    ],
                    "responses": {
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                    },
                    "callbacks": {
                        "callback_route": {
                            "{$callback_url}/callback/": {
                                "get": {
                                    "summary": "Callback Route",
                                    "operationId": "callback_route__callback_url__callback__get",
                                    "responses": {
                                        "400": {
                                            "content": {
                                                "application/json": {
                                                    "schema": {
                                                        "$ref": "#/components/schemas/CustomModel"
                                                    }
                                                }
                                            },
                                            "description": "Bad Request",
                                        },
                                        "200": {
                                            "description": "Successful Response",
                                            "content": {
                                                "application/json": {"schema": {}}
                                            },
                                        },
                                    },
                                }
                            }
                        }
                    },
                }
            }
        },
        "components": {
            "schemas": {
                "CustomModel": {
                    "title": "CustomModel",
                    "required": ["a"],
                    "type": "object",
                    "properties": {"a": {"title": "A", "type": "integer"}},
                },
                "HTTPValidationError": {
                    "title": "HTTPValidationError",
                    "type": "object",
                    "properties": {
                        "detail": {
                            "title": "Detail",
                            "type": "array",
                            "items": {"$ref": "#/components/schemas/ValidationError"},
                        }
                    },
                },
                "ValidationError": {
                    "title": "ValidationError",
                    "required": ["loc", "msg", "type"],
                    "type": "object",
                    "properties": {
                        "loc": {
                            "title": "Location",
                            "type": "array",
                            "items": {
                                "anyOf": [{"type": "string"}, {"type": "integer"}]
                            },
                        },
                        "msg": {"title": "Message", "type": "string"},
                        "type": {"title": "Error Type", "type": "string"},
                    },
                },
            }
        },
    }


--- tests/test_additional_responses_custom_validationerror.py ---
import typing

from fastapi import FastAPI
from fastapi.responses import JSONResponse
from fastapi.testclient import TestClient
from pydantic import BaseModel

app = FastAPI()


class JsonApiResponse(JSONResponse):
    media_type = "application/vnd.api+json"


class Error(BaseModel):
    status: str
    title: str


class JsonApiError(BaseModel):
    errors: typing.List[Error]


@app.get(
    "/a/{id}",
    response_class=JsonApiResponse,
    responses={422: {"description": "Error", "model": JsonApiError}},
)
async def a(id):
    pass  # pragma: no cover


client = TestClient(app)


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/a/{id}": {
                "get": {
                    "responses": {
                        "422": {
                            "description": "Error",
                            "content": {
                                "application/vnd.api+json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/JsonApiError"
                                    }
                                }
                            },
                        },
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/vnd.api+json": {"schema": {}}},
                        },
                    },
                    "summary": "A",
                    "operationId": "a_a__id__get",
                    "parameters": [
                        {
                            "required": True,
                            "schema": {"title": "Id"},
                            "name": "id",
                            "in": "path",
                        }
                    ],
                }
            }
        },
        "components": {
            "schemas": {
                "Error": {
                    "title": "Error",
                    "required": ["status", "title"],
                    "type": "object",
                    "properties": {
                        "status": {"title": "Status", "type": "string"},
                        "title": {"title": "Title", "type": "string"},
                    },
                },
                "JsonApiError": {
                    "title": "JsonApiError",
                    "required": ["errors"],
                    "type": "object",
                    "properties": {
                        "errors": {
                            "title": "Errors",
                            "type": "array",
                            "items": {"$ref": "#/components/schemas/Error"},
                        }
                    },
                },
            }
        },
    }


--- tests/test_additional_responses_default_validationerror.py ---
from fastapi import FastAPI
from fastapi.testclient import TestClient

app = FastAPI()


@app.get("/a/{id}")
async def a(id):
    pass  # pragma: no cover


client = TestClient(app)


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/a/{id}": {
                "get": {
                    "responses": {
                        "422": {
                            "description": "Validation Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/HTTPValidationError"
                                    }
                                }
                            },
                        },
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                    },
                    "summary": "A",
                    "operationId": "a_a__id__get",
                    "parameters": [
                        {
                            "required": True,
                            "schema": {"title": "Id"},
                            "name": "id",
                            "in": "path",
                        }
                    ],
                }
            }
        },
        "components": {
            "schemas": {
                "ValidationError": {
                    "title": "ValidationError",
                    "required": ["loc", "msg", "type"],
                    "type": "object",
                    "properties": {
                        "loc": {
                            "title": "Location",
                            "type": "array",
                            "items": {
                                "anyOf": [{"type": "string"}, {"type": "integer"}]
                            },
                        },
                        "msg": {"title": "Message", "type": "string"},
                        "type": {"title": "Error Type", "type": "string"},
                    },
                },
                "HTTPValidationError": {
                    "title": "HTTPValidationError",
                    "type": "object",
                    "properties": {
                        "detail": {
                            "title": "Detail",
                            "type": "array",
                            "items": {"$ref": "#/components/schemas/ValidationError"},
                        }
                    },
                },
            }
        },
    }


--- tests/test_additional_responses_router.py ---
from fastapi import APIRouter, FastAPI
from fastapi.testclient import TestClient
from pydantic import BaseModel


class ResponseModel(BaseModel):
    message: str


app = FastAPI()
router = APIRouter()


@router.get("/a", responses={501: {"description": "Error 1"}})
async def a():
    return "a"


@router.get(
    "/b",
    responses={
        502: {"description": "Error 2"},
        "4XX": {"description": "Error with range, upper"},
    },
)
async def b():
    return "b"


@router.get(
    "/c",
    responses={
        "400": {"description": "Error with str"},
        "5xx": {"description": "Error with range, lower"},
        "default": {"description": "A default response"},
    },
)
async def c():
    return "c"


@router.get(
    "/d",
    responses={
        "400": {"description": "Error with str"},
        "5XX": {"model": ResponseModel},
        "default": {"model": ResponseModel},
    },
)
async def d():
    return "d"


app.include_router(router)


client = TestClient(app)


def test_a():
    response = client.get("/a")
    assert response.status_code == 200, response.text
    assert response.json() == "a"


def test_b():
    response = client.get("/b")
    assert response.status_code == 200, response.text
    assert response.json() == "b"


def test_c():
    response = client.get("/c")
    assert response.status_code == 200, response.text
    assert response.json() == "c"


def test_d():
    response = client.get("/d")
    assert response.status_code == 200, response.text
    assert response.json() == "d"


def test_openapi_schema():
    response = client.get("/openapi.json")
    assert response.status_code == 200, response.text
    assert response.json() == {
        "openapi": "3.1.0",
        "info": {"title": "FastAPI", "version": "0.1.0"},
        "paths": {
            "/a": {
                "get": {
                    "responses": {
                        "501": {"description": "Error 1"},
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                    },
                    "summary": "A",
                    "operationId": "a_a_get",
                }
            },
            "/b": {
                "get": {
                    "responses": {
                        "502": {"description": "Error 2"},
                        "4XX": {"description": "Error with range, upper"},
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                    },
                    "summary": "B",
                    "operationId": "b_b_get",
                }
            },
            "/c": {
                "get": {
                    "responses": {
                        "400": {"description": "Error with str"},
                        "5XX": {"description": "Error with range, lower"},
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "default": {"description": "A default response"},
                    },
                    "summary": "C",
                    "operationId": "c_c_get",
                }
            },
            "/d": {
                "get": {
                    "responses": {
                        "400": {"description": "Error with str"},
                        "5XX": {
                            "description": "Server Error",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/ResponseModel"
                                    }
                                }
                            },
                        },
                        "200": {
                            "description": "Successful Response",
                            "content": {"application/json": {"schema": {}}},
                        },
                        "default": {
                            "description": "Default Response",
                            "content": {
                                "application/json": {
                                    "schema": {
                                        "$ref": "#/components/schemas/ResponseModel"
                                    }
                                }
                            },
                        },
                    },
                    "summary": "D",
                    "operationId": "d_d_get",
                }
            },
        },
        "components": {
            "schemas": {
                "ResponseModel": {
                    "title": "ResponseModel",
                    "required": ["message"],
                    "type": "object",
                    "properties": {"message": {"title": "Message", "type": "string"}},
                }
            }
        },
    }


--- AGENTS.md ---
# Task Master AI - Agent Integration Guide

## Essential Commands

### Core Workflow Commands

```bash
# Project Setup
task-master init                                    # Initialize Task Master in current project
task-master parse-prd .taskmaster/docs/prd.txt      # Generate tasks from PRD document
task-master models --setup                        # Configure AI models interactively

# Daily Development Workflow
task-master list                                   # Show all tasks with status
task-master next                                   # Get next available task to work on
task-master show <id>                             # View detailed task information (e.g., task-master show 1.2)
task-master set-status --id=<id> --status=done    # Mark task complete

# Task Management
task-master add-task --prompt="description" --research        # Add new task with AI assistance
task-master expand --id=<id> --research --force              # Break task into subtasks
task-master update-task --id=<id> --prompt="changes"         # Update specific task
task-master update --from=<id> --prompt="changes"            # Update multiple tasks from ID onwards
task-master update-subtask --id=<id> --prompt="notes"        # Add implementation notes to subtask

# Analysis & Planning
task-master analyze-complexity --research          # Analyze task complexity
task-master complexity-report                      # View complexity analysis
task-master expand --all --research               # Expand all eligible tasks

# Dependencies & Organization
task-master add-dependency --id=<id> --depends-on=<id>       # Add task dependency
task-master move --from=<id> --to=<id>                       # Reorganize task hierarchy
task-master validate-dependencies                            # Check for dependency issues
task-master generate                                         # Update task markdown files (usually auto-called)
```

## Key Files & Project Structure

### Core Files

- `.taskmaster/tasks/tasks.json` - Main task data file (auto-managed)
- `.taskmaster/config.json` - AI model configuration (use `task-master models` to modify)
- `.taskmaster/docs/prd.txt` - Product Requirements Document for parsing
- `.taskmaster/tasks/*.txt` - Individual task files (auto-generated from tasks.json)
- `.env` - API keys for CLI usage

### Claude Code Integration Files

- `CLAUDE.md` - Auto-loaded context for Claude Code (this file)
- `.claude/settings.json` - Claude Code tool allowlist and preferences
- `.claude/commands/` - Custom slash commands for repeated workflows
- `.mcp.json` - MCP server configuration (project-specific)

### Directory Structure

```
project/
├── .taskmaster/
│   ├── tasks/              # Task files directory
│   │   ├── tasks.json      # Main task database
│   │   ├── task-1.md      # Individual task files
│   │   └── task-2.md
│   ├── docs/              # Documentation directory
│   │   ├── prd.txt        # Product requirements
│   ├── reports/           # Analysis reports directory
│   │   └── task-complexity-report.json
│   ├── templates/         # Template files
│   │   └── example_prd.txt  # Example PRD template
│   └── config.json        # AI models & settings
├── .claude/
│   ├── settings.json      # Claude Code configuration
│   └── commands/         # Custom slash commands
├── .env                  # API keys
├── .mcp.json            # MCP configuration
└── CLAUDE.md            # This file - auto-loaded by Claude Code
```

## MCP Integration

Task Master provides an MCP server that Claude Code can connect to. Configure in `.mcp.json`:

```json
{
  "mcpServers": {
    "task-master-ai": {
      "command": "npx",
      "args": ["-y", "task-master-ai"],
      "env": {
        "ANTHROPIC_API_KEY": "your_key_here",
        "PERPLEXITY_API_KEY": "your_key_here",
        "OPENAI_API_KEY": "OPENAI_API_KEY_HERE",
        "GOOGLE_API_KEY": "GOOGLE_API_KEY_HERE",
        "XAI_API_KEY": "XAI_API_KEY_HERE",
        "OPENROUTER_API_KEY": "OPENROUTER_API_KEY_HERE",
        "MISTRAL_API_KEY": "MISTRAL_API_KEY_HERE",
        "AZURE_OPENAI_API_KEY": "AZURE_OPENAI_API_KEY_HERE",
        "OLLAMA_API_KEY": "OLLAMA_API_KEY_HERE"
      }
    }
  }
}
```

### Essential MCP Tools

```javascript
help; // = shows available taskmaster commands
// Project setup
initialize_project; // = task-master init
parse_prd; // = task-master parse-prd

// Daily workflow
get_tasks; // = task-master list
next_task; // = task-master next
get_task; // = task-master show <id>
set_task_status; // = task-master set-status

// Task management
add_task; // = task-master add-task
expand_task; // = task-master expand
update_task; // = task-master update-task
update_subtask; // = task-master update-subtask
update; // = task-master update

// Analysis
analyze_project_complexity; // = task-master analyze-complexity
complexity_report; // = task-master complexity-report
```

## Claude Code Workflow Integration

### Standard Development Workflow

#### 1. Project Initialization

```bash
# Initialize Task Master
task-master init

# Create or obtain PRD, then parse it
task-master parse-prd .taskmaster/docs/prd.txt

# Analyze complexity and expand tasks
task-master analyze-complexity --research
task-master expand --all --research
```

If tasks already exist, another PRD can be parsed (with new information only!) using parse-prd with --append flag. This will add the generated tasks to the existing list of tasks..

#### 2. Daily Development Loop

```bash
# Start each session
task-master next                           # Find next available task
task-master show <id>                     # Review task details

# During implementation, check in code context into the tasks and subtasks
task-master update-subtask --id=<id> --prompt="implementation notes..."

# Complete tasks
task-master set-status --id=<id> --status=done
```

#### 3. Multi-Claude Workflows

For complex projects, use multiple Claude Code sessions:

```bash
# Terminal 1: Main implementation
cd project && claude

# Terminal 2: Testing and validation
cd project-test-worktree && claude

# Terminal 3: Documentation updates
cd project-docs-worktree && claude
```

### Custom Slash Commands

Create `.claude/commands/taskmaster-next.md`:

```markdown
Find the next available Task Master task and show its details.

Steps:

1. Run `task-master next` to get the next task
2. If a task is available, run `task-master show <id>` for full details
3. Provide a summary of what needs to be implemented
4. Suggest the first implementation step
```

Create `.claude/commands/taskmaster-complete.md`:

```markdown
Complete a Task Master task: $ARGUMENTS

Steps:

1. Review the current task with `task-master show $ARGUMENTS`
2. Verify all implementation is complete
3. Run any tests related to this task
4. Mark as complete: `task-master set-status --id=$ARGUMENTS --status=done`
5. Show the next available task with `task-master next`
```

## Tool Allowlist Recommendations

Add to `.claude/settings.json`:

```json
{
  "allowedTools": [
    "Edit",
    "Bash(task-master *)",
    "Bash(git commit:*)",
    "Bash(git add:*)",
    "Bash(npm run *)",
    "mcp__task_master_ai__*"
  ]
}
```

## Configuration & Setup

### API Keys Required

At least **one** of these API keys must be configured:

- `ANTHROPIC_API_KEY` (Claude models) - **Recommended**
- `PERPLEXITY_API_KEY` (Research features) - **Highly recommended**
- `OPENAI_API_KEY` (GPT models)
- `GOOGLE_API_KEY` (Gemini models)
- `MISTRAL_API_KEY` (Mistral models)
- `OPENROUTER_API_KEY` (Multiple models)
- `XAI_API_KEY` (Grok models)

An API key is required for any provider used across any of the 3 roles defined in the `models` command.

### Model Configuration

```bash
# Interactive setup (recommended)
task-master models --setup

# Set specific models
task-master models --set-main claude-3-5-sonnet-20241022
task-master models --set-research perplexity-llama-3.1-sonar-large-128k-online
task-master models --set-fallback gpt-4o-mini
```

## Task Structure & IDs

### Task ID Format

- Main tasks: `1`, `2`, `3`, etc.
- Subtasks: `1.1`, `1.2`, `2.1`, etc.
- Sub-subtasks: `1.1.1`, `1.1.2`, etc.

### Task Status Values

- `pending` - Ready to work on
- `in-progress` - Currently being worked on
- `done` - Completed and verified
- `deferred` - Postponed
- `cancelled` - No longer needed
- `blocked` - Waiting on external factors

### Task Fields

```json
{
  "id": "1.2",
  "title": "Implement user authentication",
  "description": "Set up JWT-based auth system",
  "status": "pending",
  "priority": "high",
  "dependencies": ["1.1"],
  "details": "Use bcrypt for hashing, JWT for tokens...",
  "testStrategy": "Unit tests for auth functions, integration tests for login flow",
  "subtasks": []
}
```

## Claude Code Best Practices with Task Master

### Context Management

- Use `/clear` between different tasks to maintain focus
- This CLAUDE.md file is automatically loaded for context
- Use `task-master show <id>` to pull specific task context when needed

### Iterative Implementation

1. `task-master show <subtask-id>` - Understand requirements
2. Explore codebase and plan implementation
3. `task-master update-subtask --id=<id> --prompt="detailed plan"` - Log plan
4. `task-master set-status --id=<id> --status=in-progress` - Start work
5. Implement code following logged plan
6. `task-master update-subtask --id=<id> --prompt="what worked/didn't work"` - Log progress
7. `task-master set-status --id=<id> --status=done` - Complete task

### Complex Workflows with Checklists

For large migrations or multi-step processes:

1. Create a markdown PRD file describing the new changes: `touch task-migration-checklist.md` (prds can be .txt or .md)
2. Use Taskmaster to parse the new prd with `task-master parse-prd --append` (also available in MCP)
3. Use Taskmaster to expand the newly generated tasks into subtasks. Consdier using `analyze-complexity` with the correct --to and --from IDs (the new ids) to identify the ideal subtask amounts for each task. Then expand them.
4. Work through items systematically, checking them off as completed
5. Use `task-master update-subtask` to log progress on each task/subtask and/or updating/researching them before/during implementation if getting stuck

### Git Integration

Task Master works well with `gh` CLI:

```bash
# Create PR for completed task
gh pr create --title "Complete task 1.2: User authentication" --body "Implements JWT auth system as specified in task 1.2"

# Reference task in commits
git commit -m "feat: implement JWT auth (task 1.2)"
```

### Parallel Development with Git Worktrees

```bash
# Create worktrees for parallel task development
git worktree add ../project-auth feature/auth-system
git worktree add ../project-api feature/api-refactor

# Run Claude Code in each worktree
cd ../project-auth && claude    # Terminal 1: Auth work
cd ../project-api && claude     # Terminal 2: API work
```

## Troubleshooting

### AI Commands Failing

```bash
# Check API keys are configured
cat .env                           # For CLI usage

# Verify model configuration
task-master models

# Test with different model
task-master models --set-fallback gpt-4o-mini
```

### MCP Connection Issues

- Check `.mcp.json` configuration
- Verify Node.js installation
- Use `--mcp-debug` flag when starting Claude Code
- Use CLI as fallback if MCP unavailable

### Task File Sync Issues

```bash
# Regenerate task files from tasks.json
task-master generate

# Fix dependency issues
task-master fix-dependencies
```

DO NOT RE-INITIALIZE. That will not do anything beyond re-adding the same Taskmaster core files.

## Important Notes

### AI-Powered Operations

These commands make AI calls and may take up to a minute:

- `parse_prd` / `task-master parse-prd`
- `analyze_project_complexity` / `task-master analyze-complexity`
- `expand_task` / `task-master expand`
- `expand_all` / `task-master expand --all`
- `add_task` / `task-master add-task`
- `update` / `task-master update`
- `update_task` / `task-master update-task`
- `update_subtask` / `task-master update-subtask`

### File Management

- Never manually edit `tasks.json` - use commands instead
- Never manually edit `.taskmaster/config.json` - use `task-master models`
- Task markdown files in `tasks/` are auto-generated
- Run `task-master generate` after manual changes to tasks.json

### Claude Code Session Management

- Use `/clear` frequently to maintain focused context
- Create custom slash commands for repeated Task Master workflows
- Configure tool allowlist to streamline permissions
- Use headless mode for automation: `claude -p "task-master next"`

### Multi-Task Updates

- Use `update --from=<id>` to update multiple future tasks
- Use `update-task --id=<id>` for single task updates
- Use `update-subtask --id=<id>` for implementation logging

### Research Mode

- Add `--research` flag for research-based AI enhancement
- Requires a research model API key like Perplexity (`PERPLEXITY_API_KEY`) in environment
- Provides more informed task creation and updates
- Recommended for complex technical tasks

---

_This guide ensures Claude Code has immediate access to Task Master's essential functionality for agentic development workflows._


--- GEMINI.md ---
# Task Master AI - Agent Integration Guide

## Essential Commands

### Core Workflow Commands

```bash
# Project Setup
task-master init                                    # Initialize Task Master in current project
task-master parse-prd .taskmaster/docs/prd.txt      # Generate tasks from PRD document
task-master models --setup                        # Configure AI models interactively

# Daily Development Workflow
task-master list                                   # Show all tasks with status
task-master next                                   # Get next available task to work on
task-master show <id>                             # View detailed task information (e.g., task-master show 1.2)
task-master set-status --id=<id> --status=done    # Mark task complete

# Task Management
task-master add-task --prompt="description" --research        # Add new task with AI assistance
task-master expand --id=<id> --research --force              # Break task into subtasks
task-master update-task --id=<id> --prompt="changes"         # Update specific task
task-master update --from=<id> --prompt="changes"            # Update multiple tasks from ID onwards
task-master update-subtask --id=<id> --prompt="notes"        # Add implementation notes to subtask

# Analysis & Planning
task-master analyze-complexity --research          # Analyze task complexity
task-master complexity-report                      # View complexity analysis
task-master expand --all --research               # Expand all eligible tasks

# Dependencies & Organization
task-master add-dependency --id=<id> --depends-on=<id>       # Add task dependency
task-master move --from=<id> --to=<id>                       # Reorganize task hierarchy
task-master validate-dependencies                            # Check for dependency issues
task-master generate                                         # Update task markdown files (usually auto-called)
```

## Key Files & Project Structure

### Core Files

- `.taskmaster/tasks/tasks.json` - Main task data file (auto-managed)
- `.taskmaster/config.json` - AI model configuration (use `task-master models` to modify)
- `.taskmaster/docs/prd.txt` - Product Requirements Document for parsing
- `.taskmaster/tasks/*.txt` - Individual task files (auto-generated from tasks.json)
- `.env` - API keys for CLI usage

### Claude Code Integration Files

- `CLAUDE.md` - Auto-loaded context for Claude Code (this file)
- `.claude/settings.json` - Claude Code tool allowlist and preferences
- `.claude/commands/` - Custom slash commands for repeated workflows
- `.mcp.json` - MCP server configuration (project-specific)

### Directory Structure

```
project/
├── .taskmaster/
│   ├── tasks/              # Task files directory
│   │   ├── tasks.json      # Main task database
│   │   ├── task-1.md      # Individual task files
│   │   └── task-2.md
│   ├── docs/              # Documentation directory
│   │   ├── prd.txt        # Product requirements
│   ├── reports/           # Analysis reports directory
│   │   └── task-complexity-report.json
│   ├── templates/         # Template files
│   │   └── example_prd.txt  # Example PRD template
│   └── config.json        # AI models & settings
├── .claude/
│   ├── settings.json      # Claude Code configuration
│   └── commands/         # Custom slash commands
├── .env                  # API keys
├── .mcp.json            # MCP configuration
└── CLAUDE.md            # This file - auto-loaded by Claude Code
```

## MCP Integration

Task Master provides an MCP server that Claude Code can connect to. Configure in `.mcp.json`:

```json
{
  "mcpServers": {
    "task-master-ai": {
      "command": "npx",
      "args": ["-y", "task-master-ai"],
      "env": {
        "ANTHROPIC_API_KEY": "your_key_here",
        "PERPLEXITY_API_KEY": "your_key_here",
        "OPENAI_API_KEY": "OPENAI_API_KEY_HERE",
        "GOOGLE_API_KEY": "GOOGLE_API_KEY_HERE",
        "XAI_API_KEY": "XAI_API_KEY_HERE",
        "OPENROUTER_API_KEY": "OPENROUTER_API_KEY_HERE",
        "MISTRAL_API_KEY": "MISTRAL_API_KEY_HERE",
        "AZURE_OPENAI_API_KEY": "AZURE_OPENAI_API_KEY_HERE",
        "OLLAMA_API_KEY": "OLLAMA_API_KEY_HERE"
      }
    }
  }
}
```

### Essential MCP Tools

```javascript
help; // = shows available taskmaster commands
// Project setup
initialize_project; // = task-master init
parse_prd; // = task-master parse-prd

// Daily workflow
get_tasks; // = task-master list
next_task; // = task-master next
get_task; // = task-master show <id>
set_task_status; // = task-master set-status

// Task management
add_task; // = task-master add-task
expand_task; // = task-master expand
update_task; // = task-master update-task
update_subtask; // = task-master update-subtask
update; // = task-master update

// Analysis
analyze_project_complexity; // = task-master analyze-complexity
complexity_report; // = task-master complexity-report
```

## Claude Code Workflow Integration

### Standard Development Workflow

#### 1. Project Initialization

```bash
# Initialize Task Master
task-master init

# Create or obtain PRD, then parse it
task-master parse-prd .taskmaster/docs/prd.txt

# Analyze complexity and expand tasks
task-master analyze-complexity --research
task-master expand --all --research
```

If tasks already exist, another PRD can be parsed (with new information only!) using parse-prd with --append flag. This will add the generated tasks to the existing list of tasks..

#### 2. Daily Development Loop

```bash
# Start each session
task-master next                           # Find next available task
task-master show <id>                     # Review task details

# During implementation, check in code context into the tasks and subtasks
task-master update-subtask --id=<id> --prompt="implementation notes..."

# Complete tasks
task-master set-status --id=<id> --status=done
```

#### 3. Multi-Claude Workflows

For complex projects, use multiple Claude Code sessions:

```bash
# Terminal 1: Main implementation
cd project && claude

# Terminal 2: Testing and validation
cd project-test-worktree && claude

# Terminal 3: Documentation updates
cd project-docs-worktree && claude
```

### Custom Slash Commands

Create `.claude/commands/taskmaster-next.md`:

```markdown
Find the next available Task Master task and show its details.

Steps:

1. Run `task-master next` to get the next task
2. If a task is available, run `task-master show <id>` for full details
3. Provide a summary of what needs to be implemented
4. Suggest the first implementation step
```

Create `.claude/commands/taskmaster-complete.md`:

```markdown
Complete a Task Master task: $ARGUMENTS

Steps:

1. Review the current task with `task-master show $ARGUMENTS`
2. Verify all implementation is complete
3. Run any tests related to this task
4. Mark as complete: `task-master set-status --id=$ARGUMENTS --status=done`
5. Show the next available task with `task-master next`
```

## Tool Allowlist Recommendations

Add to `.claude/settings.json`:

```json
{
  "allowedTools": [
    "Edit",
    "Bash(task-master *)",
    "Bash(git commit:*)",
    "Bash(git add:*)",
    "Bash(npm run *)",
    "mcp__task_master_ai__*"
  ]
}
```

## Configuration & Setup

### API Keys Required

At least **one** of these API keys must be configured:

- `ANTHROPIC_API_KEY` (Claude models) - **Recommended**
- `PERPLEXITY_API_KEY` (Research features) - **Highly recommended**
- `OPENAI_API_KEY` (GPT models)
- `GOOGLE_API_KEY` (Gemini models)
- `MISTRAL_API_KEY` (Mistral models)
- `OPENROUTER_API_KEY` (Multiple models)
- `XAI_API_KEY` (Grok models)

An API key is required for any provider used across any of the 3 roles defined in the `models` command.

### Model Configuration

```bash
# Interactive setup (recommended)
task-master models --setup

# Set specific models
task-master models --set-main claude-3-5-sonnet-20241022
task-master models --set-research perplexity-llama-3.1-sonar-large-128k-online
task-master models --set-fallback gpt-4o-mini
```

## Task Structure & IDs

### Task ID Format

- Main tasks: `1`, `2`, `3`, etc.
- Subtasks: `1.1`, `1.2`, `2.1`, etc.
- Sub-subtasks: `1.1.1`, `1.1.2`, etc.

### Task Status Values

- `pending` - Ready to work on
- `in-progress` - Currently being worked on
- `done` - Completed and verified
- `deferred` - Postponed
- `cancelled` - No longer needed
- `blocked` - Waiting on external factors

### Task Fields

```json
{
  "id": "1.2",
  "title": "Implement user authentication",
  "description": "Set up JWT-based auth system",
  "status": "pending",
  "priority": "high",
  "dependencies": ["1.1"],
  "details": "Use bcrypt for hashing, JWT for tokens...",
  "testStrategy": "Unit tests for auth functions, integration tests for login flow",
  "subtasks": []
}
```

## Claude Code Best Practices with Task Master

### Context Management

- Use `/clear` between different tasks to maintain focus
- This CLAUDE.md file is automatically loaded for context
- Use `task-master show <id>` to pull specific task context when needed

### Iterative Implementation

1. `task-master show <subtask-id>` - Understand requirements
2. Explore codebase and plan implementation
3. `task-master update-subtask --id=<id> --prompt="detailed plan"` - Log plan
4. `task-master set-status --id=<id> --status=in-progress` - Start work
5. Implement code following logged plan
6. `task-master update-subtask --id=<id> --prompt="what worked/didn't work"` - Log progress
7. `task-master set-status --id=<id> --status=done` - Complete task

### Complex Workflows with Checklists

For large migrations or multi-step processes:

1. Create a markdown PRD file describing the new changes: `touch task-migration-checklist.md` (prds can be .txt or .md)
2. Use Taskmaster to parse the new prd with `task-master parse-prd --append` (also available in MCP)
3. Use Taskmaster to expand the newly generated tasks into subtasks. Consdier using `analyze-complexity` with the correct --to and --from IDs (the new ids) to identify the ideal subtask amounts for each task. Then expand them.
4. Work through items systematically, checking them off as completed
5. Use `task-master update-subtask` to log progress on each task/subtask and/or updating/researching them before/during implementation if getting stuck

### Git Integration

Task Master works well with `gh` CLI:

```bash
# Create PR for completed task
gh pr create --title "Complete task 1.2: User authentication" --body "Implements JWT auth system as specified in task 1.2"

# Reference task in commits
git commit -m "feat: implement JWT auth (task 1.2)"
```

### Parallel Development with Git Worktrees

```bash
# Create worktrees for parallel task development
git worktree add ../project-auth feature/auth-system
git worktree add ../project-api feature/api-refactor

# Run Claude Code in each worktree
cd ../project-auth && claude    # Terminal 1: Auth work
cd ../project-api && claude     # Terminal 2: API work
```

## Troubleshooting

### AI Commands Failing

```bash
# Check API keys are configured
cat .env                           # For CLI usage

# Verify model configuration
task-master models

# Test with different model
task-master models --set-fallback gpt-4o-mini
```

### MCP Connection Issues

- Check `.mcp.json` configuration
- Verify Node.js installation
- Use `--mcp-debug` flag when starting Claude Code
- Use CLI as fallback if MCP unavailable

### Task File Sync Issues

```bash
# Regenerate task files from tasks.json
task-master generate

# Fix dependency issues
task-master fix-dependencies
```

DO NOT RE-INITIALIZE. That will not do anything beyond re-adding the same Taskmaster core files.

## Important Notes

### AI-Powered Operations

These commands make AI calls and may take up to a minute:

- `parse_prd` / `task-master parse-prd`
- `analyze_project_complexity` / `task-master analyze-complexity`
- `expand_task` / `task-master expand`
- `expand_all` / `task-master expand --all`
- `add_task` / `task-master add-task`
- `update` / `task-master update`
- `update_task` / `task-master update-task`
- `update_subtask` / `task-master update-subtask`

### File Management

- Never manually edit `tasks.json` - use commands instead
- Never manually edit `.taskmaster/config.json` - use `task-master models`
- Task markdown files in `tasks/` are auto-generated
- Run `task-master generate` after manual changes to tasks.json

### Claude Code Session Management

- Use `/clear` frequently to maintain focused context
- Create custom slash commands for repeated Task Master workflows
- Configure tool allowlist to streamline permissions
- Use headless mode for automation: `claude -p "task-master next"`

### Multi-Task Updates

- Use `update --from=<id>` to update multiple future tasks
- Use `update-task --id=<id>` for single task updates
- Use `update-subtask --id=<id>` for implementation logging

### Research Mode

- Add `--research` flag for research-based AI enhancement
- Requires a research model API key like Perplexity (`PERPLEXITY_API_KEY`) in environment
- Provides more informed task creation and updates
- Recommended for complex technical tasks

---

_This guide ensures Claude Code has immediate access to Task Master's essential functionality for agentic development workflows._


--- README.md ---
---
title: "LM Studio llms.txt Generator"
description: "Generate llms.txt, llms-full, and fallback artifacts for GitHub repositories using DSPy with LM Studio."
---

## Overview

Use this CLI-first toolkit to produce LLM-friendly documentation bundles (`llms.txt`, `llms-full.txt`, optional `llms-ctx.txt`, and fallback JSON) for any GitHub repository. The generator wraps DSPy analyzers, manages LM Studio model lifecycle with the official Python SDK, and guarantees output even when the primary language model cannot respond.

<Info>
The pipeline validates curated links, detects default branches automatically, and writes artifacts to `artifacts/<owner>/<repo>/`.
</Info>

## Prerequisites

- Python 3.10 or later
- LM Studio server available locally (Developer tab → **Start Server**) or the CLI (`lms server start --port 1234`)
- GitHub API token in `GITHUB_ACCESS_TOKEN` or `GH_TOKEN`
- Optional: [`llms_txt`](https://pypi.org/project/llms-txt/) when you want to produce `llms-ctx.txt`

<Warning>
Install dependencies inside a virtual environment to avoid PEP 668 “externally managed environment” errors.
</Warning>

## Install

<Steps>
  <Step title="Create a virtual environment">
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```
  </Step>
  <Step title="Install the package with developer extras">
    ```bash
    pip install -e .[dev]
    ```
    Installing the editable package exposes the `lmstudio-llmstxt` CLI and brings in the `lmstudio` Python SDK plus pytest.
  </Step>
</Steps>

<Tip>
Keep the virtual environment active while running the CLI or tests so the SDK-based unload logic can import `lmstudio`.
</Tip>

## Configure LM Studio

<Steps>
  <Step title="Load the CLI">
    ```bash
    npx lmstudio install-cli
    lms server start --port 1234
    ```
    The server must expose an OpenAI-compatible endpoint, commonly `http://localhost:1234/v1`.
  </Step>
  <Step title="Ensure the target model is downloaded">
    Open LM Studio, download the model (for example `qwen/qwen3-4b-2507`), and confirm it appears in the **Server** tab.
  </Step>
</Steps>

## Quick start

Run the CLI against any GitHub repository:

```bash
lmstudio-llmstxt https://github.com/owner/repo \
  --model qwen/qwen3-4b-2507 \
  --api-base http://localhost:1234/v1 \
  --stamp
```

The command writes artifacts to `artifacts/owner/repo/`. Use `--output-dir` to override the destination.

### Environment variables

| Variable | Description |
|----------|-------------|
| `LMSTUDIO_MODEL` | Default LM Studio model identifier |
| `LMSTUDIO_BASE_URL` | Base URL such as `http://localhost:1234/v1` |
| `LMSTUDIO_API_KEY` | API key for secured LM Studio deployments |
| `OUTPUT_DIR` | Custom root directory for artifacts |
| `ENABLE_CTX=1` | Emit `llms-ctx.txt` using the optional `llms_txt` package |

## Generated artifacts

| Artifact | Purpose |
|----------|---------|
| `*-llms.txt` | Primary documentation synthesized by DSPy or the fallback heuristic |
| `*-llms-full.txt` | Expanded content fetched from curated GitHub links with 404 filtering |
| `*-llms.json` | Fallback JSON following `LLMS_JSON_SCHEMA` (only when LM fallback triggers) |
| `*-llms-ctx.txt` | Optional context file created when `ENABLE_CTX=1` and `llms_txt` is installed |

<Check>
The pipeline always writes `llms.txt` and `llms-full.txt`, even when the language model call fails.
</Check>

## How it works

1. **Collect repository material** – the GitHub client gathers the file tree, README, package files, repository visibility, and default branch.
2. **Prepare LM Studio** – the manager confirms the requested model is loaded, auto-loading if necessary.
3. **Generate documentation** – DSPy produces curated content; on LM failures the fallback serializer builds markdown and JSON directly.
4. **Assemble `llms-full`** – curated links are re-fetched via raw GitHub URLs for public repos or authenticated API calls for private ones, with validation to remove dead links.
5. **Unload models safely** – the workflow first uses the official `lmstudio` SDK (`model.unload()` or `list_loaded_models`), then falls back to HTTP and CLI unload requests.

## Project layout

- `src/lmstudiotxt_generator/` – configuration, GitHub utilities, DSPy analyzers, LM Studio helpers, fallback renderer, and artifact writers
- `tests/` – pytest coverage for analyzer buckets, LM Studio handshake/unload logic, and pipeline fallbacks
- `.taskmaster/` – Task Master AI configuration for agentic planning (optional)
- `artifacts/` – sample outputs generated from previous runs

## Verify your setup

```bash
source .venv/bin/activate
python -m pytest
```

All tests should pass, confirming URL validation, fallback handling, and SDK-first unload behaviour.

## Troubleshooting

<Warning>
If `pip install -e .[dev]` fails with build tool errors (`cmake` or `pyarrow`), install the missing system packages and retry the installation before running tests.
</Warning>

<Tip>
When `llms-full` surfaces `[fetch-error]`, verify the curated link uses the repository’s actual default branch (`master` vs `main`). The analyzer keeps only live URLs, but custom additions may need manual tweaks.
</Tip>

<Info>
Set `LMSTUDIO_MODEL` and `GITHUB_ACCESS_TOKEN` in your shell profile to avoid repeating flags during iterative runs.
</Info>


--- index.md ---
# Generating llms.txt for Code Documentation with DSPy

This tutorial demonstrates how to use DSPy to automatically generate an `llms.txt` file for the DSPy repository itself. The `llms.txt` standard provides LLM-friendly documentation that helps AI systems better understand codebases.

## What is llms.txt?

`llms.txt` is a proposed standard for providing structured, LLM-friendly documentation about a project. It typically includes:

- Project overview and purpose
- Key concepts and terminology
- Architecture and structure
- Usage examples
- Important files and directories

## Building a DSPy Program for llms.txt Generation

Let's create a DSPy program that analyzes a repository and generates comprehensive `llms.txt` documentation.

### Step 1: Define Our Signatures

First, we'll define signatures for different aspects of documentation generation:

```python
import dspy
from typing import List

class AnalyzeRepository(dspy.Signature):
    """Analyze a repository structure and identify key components."""
    repo_url: str = dspy.InputField(desc="GitHub repository URL")
    file_tree: str = dspy.InputField(desc="Repository file structure")
    readme_content: str = dspy.InputField(desc="README.md content")
    
    project_purpose: str = dspy.OutputField(desc="Main purpose and goals of the project")
    key_concepts: list[str] = dspy.OutputField(desc="List of important concepts and terminology")
    architecture_overview: str = dspy.OutputField(desc="High-level architecture description")

class AnalyzeCodeStructure(dspy.Signature):
    """Analyze code structure to identify important directories and files."""
    file_tree: str = dspy.InputField(desc="Repository file structure")
    package_files: str = dspy.InputField(desc="Key package and configuration files")
    
    important_directories: list[str] = dspy.OutputField(desc="Key directories and their purposes")
    entry_points: list[str] = dspy.OutputField(desc="Main entry points and important files")
    development_info: str = dspy.OutputField(desc="Development setup and workflow information")

class GenerateLLMsTxt(dspy.Signature):
    """Generate a comprehensive llms.txt file from analyzed repository information."""
    project_purpose: str = dspy.InputField()
    key_concepts: list[str] = dspy.InputField()
    architecture_overview: str = dspy.InputField()
    important_directories: list[str] = dspy.InputField()
    entry_points: list[str] = dspy.InputField()
    development_info: str = dspy.InputField()
    usage_examples: str = dspy.InputField(desc="Common usage patterns and examples")
    
    llms_txt_content: str = dspy.OutputField(desc="Complete llms.txt file content following the standard format")
```

### Step 2: Create the Repository Analyzer Module

```python
class RepositoryAnalyzer(dspy.Module):
    def __init__(self):
        super().__init__()
        self.analyze_repo = dspy.ChainOfThought(AnalyzeRepository)
        self.analyze_structure = dspy.ChainOfThought(AnalyzeCodeStructure)
        self.generate_examples = dspy.ChainOfThought("repo_info -> usage_examples")
        self.generate_llms_txt = dspy.ChainOfThought(GenerateLLMsTxt)
    
    def forward(self, repo_url, file_tree, readme_content, package_files):
        # Analyze repository purpose and concepts
        repo_analysis = self.analyze_repo(
            repo_url=repo_url,
            file_tree=file_tree,
            readme_content=readme_content
        )
        
        # Analyze code structure
        structure_analysis = self.analyze_structure(
            file_tree=file_tree,
            package_files=package_files
        )
        
        # Generate usage examples
        usage_examples = self.generate_examples(
            repo_info=f"Purpose: {repo_analysis.project_purpose}\nConcepts: {repo_analysis.key_concepts}"
        )
        
        # Generate final llms.txt
        llms_txt = self.generate_llms_txt(
            project_purpose=repo_analysis.project_purpose,
            key_concepts=repo_analysis.key_concepts,
            architecture_overview=repo_analysis.architecture_overview,
            important_directories=structure_analysis.important_directories,
            entry_points=structure_analysis.entry_points,
            development_info=structure_analysis.development_info,
            usage_examples=usage_examples.usage_examples
        )
        
        return dspy.Prediction(
            llms_txt_content=llms_txt.llms_txt_content,
            analysis=repo_analysis,
            structure=structure_analysis
        )
```

### Step 3: Gather Repository Information

Let's create helper functions to extract repository information:

```python
import requests
import os
from pathlib import Path

os.environ["GITHUB_ACCESS_TOKEN"] = "<your_access_token>"

def get_github_file_tree(repo_url):
    """Get repository file structure from GitHub API."""
    # Extract owner/repo from URL
    parts = repo_url.rstrip('/').split('/')
    owner, repo = parts[-2], parts[-1]
    
    api_url = f"https://api.github.com/repos/{owner}/{repo}/git/trees/main?recursive=1"
    response = requests.get(api_url, headers={
        "Authorization": f"Bearer {os.environ.get('GITHUB_ACCESS_TOKEN')}"
    })
    
    if response.status_code == 200:
        tree_data = response.json()
        file_paths = [item['path'] for item in tree_data['tree'] if item['type'] == 'blob']
        return '\n'.join(sorted(file_paths))
    else:
        raise Exception(f"Failed to fetch repository tree: {response.status_code}")

def get_github_file_content(repo_url, file_path):
    """Get specific file content from GitHub."""
    parts = repo_url.rstrip('/').split('/')
    owner, repo = parts[-2], parts[-1]
    
    api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}"
    response = requests.get(api_url, headers={
        "Authorization": f"Bearer {os.environ.get('GITHUB_ACCESS_TOKEN')}"
    })
    
    if response.status_code == 200:
        import base64
        content = base64.b64decode(response.json()['content']).decode('utf-8')
        return content
    else:
        return f"Could not fetch {file_path}"

def gather_repository_info(repo_url):
    """Gather all necessary repository information."""
    file_tree = get_github_file_tree(repo_url)
    readme_content = get_github_file_content(repo_url, "README.md")
    
    # Get key package files
    package_files = []
    for file_path in ["pyproject.toml", "setup.py", "requirements.txt", "package.json"]:
        try:
            content = get_github_file_content(repo_url, file_path)
            if "Could not fetch" not in content:
                package_files.append(f"=== {file_path} ===\n{content}")
        except:
            continue
    
    package_files_content = "\n\n".join(package_files)
    
    return file_tree, readme_content, package_files_content
```

### Step 4: Configure DSPy and Generate llms.txt

```python
def generate_llms_txt_for_dspy():
    # Configure DSPy (use your preferred LM)
    lm = dspy.LM(model="gpt-4o-mini")
    dspy.configure(lm=lm)
    os.environ["OPENAI_API_KEY"] = "<YOUR OPENAI KEY>"
    
    # Initialize our analyzer
    analyzer = RepositoryAnalyzer()
    
    # Gather DSPy repository information
    repo_url = "https://github.com/stanfordnlp/dspy"
    file_tree, readme_content, package_files = gather_repository_info(repo_url)
    
    # Generate llms.txt
    result = analyzer(
        repo_url=repo_url,
        file_tree=file_tree,
        readme_content=readme_content,
        package_files=package_files
    )
    
    return result

# Run the generation
if __name__ == "__main__":
    result = generate_llms_txt_for_dspy()
    
    # Save the generated llms.txt
    with open("llms.txt", "w") as f:
        f.write(result.llms_txt_content)
    
    print("Generated llms.txt file!")
    print("\nPreview:")
    print(result.llms_txt_content[:500] + "...")
```

## Expected Output Structure

The generated `llms.txt` for DSPy would follow this structure:

```
# DSPy: Programming Language Models

## Project Overview
DSPy is a framework for programming—rather than prompting—language models...

## Key Concepts
- **Modules**: Building blocks for LM programs
- **Signatures**: Input/output specifications  
- **Teleprompters**: Optimization algorithms
- **Predictors**: Core reasoning components

## Architecture
- `/dspy/`: Main package directory
  - `/adapters/`: Input/output format handlers
  - `/clients/`: LM client interfaces
  - `/predict/`: Core prediction modules
  - `/teleprompt/`: Optimization algorithms

## Usage Examples
1. **Building a Classifier**: Using DSPy, a user can define a modular classifier that takes in text data and categorizes it into predefined classes. The user can specify the classification logic declaratively, allowing for easy adjustments and optimizations.
2. **Creating a RAG Pipeline**: A developer can implement a retrieval-augmented generation pipeline that first retrieves relevant documents based on a query and then generates a coherent response using those documents. DSPy facilitates the integration of retrieval and generation components seamlessly.
3. **Optimizing Prompts**: Users can leverage DSPy to create a system that automatically optimizes prompts for language models based on performance metrics, improving the quality of responses over time without manual intervention.
4. **Implementing Agent Loops**: A user can design an agent loop that continuously interacts with users, learns from feedback, and refines its responses, showcasing the self-improving capabilities of the DSPy framework.
5. **Compositional Code**: Developers can write compositional code that allows different modules of the AI system to interact with each other, enabling complex workflows that can be easily modified and extended.
```

The resulting `llms.txt` file provides a comprehensive, LLM-friendly overview of the DSPy repository that can help other AI systems better understand and work with the codebase.

## Next Steps

- Extend the program to analyze multiple repositories
- Add support for different documentation formats
- Create metrics for documentation quality assessment
- Build a web interface for interactive repository analysis


--- pytest.txt ---
   Building lmstudio-llmstxt-generator @ file:///C:/Users/user/projects/WIP/lms-llmsTxt-generator
      Built lmstudio-llmstxt-generator @ file:///C:/Users/user/projects/WIP/lms-llmsTxt-generator
Uninstalled 1 package in 9ms
Installed 1 package in 7ms
============================= test session starts =============================
platform win32 -- Python 3.13.5, pytest-8.4.2, pluggy-1.6.0
rootdir: C:\Users\user\projects\WIP\lms-llmsTxt-generator
configfile: pyproject.toml
plugins: anyio-4.11.0
collected 10 items

tests\test_analyzer.py .                                                 [ 10%]
tests\test_full_builder.py ...                                           [ 40%]
tests\test_lmstudio.py ......                                            [100%]

============================= 10 passed in 5.19s ==============================


--- artifacts/AcidicSoil/dspy-file/dspy-file-llms.txt ---
# Dspy File

> dspyteach is a DSPy-powered CLI tool that analyzes source files to generate teaching briefs or refactor templates, aiding developers in understanding code structure, workflows, and best practices. It supports multiple file types and integrates with various language models for analysis.

**Remember:**
- DSPy integration for structured AI inference
- CLI-based file analysis with customizable providers
- Support for teaching briefs and refactor templates
- Multi-file and directory processing capabilities
- Environment configuration via .env or CLI flags
- Output generation as markdown reports

## Docs
- [Shared  Tm  Overview.Tm.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/_shared__tm__overview.tm.refactor.md): install & quickstart.
- [Tm Overview.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/codex-prompts/tm-overview.refactor.md): install & quickstart.
- [10 Scaffold  Conventions  Version Control Guide.Conventions.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/10-scaffold__conventions__version-control-guide.conventions.refactor.md): worked example.
- [40 Testing  Coverage  Guide.Coverage.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/40-testing__coverage__guide.coverage.refactor.md): worked example.
- [50 Docs  Api Docs  Api Docs Local.Api Docs.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/50-docs__api-docs__api-docs-local.api-docs.refactor.md): worked example.
- [50 Docs  Api Docs  Openapi Generate.Api Docs.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/50-docs__api-docs__openapi-generate.api-docs.refactor.md): worked example.
- [50 Docs  Doc Plan  Gemini Map.Doc Plan.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/50-docs__doc-plan__gemini-map.doc-plan.refactor.md): worked example.
- [50 Docs  Doc Plan  Owners.Doc Plan.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/50-docs__doc-plan__owners.doc-plan.refactor.md): worked example.
- [50 Docs  Examples  Api Usage.Examples.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/50-docs__examples__api-usage.examples.refactor.md): worked example.
- [50 Docs  Examples  Reference Implementation.Examples.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/50-docs__examples__reference-implementation.examples.refactor.md): API reference.

## Tutorials
- [Example Response Format Usage](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-response-format-usage.py): worked example.
- [00 Ideation  Architecture  Adr New.Architecture.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/00-ideation__architecture__adr-new.architecture.refactor.md): worked example.
- [00 Ideation  Architecture  Logging Strategy.Architecture.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/00-ideation__architecture__logging-strategy.architecture.refactor.md): worked example.
- [00 Ideation  Architecture  Modular Architecture.Architecture.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/00-ideation__architecture__modular-architecture.architecture.refactor.md): worked example.
- [00 Ideation  Architecture  Stack Evaluation.Architecture.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/00-ideation__architecture__stack-evaluation.architecture.refactor.md): worked example.
- [00 Ideation  Design  Action Diagram.Design.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/00-ideation__design__action-diagram.design.refactor.md): worked example.
- [00 Ideation  Design  Api Contract.Design.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/00-ideation__design__api-contract.design.refactor.md): worked example.
- [00 Ideation  Design  Design Assets.Design.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/00-ideation__design__design-assets.design.refactor.md): worked example.
- [00 Ideation  Design  Ui Screenshots.Design.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/00-ideation__design__ui-screenshots.design.refactor.md): worked example.
- [00 Ideation  Requirements  Plan Delta.Requirements.Refactor](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/example-data/prompt-front-matter/00-ideation__requirements__plan-delta.requirements.refactor.md): worked example.

## Optional
- [Bump](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/.taskmaster/scripts/bump.md): docs page.
- [Agents](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/AGENTS.md): docs page.
- [Gemini](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/GEMINI.md): docs page.
- [README](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/README.md): docs page.
- [Json Response Integration Report](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/json_response_integration_report.md): docs page.

## Dspy File
- [Analyze File Cli](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/dspy_file/analyze_file_cli.py): docs page.
- [File Analyzer](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/dspy_file/file_analyzer.py): docs page.
- [File Helpers](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/dspy_file/file_helpers.py): docs page.
- [Init](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/dspy_file/__init__.py): docs page.
- [Refactor Analyzer](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/dspy_file/refactor_analyzer.py): docs page.
- [Signatures](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/dspy_file/signatures.py): docs page.
- [Code Fence](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/dspy_file/prompts/code-fence.md): docs page.
- [Front Matter V2](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/dspy_file/prompts/front-matter-v2.md): docs page.
- [Gemini Cli Command Prompt Template](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/dspy_file/prompts/gemini-cli-command_prompt-template.md): docs page.
- [Gemini Cli Extension Command Prompt Template](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/dspy_file/prompts/gemini-cli_extension-command_prompt-template.md): docs page.

## Tests
- [Conftest](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/tests/conftest.py): docs page.
- [Smoke Test](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/tests/smoke_test.py): docs page.
- [Test Cli Connectivity](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/tests/test_cli_connectivity.py): docs page.
- [Test File Helpers](https://raw.githubusercontent.com/AcidicSoil/dspy-file/main/tests/test_file_helpers.py): docs page.


--- artifacts/AcidicSoil/dspy-file/dspy-file-llms-full.txt ---
# llms-full (private-aware)
> Built by GitHub fetches (raw or authenticated). Large files may be truncated.

--- example-data/prompt-front-matter/_shared__tm__overview.tm.refactor.md ---
# TaskMaster Overview

## Metadata

- **identifier**: tm-overview  
- **category**: summarization  
- **lifecycle_stage**: inspection  
- **dependencies**: tasks.json  
- **provided_artifacts**: overview bullets, totals table, top pending list, critical path list, issues list  
- **summary**: Summarize TaskMaster tasks.json by status, priority, and dependency health to orient work.

## Inputs

- `tasks.json` path (optional; defaults to repo root)

## Canonical taxonomy (exact strings)

- summarization
- analysis
- reporting

### Stage hints (for inference)

- inspection → summarizing state, reading data
- analysis → detecting cycles, computing paths
- reporting → outputting tables and lists

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).  

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- Body text is not altered.

## Validation

- Identifier matches a normalized id pattern (e.g., kebab-case).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints (inspection, analysis, reporting).
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Input: `/tm-overview`  
  Output:  
    # Overview  
    - Bullet summary of status, priority, dependencies  
    ## Totals  
    | status       | count | percent | notes         |  
    |--------------|-------|---------|---------------|  
    | pending      | 5     | 40%     | high volume   |  
    | in_progress  | 3     | 25%     | active        |  
    | blocked      | 1     | 8%      | dependency    |  
    | done         | 6     | 50%     | completed     |  
    ## Top Pending  
    | id   | title               | priority | unblockers          |  
    |------|---------------------|----------|---------------------|  
    | t-12 | Fix login timeout   | high     | resolve API error   |  
    | t-34 | Deploy frontend     | medium   | wait for backend    |  
    ## Critical Path  
    - t-12 → t-34 → t-56  
    ## Issues  
    - Cycle detected: t-78 → t-90 → t-78  
    - Missing reference: t-11 (no dependencies)  
    - Duplicate entry: t-44 appears twice  

---

# TaskMaster Overview

Trigger: /tm-overview

Purpose: Summarize the current TaskMaster tasks.json by status, priority, dependency health, and critical path to orient work.

Steps:

1. Locate the active tasks.json at repo root or the path supplied in the user message. Do not modify it.
2. Parse fields: id, title, description, status, priority, dependencies, subtasks.
3. Compute counts per status and a table of top pending items by priority.
4. Detect dependency issues: cycles, missing ids, orphans (no deps and not depended on).
5. Approximate a critical path: longest dependency chain among pending→in_progress tasks.

Output format:

- "# Overview" then a bullets summary.
- "## Totals" as a 4-column table: status | count | percent | notes.
- "## Top Pending" table: id | title | priority | unblockers.
- "## Critical Path" as an ordered list of ids with short titles.
- "## Issues" list for cycles, missing references, duplicates.

Examples:

- Input (Codex TUI): /tm-overview
- Output: tables and lists as specified. Keep to <= 200 lines.

Notes:

- Read-only. Assume statuses: pending | in_progress | blocked | done.
- If tasks.json is missing or invalid, output an "## Errors" section with a concise diagnosis.

Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## template_markdown ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.


--- example-data/codex-prompts/tm-overview.refactor.md ---
# TaskMaster Overview

Trigger: $1

Purpose: Summarize the current TaskMaster tasks.json by status, priority, dependency health, and critical path to orient work.

Steps:

1. Locate the active tasks.json at repo root or the path supplied in the user message. Do not modify it.
2. Parse fields: id, title, description, status, priority, dependencies, subtasks.
3. Compute counts per status and a table of top pending items by priority.
4. Detect dependency issues: cycles, missing ids, orphans (no deps and not depended on).
5. Approximate a critical path: longest dependency chain among pending→in_progress tasks.

Output format:

- "# Overview" then a bullets summary.
- "## Totals" as a 4-column table: status | count | percent | notes.
- "## Top Pending" table: id | title | priority | unblockers.
- "## Critical Path" as an ordered list of ids with short titles.
- "## Issues" list for cycles, missing references, duplicates.

Examples:

- Input (Codex TUI): $2
- Output: tables and lists as specified. Keep to <= 200 lines.

Notes:

- Read-only. Assume statuses: pending | in_progress | blocked | done.
- If tasks.json is missing or invalid, output an "## Errors" section with a concise diagnosis.

---

### Affected files
$3

### Root cause
$4

### Proposed fix
$5

### Tests
$6

### Docs gaps
$7

### Open questions
$8


--- example-data/prompt-front-matter/10-scaffold__conventions__version-control-guide.conventions.refactor.md ---
# Version Control Guide

## Metadata

- **Identifier**: version-control-guide
- **Categories**: development practice, workflow guide, code hygiene
- **Stage**: implementation
- **Dependencies**: none
- **Provided Artifacts**: checklist, suggested commands
- **Summary**: Enforce clean incremental commits and clean-room re-implementation to ensure reproducible and safe changes.

## Inputs

- Trigger: /version-control-guide
- Purpose: Enforce clean incremental commits and clean-room re-implementation when finalizing.
- Output format: Checklist plus suggested commands for the current repo state.
- Examples: Convert messy spike into three commits: setup, feature, tests.
- Notes: Never modify remote branches without confirmation.

## Canonical taxonomy (exact strings)

- development practice
- workflow guide
- code hygiene

### Stage hints (for inference)

- implementation
- commit workflow
- development lifecycle

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- Output body unchanged.

## Validation

- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Identifier: version-control-guide  
- Categories: development practice, workflow guide, code hygiene  
- Stage: implementation  
- Dependencies: none  
- Provided Artifacts: checklist, suggested commands  
- Summary: Enforce clean incremental commits and clean-room re-implementation to ensure reproducible and safe changes.

# Version Control Guide

Trigger: /version-control-guide

Purpose: Enforce clean incremental commits and clean-room re-implementation when finalizing.

## Steps

1. Start each feature from a clean branch: `git switch -c <feat>`.
2. Commit in vertical slices with passing tests: `git add -p && git commit`.
3. When solution is proven, recreate a minimal clean diff: stash or copy results, reset, then apply only the final changes.
4. Use `git revert` for bad commits instead of force-pushing shared branches.

## Output format

- Checklist plus suggested commands for the current repo state.

## Examples

- Convert messy spike into three commits: setup, feature, tests.

## Notes

- Never modify remote branches without confirmation.


--- example-data/prompt-front-matter/40-testing__coverage__guide.coverage.refactor.md ---
# Coverage Plan

## Inputs
- Command: `/coverage-guide`
- Input context: none (command runs without arguments)
- Expected output format: concise summary, prioritized recommendations with rationale, coverage gaps and validation steps

## Canonical taxonomy (exact strings)
- testing
- analysis
- prioritization

### Stage hints (for inference)
- analyze → gather data and propose insights
- plan → suggest actionable items
- execute → run tests or apply changes

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.  
   → Identifier: coverage-plan

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).  
   → Categories: testing, analysis, prioritization

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.  
   → Stage: analyze

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).  
   → Dependencies: find . -name 'coverage*', git ls-files

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.  
   → Artifacts: prioritized test recommendations, coverage gap identification, validation steps

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”  
   → Summary: Suggest a plan to raise coverage based on uncovered areas to achieve actionable, high-ROI test additions.

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.  

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤7.
- All values are derived from content or inference using canonical taxonomy and stage hints.

## Validation
- Identifier matches a normalized id pattern → yes (coverage-plan)
- Categories non-empty and drawn from canonical taxonomy (≤3) → yes
- Stage, if present, is one of the allowed stages implied by stage hints → yes (analyze)
- Dependencies, if present, are id-shaped (≤5) → yes
- Summary ≤120 chars; punctuation coherent → 118 characters
- Body text $1 is not altered.

## Output format examples
- Focus on src/auth/login.ts — 0% branch coverage; add error path test.
- Prioritize authentication modules with low branch coverage (e.g., login, token validation).
- Identify missing edge cases in user input handling and validate via unit tests.


--- example-data/prompt-front-matter/50-docs__api-docs__api-docs-local.api-docs.refactor.md ---
# API Docs Local

## Metadata

- **identifier**: api-docs-local
- **categories**: [documentation, retrieval, storage]
- **lifecycle_stage**: configuration
- **dependencies**: []
- **provided_artifacts**: ["docs/apis/ directory", "DOCS.md index file"]
- **summary**: Do fetch API docs and store locally to achieve offline, deterministic reference.

## Inputs

- URLs or package names to retrieve documentation from.

## Canonical taxonomy (exact strings)

- documentation
- retrieval
- storage
- configuration
- generation
- deployment
- validation

### Stage hints (for inference)

- configuration → setup of environment or initial state
- retrieval → fetching data from external sources
- storage → saving content locally
- deployment → making system available to users

## Algorithm

1. Extract signals from $1  
   *Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.*

2. Determine the primary identifier  
   *Prefer explicit input; otherwise infer from main action + object.*  
   *Normalize (lowercase, kebab-case, length-capped, starts with a letter).*  
   *De-duplicate.*

3. Determine categories  
   *Prefer explicit input; otherwise infer from verbs/headings vs $5.*  
   *Validate, sort deterministically, and de-dupe (≤3).*

4. Determine lifecycle/stage (optional)  
   *Prefer explicit input; otherwise map categories via $6.*  
   *Omit if uncertain.*

5. Determine dependencies (optional)  
   *Parse phrases implying order or prerequisites; keep id-shaped items (≤5).*

6. Determine provided artifacts (optional)  
   *Short list (≤3) of unlocked outputs.*

7. Compose summary  
   *One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”*

8. Produce metadata in the requested format  
   *Default to a human-readable serialization; honor any requested alternative.*

9. Reconcile if input already contains metadata  
   *Merge: explicit inputs > existing > inferred.*  
   *Validate lists; move unknowns to an extension field if needed.*  
   *Remove empty keys.*

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.

## Validation

- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Command list and file paths to place docs under `docs/apis/`.
- Example: curl -o docs/apis/github.com/api.json https://api.github.com/docs
- Example: npm view express docs --json > docs/apis/express/README.md

# API Docs Local

Trigger: /api-docs-local

Purpose: Fetch API docs and store locally for offline, deterministic reference.

## Steps

1. Create `docs/apis/` directory.
2. For each provided URL or package, write retrieval commands (curl or `npm view` docs links). Do not fetch automatically without confirmation.
3. Add `DOCS.md` index linking local copies.

## Output format

- Command list and file paths to place docs under `docs/apis/`.


--- example-data/prompt-front-matter/50-docs__api-docs__openapi-generate.api-docs.refactor.md ---
# OpenAPI Generate

## Metadata

- **Identifier**: generate-api  
- **Categories**: code generation, api scaffolding, build  
- **Stage**: build  
- **Dependencies**: none  
- **Provided Artifacts**: 
  - Summary table of generated paths  
  - Scripts to add (e.g., `make generate-api`, `pnpm sdk:gen`)  
  - TODO list for unimplemented handlers  
- **Summary**: Generate server stubs or typed clients from an OpenAPI spec to achieve code scaffolding with validation and CI checks.

## Inputs

- Command: `/openapi-generate <server|client> <lang> <spec-path>`
- Parameters:
  - `<server>`: Generates controllers, routers, validation, and error middleware into `apps/api`
  - `<client>`: Generates a typed SDK into `packages/sdk` with fetch wrapper and retry/backoff
  - `<spec-path>`: Path to OpenAPI spec (e.g., `apis/auth/openapi.yaml`)
- Output format: Summary table of generated paths, scripts to add, and next actions

## Canonical taxonomy (exact strings)

- code generation  
- api scaffolding  
- build  

### Stage hints (for inference)

- generate → build  
- scaffold → build  
- script addition → build  

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.  

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.  

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).  

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.  

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).  

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.  

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”  

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.  

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.  

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.

## Validation

- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- `/openapi-generate client ts apis/auth/openapi.yaml`
- Output: Summary table of generated paths, scripts to add, and next actions
- Notes: Prefer openapi-typescript + zod for TS clients when possible

---

# OpenAPI Generate

Trigger: /openapi-generate <server|client> <lang> <spec-path>

Purpose: Generate server stubs or typed clients from an OpenAPI spec.

**Steps:**

1. Validate `<spec-path>`; fail with actionable errors.
2. For `server`, generate controllers, routers, validation, and error middleware into `apps/api`.
3. For `client`, generate a typed SDK into `packages/sdk` with fetch wrapper and retry/backoff.
4. Add `make generate-api` or `pnpm sdk:gen` scripts and CI step to verify no drift.
5. Produce a diff summary and TODO list for unimplemented handlers.

**Output format:** summary table of generated paths, scripts to add, and next actions.

**Examples:** `/openapi-generate client ts apis/auth/openapi.yaml`.

**Notes:** Prefer openapi-typescript + zod for TS clients when possible.


--- example-data/prompt-front-matter/50-docs__doc-plan__gemini-map.doc-plan.refactor.md ---
# Gemini→Codex Mapper

Task: Given a TOML configuration for a Gemini CLI command, produce a structured Codex prompt file with metadata and example usage. The output must be ready to run via bash.

## Inputs
- TOML input containing `description`, `prompt`, and optional `Expected output` or `Usage`
- Target output format constraints (≤300 words, specific sections)

## Canonical taxonomy (exact strings)
- migration
- prompts
- tooling
- transform
- build
- validate

### Stage hints (for inference)
- "translation" → transform  
- "generates", "writes", "creates" → build  
- "validates" → validate  

## Algorithm
1. Extract signals from TOML:
   - Description and prompt define intent.
   - Expected output defines structure of result.

2. Determine the primary identifier:
   - Prefer explicit input; otherwise infer from main action + object.
   - Normalize to lowercase, kebab-case, length-capped (≤32), starts with letter.
   - Result: `gemini-map`

3. Determine categories:
   - Prefer explicit tags: migration, prompts, tooling
   - Validate and de-dupe → [migration, prompts, tooling]

4. Determine lifecycle/stage:
   - Map from "translation" to "transform"
   - Stage: transform

5. Determine dependencies:
   - No prerequisites mentioned.
   - Dependencies: []

6. Determine provided artifacts:
   - Codex prompt file (structured with role, steps, output, example)
   - Bash snippet for writing the file to `~/.codex/prompts/<filename>.md`

7. Compose summary:
   - "Do translate a Gemini CLI TOML command into a Codex prompt file to achieve structured, reusable prompt generation."

8. Produce metadata in human-readable format:
   - identifier: gemini-map
   - categories: migration, prompts, tooling
   - stage: transform
   - dependencies: []
   - artifacts: codex-prompt-file, bash-write-snippet
   - summary: Do translate a Gemini CLI TOML command into a Codex prompt file to achieve structured, reusable prompt generation.

9. Reconcile if input already contains metadata:
   - No existing metadata; all derived from explicit or inferable signals.

## Assumptions & Constraints
- Output must include metadata block followed by blank line and original body unchanged.
- All identifiers normalized and within constraints.
- Categories strictly from canonical taxonomy.
- Stage inferred via stage hints only if not explicit.
- Artifacts are short-listed (≤3).
- Summary ≤120 characters.

## Validation
- Identifier: `gemini-map` → valid kebab-case, lowercase.
- Categories: [migration, prompts, tooling] → all in taxonomy, non-empty, de-duplicated.
- Stage: transform → valid and implied by translation workflow.
- Dependencies: empty list → valid.
- Artifacts: codex-prompt-file, bash-write-snippet → both valid and ≤3.
- Summary: 108 characters; coherent and punctuated correctly.

## Output format examples
```markdown
# Gemini→Codex Mapper

identifier: gemini-map  
categories: migration, prompts, tooling  
stage: transform  
dependencies: []  
artifacts: codex-prompt-file, bash-write-snippet  
summary: Do translate a Gemini CLI TOML command into a Codex prompt file to achieve structured, reusable prompt generation.

You are a translator that converts a Gemini CLI TOML command into a Codex prompt file.

Steps:

1) Read TOML with `description` and `prompt`.
2) Extract the task, inputs, and outputs implied by the TOML.
3) Write a Codex prompt file ≤ 300 words:

    - Role line `You are ...`
    - Numbered steps
    - Output section
    - Example input and expected output
    - `Usage: /<command>` line
    - YAML-like metadata at top

4) Choose a short, hyphenated filename ≤ 32 chars.
5) Emit a ready-to-run bash snippet:
`cat > ~/.codex/prompts/<filename>.md << 'EOF'` … `EOF`.
6) Do not include destructive commands or secrets.

Example input:

```toml
description = "Draft a PR description"
prompt = "Create sections Summary, Context, Changes from diff stats"
Expected output:

A pr-desc.md file with the structure above and a bash cat > block.

Usage: /gemini-map
```
```


--- example-data/prompt-front-matter/50-docs__doc-plan__owners.doc-plan.refactor.md ---
# Owners

## Inputs
- Path to analyze (e.g., `src/components/Button.tsx`)
- Access to `.github/CODEOWNERS` file
- Git repository with recent commit logs (`git log --pretty='- %an %ae: %s'`)

## Canonical taxonomy (exact strings)
- CLI
- ownership
- review

### Stage hints (for inference)
- discovery
- analysis
- suggestion

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤7.

## Validation
- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: owners  
- Categories: CLI, ownership, review  
- Stage: discovery  
- Dependencies: CODEOWNERS file, git log access  
- Artifacts: @frontend-team (CODEOWNERS), @jane (last 5 commits)  
- Summary: Suggest owners/reviewers for a path using CODEOWNERS and commit history.

---

Trigger: /owners <path>

Purpose: Suggest likely owners or reviewers for the specified path.

You are a CLI assistant focused on helping contributors with the task: Suggest likely owners/reviewers for a path.

1. Gather context by inspecting `.github/CODEOWNERS` for the codeowners (if present); running `git log --pretty='- %an %ae: %s' -- {{args}} | sed -n '1,50p'` for the recent authors for the path.
2. Based on CODEOWNERS and git history, suggest owners.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Suggest likely owners/reviewers for a path.
- Reference evidence from CODEOWNERS or git history for each owner suggestion.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
src/components/Button.tsx

Expected Output:

- Likely reviewers: @frontend-team (CODEOWNERS), @jane (last 5 commits).


--- example-data/prompt-front-matter/50-docs__examples__api-usage.examples.refactor.md ---
# API Usage Analysis

## Metadata

- **identifier**: http-client
- **category**: API Usage Analysis
- **lifecycle_stage**: analysis
- **dependencies**: [rg, grep]
- **provided_artifacts**: 
  - Definition: src/network/httpClient.ts line 42
  - Key usages: services/userService.ts, hooks/useRequest.ts
- **summary**: Do analyze how an internal API is used to achieve clear documentation and visibility into its real-world applications.

## Inputs

- Input symbol: HttpClient
- Tool commands: `rg -n {{args}} . || grep -RIn {{args}} .`

## Canonical taxonomy (exact strings)

- API Usage Analysis
- Code Inspection
- Dependency Mapping
- Documentation Generation

### Stage hints (for inference)

- analysis
- inspection
- gathering
- review
- synthesis

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- All fields must be derived from content or logical inference.

## Validation

- Identifier matches a normalized id pattern (kebab-case, lowercase).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- identifier: http-client  
- category: API Usage Analysis  
- lifecycle_stage: analysis  
- dependencies: [rg, grep]  
- provided_artifacts: 
  - Definition: src/network/httpClient.ts line 42
  - Key usages: services/userService.ts, hooks/useRequest.ts
- summary: Do analyze how an internal API is used to achieve clear documentation and visibility into its real-world applications.


--- example-data/prompt-front-matter/50-docs__examples__reference-implementation.examples.refactor.md ---
# Reference Implementation

## Metadata

- **Identifier**: reference-implementation
- **Categories**: code-generation, api-mapping, diff-generation
- **Lifecycle Stage**: implementation
- **Dependencies**: target-module-path, example-url
- **Provided Artifacts**: side-by-side API table, patch suggestions
- **Summary**: Do map target module's API to reference to achieve consistent structure and naming.

## Steps

1. Accept a path or URL to an example. Extract its public API and patterns.
2. Map target module’s API to the reference.
3. Generate diffs that adopt the same structure and naming.

## Output format

- Side-by-side API table and patch suggestions.


--- example-response-format-usage.py ---
# main.py
# Purpose: Automated documentation-powered code generation pipeline using DSPy.
# Now with integrated GitHub repository analysis.

import dspy
import requests
from bs4 import BeautifulSoup
import html2text
from typing import List, Dict, Any
import json
import time
import subprocess

# Import the GitHub helper functions
from repo_helpers import gather_repository_info

MODEL_NAME = "hf.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF:Q4_K_S"


def stop_ollama_model(model_name: str) -> None:
    """Stop the specified Ollama model to free server resources."""
    try:
        subprocess.run(
            ["ollama", "stop", model_name],
            check=True,
            capture_output=True,
        )
    except subprocess.CalledProcessError as exc:  # pragma: no cover - log warning only
        print(f"Warning: Failed to stop model {model_name}: {exc}")


# --- Data Fetching Classes ---


class DocumentationFetcher:
    """
    Fetches and processes documentation from both standard URLs and GitHub repositories.
    """

    def __init__(self, max_retries=3, delay=1):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
        )
        self.max_retries = max_retries
        self.delay = delay
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = False
        self.html_converter.ignore_images = True

    def fetch_website_url(self, url: str) -> dict:
        """Fetches and cleans content from a standard website URL."""
        for attempt in range(self.max_retries):
            try:
                print(f"📡 Fetching Website: {url} (attempt {attempt + 1})")
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, "html.parser")
                for script in soup(["script", "style", "nav", "footer", "header"]):
                    script.decompose()
                markdown_content = self.html_converter.handle(str(soup))
                return {
                    "url": url,
                    "title": soup.title.string if soup.title else "No title",
                    "content": markdown_content,
                    "success": True,
                }
            except Exception as e:
                print(f"❌ Error fetching {url}: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.delay)
        return {"url": url, "title": "Failed to fetch", "content": "", "success": False}

    def fetch_github_repo(self, url: str) -> dict:
        """Fetches and consolidates content from a GitHub repository."""
        print(f"📦 Fetching GitHub Repo: {url}")
        try:
            # Use the helper function to get README and example files
            combined_content, details = gather_repository_info(url)
            if "Could not access" in combined_content:
                return {
                    "url": url,
                    "title": "Failed to fetch repo",
                    "content": "",
                    "success": False,
                }

            return {
                "url": url,
                "title": f"GitHub Repo: {url.split('/')[-1]}",
                "content": combined_content,
                "success": True,
                "details": details,
            }
        except Exception as e:
            print(f"❌ Error fetching GitHub repo {url}: {e}")
            return {
                "url": url,
                "title": "Failed to fetch repo",
                "content": f"Error: {e}",
                "success": False,
            }

    def fetch_documentation(self, urls: list[str]) -> list[dict]:
        """
        Fetches documentation from a list of URLs, routing to the correct
        fetcher (website or GitHub) based on the URL.
        """
        results = []
        for url in urls:
            if "github.com" in url:
                result = self.fetch_github_repo(url)
            else:
                result = self.fetch_website_url(url)

            results.append(result)
            time.sleep(self.delay)  # Be respectful to servers
        return results


# --- DSPy Signatures and Modules ---


class LibraryAnalyzer(dspy.Signature):
    """Analyze library documentation to understand core concepts, patterns, and examples."""

    library_name: str = dspy.InputField(desc="Name of the library to analyze")
    documentation_content: str = dspy.InputField(
        desc="Combined documentation from websites, READMEs, and code examples"
    )

    core_concepts: list[str] = dspy.OutputField(
        desc="Main concepts, classes, and components of the library."
    )
    common_patterns: list[str] = dspy.OutputField(
        desc="Common usage patterns or workflows."
    )
    key_methods: list[str] = dspy.OutputField(
        desc="List of important methods or functions and their purpose."
    )
    installation_info: str = dspy.OutputField(
        desc="How to install the library (e.g., 'pip install ...')."
    )
    code_examples: list[str] = dspy.OutputField(
        desc="Key code snippets found in the documentation."
    )


class CodeGenerator(dspy.Signature):
    """Generate a complete, working code example for a specific use case."""

    library_info: str = dspy.InputField(
        desc="Summary of the library's concepts, patterns, and methods."
    )
    use_case: str = dspy.InputField(
        desc="The specific task to accomplish with the code."
    )
    requirements: str = dspy.InputField(
        desc="Additional requirements or constraints for the code."
    )

    code_example: str = dspy.OutputField(
        desc="A single, complete, and runnable code block."
    )
    explanation: str = dspy.OutputField(
        desc="A step-by-step explanation of the generated code."
    )
    best_practices: list[str] = dspy.OutputField(
        desc="Tips and best practices for using the library."
    )
    imports_needed: list[str] = dspy.OutputField(
        desc="A list of necessary import statements."
    )


class DocumentationLearningAgent(dspy.Module):
    """Agent that learns from documentation and generates code."""

    def __init__(self):
        super().__init__()
        self.fetcher = DocumentationFetcher()
        self.analyze_docs = dspy.ChainOfThought(LibraryAnalyzer)
        self.generate_code = dspy.ChainOfThought(CodeGenerator)

    def learn_from_urls(self, library_name: str, doc_urls: list[str]) -> Dict:
        """Learns about a library from documentation URLs and GitHub repos."""
        print(f"📚 Learning about {library_name} from {len(doc_urls)} sources...")
        docs = self.fetcher.fetch_documentation(doc_urls)

        combined_content = "\n\n---\n\n".join(
            [
                f"SOURCE: {doc['url']}\n\n{doc['content']}"
                for doc in docs
                if doc["success"]
            ]
        )

        if not combined_content:
            raise ValueError("No documentation could be fetched successfully.")

        analysis = self.analyze_docs(
            library_name=library_name, documentation_content=combined_content
        )

        return {
            "library": library_name,
            "source_urls": [doc["url"] for doc in docs if doc["success"]],
            "core_concepts": analysis.core_concepts,
            "patterns": analysis.common_patterns,
            "methods": analysis.key_methods,
            "installation": analysis.installation_info,
            "examples": analysis.code_examples,
        }

    def generate_example(
        self, library_info: Dict, use_case: str, requirements: str = ""
    ) -> Dict:
        """Generates a code example for a specific use case."""
        info_text = f"""
        Library: {library_info["library"]}
        Core Concepts: {", ".join(library_info["core_concepts"])}
        Common Patterns: {", ".join(library_info["patterns"])}
        Key Methods: {", ".join(library_info["methods"])}
        Installation: {library_info["installation"]}
        """

        code_result = self.generate_code(
            library_info=info_text, use_case=use_case, requirements=requirements
        )

        return {
            "code": code_result.code_example,
            "explanation": code_result.explanation,
            "best_practices": code_result.best_practices,
            "imports": code_result.imports_needed,
        }

def interactive_learning_session():
    """Main interactive session for the library learning system."""
    lm = dspy.LM(
        f"ollama_chat/{MODEL_NAME}",
        api_base="http://localhost:11434",
        api_key="",
        streaming=False,
        cache=False,
        response_format={
        "type": "json_schema",
        "json_schema": {
            "schema": {
                "type": "object",
                "properties": {
                    "project": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string"},
                            "description": {"type": "string"},
                        },
                        "required": ["name", "description"],
                    },
                    "key_concepts": {
                        "type": "array",
                        "items": {"type": "string"},
                        "minItems": 1,
                    },
                    "architecture_overview": {"type": "string"},
                    "important_directories": {
                        "type": "array",
                        "items": {"type": "string"},
                        "minItems": 1,
                    },
                    "entry_points": {
                        "type": "array",
                        "items": {"type": "string"},
                        "minItems": 1,
                    },
                    "development_info": {
                        "type": "object",
                        "properties": {
                            "test_dependencies": {
                                "type": "array",
                                "items": {"type": "string"},
                                "minItems": 1,
                            },
                            "linting_tools": {
                                "type": "array",
                                "items": {"type": "string"},
                                "minItems": 1,
                            },
                            "optional_dependencies": {
                                "type": "array",
                                "items": {"type": "string"},
                            },
                        },
                        "required": ["test_dependencies", "linting_tools"],
                    },
                    "usage_examples": {
                        "type": "array",
                        "items": {"type": "string"},
                        "minItems": 1,
                    },
                },
                "required": [
                    "project",
                    "key_concepts",
                    "architecture_overview",
                    "important_directories",
                    "entry_points",
                    "development_info",
                    "usage_examples",
                ],
            }
        },
    },
    )
    dspy.configure(lm=lm)

    learned_libraries = {}

    try:
        print("🎯 Welcome to the Interactive Library Learning System!")
        print("This system now supports learning from websites AND GitHub repositories.\n")

        agent = DocumentationLearningAgent()

        while True:
            print("\n" + "=" * 60)
            library_name = input(
                "\n📚 Enter the library name (or 'quit' to exit): "
            ).strip()
            if library_name.lower() in ["quit", "exit", "q"]:
                break

            print(
                f"\n🔗 Enter documentation URLs or a GitHub repo URL for {library_name} (one per line, empty line to finish):"
            )
            urls = []
            while True:
                url = input("  URL: ").strip()
                if not url:
                    break
                urls.append(url)

            if not urls:
                continue

            try:
                # Step 1: Learn about the library from the provided sources
                library_info = agent.learn_from_urls(library_name, urls)
                print(f"\n✅ Successfully learned {library_name}!")
                print(f"   - Core Concepts: {library_info.get('core_concepts', 'N/A')}")
                print(f"   - Installation: {library_info.get('installation', 'N/A')}")

                # Step 2: Get all use cases from the user upfront
                print(
                    f"\n🎯 Define use cases for {library_name} (one per line, empty line to finish):"
                )
                use_cases = []
                while True:
                    use_case = input("     Use case: ").strip()
                    if not use_case:
                        break
                    use_cases.append(use_case)

                if not use_cases:
                    print("No use cases provided. Moving on.")
                    continue

                # Step 3: Generate and display all examples
                print(f"\n🔧 Generating {len(use_cases)} examples for {library_name}...")
                all_examples = []
                for i, use_case in enumerate(use_cases, 1):
                    print(f"\n--- EXAMPLE {i}/{len(use_cases)}: {use_case} ---")

                    example = agent.generate_example(library_info, use_case)
                    all_examples.append({"use_case": use_case, **example})

                    print("\n💻 Code Example:")
                    print(f"```python\n{example['code']}\n```")

                    print("\n📦 Required Imports:")
                    print("\n".join([f"  • {imp}" for imp in example["imports"]]))

                    print("\n📝 Explanation:")
                    print(example["explanation"])

                    print("\n✅ Best Practices:")
                    print("\n".join([f"  • {bp}" for bp in example["best_practices"]]))
                    print("--- END OF EXAMPLE ---")

                # Store the complete results
                learned_libraries[library_name] = {
                    "library_info": library_info,
                    "examples": all_examples,
                }

                # Step 4: Offer to save the results to a file
                save_results = (
                    input(f"\n💾 Save learning results for {library_name} to file? (y/n): ")
                    .strip()
                    .lower()
                )
                if save_results in ["y", "yes"]:
                    filename = input(
                        f"   Enter filename (default: {library_name.lower()}_learning.json): "
                    ).strip()
                    if not filename:
                        filename = f"{library_name.lower()}_learning.json"

                    try:
                        with open(filename, "w", encoding="utf-8") as f:
                            json.dump(
                                learned_libraries[library_name], f, indent=2, default=str
                            )
                        print(f"   ✅ Results saved to {filename}")
                    except Exception as e:
                        print(f"   ❌ Error saving file: {e}")

            except Exception as e:
                print(f"❌ An error occurred while learning {library_name}: {e}")

        print("\n👋 Thanks for using the Interactive Library Learning System!")
        if learned_libraries:
            print(f"\n🎉 Session Summary:")
            print(
                f"Successfully learned {len(learned_libraries)} libraries: {list(learned_libraries.keys())}"
            )
    finally:
        stop_ollama_model(MODEL_NAME)


if __name__ == "__main__":
    interactive_learning_session()


--- example-data/prompt-front-matter/00-ideation__architecture__adr-new.architecture.refactor.md ---
# ADR Drafting Assistant

Task: Given the following prompt, produce a structured **metadata block** and then emit the original body unchanged. The metadata must expose identifiers, categories, optional lifecycle/stage, optional dependencies, optional provided artifacts, and a concise summary. Output = metadata, blank line, then the input text.

## Inputs
- Input prompt: "You are a CLI assistant focused on helping contributors with the task: Draft an Architecture Decision Record with pros/cons."
- Workflow steps: Gather context from `README.md`, draft ADR (Context, Decision, Status, Consequences), synthesize insights.
- Output requirements: Concise summary of goal; workflow triggers/failing jobs/proposed fixes; documented evidence for maintainers' trust.

## Canonical taxonomy (exact strings)
- architecture
- decision-making
- documentation

### Stage hints (for inference)
- ideation → early drafting, context gathering
- planning → structured output design
- implementation → actual code changes
- review → peer feedback or approval

## Algorithm
1. Extract signals from input:
   - Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.
2. Determine the primary identifier:
   - Prefer explicit input; otherwise infer from main action + object.
   - Normalize (lowercase, kebab-case, length-capped, starts with a letter).
   - De-duplicate.
3. Determine categories:
   - Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.
   - Validate, sort deterministically, and de-dupe (≤3).
4. Determine lifecycle/stage (optional):
   - Prefer explicit input; otherwise map categories via stage hints.
   - Omit if uncertain.
5. Determine dependencies (optional):
   - Parse phrases implying order or prerequisites; keep id-shaped items (≤5).
6. Determine provided artifacts (optional):
   - Short list (≤3) of unlocked outputs.
7. Compose summary:
   - One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”
8. Produce metadata in the requested format:
   - Default to a human-readable serialization; honor any requested alternative.
9. Reconcile if input already contains metadata:
   - Merge: explicit inputs > existing > inferred.
   - Validate lists; move unknowns to an extension field if needed.
   - Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then the original body.
- Limit distinct placeholders to ≤7.

## Validation
- Identifier matches a normalized id pattern (e.g., kebab-case, lowercase).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Artifacts are short (≤3) and relevant to output.
- Summary ≤120 chars; punctuation coherent.
- Body text is not altered.

## Output format examples
- Identifier: `adr-draft`
- Categories: architecture, decision-making, documentation
- Lifecycle stage: ideation
- Dependencies: README.md
- Provided artifacts: ADR with pros/cons, evidence summary, workflow insights
- Summary: "Draft an Architecture Decision Record with pros/cons to achieve transparent decision documentation."


--- example-data/prompt-front-matter/00-ideation__architecture__logging-strategy.architecture.refactor.md ---
# Logging Strategy

## Metadata

- identifier: logging-strategy
- categories: [observability, operations, security]
- stage: design
- dependencies: []
- provided_artifacts: ["diff hunks", "short guideline section"]
- summary: Do add or remove diagnostic logs with privacy in mind to achieve structured observability.

## Steps

1. Identify hotspots from recent failures.
2. Insert structured logs with contexts and correlation IDs.
3. Remove noisy or PII-leaking logs.
4. Document log levels and sampling in `OBSERVABILITY.md`.

## Output format

- Diff hunks and a short guideline section.


--- example-data/prompt-front-matter/00-ideation__architecture__modular-architecture.architecture.refactor.md ---
# Modular Architecture

## Metadata

- **identifier**: modular-architecture  
- **categories**: architecture  
- **stage**: design  
- **dependencies**: [module-boundaries-identification]  
- **provided-artifacts**: [module-graph, dependency-diff, contract-test-plan]  
- **summary**: Do modularize services to achieve clear boundaries and testable interfaces.

## Steps

1. Identify services/modules and their public contracts.
2. Flag cross-module imports and circular deps.
3. Propose boundaries, facades, and internal folders.
4. Add "contract tests" for public APIs.

## Output format

- Diagram-ready list of modules and edges, plus diffs.


--- example-data/prompt-front-matter/00-ideation__architecture__stack-evaluation.architecture.refactor.md ---
# Stack Evaluation

## Metadata

- identifier: stack-evaluation
- categories: [evaluation, analysis, recommendation]
- stage: evaluation
- dependencies: []
- provided_artifacts: ["decision memo", "next steps"]
- summary: Evaluate language/framework choices to achieve informed stay-or-switch decisions.

## Steps

1. Detect current stack and conventions.
2. List tradeoffs: maturity, tooling, available examples, hiring, and AI training coverage.
3. Recommend stay-or-switch with migration outline if switching.

## Output format

- Decision memo with pros/cons and next steps.


--- example-data/prompt-front-matter/00-ideation__design__action-diagram.design.refactor.md ---
# Action Diagram Metadata

## Inputs
- Source file path: C:\Users\user\projects\prompts\temp-prompts\00-ideation\design\action-diagram.design.md
- Maximum placeholders allowed: 7

## Canonical taxonomy (exact strings)
- devops
- pipeline
- workflow

### Stage hints (for inference)
- build → development stage
- deploy → production stage
- push → trigger stage

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.

## Validation
- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: build  
- Categories: devops, pipeline, workflow  
- Lifecycle stage: none  
- Dependencies: push  
- Provided artifacts: deployment artifact  
- Summary: Do build to achieve deployment after push

## Metadata
- identifier: build
- categories: ["devops", "pipeline", "workflow"]
- lifecycle_stage: null
- dependencies: ["push"]
- provided_artifacts: ["deployment artifact"]
- summary: Do build to achieve deployment after push

## Nodes
- build
- deploy

## Edges
- push -> build
- build -> deploy


--- example-data/prompt-front-matter/00-ideation__design__api-contract.design.refactor.md ---
# API Contract Design

## Inputs
- Feature or domain string (e.g., "accounts & auth")
- Existing documentation and requirements
- Preference for OpenAPI 3.1 or GraphQL SDL

## Canonical taxonomy (exact strings)
- design
- specification
- contract generation

### Stage hints (for inference)
- design → initial creation of a contract from inputs
- specification → detailed schema definition
- implementation → code generation phase

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- All categories must be from the canonical taxonomy.
- Stage mapping is deterministic and context-aware.

## Validation
- Identifier matches a normalized id pattern (e.g., api-contract).
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Dependencies must be explicit or inferable from input structure.
- Artifacts list ≤3 items; all valid outputs.
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: `api-contract`  
- Categories: design, specification, contract generation  
- Stage: design  
- Dependencies: feature/domain input, existing documentation  
- Artifacts: openapi.yaml, schema.graphql, changelog entry  
- Summary: "Do generate an API contract from requirements to achieve a standardized specification for endpoints."

---

# API Contract

Trigger: /api-contract "<feature or domain>"

Purpose: Author an initial OpenAPI 3.1 or GraphQL SDL contract from requirements.

**Steps:**

1. Parse inputs and existing docs. If REST, prefer OpenAPI 3.1 YAML; if GraphQL, produce SDL.
2. Define resources, operations, request/response schemas, error model, auth, and rate limit headers.
3. Add examples for each endpoint or type. Include pagination and filtering conventions.
4. Save to `apis/<domain>/openapi.yaml` or `apis/<domain>/schema.graphql`.
5. Emit changelog entry `docs/api/CHANGELOG.md` with rationale and breaking-change flags.

**Output format:**

- `Contract Path`, `Design Notes`, and a fenced code block with the spec body.

**Examples:**

- `/api-contract "accounts & auth"` → `apis/auth/openapi.yaml` with OAuth 2.1 flows.

**Notes:**

- Follow JSON:API style for REST unless caller specifies otherwise. Include `429` and `5xx` models.


--- example-data/prompt-front-matter/00-ideation__design__design-assets.design.refactor.md ---
# Design Assets

## Metadata

- **Identifier**: design-assets
- **Categories**: design, brand assets
- **Stage**: generate
- **Dependencies**: brand-colors, brand-name
- **Provided Artifacts**: asset-checklist, generation-commands
- **Summary**: Generate favicons and small design snippets from product brand to achieve consistent visual identity.

## Steps

1. Extract brand colors and name from README or config.
2. Produce favicon set, social preview, and basic UI tokens.
3. Document asset locations and references.

## Output format

- Asset checklist and generation commands.


--- example-data/prompt-front-matter/00-ideation__design__ui-screenshots.design.refactor.md ---
# UI Screenshots

## Metadata

- **Identifier**: ui-screenshots
- **Categories**: analysis, design, code-generation
- **Stage**: design-review
- **Dependencies**: []
- **Provided Artifacts**: issue-list, css-changes, component-updates
- **Summary**: Analyze UI screenshots to identify visual issues and generate actionable CSS or component changes

## Steps

1. Accept screenshot paths or links.
2. Describe visual hierarchy, spacing, contrast, and alignment issues.
3. Output concrete CSS or component changes.

## Output format

- Issue list and code snippets to fix visuals.


--- example-data/prompt-front-matter/00-ideation__requirements__plan-delta.requirements.refactor.md ---
# plan-delta

## Metadata

- **identifier**: plan-delta  
- **categories**: Planning, Task Management, Graph Maintenance  
- **lifecycle_stage**: Mid-Project Adjustment  
- **dependencies**: task graph history, user delta input  
- **provided_artifacts**: 
  - Updated tasks file (valid JSON)  
  - Delta document (Markdown with # Delta, ## Objectives, ## Constraints, ## Impacts, ## Decisions, ## Evidence)  
  - Readiness report (plain text: READY | BLOCKED | DEPRECATED)  
- **summary**: Orchestrate mid-project planning deltas to preserve history and update task graph readiness.

## Inputs

- User-provided delta text with objectives, constraints, findings
- Selection mode: Continue, Hybrid Rebaseline, Full Rebaseline
- Existing tasks file (tasks.json or equivalent)
- Repository context path for task and plan files

## Canonical taxonomy (exact strings)

Planning, Task Management, Graph Maintenance

### Stage hints (for inference)

Mid-project adjustment, delta update, planning revision, graph maintenance, readiness recalculation

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤7.
- All outputs are strictly defined in the output format section.

## Validation

- Identifier matches a normalized id pattern (lowercase, kebab-case).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped or context-based (≤5).
- Provided artifacts match exactly those listed in output format.
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Input →  
  ```
  Mode: Continue
  New objectives: add offline export for tasks
  Constraints: no DB migrations
  Findings: existing export lib supports JSON only
  ```  

  Output →  
  - Updated `tasks.json` with new task `T-342` { title: "Add CSV export", dependencies: ["T-120"], source_doc: "delta-20250921.md", lineage: ["T-120"], supersedes: [] }.  
  - `artifacts/delta-20250921-160500.md` populated with objectives, constraints, impacts, decisions, evidence.  
  - Readiness report lists `T-342` under READY if deps done.

- Input →  
  ```
  Mode: Hybrid Rebaseline
  Changes: ~30% of scope affected by auth provider swap
  ```  

  Output →  
  - Minor-plan version bump recorded in Delta Doc.  
  - New tasks added for provider swap; prior tasks kept with `deprecated` or `blocked` and lineage links.


--- .taskmaster/scripts/bump.md ---
# bump version and commit workflow

uv build
uv version --bump patch --bump beta
git add .
git commit -m "Release: 0.1.4b1"
git tag -a v0.1.4b1 -m "Release 0.1.4b1"
git push origin HEAD --tags

{ yes "" | dt -m refactor ~/.gemini/commands \
    --provider lmstudio \
    --api-base <http://10.0.0.81:1234/v1> --confirm-each; } \
  |& tee "dspyteach.all.$(date +%Y%m%d-%H%M%S).log"

{ dt generate-sequence \
  --goal "Cut a release" ~/.codex/prompts \
    --provider lmstudio \
    --api-base http://10.0.0.81:1234/v1; }

dspyteach rank-prompts "How do I commit my changes?" ~/.codex/prompts 0.6

dt generate-sequence \
  --goal "Cut a release"


{ dspyteach generate-sequence "Cut a release" -P lmstudio -B 10.0.0.81:1234/v1 ;} |& tee "dspyteach.all.$(date +%Y%m%d-%H%M%S).log"


dspyteach -P lmstudio -B 10.0.0.81:1234/v1 generate-sequence "Cut a release"


 dspyteach generate-sequence "Cut a release"
  dspyteach -P lmstudio -B <http://localhost:1234/v1> generate-sequence
  --goal "Cut a release"


--- AGENTS.md ---
<!-- path: ~/projects/dspy-file/AGENTS.md -->

# AGENTS.md — Tool Selection (Python)

## DSPy Framework Playbook (All‑Purpose)

**When to apply:** Use these rules whenever creating or editing DSPy **Signatures**, **Modules**, **Programs**, **Optimizers**, **Metrics**, or **RAG** components in any repository.

**Canonical docs**

- Signatures: <https://dspy.ai/learn/programming/signatures/>
- Modules (Predict / ChainOfThought / ReAct): <https://dspy.ai/learn/programming/modules/>
- Optimizers overview: <https://dspy.ai/learn/optimization/optimizers/>
- MIPROv2: <https://dspy.ai/api/optimizers/MIPROv2/>
- BootstrapFewShot: <https://dspy.ai/api/optimizers/BootstrapFewShot/>
- Assertions: <https://dspy.ai/learn/programming/7-assertions/>
- Metrics: <https://dspy.ai/learn/evaluation/metrics/>
- RAG tutorial: <https://dspy.ai/tutorials/rag/>

### Quick posture

- **Program, don’t prompt.** Encode task instructions in **Signature docstrings**; compose behavior from Modules. Avoid ad‑hoc long prompts.
- **Provider‑agnostic.** Support OpenAI/Ollama/Anthropic/etc. by configuring the LM once via `dspy.settings.configure(lm=...)` before building modules.
- **Composable by default.** Prefer several small Modules over a single monolith; pass data via fields, not globals.
- **Repro first.** Pin DSPy and core deps in `pyproject.toml`; commit seeds and compile artifacts for deterministic rebuilds.

### Order of operations (universal)

1) **Define Signature(s)** — Put task instructions in the **docstring**; declare inputs with `dspy.InputField`, outputs with `dspy.OutputField`. (Docs: Signatures)
2) **Compose Modules** — Start with `dspy.Predict`; add `dspy.ChainOfThought` when rationale is useful; use `dspy.ReAct` for tools/agents. (Docs: Modules)
3) **Choose a Metric** — Make it concrete: e.g., `accuracy`/`F1` (classification), `nDCG@k` (ranking), exact/EM (QA). (Docs: Metrics)
4) **Add Assertions** — Specify schema+constraints and enable automatic retries on violation. (Docs: Assertions)
5) **(Optional) Retrieval** — Add a retriever and route evidence to the program for RAG tasks. (Docs: RAG tutorial)
6) **Optimize** — Run `BootstrapFewShot` to assemble demos → then `MIPROv2` to jointly tune instructions+demos against your metric. (Docs: Optimizers)

### Minimum rules (Codex must enforce)

- **Instruction location:** All task guidance lives in Signature docstrings; no hidden prompts in module code.
- **LM config timing:** Call `dspy.settings.configure(lm=...)` **before** constructing Modules/Programs so compiles/optimizers see the right LM.
- **Schema guard (example):** For ranked outputs, emit a 3‑column Markdown table: `prompt | score | rationale`, where `score` ∈ `[0,1]`.
- **Metrics defaults:** Classification → `accuracy` or `F1`; Ranking → `nDCG@k`; QA → exact match / token‑F1. Document any custom metric in one line.
- **Tracing & artifacts:** Enable tracing during iteration; persist optimized programs/artifacts after `compile()` for reproducibility.
- **Small pieces, loosely joined:** Prefer many tiny units; inject dependencies; avoid singletons and global state.

### Environment & compatibility

- **Versioning:** Prefer `dspy>=2` (or latest stable); if upgrading major versions, re‑run optimizers and refresh assertions.
- **Providers:** Keep provider/model/api‑base in env (e.g., `DSPY_OPENAI_API_KEY`, `OPENAI_API_BASE`, etc.). Do not hardcode keys.
- **Eval data:** Keep small, labeled eval sets under `eval/` to make metrics and compilation meaningful.

### Long‑form guide (for humans)

See **docs/pre-work-dspy.md** for the expanded checklist and examples
-------

When you need to call tools from the shell, use this rubric:

File & Text

-------

- Find files by file name: `fd`

- Find files with path name: `fd -p <file-path>`

- List files in a directory: `fd . <directory>`

- Find files with extension and pattern: `fd -e <extension> <pattern>`

- Find Text: `rg` (ripgrep)

- Find Code Structure: `ast-grep`

  - Common languages:

    - Python → `ast-grep --lang python -p '<pattern>'`

    - TypeScript → `ast-grep --lang ts -p '<pattern>'`

    - Bash → `ast-grep --lang bash -p '<pattern>'`

    - TSX (React) → `ast-grep --lang tsx -p '<pattern>'`

    - JavaScript → `ast-grep --lang js -p '<pattern>'`

    - Rust → `ast-grep --lang rust -p '<pattern>'`

    - JSON → `ast-grep --lang json -p '<pattern>'`

  - Prefer `ast-grep` over ripgrep/grep unless a plain-text search is explicitly requested.

- Select among matches: pipe to `fzf`

Data
----

- JSON: `jq`

- YAML/XML: `yq`

Python Tooling
--------------

- Package Management & Virtual Envs: `uv`
    (fast replacement for pip/pip-tools/virtualenv; use `uv pip install ...`, `uv run ...`)

- Linting & Formatting: `ruff`
    (linter + formatter; use `ruff check .`, `ruff format .`)

- Static Typing: `mypy`
    (type checking; use `mypy .`)

- Security: `bandit`
    (Python security linter; use `bandit -r .`)

- Testing: `pytest`
    (test runner; use `pytest -q`, `pytest -k <pattern>` to filter tests)

- Logging: `loguru`
    (runtime logging utility; import in code:)

        from loguru import logger
        logger.info("message")

Notes
-----

- Prefer `uv` for Python dependency and environment management instead of pip/venv/poetry/pip-tools.

MCP\_SERVERS
------------

- Use the `dspy_Docs` MCP server to get latest docs for DSPy usage.

- Use the `lmstudio_docs` MCP server to get latest docs for LM Studio API usage.

-------

## Proactive TODO/FIXME Annotations

Add TODO/FIXME notes as you work—don’t wait for a cleanup pass. Use them to mark: missing tests, unclear contracts, temporary workarounds, performance/security concerns, or places where design choices need follow-up.

**Format (single line):**

    TODO(scope|owner): short, imperative next step — why it matters [evidence: <source|cmd|ticket>]
    FIXME(scope|owner): what is broken — minimal repro or constraint [evidence: <source|cmd|ticket>]

- `scope|owner` is optional but encouraged (e.g., `ui`, `backend`, `deps`, or a handle like `@alice`).

- Keep it ≤120 chars when possible; link to issues for details.

**Examples (per language comment style):**

    # TODO(domain|@alice): replace naive parse with streaming parser — OOM on large inputs [evidence: profile.txt]
    # FIXME(api): 500 on empty payload — add validation + test [evidence: pytest -k empty_payload]


    // TODO(ui): debounce search — noisy network on fast typing [evidence: trace.log]
    // FIXME(auth|@bob): refresh token race — guard with mutex [evidence: unit test 'refresh-concurrency']


    # TODO(devex): switch to uv task for one-liners [evidence: uv run --help]

### Workflow (aligned with `todos.md`)

1. Gather evidence with the command you used during investigation and reference it in the note’s `[evidence: ...]`.

2. Add the TODO/FIXME in the code at the closest actionable location.

3. Commit with a concise message (e.g., `chore(todos): mark debounce + auth race with evidence`).

4. Before opening a PR, **find and group** all annotations as described in `todos.md` so maintainers can review them together.

### Discover & verify (standard commands)

- Plain-text sweep:

```bash
        rg -n "TODO|FIXME" | fzf
```

- Syntax-aware matches (prefer this when patterns are noisy):

```bash
        ast-grep --lang python -p "// TODO(_) (_) : (_)"
        ast-grep --lang ts -p "// FIXME(_) : (_)"
```

- File targeting:

```bash
        fd -e py -e ts | xargs rg -n "TODO|FIXME"
```

### PR checklist (copy into your PR template)

- Added TODO/FIXME where follow-ups are needed, with `[evidence: ...]`.

- Ran `rg`/`ast-grep` to list all annotations and grouped them per `todos.md` for reviewers.

- Linked or opened issues for any TODO expected to live >2 sprints.

### Retirement policy

- Convert TODO → issue if it will outlive the current PR.

- Remove the annotation when addressed; reference the commit/issue that resolves it.

-------

Rules for Best-Practice

-------

<file\_length\_and\_structure>

- Prefer maintainability signals over fixed line caps.

- Split when cognitive complexity > 15, cohesion drops, or fan-in/out spikes.

- Group by feature. Keep a file to one capability plus its close helpers.

- Use clear folder names and consistent naming.

</file\_length\_and\_structure>

<paradigm\_and\_style>

- Use OOP, functional, or data-oriented styles as idiomatic for the language.

- Favor composition. In OOP, model behavior behind small interfaces or protocols.

- Prefer pure functions and algebraic data types where natural.

</paradigm\_and\_style>

<single\_responsibility\_principle>

- Aim for one capability and its close helpers. Avoid micro-files.

- Enforce through module boundaries and public APIs, not line counts.

</single\_responsibility\_principle>

<modular\_design>

- Design modules to be interchangeable, testable, and isolated.

- Keep public surfaces small. Inject dependencies. Avoid tight coupling.

- Optimize for replaceability and test seams over premature reuse.

</modular\_design>

<roles\_by\_platform>

- UI stacks: ViewModel for UI logic, Manager for business logic, Coordinator for navigation and state flow.

- Backend and CLI: Service, Handler, Repository, Job, Workflow.

- Do not mix view code with business logic.

</roles\_by\_platform>

<function\_and\_class\_size>

- Size by behavior, not lines.

- Functions ≤ 20–30 cognitive steps.

- Split a class when it owns more than one lifecycle or more than one external dependency graph.

</function\_and\_class\_size>

<naming\_and\_readability>

- Use intention revealing names.

- Allow domain terms with qualifiers, for example `UserData`, `BillingInfo`.

- Forbid empty suffixes like `Helper` or `Utils` unless tightly scoped.

</naming\_and\_readability>

<scalability\_mindset>

- Build for extension points from day one, such as interfaces, protocols, and constructor injection.

- Prefer local duplication over unstable abstractions.

- Document contracts at module seams.

</scalability\_mindset>

<avoid\_god\_classes>

- Do not centralize everything in one file or class.

- Split into UI, State, Handlers, Networking, and other focused parts.

</avoid\_god\_classes>

<dependency\_injection>

- Backends: prefer constructor injection. Keep containers optional.

- Swift, Kotlin, TypeScript: use protocols or interfaces. Inject by initializer or factory.

- Limit global singletons. Provide test doubles at seams.

</dependency\_injection>

<testing>

- Require deterministic seams.

- Add contract tests for modules and layers.

- Use snapshot or golden tests for UI and renderers.

</testing>

<architecture\_boundaries>

- Feature oriented packaging with clear dependency direction: UI → app → domain → infra.

- Stabilize domain modules. Keep infra replaceable.

- Enforce imports with rules or module maps.

</architecture\_boundaries>

-------


--- GEMINI.md ---
<!-- path: ~/projects/dspy-file/AGENTS.md -->

# AGENTS.md — Tool Selection (Python)

## DSPy Framework Playbook (All‑Purpose)

**When to apply:** Use these rules whenever creating or editing DSPy **Signatures**, **Modules**, **Programs**, **Optimizers**, **Metrics**, or **RAG** components in any repository.

**Canonical docs**

- Signatures: <https://dspy.ai/learn/programming/signatures/>
- Modules (Predict / ChainOfThought / ReAct): <https://dspy.ai/learn/programming/modules/>
- Optimizers overview: <https://dspy.ai/learn/optimization/optimizers/>
- MIPROv2: <https://dspy.ai/api/optimizers/MIPROv2/>
- BootstrapFewShot: <https://dspy.ai/api/optimizers/BootstrapFewShot/>
- Assertions: <https://dspy.ai/learn/programming/7-assertions/>
- Metrics: <https://dspy.ai/learn/evaluation/metrics/>
- RAG tutorial: <https://dspy.ai/tutorials/rag/>

### Quick posture

- **Program, don’t prompt.** Encode task instructions in **Signature docstrings**; compose behavior from Modules. Avoid ad‑hoc long prompts.
- **Provider‑agnostic.** Support OpenAI/Ollama/Anthropic/etc. by configuring the LM once via `dspy.settings.configure(lm=...)` before building modules.
- **Composable by default.** Prefer several small Modules over a single monolith; pass data via fields, not globals.
- **Repro first.** Pin DSPy and core deps in `pyproject.toml`; commit seeds and compile artifacts for deterministic rebuilds.

### Order of operations (universal)

1) **Define Signature(s)** — Put task instructions in the **docstring**; declare inputs with `dspy.InputField`, outputs with `dspy.OutputField`. (Docs: Signatures)
2) **Compose Modules** — Start with `dspy.Predict`; add `dspy.ChainOfThought` when rationale is useful; use `dspy.ReAct` for tools/agents. (Docs: Modules)
3) **Choose a Metric** — Make it concrete: e.g., `accuracy`/`F1` (classification), `nDCG@k` (ranking), exact/EM (QA). (Docs: Metrics)
4) **Add Assertions** — Specify schema+constraints and enable automatic retries on violation. (Docs: Assertions)
5) **(Optional) Retrieval** — Add a retriever and route evidence to the program for RAG tasks. (Docs: RAG tutorial)
6) **Optimize** — Run `BootstrapFewShot` to assemble demos → then `MIPROv2` to jointly tune instructions+demos against your metric. (Docs: Optimizers)

### Minimum rules (Codex must enforce)

- **Instruction location:** All task guidance lives in Signature docstrings; no hidden prompts in module code.
- **LM config timing:** Call `dspy.settings.configure(lm=...)` **before** constructing Modules/Programs so compiles/optimizers see the right LM.
- **Schema guard (example):** For ranked outputs, emit a 3‑column Markdown table: `prompt | score | rationale`, where `score` ∈ `[0,1]`.
- **Metrics defaults:** Classification → `accuracy` or `F1`; Ranking → `nDCG@k`; QA → exact match / token‑F1. Document any custom metric in one line.
- **Tracing & artifacts:** Enable tracing during iteration; persist optimized programs/artifacts after `compile()` for reproducibility.
- **Small pieces, loosely joined:** Prefer many tiny units; inject dependencies; avoid singletons and global state.

### Environment & compatibility

- **Versioning:** Prefer `dspy>=2` (or latest stable); if upgrading major versions, re‑run optimizers and refresh assertions.
- **Providers:** Keep provider/model/api‑base in env (e.g., `DSPY_OPENAI_API_KEY`, `OPENAI_API_BASE`, etc.). Do not hardcode keys.
- **Eval data:** Keep small, labeled eval sets under `eval/` to make metrics and compilation meaningful.

### Long‑form guide (for humans)

See **docs/pre-work-dspy.md** for the expanded checklist and examples
-------

When you need to call tools from the shell, use this rubric:

File & Text

-------

- Find files by file name: `fd`

- Find files with path name: `fd -p <file-path>`

- List files in a directory: `fd . <directory>`

- Find files with extension and pattern: `fd -e <extension> <pattern>`

- Find Text: `rg` (ripgrep)

- Find Code Structure: `ast-grep`

  - Common languages:

    - Python → `ast-grep --lang python -p '<pattern>'`

    - TypeScript → `ast-grep --lang ts -p '<pattern>'`

    - Bash → `ast-grep --lang bash -p '<pattern>'`

    - TSX (React) → `ast-grep --lang tsx -p '<pattern>'`

    - JavaScript → `ast-grep --lang js -p '<pattern>'`

    - Rust → `ast-grep --lang rust -p '<pattern>'`

    - JSON → `ast-grep --lang json -p '<pattern>'`

  - Prefer `ast-grep` over ripgrep/grep unless a plain-text search is explicitly requested.

- Select among matches: pipe to `fzf`

Data
----

- JSON: `jq`

- YAML/XML: `yq`

Python Tooling
--------------

- Package Management & Virtual Envs: `uv`
    (fast replacement for pip/pip-tools/virtualenv; use `uv pip install ...`, `uv run ...`)

- Linting & Formatting: `ruff`
    (linter + formatter; use `ruff check .`, `ruff format .`)

- Static Typing: `mypy`
    (type checking; use `mypy .`)

- Security: `bandit`
    (Python security linter; use `bandit -r .`)

- Testing: `pytest`
    (test runner; use `pytest -q`, `pytest -k <pattern>` to filter tests)

- Logging: `loguru`
    (runtime logging utility; import in code:)

        from loguru import logger
        logger.info("message")

Notes
-----

- Prefer `uv` for Python dependency and environment management instead of pip/venv/poetry/pip-tools.

MCP\_SERVERS
------------

- Use the `dspy_Docs` MCP server to get latest docs for DSPy usage.

- Use the `lmstudio_docs` MCP server to get latest docs for LM Studio API usage.

-------

## Proactive TODO/FIXME Annotations

Add TODO/FIXME notes as you work—don’t wait for a cleanup pass. Use them to mark: missing tests, unclear contracts, temporary workarounds, performance/security concerns, or places where design choices need follow-up.

**Format (single line):**

    TODO(scope|owner): short, imperative next step — why it matters [evidence: <source|cmd|ticket>]
    FIXME(scope|owner): what is broken — minimal repro or constraint [evidence: <source|cmd|ticket>]

- `scope|owner` is optional but encouraged (e.g., `ui`, `backend`, `deps`, or a handle like `@alice`).

- Keep it ≤120 chars when possible; link to issues for details.

**Examples (per language comment style):**

    # TODO(domain|@alice): replace naive parse with streaming parser — OOM on large inputs [evidence: profile.txt]
    # FIXME(api): 500 on empty payload — add validation + test [evidence: pytest -k empty_payload]


    // TODO(ui): debounce search — noisy network on fast typing [evidence: trace.log]
    // FIXME(auth|@bob): refresh token race — guard with mutex [evidence: unit test 'refresh-concurrency']


    # TODO(devex): switch to uv task for one-liners [evidence: uv run --help]

### Workflow (aligned with `todos.md`)

1. Gather evidence with the command you used during investigation and reference it in the note’s `[evidence: ...]`.

2. Add the TODO/FIXME in the code at the closest actionable location.

3. Commit with a concise message (e.g., `chore(todos): mark debounce + auth race with evidence`).

4. Before opening a PR, **find and group** all annotations as described in `todos.md` so maintainers can review them together.

### Discover & verify (standard commands)

- Plain-text sweep:

```bash
        rg -n "TODO|FIXME" | fzf
```

- Syntax-aware matches (prefer this when patterns are noisy):

```bash
        ast-grep --lang python -p "// TODO(_) (_) : (_)"
        ast-grep --lang ts -p "// FIXME(_) : (_)"
```

- File targeting:

```bash
        fd -e py -e ts | xargs rg -n "TODO|FIXME"
```

### PR checklist (copy into your PR template)

- Added TODO/FIXME where follow-ups are needed, with `[evidence: ...]`.

- Ran `rg`/`ast-grep` to list all annotations and grouped them per `todos.md` for reviewers.

- Linked or opened issues for any TODO expected to live >2 sprints.

### Retirement policy

- Convert TODO → issue if it will outlive the current PR.

- Remove the annotation when addressed; reference the commit/issue that resolves it.

-------

Rules for Best-Practice

-------

<file\_length\_and\_structure>

- Prefer maintainability signals over fixed line caps.

- Split when cognitive complexity > 15, cohesion drops, or fan-in/out spikes.

- Group by feature. Keep a file to one capability plus its close helpers.

- Use clear folder names and consistent naming.

</file\_length\_and\_structure>

<paradigm\_and\_style>

- Use OOP, functional, or data-oriented styles as idiomatic for the language.

- Favor composition. In OOP, model behavior behind small interfaces or protocols.

- Prefer pure functions and algebraic data types where natural.

</paradigm\_and\_style>

<single\_responsibility\_principle>

- Aim for one capability and its close helpers. Avoid micro-files.

- Enforce through module boundaries and public APIs, not line counts.

</single\_responsibility\_principle>

<modular\_design>

- Design modules to be interchangeable, testable, and isolated.

- Keep public surfaces small. Inject dependencies. Avoid tight coupling.

- Optimize for replaceability and test seams over premature reuse.

</modular\_design>

<roles\_by\_platform>

- UI stacks: ViewModel for UI logic, Manager for business logic, Coordinator for navigation and state flow.

- Backend and CLI: Service, Handler, Repository, Job, Workflow.

- Do not mix view code with business logic.

</roles\_by\_platform>

<function\_and\_class\_size>

- Size by behavior, not lines.

- Functions ≤ 20–30 cognitive steps.

- Split a class when it owns more than one lifecycle or more than one external dependency graph.

</function\_and\_class\_size>

<naming\_and\_readability>

- Use intention revealing names.

- Allow domain terms with qualifiers, for example `UserData`, `BillingInfo`.

- Forbid empty suffixes like `Helper` or `Utils` unless tightly scoped.

</naming\_and\_readability>

<scalability\_mindset>

- Build for extension points from day one, such as interfaces, protocols, and constructor injection.

- Prefer local duplication over unstable abstractions.

- Document contracts at module seams.

</scalability\_mindset>

<avoid\_god\_classes>

- Do not centralize everything in one file or class.

- Split into UI, State, Handlers, Networking, and other focused parts.

</avoid\_god\_classes>

<dependency\_injection>

- Backends: prefer constructor injection. Keep containers optional.

- Swift, Kotlin, TypeScript: use protocols or interfaces. Inject by initializer or factory.

- Limit global singletons. Provide test doubles at seams.

</dependency\_injection>

<testing>

- Require deterministic seams.

- Add contract tests for modules and layers.

- Use snapshot or golden tests for UI and renderers.

</testing>

<architecture\_boundaries>

- Feature oriented packaging with clear dependency direction: UI → app → domain → infra.

- Stabilize domain modules. Keep infra replaceable.

- Enforce imports with rules or module maps.

</architecture\_boundaries>

-------


--- README.md ---
# dspyteach – DSPy File Teaching Analyzer

---

[![PyPI](https://img.shields.io/pypi/v/dspyteach.svg?include_prereleases&cacheSeconds=60&t=1)](https://pypi.org/project/dspyteach/)
[![Downloads](https://img.shields.io/pypi/dm/dspyteach.svg?cacheSeconds=300)](https://pypi.org/project/dspyteach/)
[![TestPyPI](https://img.shields.io/badge/TestPyPI-dspyteach-informational?cacheSeconds=300)](https://test.pypi.org/project/dspyteach/)
[![CI](https://github.com/AcidicSoil/dspy-file/actions/workflows/release.yml/badge.svg)](…)
[![Repo](https://img.shields.io/badge/GitHub-AcidicSoil%2Fdspy--file-181717?logo=github)](https://github.com/AcidicSoil/dspy-file)

---

## DSPy-powered CLI that analyzes source files (one or many) and produces teaching briefs

**Each run captures:**

- an overview of the file and its major sections
- key teaching points, workflows, and pitfalls highlighted in the material
- a polished markdown brief suitable for sharing with learners

The implementation mirrors the multi-file tutorial (`tutorials/multi-llmtxt_generator`) but focuses on per-file inference. The program is split into:

- `dspy_file/signatures.py` – DSPy signatures that define inputs/outputs for each step
- `dspy_file/file_analyzer.py` – the main DSPy module that orchestrates overview, teaching extraction, and report composition. It now wraps the final report stage with `dspy.Refine`, pushing for 450–650+ word briefs.
- `dspy_file/file_helpers.py` – utilities for loading files and rendering the markdown brief
- `dspy_file/analyze_file_cli.py` – command line entry point that configures the local model and prints results. It can walk directories, apply glob filters, and batch-generate briefs.

---

## Quick start

1. Confirm Python 3.10–3.12 is available and pull at least one OpenAI-compatible model (Ollama, LM Studio, or a hosted provider).
2. From the repository root, create an isolated environment and install dependencies:

   ```bash
   uv venv -p 3.12
   source .venv/bin/activate
   uv sync
   ```

3. Run a smoke test to confirm the CLI is wired up:

   ```bash
   dspyteach --help
   ```

   Expected result: the help output lists available flags and displays the active version string.

4. Analyze a sample file to confirm end-to-end output:

   ```bash
   dspyteach path/to/example.py
   ```

   Expected result: the command prints a teaching brief to stdout and writes a `.teaching.md` file under `dspy_file/data/`.

---

## Requirements

- Python 3.10-3.12+
- DSPy installed in the environment
- A language-model backend. You can choose between:
  - **Ollama** (default): run it locally with the model `hf.co/unsloth/Qwen3-4B-Instruct-2507-GGUF:Q6_K_XL` pulled.
  - **LM Studio** (OpenAI-compatible): start the LM Studio server (`lms server start`) and download a model such as `qwen3-4b-instruct-2507@q6_k_xl`.
  - **Any other OpenAI-compatible endpoint**: point the CLI at a hosted provider by supplying an API base URL and key (defaults to `gpt-5`).
- (Optional) `.env` file for DSPy configuration. `dotenv` loads variables such as `DSPYTEACH_PROVIDER`, `DSPYTEACH_MODEL`, `DSPYTEACH_API_BASE`, `DSPYTEACH_API_KEY`, and `OPENAI_API_KEY`.

---

## Example output

[[example-data after running a few passes](example-data/)]

---

## Installation

### Install with uv (recommended for local development)

```bash
uv venv -p 3.12
source .venv/bin/activate
uv sync
```

Expected result: the virtual environment contains the project dependencies and `dspyteach --version` reports the local build.

### Install from PyPI

```bash
pip install dspyteach
```

Expected result: running `dspyteach --help` prints the CLI usage banner from the installed package.

### Configure the language model

The CLI now supports configurable OpenAI-compatible providers in addition to the default Ollama runtime. You can override the backend via CLI options or environment variables:

```bash
# Use LM Studio's OpenAI-compatible server with its default port
dspyteach path/to/project \
  --provider lmstudio \
  --model osmosis-mcp-4b@q8_0 \
  --api-base http://localhost:1234/v1
```

```bash
# Environment variable alternative (e.g. inside .env)
export DSPYTEACH_PROVIDER=lmstudio
export DSPYTEACH_MODEL=osmosis-mcp-4b@q8_0
export DSPYTEACH_API_BASE=http://localhost:1234/v1
dspyteach path/to/project
```

### LM-Studio Usage Notes

[LM Studio configuration guide](docs/lm-studio-provider.md)

LM Studio must expose its local server before you run the CLI. Start it from the Developer tab inside the LM Studio app or via `lms server start` (details in the [LM Studio configuration guide](docs/lm-studio-provider.md)); otherwise the CLI will exit early with a connection warning.

### OpenAI-compatible others usage

For hosted OpenAI-compatible services, set `--provider openai`, supply `--api-base` if needed, and pass an API key either through `--api-key`, `DSPYTEACH_API_KEY`, or the standard `OPENAI_API_KEY`. To keep a local Ollama model running after the CLI finishes, add `--keep-provider-alive`.

## Usage

Run the CLI to extract a teaching brief from a single file:

```bash
dspyteach path/to/your_file
```

Expected result: the CLI prints a markdown teaching brief to stdout and saves a copy under `dspy_file/data/`.

You can also point the CLI at a directory. The tool will recurse by default:

```bash
dspyteach path/to/project --glob "**/*.py" --glob "**/*.md"
```

Expected result: each matched file produces its own `.teaching.md` report in the output directory.

Use `--non-recursive` to stay in the top-level directory, add `--glob` repeatedly to narrow the target set, and pass `--raw` to print the raw DSPy prediction object instead of the formatted report.

### Command examples

- **Analyze a single markdown file**

  ```bash
  dspyteach docs/example.md
  ```

  Expected result: the CLI prints a teaching brief and stores `docs__example.teaching.md` in the output directory.

- **Process a repository while skipping generated assets**

  ```bash
  dspyteach ./repo \
    --glob "**/*.py" \
    --glob "**/*.md" \
    --exclude-dirs "build/,dist/,data/"
  ```

  Expected result: only `.py` and `.md` files outside the excluded directories are analyzed.

- **Generate refactor templates instead of teaching briefs**

  ```bash
  dspyteach ./repo --mode refactor --prompt refactor_prompt_template
  ```

  Expected result: `.refactor.md` files appear alongside the teaching outputs with guidance tailored to the selected prompt.

Need to double-check files before the model runs? Add `--confirm-each` (alias `--interactive`) to prompt before every file, accepting with Enter or skipping with `n`.

To omit specific subdirectories entirely, pass one or more `--exclude-dirs` options. Each value can list comma-separated relative paths (for example `--exclude-dirs "build/,venv/" --exclude-dirs data/raw`). The analyzer ignores any files whose path begins with the provided prefixes.

Prefer short flags? The common options include `-r` (`--raw`), `-m` (`--mode`), `-nr` (`--non-recursive`), `-g` (`--glob`), `-i` (`--confirm-each`), `-ed` (`--exclude-dirs`), and `-o` (`--output-dir`). Mix and match them as needed.

## Refactor files/dirs

Want to scaffold refactor prompt templates instead of teaching briefs? Switch the mode:

```bash
dspyteach path/to/project --mode refactor --glob "**/*.md"
```

---

---

## **clarity on what happens when in teaching mode**

### both of these commands shown below would create new directories in the path outside the cwd that you ran the commands from and the directories would be the following: so in this case it would be exactly ["C:\Users\user\projects\WIP\NAME-OF-CWD + (the new files it creates which will be...)dspyteach\teach\data\00-ideation\architecture\adr-new.architecture.md]"

#### "00-ideation\architecture\adr-new.architecture.md" are unique to my personal setup so your output would be a mirrored version of the target path recursively

directory analyzed --> "~\projects\WIP\ .__pre-temp-prompts\temp-prompts-organized" so all under temp-prompts-organized are analyzed unless flag is passed to do otherwise, ie., non-recursive or -i AKA --interactive (file by file of target path).

---

```bash
dt -m refactor C:\Users\user\projects\WIP\.__pre-temp-prompts\temp-prompts-organized\ --provider lmstudio --api-base <http://127.0.0.1:1234/v1> -ed prompt-front-matter/ -o ..\dspyteach\data -i
```

```bash
dt C:\Users\user\projects\WIP\.__pre-temp-prompts\temp-prompts-organized\ --provider lmstudio --api-base <http://127.0.0.1:1234/v1> -ed prompt-front-matter/ -o ..\dspyteach\teach\data -i
```

---


## Additional Information

The CLI reuses the same file resolution pipeline but feeds each document through the bundled `dspy-file_refactor-prompt_template.md` instructions (packaged under `dspy_file/prompts/`), saving `.refactor.md` files alongside the teaching reports. Teaching briefs remain the default (`--mode teach`), so existing workflows continue to work unchanged.

When multiple templates live in `dspy_file/prompts/`, the refactor mode surfaces a picker so you can choose which one to use. You can also point at a specific template explicitly with `-p/--prompt`, passing either a bundled name (`-p refactor_prompt_template`) or an absolute path to your own Markdown prompt.

Each run only executes the analyzer for the chosen mode. When you pass `--mode refactor` the teaching inference pipeline stays idle, and you can alias the command (for example `alias dspyrefactor='dspyteach --mode refactor'`) if you prefer refactor templates to be the default in your shell.

To change where reports land, supply `--output-dir /path/to/reports`. When omitted the CLI writes to `dspy_file/data/` next to the module. Every run prints the active model name and the resolved output directory before analysis begins so you can confirm the environment at a glance. For backwards compatibility the installer also registers `dspy-file-teaching` as an alias.

Each analyzed file is saved under the chosen directory with a slugged name (e.g. `src__main.teaching.md` or `src__main.refactor.md`). If a file already exists, the CLI appends a numeric suffix to avoid overwriting previous runs.

The generated brief is markdown that mirrors the source material:

- Overview paragraphs for quick orientation
- Section-by-section bullets capturing the narrative
- Key concepts, workflows, pitfalls, and references learners should review
- A `dspy.Refine` wrapper keeps retrying until the report clears a length reward (defaults scale to ~50% of the source word count, with min/max clamps), so the content tends to be substantially longer than a single LM call.
- If a model cannot honour DSPy's structured-output schema, the CLI prints a `Structured output fallback` notice and heuristically parses the textual response so you still get usable bullets.

Behind the scenes the CLI:

1. Loads environment variables via `python-dotenv`.
2. Configures DSPy with the provider selected via CLI or environment variables (Ollama by default).
3. Resolves all requested files, reads contents, runs the DSPy `FileTeachingAnalyzer` module, and prints a human-friendly report for each.
4. Persists each report to the configured output directory so results are easy to revisit.
5. Stops the Ollama model when appropriate so local resources are returned to the pool.

### Extending

- Adjust the `TeachingReport` signature or add new chains in `dspy_file/file_analyzer.py` to capture additional teaching metadata.
- Customize the render logic in `dspy_file.file_helpers.render_prediction` if you want richer CLI output or structured JSON.
- Tune `TeachingConfig` inside `file_analyzer.py` to raise `max_tokens`, adjust the `Refine` word-count reward, or add extra LM kwargs.
- Add more signatures and module stages to capture additional metadata (e.g., security checks) and wire them into `FileAnalyzer`.

---

## Releasing

Maintainer release steps live in [docs/RELEASING.md](docs/RELEASING.md).

## Troubleshooting

- If the program cannot connect to Ollama, verify that the server is running on `http://localhost:11434` and the requested model has been pulled.
- When you see `ollama command not found`, ensure the `ollama` binary is on your `PATH`.
- For encoding errors, the helper already falls back to `latin-1`, but you can add more fallbacks in `file_helpers.read_file_content` if needed.


--- json_response_integration_report.md ---
# Integration Report: JSON Response Formatting

**Date:** October 17, 2025
**Status:** Proposed

## 1. Objective

This document outlines a proposal to integrate a feature into the `dspy-file` command-line tool (`analyze_file_cli.py`) to enforce structured JSON output from language models. This is based on the `response_format` parameter within the **DSPy framework**, which instructs DSPy to request JSON-formatted output from the underlying model provider.

## 2. DSPy Framework Abstraction

It is important to note that `response_format` is a feature of the `dspy.LM` abstraction layer, not necessarily a native parameter of the end-provider's API. The DSPy framework is responsible for translating this parameter into the correct, provider-specific API call.

- When `response_format` is passed to `dspy.LM` for an **Ollama** model, DSPy's Ollama client handler converts this into the native `format: 'json'` parameter in the request sent to the Ollama server.
- Similarly, for **OpenAI-compatible** endpoints, DSPy would use the provider's required format, such as `response_format={"type": "json_object"}`.

The code proposed in this document correctly interacts with the DSPy abstraction layer.

## 3. Initial Proposal (Ollama-Specific)

The initial goal was to create an optional, provider-specific feature for users of Ollama.

### 3.1. Key Mechanisms

1. **CLI Flag:** A new `--json-response` command-line flag was proposed to allow users to explicitly enable this functionality.
2. **Provider Gating:** The core logic is enclosed in `if provider is Provider.OLLAMA:` checks to ensure it only runs when Ollama is the selected provider.
3. **External Schemas:** The proposal involves loading the required JSON schema from external files (e.g., `teach.schema.json`, `refactor.schema.json`) based on the selected analysis mode.

## 4. Generalized Proposal (Provider-Agnostic)

To extend this feature to other providers, the Ollama-specific gating can be removed.

### 4.1. Key Mechanisms

1. **CLI Flag:** The `--json-response` flag remains the sole controller for the feature.
2. **Generalized Logic:** The `if provider is Provider.OLLAMA` checks are removed, allowing the `response_format` to be passed to any configured provider via the `dspy.LM` constructor.

### 4.2. Considerations & Caveats

- **Provider Capability:** The success of this feature depends on both the DSPy provider implementation and the underlying model's ability to generate valid JSON. While the `response_format` parameter instructs the model, it does not guarantee schema conformance, which still relies on the model's instruction-following capabilities.

## 5. Final Proposed Code (Provider-Agnostic)

The following represents the final proposed code structure for a generalized implementation.

```python
# In dspy_file/analyze_file_cli.py

# 1. In build_parser()
parser.add_argument(
    "--json-response",
    action="store_true",
    dest="json_response",
    help="Request a JSON response using a schema (if supported by the provider's DSPy handler).",
)

# 2. In configure_model()
def configure_model(
    # ...,
    response_format: dict[str, Any] | None = None,
):
    lm_kwargs: dict[str, Any] = {"streaming": False, "cache": False}

    if response_format:
        lm_kwargs["response_format"] = response_format

    if provider is Provider.OLLAMA:
        # ...
    else:
        # ...

    lm = dspy.LM(identifier, **lm_kwargs)
    dspy.configure(lm=lm)

# 3. In main()
# ...
analysis_mode = AnalysisMode(args.mode)
response_format = None
if args.json_response:
    schema_filename = f"{analysis_mode.value}.schema.json"
    schema_path = Path(__file__).parent / "prompts" / schema_filename
    try:
        with schema_path.open("r", encoding="utf-8") as f:
            schema = json.load(f)
        response_format = {
            "type": "json_schema",
            "json_schema": {"schema": schema},
        }
    except Exception as e:
        print(f"Warning: Could not load schema from {schema_path}. Error: {e}")

configure_model(..., response_format=response_format)
# ...
```


--- dspy_file/analyze_file_cli.py ---
# path: analyze_file_cli.py
# analyze_file_cli.py - command line entry point for DSPy file analyzer
from __future__ import annotations

import argparse
import os
import subprocess
import sys
from enum import Enum
from urllib import error as urlerror
from urllib import request
from pathlib import Path
from typing import Any, Final

import dspy
from dotenv import load_dotenv

from .file_analyzer import FileTeachingAnalyzer
from .file_helpers import collect_source_paths, read_file_content, render_prediction
from .prompts import PromptTemplate, list_bundled_prompts, load_prompt_text
from .refactor_analyzer import FileRefactorAnalyzer

try:  # dspy depends on litellm; guard in case import path changes.
    from litellm.exceptions import InternalServerError as LiteLLMInternalServerError
except Exception:  # pragma: no cover - defensive fallback if litellm API shifts
    LiteLLMInternalServerError = None  # type: ignore[assignment]


class Provider(str, Enum):
    """Supported language model providers."""

    OLLAMA = "ollama"
    OPENAI = "openai"
    LMSTUDIO = "lmstudio"

    @property
    def is_openai_compatible(self) -> bool:
        return self in {Provider.OPENAI, Provider.LMSTUDIO}


DEFAULT_PROVIDER: Final[Provider] = Provider.OLLAMA
DEFAULT_OUTPUT_DIR = Path(__file__).parent / "data"
DEFAULT_OLLAMA_MODEL = "hf.co/Mungert/osmosis-mcp-4b-GGUF:Q5_K_M"
DEFAULT_LMSTUDIO_MODEL = "osmosis-mcp-4b@q8_0"
DEFAULT_OPENAI_MODEL = "gpt-5"
OLLAMA_BASE_URL = "http://localhost:11434"
LMSTUDIO_BASE_URL = "http://localhost:1234/v1"

PROVIDER_DEFAULTS: Final[dict[Provider, dict[str, Any]]] = {
    Provider.OLLAMA: {"model": DEFAULT_OLLAMA_MODEL, "api_base": OLLAMA_BASE_URL},
    Provider.LMSTUDIO: {"model": DEFAULT_LMSTUDIO_MODEL, "api_base": LMSTUDIO_BASE_URL},
    Provider.OPENAI: {"model": DEFAULT_OPENAI_MODEL, "api_base": None},
}


def _resolve_option(
    cli_value: str | None, env_var: str, default: str | None = None
) -> str | None:
    """Return the CLI value if provided, otherwise fall back to env or default."""

    if cli_value is not None:
        return cli_value
    env_value = os.getenv(env_var)
    if env_value not in ("", None):
        return env_value
    return default


def _normalize_model_name(provider: Provider, raw_model: str) -> str:
    """Attach the appropriate provider prefix to the model identifier."""

    if provider is Provider.OLLAMA:
        return (
            raw_model
            if raw_model.startswith("ollama_chat/")
            else f"ollama_chat/{raw_model}"
        )

    if raw_model.startswith("openai/"):
        return raw_model
    return f"openai/{raw_model}"


def configure_model(
    provider: Provider,
    model_name: str,
    *,
    api_base: str | None,
    api_key: str | None,
) -> None:
    """Configure DSPy with the selected provider and model."""

    lm_kwargs: dict[str, Any] = {"streaming": False, "cache": False}
    if provider is Provider.OLLAMA:
        lm_kwargs["api_base"] = api_base or OLLAMA_BASE_URL
        # Ollama's OpenAI compatibility ignores api_key, so pass an empty string.
        lm_kwargs["api_key"] = ""
    else:
        if api_base:
            lm_kwargs["api_base"] = api_base
        if api_key:
            lm_kwargs["api_key"] = api_key

    identifier = _normalize_model_name(provider, model_name)
    lm = dspy.LM(identifier, **lm_kwargs)
    dspy.configure(lm=lm)
    provider_label = "LM Studio" if provider is Provider.LMSTUDIO else provider.value
    suffix = f" via {api_base}" if provider.is_openai_compatible and api_base else ""
    print(f"Configured DSPy LM ({provider_label}): {model_name}{suffix}")


class ProviderConnectivityError(RuntimeError):
    """Raised when a provider cannot be reached before running analysis."""


def _probe_openai_provider(
    api_base: str, api_key: str | None, *, timeout: float = 3.0
) -> None:
    """Make a lightweight request against an OpenAI-compatible endpoint."""

    endpoint = api_base.rstrip("/") + "/models"
    headers = {"Authorization": f"Bearer {api_key or ''}"}
    request_obj = request.Request(endpoint, headers=headers, method="GET")

    try:
        with request.urlopen(request_obj, timeout=timeout):
            return
    except urlerror.HTTPError as exc:
        raise ProviderConnectivityError(
            f"Endpoint {endpoint} responded with HTTP {exc.code}: {exc.reason}"
        ) from exc
    except urlerror.URLError as exc:
        reason = getattr(exc, "reason", exc)
        raise ProviderConnectivityError(
            f"Failed to reach {endpoint}: {reason}"
        ) from exc


def stop_ollama_model(model_name: str) -> None:
    """Stop the Ollama model to free server resources."""

    try:
        subprocess.run(
            ["ollama", "stop", model_name],
            check=True,
            capture_output=True,
        )
    except subprocess.CalledProcessError as exc:  # pragma: no cover - warn only
        print(f"Warning: Failed to stop model {model_name}: {exc}")
    except FileNotFoundError:
        print("Warning: ollama command not found while attempting to stop the model.")


class AnalysisMode(str, Enum):
    TEACH = "teach"
    REFACTOR = "refactor"

    @property
    def render_key(self) -> str:
        return self.value

    @property
    def output_description(self) -> str:
        return "teaching report" if self is AnalysisMode.TEACH else "refactor template"

    @property
    def file_suffix(self) -> str:
        # No suffixing. Preserve original filename.
        return ""


def _prompt_for_template_selection(prompts: list[PromptTemplate]) -> PromptTemplate:
    while True:
        print("Available prompt templates:")
        for idx, template in enumerate(prompts, 1):
            print(f"  [{idx}] {template.name} ({template.path.name})")
        try:
            choice = input(f"Select a template [1-{len(prompts)}] (default 1): ")
        except EOFError:
            print("No selection provided; using first template.")
            return prompts[0]

        stripped = choice.strip()
        if not stripped:
            return prompts[0]
        if stripped.isdigit():
            idx = int(stripped)
            if 1 <= idx <= len(prompts):
                return prompts[idx - 1]
        print(f"Please enter a number between 1 and {len(prompts)}.")


def _resolve_prompt_text(prompt_arg: str | None) -> str:
    if prompt_arg:
        return load_prompt_text(prompt_arg)

    prompts = list_bundled_prompts()
    if not prompts:
        raise FileNotFoundError("No prompt templates found in prompts directory.")
    if len(prompts) == 1:
        return prompts[0].path.read_text(encoding="utf-8")

    selected = _prompt_for_template_selection(prompts)
    return selected.path.read_text(encoding="utf-8")


def _write_output(
    source_path: Path,
    content: str,
    *,
    root: Path | None = None,
    output_dir: Path | None = None,
    suffix: str = "",
) -> Path:
    """Write output to the same filename and directory layout as the analyzed path.

    If output_dir is provided, mirror the relative directory structure inside it and
    keep the original filename. Otherwise overwrite the source file in place.
    """

    try:
        relative_path = (
            source_path.relative_to(root) if root else Path(source_path.name)
        )
    except ValueError:
        relative_path = Path(source_path.name)

    if output_dir:
        dest_path = (output_dir / relative_path).resolve()
        dest_path.parent.mkdir(parents=True, exist_ok=True)
    else:
        dest_path = source_path

    if not content.endswith("\n"):
        content = content + "\n"

    dest_path.write_text(content, encoding="utf-8")
    return dest_path


def _confirm_analyze(path: Path) -> bool:
    """Prompt the user for confirmation before analyzing a file."""

    prompt = f"Analyze {path}? [Y/n]: "
    while True:
        try:
            response = input(prompt)
        except EOFError:
            print("No input received; skipping.")
            return False

        normalized = response.strip().lower()
        if normalized in {"", "y", "yes"}:
            return True
        if normalized in {"n", "no"}:
            return False
        print("Please answer 'y' or 'n'.")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Analyze a single file using DSPy signatures and modules",
    )
    parser.add_argument("path", help="Path to the file to analyze")
    parser.add_argument(
        "--provider",
        choices=[provider.value for provider in Provider],
        default=None,
        help=(
            "Language model provider to use (env: DSPYTEACH_PROVIDER). "
            "Choose from 'ollama', 'lmstudio', or 'openai'."
        ),
    )
    parser.add_argument(
        "--model",
        dest="model_name",
        default=None,
        help=(
            "Override the model identifier for the selected provider "
            "(env: DSPYTEACH_MODEL)."
        ),
    )
    parser.add_argument(
        "--api-base",
        dest="api_base",
        default=None,
        help=("Override the OpenAI-compatible API base URL (env: DSPYTEACH_API_BASE)."),
    )
    parser.add_argument(
        "--api-key",
        dest="api_key",
        default=None,
        help=(
            "API key for OpenAI-compatible providers (env: DSPYTEACH_API_KEY). "
            "Falls back to OPENAI_API_KEY for the OpenAI provider."
        ),
    )
    parser.add_argument(
        "--keep-provider-alive",
        action="store_true",
        dest="keep_provider_alive",
        help="Skip stopping the local Ollama model when execution completes.",
    )
    parser.add_argument(
        "-r",
        "--raw",
        action="store_true",
        help="Print raw DSPy prediction repr instead of formatted text",
    )
    parser.add_argument(
        "-m",
        "--mode",
        choices=[mode.value for mode in AnalysisMode],
        default=AnalysisMode.TEACH.value,
        help="Select output mode: teaching report (default) or refactor prompt template.",
    )
    parser.add_argument(
        "-nr",
        "--non-recursive",
        action="store_true",
        help="When path is a directory, only analyze files in the top-level directory",
    )
    parser.add_argument(
        "-g",
        "--glob",
        action="append",
        dest="include_globs",
        default=None,
        help=(
            "Optional glob pattern(s) applied relative to the directory. Repeat to combine."
        ),
    )
    parser.add_argument(
        "-p",
        "--prompt",
        dest="prompt",
        default=None,
        help=(
            "Prompt template to use in refactor mode. Provide a name, bundled filename, or path."
        ),
    )
    parser.add_argument(
        "-i",
        "--confirm-each",
        "--interactive",
        action="store_true",
        dest="confirm_each",
        help="Prompt for confirmation before analyzing each file.",
    )
    parser.add_argument(
        "-ed",
        "--exclude-dirs",
        action="append",
        dest="exclude_dirs",
        default=None,
        help=(
            "Comma-separated relative directory paths to skip entirely when scanning."
        ),
    )
    parser.add_argument(
        "-o",
        "--output-dir",
        dest="output_dir",
        default=None,
        help=("Directory to write outputs. If omitted, overwrite files in place."),
    )
    return parser


def analyze_path(
    path: str,
    *,
    raw: bool,
    recursive: bool,
    include_globs: list[str] | None,
    confirm_each: bool,
    exclude_dirs: list[str] | None,
    output_dir: Path | None,
    mode: AnalysisMode,
    prompt_text: str | None = None,
) -> int:
    """Run the DSPy pipeline and render results to stdout for one or many files."""

    resolved = Path(path).expanduser().resolve()
    targets = collect_source_paths(
        path,
        recursive=recursive,
        include_globs=include_globs,
        exclude_dirs=exclude_dirs,
    )

    if not targets:
        print(f"No files found under {resolved}")
        return 0

    analyzer: dspy.Module
    if mode is AnalysisMode.TEACH:
        analyzer = FileTeachingAnalyzer()
    else:
        analyzer = FileRefactorAnalyzer(template_text=prompt_text)
    root: Path | None = resolved if resolved.is_dir() else None

    exit_code = 0
    for target in targets:
        if confirm_each and not _confirm_analyze(target):
            print(f"Skipping {target} at user request.")
            continue

        try:
            content = read_file_content(target)
        except (FileNotFoundError, UnicodeDecodeError) as exc:
            print(f"Skipping {target}: {exc}")
            exit_code = 1
            continue

        print(f"\n=== Analyzing {target} ===")
        prediction = analyzer(file_path=str(target), file_content=content)

        if raw:
            output_text = repr(prediction)
            print(output_text)
        else:
            output_text = render_prediction(prediction, mode=mode.render_key)
            print(output_text, end="")

        output_path = _write_output(
            target,
            output_text,
            root=root,
            output_dir=output_dir,
            suffix=mode.file_suffix,
        )
        print(f"Saved {mode.output_description} to {output_path}")

    return exit_code


def main(argv: list[str] | None = None) -> int:
    load_dotenv()

    parser = build_parser()
    args = parser.parse_args(argv)

    provider_value = _resolve_option(
        args.provider, "DSPYTEACH_PROVIDER", DEFAULT_PROVIDER.value
    )
    try:
        provider = Provider(provider_value)
    except ValueError:  # pragma: no cover - argparse handles this
        valid = ", ".join(p.value for p in Provider)
        parser.error(f"Unsupported provider '{provider_value}'. Choose from: {valid}.")

    defaults = PROVIDER_DEFAULTS[provider]
    model_name = _resolve_option(args.model_name, "DSPYTEACH_MODEL", defaults["model"])
    api_base_default = defaults.get("api_base")
    api_base = _resolve_option(args.api_base, "DSPYTEACH_API_BASE", api_base_default)
    api_key = _resolve_option(args.api_key, "DSPYTEACH_API_KEY", None)
    if provider is Provider.OPENAI and not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
    if provider is Provider.LMSTUDIO and not api_key:
        api_key = "lm-studio"

    if provider is Provider.LMSTUDIO and api_base:
        try:
            _probe_openai_provider(api_base, api_key)
        except ProviderConnectivityError as exc:
            print("Unable to reach the LM Studio server before starting analysis.")
            print(f"Details: {exc}")
            print(
                "Start LM Studio's local API server (Developer tab → Start Server or "
                "`lms server start`) and re-run, or pass --api-base to match the running port."
            )
            return 1

    configure_model(provider, model_name, api_base=api_base, api_key=api_key)
    stop_model: str | None = model_name if provider is Provider.OLLAMA else None

    exit_code = 0
    try:
        analysis_mode = AnalysisMode(args.mode)
        prompt_text: str | None = None
        if analysis_mode is AnalysisMode.REFACTOR:
            try:
                prompt_text = _resolve_prompt_text(args.prompt)
            except (FileNotFoundError, ValueError) as exc:
                print(f"Error resolving prompt: {exc}")
                return 2
        elif args.prompt:
            print("Warning: --prompt is ignored outside refactor mode.")
        output_dir = (
            Path(args.output_dir).expanduser().resolve() if args.output_dir else None
        )
        if output_dir:
            print(f"Writing {analysis_mode.output_description}s to {output_dir}")
        else:
            print(f"Writing {analysis_mode.output_description}s in place")
        exclude_dirs = None
        if args.exclude_dirs:
            parsed: list[str] = []
            for entry in args.exclude_dirs:
                parsed.extend(
                    segment.strip() for segment in entry.split(",") if segment.strip()
                )
            exclude_dirs = parsed or None
        try:
            exit_code = analyze_path(
                args.path,
                raw=args.raw,
                recursive=not args.non_recursive,
                include_globs=args.include_globs,
                confirm_each=args.confirm_each,
                exclude_dirs=exclude_dirs,
                output_dir=output_dir,
                mode=analysis_mode,
                prompt_text=prompt_text,
            )
        except Exception as exc:
            if LiteLLMInternalServerError and isinstance(
                exc, LiteLLMInternalServerError
            ):
                message = str(exc)
                if exc.__cause__:
                    message = f"{message} (cause: {exc.__cause__})"
                print("Model request failed while generating the report.")
                print(f"Details: {message}")
                if provider is Provider.LMSTUDIO:
                    print(
                        "Confirm the LM Studio server is running and reachable at "
                        f"{api_base}."
                    )
                return 1
            raise
    except (FileNotFoundError, IsADirectoryError) as exc:
        parser.print_usage(sys.stderr)
        print(f"{parser.prog}: error: {exc}", file=sys.stderr)
        exit_code = 2
    except KeyboardInterrupt:
        exit_code = 1
    finally:
        if provider is Provider.OLLAMA and not args.keep_provider_alive and stop_model:
            stop_ollama_model(stop_model)

    return exit_code


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    sys.exit(main())


--- dspy_file/file_analyzer.py ---
# file_analyzer.py - DSPy module deriving a learning brief from a single file
from __future__ import annotations

import json
import re
from collections.abc import Iterable, Mapping
from dataclasses import dataclass, field
from typing import Any

import dspy

from .signatures import FileOverview, TeachingPoints, TeachingReport


@dataclass
class TeachingConfig:
    section_bullet_prefix: str = "- "
    overview_max_tokens: int = 2000
    teachings_max_tokens: int = 2000
    report_max_tokens: int = 2000
    temperature: float | None = 0.3
    top_p: float | None = None
    n_completions: int | None = None
    extra_lm_kwargs: dict[str, Any] = field(default_factory=dict)
    report_refine_attempts: int = 3
    report_reward_threshold: float = 0.8
    report_min_word_count: int = 400
    report_max_word_count: int = 2800
    report_target_ratio: float = 0.5
    report_soft_cap_ratio: float = 0.8

    def lm_args_for(self, scope: str) -> dict[str, Any]:
        """Return per-module LM kwargs without mutating shared config."""
        scope_tokens = {
            "overview": self.overview_max_tokens,
            "teachings": self.teachings_max_tokens,
            "report": self.report_max_tokens,
        }

        kwargs: dict[str, Any] = {**self.extra_lm_kwargs}
        kwargs["max_tokens"] = scope_tokens.get(scope, self.report_max_tokens)

        if self.temperature is not None:
            kwargs["temperature"] = self.temperature
        if self.top_p is not None:
            kwargs["top_p"] = self.top_p
        if self.n_completions is not None:
            kwargs["n"] = self.n_completions

        return kwargs


def _fallback_list(message: str) -> list[str]:
    return [message]


def _ensure_text(value: str | None, fallback: str) -> str:
    if value and value.strip():
        return value
    return fallback


def _ensure_list(
    values: Iterable[str] | None,
    fallback: str,
    *,
    strip_entries: bool = True,
    field_name: str | None = None,
) -> list[str]:
    coerced, used_fallback = _coerce_iterable(values, strip_entries=strip_entries)

    if coerced:
        if used_fallback and field_name:
            _structured_output_notice(field_name)
        return coerced

    if used_fallback and field_name:
        _structured_output_notice(field_name)

    return _fallback_list(fallback)


def _clean_list(
    values: Iterable[str] | None,
    *,
    strip_entries: bool = True,
    field_name: str | None = None,
) -> list[str]:
    if not values:
        return []

    coerced, used_fallback = _coerce_iterable(values, strip_entries=strip_entries)

    if used_fallback and field_name:
        _structured_output_notice(field_name)

    return coerced


_STRUCTURED_NOTICE_CACHE: set[str] = set()


def _structured_output_notice(field: str) -> None:
    if field in _STRUCTURED_NOTICE_CACHE:
        return
    _STRUCTURED_NOTICE_CACHE.add(field)
    print(
        f"Structured output fallback applied for '{field}'. Parsed textual response."
    )


_LEADING_MARKER_PATTERN = re.compile(r"^[\s\-\*•·\u2022\d\.\)\(]+")


def _coerce_iterable(
    values: Iterable[str] | None,
    *,
    strip_entries: bool,
) -> tuple[list[str], bool]:
    if values is None:
        return [], False

    if isinstance(values, str):
        return _coerce_string(values, strip_entries=strip_entries), True

    if isinstance(values, Mapping):
        items: list[str] = []
        for key, val in values.items():
            key_text = str(key).strip()
            val_text = str(val).strip()
            combined = f"{key_text}: {val_text}" if val_text else key_text
            candidate = combined.rstrip() if not strip_entries else combined.strip()
            if candidate:
                items.append(candidate if strip_entries else candidate.rstrip())
        return items, True

    if isinstance(values, Iterable):
        cleaned: list[str] = []
        used_fallback = not isinstance(values, (list, tuple, set))
        for entry in values:
            if entry is None:
                continue
            if isinstance(entry, str):
                candidate = entry.rstrip() if not strip_entries else entry.strip()
            else:
                candidate = str(entry).strip()
            if strip_entries:
                candidate = _LEADING_MARKER_PATTERN.sub("", candidate).strip()
            if candidate:
                cleaned.append(candidate if strip_entries else candidate.rstrip())
        return cleaned, used_fallback

    return _coerce_string(str(values), strip_entries=strip_entries), True


def _coerce_string(value: str, *, strip_entries: bool) -> list[str]:
    text = value.strip()
    if not text:
        return []

    try:
        parsed = json.loads(text)
    except json.JSONDecodeError:
        parsed = None

    if isinstance(parsed, list):
        coerced: list[str] = []
        for item in parsed:
            candidate = str(item)
            candidate = candidate.rstrip() if not strip_entries else candidate.strip()
            if strip_entries:
                candidate = _LEADING_MARKER_PATTERN.sub("", candidate).strip()
            if candidate:
                coerced.append(candidate if strip_entries else candidate.rstrip())
        if coerced:
            return coerced
    elif isinstance(parsed, Mapping):
        mapped: list[str] = []
        for key, val in parsed.items():
            key_text = str(key).strip()
            val_text = str(val).strip()
            candidate = f"{key_text}: {val_text}" if val_text else key_text
            candidate = candidate.rstrip() if not strip_entries else candidate.strip()
            if candidate:
                mapped.append(candidate if strip_entries else candidate.rstrip())
        if mapped:
            return mapped

    lines = value.replace("\r", "\n").split("\n")
    normalized: list[str] = []
    for raw_line in lines:
        candidate = raw_line.rstrip() if not strip_entries else raw_line.strip()
        if strip_entries:
            candidate = _LEADING_MARKER_PATTERN.sub("", candidate).strip()
        if candidate:
            normalized.append(candidate if strip_entries else candidate.rstrip())

    if len(normalized) <= 1:
        delimiters = [";", "•", "·", " | "]
        for delimiter in delimiters:
            if delimiter in value:
                parts = [part.strip() for part in value.split(delimiter) if part.strip()]
                if parts:
                    return [
                        _LEADING_MARKER_PATTERN.sub("", part).strip()
                        if strip_entries
                        else part.rstrip()
                    ]

    return normalized


def _with_prefix(items: Iterable[str], prefix: str) -> list[str]:
    if not prefix:
        return [item for item in items if item.strip()]

    prefix_char = prefix.strip()[:1] if prefix.strip() else ""
    prefixed: list[str] = []

    for item in items:
        stripped = item.strip()
        if not stripped:
            continue
        if prefix_char and stripped.startswith(prefix_char):
            prefixed.append(stripped)
        else:
            prefixed.append(f"{prefix}{stripped}")

    return prefixed


def _word_count(text: str) -> int:
    return len(text.split())


class FileTeachingAnalyzer(dspy.Module):
    """Generate a teaching-focused summary using DSPy chains of thought."""

    def __init__(self, config: TeachingConfig | None = None) -> None:
        super().__init__()
        self.config = config or TeachingConfig()

        overview_signature = FileOverview.with_instructions(
            """
            Craft a thorough multi-section narrative that orients a senior learner.
            Describe the file's purpose, high-level architecture, main responsibilities,
            how data flows through each part, and any noteworthy patterns or dependencies.
            Aim for around five paragraphs that highlight why each section exists and
            how it contributes to the overall behavior.
            """
        )

        teachings_signature = TeachingPoints.with_instructions(
            """
            Extract every insight the learner would need for deep comprehension.
            Provide generous bullet lists (>=6 items when possible) covering concepts,
            workflows, pitfalls, integration guidance, and areas needing validation.
            When referencing identifiers, include the role they play.
            Prefer complete sentences that can stand alone in teaching materials.
            """
        )

        report_signature = TeachingReport.with_instructions(
            """
            Assemble a long-form teaching brief in Markdown. Include:
            - An opening context block with file path and intent.
            - Headed sections for overview, section walkthrough, key concepts, workflows,
              pitfalls, integration notes, tests/validation, and references.
            - Expand each bullet into full sentences or sub-bullets to help instructors
              speak to the content without the source file open.
            Ensure the report comfortably exceeds 400 words when source material allows.
            """
        )

        self.overview = dspy.ChainOfThought(
            overview_signature, **self.config.lm_args_for("overview")
        )
        self.teachings = dspy.ChainOfThought(
            teachings_signature, **self.config.lm_args_for("teachings")
        )

        base_report = dspy.ChainOfThought(
            report_signature, **self.config.lm_args_for("report")
        )

        if self.config.report_refine_attempts > 1:

            def report_length_reward(args: dict[str, Any], pred: dspy.Prediction) -> float:
                text = getattr(pred, "report_markdown", "") or ""
                words = _word_count(text)
                source_words = max(int(args.get("source_word_count", 0)), 0)

                dynamic_target = max(
                    self.config.report_min_word_count,
                    int(source_words * self.config.report_target_ratio),
                )

                soft_cap = max(
                    dynamic_target + 150,
                    int(source_words * self.config.report_soft_cap_ratio),
                )

                dynamic_cap = min(self.config.report_max_word_count, soft_cap)

                if words < dynamic_target:
                    return 0.0

                if words >= dynamic_cap:
                    return 1.0

                span = max(dynamic_cap - dynamic_target, 1)
                progress = (words - dynamic_target) / span
                return min(1.0, 0.6 + 0.4 * progress)

            self.report = dspy.Refine(
                module=base_report,
                N=self.config.report_refine_attempts,
                reward_fn=report_length_reward,
                threshold=self.config.report_reward_threshold,
            )
        else:
            self.report = base_report

    def forward(self, *, file_path: str, file_content: str) -> dspy.Prediction:
        overview_pred = self.overview(
            file_path=file_path,
            file_content=file_content,
        )

        teaching_pred = self.teachings(
            file_content=file_content,
        )

        overview_text = _ensure_text(
            getattr(overview_pred, "overview", None),
            "Overview unavailable.",
        )

        section_notes = _with_prefix(
            _ensure_list(
                getattr(overview_pred, "section_notes", None),
                "Section-level breakdown unavailable.",
                field_name="section_notes",
            ),
            self.config.section_bullet_prefix,
        )

        key_concepts = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "key_concepts", None),
                "Clarify core concepts manually.",
                field_name="key_concepts",
            ),
            self.config.section_bullet_prefix,
        )

        practical_steps = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "practical_steps", None),
                "Document workflow steps explicitly.",
                field_name="practical_steps",
            ),
            self.config.section_bullet_prefix,
        )

        pitfalls = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "pitfalls", None),
                "No pitfalls identified; review source for potential caveats.",
                field_name="pitfalls",
            ),
            self.config.section_bullet_prefix,
        )

        references = _with_prefix(
            _clean_list(
                getattr(teaching_pred, "references", None),
                field_name="references",
            ),
            self.config.section_bullet_prefix,
        )

        usage_patterns = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "usage_patterns", None),
                "Document how this file is applied in real flows.",
                field_name="usage_patterns",
            ),
            self.config.section_bullet_prefix,
        )

        key_functions = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "key_functions", None),
                "Identify primary interfaces and responsibilities manually.",
                field_name="key_functions",
            ),
            self.config.section_bullet_prefix,
        )

        code_walkthroughs = _ensure_list(
            getattr(teaching_pred, "code_walkthroughs", None),
            "Prepare short code walkthroughs for learners.",
            strip_entries=False,
            field_name="code_walkthroughs",
        )

        integration_notes = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "integration_notes", None),
                "Outline integration touchpoints manually.",
                field_name="integration_notes",
            ),
            self.config.section_bullet_prefix,
        )

        testing_focus = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "testing_focus", None),
                "Highlight testing priorities in a follow-up review.",
                field_name="testing_focus",
            ),
            self.config.section_bullet_prefix,
        )

        source_word_count = _word_count(file_content)

        report_pred = self.report(
            file_path=file_path,
            overview=overview_text,
            section_notes=section_notes,
            key_concepts=key_concepts,
            practical_steps=practical_steps,
            pitfalls=pitfalls,
            references=references,
            usage_patterns=usage_patterns,
            key_functions=key_functions,
            code_walkthroughs=code_walkthroughs,
            integration_notes=integration_notes,
            testing_focus=testing_focus,
            source_word_count=source_word_count,
        )

        return dspy.Prediction(
            overview=overview_pred,
            teachings=teaching_pred,
            report=report_pred,
            structured={
                "overview_text": overview_text,
                "section_notes": section_notes,
                "key_concepts": key_concepts,
                "practical_steps": practical_steps,
                "pitfalls": pitfalls,
                "references": references,
                "usage_patterns": usage_patterns,
                "key_functions": key_functions,
                "code_walkthroughs": code_walkthroughs,
                "integration_notes": integration_notes,
                "testing_focus": testing_focus,
            },
        )


--- dspy_file/file_helpers.py ---
# file_helpers.py - utilities for loading files and presenting DSPy results
from __future__ import annotations

import os
from pathlib import Path
from typing import Iterable

import dspy


# Directories that should never be traversed when collecting source files.
ALWAYS_IGNORED_DIRS: set[str] = {
    "__pycache__",
    ".git",
    ".hg",
    ".mypy_cache",
    ".pytest_cache",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    ".vscode",
    ".idea",
    "venv",
}

# Individual files or suffixes that should never be analyzed.
ALWAYS_IGNORED_FILES: set[str] = {".DS_Store"}
ALWAYS_IGNORED_SUFFIXES: set[str] = {".pyc", ".pyo"}


def _normalize_relative_parts(value: Path | str) -> tuple[str, ...]:
    """Return normalized path segments for relative comparisons."""

    text = str(value).replace("\\", "/").strip()
    if not text:
        return ()
    text = text.strip("/")
    if not text or text in {"", "."}:
        return ()

    parts: list[str] = []
    for segment in text.split("/"):
        if not segment or segment == ".":
            continue
        if segment == "..":
            if parts:
                parts.pop()
            continue
        parts.append(segment)
    return tuple(parts)


def _matches_excluded_parts(
    parts: tuple[str, ...],
    excluded_parts: set[tuple[str, ...]],
) -> bool:
    for excluded in excluded_parts:
        if len(parts) < len(excluded):
            continue
        if parts[: len(excluded)] == excluded:
            return True
    return False


def _normalize_excluded_dirs(exclude_dirs: Iterable[str] | None) -> set[tuple[str, ...]]:
    """Normalize raw exclude strings into comparable path segments."""

    normalized: set[tuple[str, ...]] = set()
    if not exclude_dirs:
        return normalized

    for raw in exclude_dirs:
        cleaned = raw.strip()
        if not cleaned:
            continue
        parts = _normalize_relative_parts(cleaned)
        if parts:
            normalized.add(parts)
    return normalized


def _relative_path_is_excluded(
    relative_path: Path,
    excluded_parts: set[tuple[str, ...]],
) -> bool:
    if not excluded_parts:
        return False
    parts = _normalize_relative_parts(relative_path)
    if not parts:
        return False
    return _matches_excluded_parts(parts, excluded_parts)


def resolve_file_path(raw_path: str) -> Path:
    """Expand user shortcuts and validate that the target file exists."""

    path = Path(raw_path).expanduser().resolve()
    if not path.exists():
        raise FileNotFoundError(f"File not found: {path}")
    if not path.is_file():
        raise IsADirectoryError(f"Expected a file path but received: {path}")
    return path


def _pattern_targets_hidden(pattern: str) -> bool:
    pattern = pattern.strip()
    if not pattern:
        return False
    normalized = pattern[2:] if pattern.startswith("./") else pattern
    return normalized.startswith(".") or "/." in normalized


def _should_skip_dir(name: str, *, ignore_hidden: bool) -> bool:
    if name in ALWAYS_IGNORED_DIRS:
        return True
    if ignore_hidden and name.startswith("."):
        return True
    return False


def _should_skip_file(name: str, *, ignore_hidden: bool) -> bool:
    if name in ALWAYS_IGNORED_FILES:
        return True
    if any(name.endswith(suffix) for suffix in ALWAYS_IGNORED_SUFFIXES):
        return True
    if ignore_hidden and name.startswith("."):
        return True
    return False


def _should_skip_relative_path(
    relative_path: Path,
    *,
    ignore_hidden: bool,
    excluded_parts: set[tuple[str, ...]] | None = None,
) -> bool:
    parts = _normalize_relative_parts(relative_path)
    if not parts:
        return False

    if excluded_parts and _matches_excluded_parts(parts, excluded_parts):
        return True

    # Check intermediate directories for ignore rules.
    for segment in parts[:-1]:
        if segment in ALWAYS_IGNORED_DIRS:
            return True
        if ignore_hidden and segment.startswith("."):
            return True

    return _should_skip_file(parts[-1], ignore_hidden=ignore_hidden)


def collect_source_paths(
    raw_path: str,
    *,
    recursive: bool = True,
    include_globs: Iterable[str] | None = None,
    exclude_dirs: Iterable[str] | None = None,
) -> list[Path]:
    """Resolve a single file or directory into an ordered list of file paths."""

    path = Path(raw_path).expanduser().resolve()
    if not path.exists():
        raise FileNotFoundError(f"Target not found: {path}")

    if path.is_file():
        return [path]

    if not path.is_dir():
        raise IsADirectoryError(f"Expected file or directory path but received: {path}")

    candidates: set[Path] = set()
    patterns = list(include_globs) if include_globs else None
    allow_hidden = any(_pattern_targets_hidden(pattern) for pattern in patterns) if patterns else False
    ignore_hidden = not allow_hidden
    excluded_parts = _normalize_excluded_dirs(exclude_dirs)

    if patterns:
        for pattern in patterns:
            for candidate in path.glob(pattern):
                if not candidate.is_file():
                    continue

                relative_candidate = candidate.relative_to(path)
                if _should_skip_relative_path(
                    relative_candidate,
                    ignore_hidden=ignore_hidden,
                    excluded_parts=excluded_parts,
                ):
                    continue

                candidates.add(candidate.resolve())
    else:
        for root_dir, dirnames, filenames in os.walk(path):
            root_path = Path(root_dir)
            relative_root = Path(".") if root_path == path else root_path.relative_to(path)

            if not recursive and root_path != path:
                dirnames[:] = []
                continue

            if _relative_path_is_excluded(relative_root, excluded_parts):
                dirnames[:] = []
                continue

            dirnames[:] = sorted(
                name
                for name in dirnames
                if not _should_skip_dir(name, ignore_hidden=ignore_hidden)
                and not _relative_path_is_excluded(relative_root / name, excluded_parts)
            )

            for filename in filenames:
                candidate = root_path / filename
                relative_candidate = candidate.relative_to(path)

                if _should_skip_relative_path(
                    relative_candidate,
                    ignore_hidden=ignore_hidden,
                    excluded_parts=excluded_parts,
                ):
                    continue

                candidates.add(candidate.resolve())

    return sorted(candidates)


def _strip_front_matter(text: str) -> str:
    if not text.startswith("---"):
        return text
    end_idx = text.find("\n---", 3)
    if end_idx == -1:
        return text
    return text[end_idx + 4 :]


def _trim_to_first_heading(text: str) -> str:
    lines = text.splitlines()
    for idx, line in enumerate(lines):
        if line.lstrip().startswith("#"):
            return "\n".join(lines[idx:])
    return text


def read_file_content(path: Path) -> str:
    """Read file contents using utf-8 and fall back to latin-1 if needed."""

    try:
        raw = path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        raw = path.read_text(encoding="latin-1")

    cleaned = _strip_front_matter(raw)
    cleaned = _trim_to_first_heading(cleaned)
    return cleaned


def _ensure_trailing_newline(text: str) -> str:
    return text if text.endswith("\n") else text + "\n"


def _teaching_output(result: dspy.Prediction) -> str:
    try:
        report = result.report.report_markdown  # type: ignore[attr-defined]
    except AttributeError:
        report = "# Teaching Brief\n\nThe DSPy pipeline did not produce a report."
    return _ensure_trailing_newline(report)


def _refactor_output(result: dspy.Prediction) -> str:
    template = getattr(result, "template_markdown", None)
    if not template:
        template = getattr(getattr(result, "template", None), "template_markdown", None)
    text = str(template).strip() if template else ""
    if not text:
        text = "# Refactor Template\n\nTemplate generation failed."
    return _ensure_trailing_newline(text)


def render_prediction(result: dspy.Prediction, *, mode: str = "teach") -> str:
    """Return the generated markdown for the selected analysis mode."""

    if mode == "refactor":
        return _refactor_output(result)
    return _teaching_output(result)


--- dspy_file/__init__.py ---
"""DSPy file teaching analyzer package."""

from .file_analyzer import FileTeachingAnalyzer, TeachingConfig
from .refactor_analyzer import FileRefactorAnalyzer, RefactorTeachingConfig

__all__ = [
    "FileTeachingAnalyzer",
    "TeachingConfig",
    "FileRefactorAnalyzer",
    "RefactorTeachingConfig",
]


--- dspy_file/refactor_analyzer.py ---
# refactor_analyzer.py - DSPy module that prepares per-file refactor prompt templates
from __future__ import annotations

from dataclasses import dataclass, field
from functools import lru_cache
from typing import Any

import dspy

from .prompts import load_prompt_text


class RefactorTemplateSignature(dspy.Signature):
    """Generate a reusable refactor prompt template from a source document."""

    file_path: str = dspy.InputField(desc="Path to the source file for context")
    file_content: str = dspy.InputField(desc="Full raw text of the file")

    template_markdown: str = dspy.OutputField(
        desc="Markdown template with numbered placeholders and section scaffolding"
    )


@dataclass
class RefactorTeachingConfig:
    """Configuration for the refactor template generator."""

    max_tokens: int = 8000
    temperature: float | None = 0.7
    top_p: float | None = 0.80
    n_completions: int | None = None
    extra_lm_kwargs: dict[str, Any] = field(default_factory=dict)

    def lm_kwargs(self) -> dict[str, Any]:
        """Return the language model arguments for DSPy modules."""

        kwargs: dict[str, Any] = {**self.extra_lm_kwargs, "max_tokens": self.max_tokens}
        if self.temperature is not None:
            kwargs["temperature"] = self.temperature
        if self.top_p is not None:
            kwargs["top_p"] = self.top_p
        if self.n_completions is not None:
            kwargs["n"] = self.n_completions
        return kwargs


@lru_cache(maxsize=1)
def _load_default_template() -> str:
    """Load the bundled refactor prompt template text."""

    return load_prompt_text(None).strip()


def _ensure_template_text(value: str | None) -> str:
    if value and value.strip():
        text = value.rstrip()
    else:
        text = "# Refactor Template\n\nTemplate generation failed."
    return text if text.endswith("\n") else text + "\n"


class FileRefactorAnalyzer(dspy.Module):
    """Generate a refactor-focused prompt template for a single file."""

    def __init__(
        self,
        *,
        template_text: str | None = None,
        config: RefactorTeachingConfig | None = None,
    ) -> None:
        super().__init__()
        self.config = config or RefactorTeachingConfig()
        instructions = template_text.strip() if template_text else _load_default_template()
        signature = RefactorTemplateSignature.with_instructions(instructions)
        self.generator = dspy.ChainOfThought(signature, **self.config.lm_kwargs())

    def forward(self, *, file_path: str, file_content: str) -> dspy.Prediction:
        raw_prediction = self.generator(
            file_path=file_path,
            file_content=file_content,
        )

        template_markdown = _ensure_template_text(
            getattr(raw_prediction, "template_markdown", None)
        )

        return dspy.Prediction(
            template=raw_prediction,
            template_markdown=template_markdown,
        )


--- dspy_file/signatures.py ---
# signatures.py - DSPy signatures focused on extracting teachings from a single file
from typing import List

import dspy


class FileOverview(dspy.Signature):
    """Summarize the file structure and core narrative with room for depth."""

    file_path: str = dspy.InputField(desc="Path to the source file")
    file_content: str = dspy.InputField(desc="Full raw text of the file")

    overview: str = dspy.OutputField(
        desc="Detailed multi-section overview (aim for 4-6 paragraphs capturing scope, intent, and flow)"
    )
    section_notes: List[str] = dspy.OutputField(
        desc="Comprehensive bullet list summarizing each major section, include headings when possible"
    )


class TeachingPoints(dspy.Signature):
    """Extract teachable concepts, workflows, and cautions."""

    file_content: str = dspy.InputField(desc="Full raw text of the file")

    key_concepts: List[str] = dspy.OutputField(desc="Essential ideas learners must retain")
    practical_steps: List[str] = dspy.OutputField(desc="Actionable steps or workflows described")
    pitfalls: List[str] = dspy.OutputField(desc="Warnings, gotchas, or misconceptions to avoid")
    references: List[str] = dspy.OutputField(desc="Follow-up links, exercises, or related material")
    usage_patterns: List[str] = dspy.OutputField(
        desc="Common usage patterns, scenarios, or recipes that appear"
    )
    key_functions: List[str] = dspy.OutputField(
        desc="Important functions, classes, or hooks with quick rationale"
    )
    code_walkthroughs: List[str] = dspy.OutputField(
        desc="Short code snippets or walkthroughs learners should discuss"
    )
    integration_notes: List[str] = dspy.OutputField(
        desc="Guidance for connecting this file with the rest of the system"
    )
    testing_focus: List[str] = dspy.OutputField(
        desc="Areas that need tests, validations, or monitoring"
    )


class TeachingReport(dspy.Signature):
    """Compose a concise but comprehensive markdown teaching brief."""

    file_path: str = dspy.InputField(desc="Original file path for context header")
    overview: str = dspy.InputField()
    section_notes: List[str] = dspy.InputField()
    key_concepts: List[str] = dspy.InputField()
    practical_steps: List[str] = dspy.InputField()
    pitfalls: List[str] = dspy.InputField()
    references: List[str] = dspy.InputField()
    usage_patterns: List[str] = dspy.InputField()
    key_functions: List[str] = dspy.InputField()
    code_walkthroughs: List[str] = dspy.InputField()
    integration_notes: List[str] = dspy.InputField()
    testing_focus: List[str] = dspy.InputField()

    report_markdown: str = dspy.OutputField(desc="Final markdown document capturing key teachings")


--- dspy_file/prompts/code-fence.md ---
# Command: /markdown:wrap-md-fence

# Usage: /markdown:wrap-md-fence "your content here"

# Args

# - {{content}}: raw bytes to wrap verbatim inside the fence

prompt = """
Wrap the provided {{content}} verbatim with a Markdown code fence labeled md.

Rules:

* Zero changes to {{content}} (byte-for-byte).
* Preserve encoding, line endings, and terminal newline presence/absence.
* No additional output or whitespace outside the fence.

Output exactly:

```md
{{content}}
```

Acceptance:

* Inner bytes are identical to {{content}}.
* Only the opening line `md and the closing` are added.
  """


--- dspy_file/prompts/front-matter-v2.md ---
<!-- $1 = source Markdown text; $2 = template name/title (optional; infer if missing); $3 = maximum placeholders allowed (1–9; default 7); $4 = input parameters block; $5 = controlled taxonomy/list block; $6 = stage mapping/rules block; $7 = output examples block -->

# $2

Task: Given $1, produce a structured **metadata block** and then emit the original body unchanged. The metadata must expose identifiers, categories, optional lifecycle/stage, optional dependencies, optional provided artifacts, and a concise summary. Output = metadata, blank line, then $1.

## Inputs

$4

## Canonical taxonomy (exact strings)

$5

### Stage hints (for inference)

$6

## Algorithm

1. Extract signals from $1

   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier

   * Prefer explicit input; otherwise infer from main action + object.
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).
   * De-duplicate.

3. Determine categories

   * Prefer explicit input; otherwise infer from verbs/headings vs $5.
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)

   * Prefer explicit input; otherwise map categories via $6.
   * Omit if uncertain.

5. Determine dependencies (optional)

   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)

   * Short list (≤3) of unlocked outputs.

7. Compose summary

   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format

   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata

   * Merge: explicit inputs > existing > inferred.
   * Validate lists; move unknowns to an extension field if needed.
   * Remove empty keys.

## Assumptions & Constraints

* Emit exactly one document: metadata, a single blank line, then $1.
* Limit distinct placeholders to ≤ $3.

## Validation

* Identifier matches a normalized id pattern.
* Categories non-empty and drawn from $5 (≤3).
* Stage, if present, is one of the allowed stages implied by $6.
* Dependencies, if present, are id-shaped (≤5).
* Summary ≤120 chars; punctuation coherent.
* Body text $1 is not altered.

## Output format examples

$7


--- dspy_file/prompts/gemini-cli-command_prompt-template.md ---
# gemini-cli-command_prompt-template_v2

Task: From {prompt_or_md} and {user_context}, synthesize a Gemini CLI custom command TOML that generalizes the task via inferred placeholders.

Heuristics

1) Mark user-changeable text (queries, titles) → {{query}}
2) Paths/globs → {{path}}
3) Targets/entities (repo/service/ticket) → {{target}}
4) Quantities/limits → {{limit}}
If uncertainty remains, collapse everything into {{args}}. Never exceed 3 distinct placeholders unless the text explicitly lists more inputs.

Transformations

- Preserve order and intent; remove fluff.
- If the source includes code or CLI calls, render them as `!{...}` lines inside the prompt.
- Prefer imperative voice (“Do X, then Y”). Add brief, testable acceptance lines inside the prompt if helpful.

Deliverable (only TOML; no extra prose)

# Command: /{namespace:=user}:{command:=auto-from-title}

# Usage: /{namespace}:{command} "example value(s)"

# Args

# - {{query}}: what to search or summarize

# - {{path}}: file or directory (optional)

prompt = """
<final instruction with placeholders and any !{...} inserts>
"""

Checks

- Valid TOML; balanced triple quotes.
- Includes ≥1 placeholder with matching Usage.
- Names are lowercase, kebab-case.
- No commentary outside TOML.


--- dspy_file/prompts/gemini-cli_extension-command_prompt-template.md ---
# gemini-cli_commands-prompt-template_v1

Goal: Produce a parameterized template from {source_text} using positional placeholders and a machine-checkable Arg spec.

Requirements

- Use {placeholder_syntax} (default: `$1..$9`).
- Insert ≤ {max_placeholders} high-impact placeholders; deduplicate repeats.
- Emit JSON Arg spec immediately after the templated text, with this shape:
    {
      "args": [
        { "id": "$1", "name": "{name}", "hint": "{short_hint}", "example": "{example}", "required": true, "validate": "{regex|rule}" }
      ]
    }
- Preserve markdown/code formatting byte-for-byte except at replacement spans.
- Do not change meaning, tone, or constraints of {source_text}.

Heuristics (apply in order)

1) User-owned identifiers: paths, repo/org names, endpoints, secrets placeholders.
2) Content slots: problem statement, target audience/domain, primary input.
3) Tunables: N/limits/timeouts only if not hard requirements.
4) Repeated literals → one arg; propagate to all occurrences.
5) Skip boilerplate constants (e.g., license names, standard flags) unless context marks them variable.

Edge cases

- If already templated, extend only with missing args; do not renumber existing placeholders.
- If no clear candidates, introduce `$1` as `topic_or_input` at the primary noun phrase of the opening sentence and document it.
- For JSON/YAML in code fences, ensure placeholders remain valid strings (quote if needed).

Acceptance tests (must pass)

- T1: All placeholders appear in the Arg spec; counts match.
- T2: Substituting provided examples yields valid markdown and runnable snippets.
- T3: Repeated concepts map to a single placeholder consistently.
- T4: Total placeholders ≤ {max_placeholders}; none are trivial.

Deliverables

1) **Templated Text** — {source_text} with placeholders inserted per heuristics.
2) **Args JSON** — machine-checkable spec as shown above.


--- tests/conftest.py ---
from __future__ import annotations

import os
from pathlib import Path


_CACHE_DIR = Path(__file__).parent / ".dspy-cache"
_CACHE_DIR.mkdir(exist_ok=True)

os.environ.setdefault("DISKCACHE_DEFAULT_DIRECTORY", str(_CACHE_DIR))
os.environ.setdefault("DSPY_CACHE_DIR", str(_CACHE_DIR))
os.environ.setdefault("DSPY_CACHEDIR", str(_CACHE_DIR))


--- tests/smoke_test.py ---
from __future__ import annotations

from pathlib import Path
from types import SimpleNamespace
from unittest import mock

from dspy_file import analyze_file_cli


class DummyAnalyzer:
    def __init__(self) -> None:
        self.calls: list[tuple[str, str]] = []

    def __call__(self, *, file_path: str, file_content: str):  # type: ignore[override]
        self.calls.append((file_path, file_content))
        return SimpleNamespace(
            report=SimpleNamespace(
                report_markdown="# Teaching Brief\n\n- Generated by DummyAnalyzer."
            )
        )


def test_analyze_path_writes_markdown(tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("# Title\n\nSome content", encoding="utf-8")

    output_dir = tmp_path / "reports"
    dummy = DummyAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileTeachingAnalyzer", return_value=dummy
    ):
        exit_code = analyze_file_cli.analyze_path(
            str(source),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=False,
            exclude_dirs=None,
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.TEACH,
            prompt_text=None,
        )

    assert exit_code == 0
    assert dummy.calls == [(str(source), "# Title\n\nSome content")]

    generated_files = list(output_dir.glob("*.md"))
    assert len(generated_files) == 1
    generated_text = generated_files[0].read_text(encoding="utf-8")
    assert generated_text.startswith("# Teaching Brief\n")
    assert generated_text.endswith("\n")


@mock.patch.object(analyze_file_cli, "_confirm_analyze", return_value=False)
def test_analyze_path_confirm_each_skips_when_declined(confirm_mock: mock.Mock, tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("# Title\n\nSome content", encoding="utf-8")

    output_dir = tmp_path / "reports"
    dummy = DummyAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileTeachingAnalyzer", return_value=dummy
    ):
        exit_code = analyze_file_cli.analyze_path(
            str(source),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=True,
            exclude_dirs=None,
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.TEACH,
            prompt_text=None,
        )

    assert exit_code == 0
    assert dummy.calls == []
    assert list(output_dir.glob("*.md")) == []
    confirm_mock.assert_called_once()


def test_analyze_path_respects_exclude_dirs(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    include_dir = project_root / "include"
    skip_dir = project_root / "skip"
    include_dir.mkdir(parents=True)
    skip_dir.mkdir(parents=True)

    include_file = include_dir / "keep.py"
    skip_file = skip_dir / "ignore.py"
    include_file.write_text("print('keep')\n", encoding="utf-8")
    skip_file.write_text("print('ignore')\n", encoding="utf-8")

    output_dir = tmp_path / "reports"
    dummy = DummyAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileTeachingAnalyzer", return_value=dummy
    ):
        exit_code = analyze_file_cli.analyze_path(
            str(project_root),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=False,
            exclude_dirs=["skip"],
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.TEACH,
            prompt_text=None,
        )

    assert exit_code == 0
    assert dummy.calls == [(str(include_file), "print('keep')\n")]
    generated = list(output_dir.glob("*.md"))
    assert len(generated) == 1
    assert "ignore" not in generated[0].read_text(encoding="utf-8")


def test_analyze_path_refactor_mode_uses_refactor_analyzer(tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("# Title\n\nSome content", encoding="utf-8")

    output_dir = tmp_path / "reports"

    class DummyRefactorAnalyzer:
        def __init__(self) -> None:
            self.calls: list[tuple[str, str]] = []

        def __call__(self, *, file_path: str, file_content: str):  # type: ignore[override]
            self.calls.append((file_path, file_content))
            return SimpleNamespace(template_markdown="# Template\n\nValue.")

    dummy = DummyRefactorAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileRefactorAnalyzer", return_value=dummy
    ) as refactor_cls:
        exit_code = analyze_file_cli.analyze_path(
            str(source),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=False,
            exclude_dirs=None,
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.REFACTOR,
            prompt_text="Custom prompt",
        )

    assert exit_code == 0
    assert dummy.calls == [(str(source), "# Title\n\nSome content")]
    generated_files = list(output_dir.glob("*.refactor.md"))
    assert len(generated_files) == 1
    assert generated_files[0].read_text(encoding="utf-8").endswith("Value.\n")
    refactor_cls.assert_called_once()
    assert refactor_cls.call_args.kwargs["template_text"] == "Custom prompt"


def test_resolve_prompt_text_with_menu(tmp_path: Path) -> None:
    prompt_one = tmp_path / "first.md"
    prompt_two = tmp_path / "second.md"
    prompt_one.write_text("First template", encoding="utf-8")
    prompt_two.write_text("Second template", encoding="utf-8")

    options = [
        analyze_file_cli.PromptTemplate(name="first", path=prompt_one),
        analyze_file_cli.PromptTemplate(name="second", path=prompt_two),
    ]

    with mock.patch.object(
        analyze_file_cli, "list_bundled_prompts", return_value=options
    ), mock.patch("builtins.input", side_effect=["2"]):
        text = analyze_file_cli._resolve_prompt_text(None)

    assert text == "Second template"


def test_resolve_prompt_text_with_explicit_path(tmp_path: Path) -> None:
    prompt_path = tmp_path / "custom.md"
    prompt_path.write_text("Custom prompt text", encoding="utf-8")

    text = analyze_file_cli._resolve_prompt_text(str(prompt_path))

    assert text == "Custom prompt text"


def test_parser_short_options_are_supported() -> None:
    parser = analyze_file_cli.build_parser()
    args = parser.parse_args(
        [
            "sample.md",
            "-r",
            "-m",
            "refactor",
            "-nr",
            "-g",
            "**/*.py",
            "-g",
            "**/*.md",
            "-i",
            "-ed",
            "skip,temp",
            "-o",
            "reports",
            "-p",
            "custom-prompt",
        ]
    )

    assert args.path == "sample.md"
    assert args.raw is True
    assert args.mode == "refactor"
    assert args.non_recursive is True
    assert args.include_globs == ["**/*.py", "**/*.md"]
    assert args.confirm_each is True
    assert args.exclude_dirs == ["skip,temp"]
    assert args.output_dir == "reports"
    assert args.prompt == "custom-prompt"


--- tests/test_cli_connectivity.py ---
from __future__ import annotations

from pathlib import Path
from unittest import mock

import pytest

from dspy_file import analyze_file_cli


def test_probe_openai_provider_success() -> None:
    mock_response = mock.MagicMock()
    urlopen_mock = mock.MagicMock()
    urlopen_mock.return_value.__enter__.return_value = mock_response

    with mock.patch(
        "dspy_file.analyze_file_cli.request.urlopen", urlopen_mock
    ):
        analyze_file_cli._probe_openai_provider("http://localhost:1234/v1", "token")

    urlopen_mock.assert_called_once()


def test_probe_openai_provider_raises_on_failure() -> None:
    with mock.patch(
        "dspy_file.analyze_file_cli.request.urlopen",
        side_effect=analyze_file_cli.urlerror.URLError("connection refused"),
    ):
        with pytest.raises(analyze_file_cli.ProviderConnectivityError):
            analyze_file_cli._probe_openai_provider("http://localhost:1234/v1", "token")


def test_main_exits_early_when_lmstudio_unreachable(tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("content", encoding="utf-8")

    with mock.patch.object(
        analyze_file_cli,
        "_probe_openai_provider",
        side_effect=analyze_file_cli.ProviderConnectivityError("unreachable"),
    ), mock.patch.object(analyze_file_cli, "configure_model") as configure_mock:
        exit_code = analyze_file_cli.main(
            ["--provider", "lmstudio", str(source)]
        )

    assert exit_code == 1
    configure_mock.assert_not_called()


--- tests/test_file_helpers.py ---
from __future__ import annotations

from pathlib import Path
from types import SimpleNamespace

from dspy_file.file_helpers import collect_source_paths, render_prediction


def _touch(path: Path, content: str = "") -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(content, encoding="utf-8")


def test_collect_source_paths_skips_hidden_and_config_files(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    project_root.mkdir()

    allowed_file = project_root / "main.py"
    _touch(allowed_file, "print('ok')\n")

    _touch(project_root / ".env", "SECRET=1\n")
    _touch(project_root / ".venv" / "lib" / "ignore.py", "print('ignored')\n")
    _touch(project_root / "nested" / ".secrets", "hidden\n")

    collected = collect_source_paths(str(project_root))

    assert collected == [allowed_file.resolve()]


def test_hidden_files_can_be_included_with_explicit_glob(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    project_root.mkdir()

    hidden_file = project_root / ".config" / "ci.yml"
    _touch(hidden_file, "name: ci\n")

    collected = collect_source_paths(
        str(project_root),
        include_globs=[".config/**/*.yml"],
    )

    assert collected == [hidden_file.resolve()]


def test_collect_source_paths_honors_exclude_dirs(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    project_root.mkdir()

    kept = project_root / "keep" / "file.py"
    skipped = project_root / "skip" / "ignored.py"
    nested_skip = project_root / "nested" / "deep" / "hidden.py"
    _touch(kept, "print('keep')\n")
    _touch(skipped, "print('ignore')\n")
    _touch(nested_skip, "print('nested ignore')\n")

    collected = collect_source_paths(
        str(project_root),
        exclude_dirs=["skip", "nested/deep"],
    )
    collected_glob = collect_source_paths(
        str(project_root),
        include_globs=["**/*.py"],
        exclude_dirs=["skip", "nested/deep"],
    )

    assert collected == [kept.resolve()]
    assert collected_glob == [kept.resolve()]


def test_render_prediction_teach_mode_uses_report() -> None:
    prediction = SimpleNamespace(
        report=SimpleNamespace(report_markdown="# Brief\n\nContent."),
    )
    output = render_prediction(prediction, mode="teach")
    assert output.endswith("Content.\n")


def test_render_prediction_refactor_mode_prefers_template_markdown() -> None:
    prediction = SimpleNamespace(template_markdown="# Template\n\nValue.")
    output = render_prediction(prediction, mode="refactor")
    assert output.endswith("Value.\n")


--- artifacts/stanfordnlp/dspy/dspy-llms.txt ---
# Dspy

> DSPy is designed to enable developers to build modular AI systems by programming language models through declarative Python code, rather than using prompts. Its primary goals include optimizing LM outputs via self-improving pipelines, supporting complex tasks like RAG pipelines and agent loops, and providing tools for prompt optimization, evaluation, and deployment. The framework aims to simplify the development of high-quality AI systems by abstracting away the complexity of traditional prompting methods.

**Remember:**
- Declarative Self-improving Python: Writing compositional Python code to teach LMs to generate high-quality outputs
- Modular AI Systems: Building complex systems through reusable components and pipelines
- Prompt Optimization: Algorithms for refining prompts and weights in language model programs
- Teleprompting: Using instruction proposals and optimization techniques (e.g., GEPA, BootstrapFewShot) to improve LM performance
- Retrieval-Augmented Generation (RAG): Integrating retrieval systems with language models for knowledge-intensive tasks
- Evaluation Metrics: Tools for assessing model performance using semantic F1, exact match, and passage match

## Docs
- [README](https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/README.md): install & quickstart.
- [Tutorials](https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/tutorials/index.md): worked example.
- [Ai Text Game](https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/tutorials/ai_text_game/index.md): worked example.
- [Async](https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/tutorials/async/index.md): worked example.
- [Build Ai Program](https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/tutorials/build_ai_program/index.md): worked example.
- [Cache](https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/tutorials/cache/index.md): worked example.
- [Classification](https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/tutorials/classification/index.md): worked example.
- [Conversation History](https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/tutorials/conversation_history/index.md): worked example.
- [Core Development](https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/tutorials/core_development/index.md): worked example.
- [Deployment](https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/tutorials/deployment/index.md): worked example.

## Tutorials
- [Example](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/primitives/example.py): worked example.
- [Test Baleen](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/examples/test_baleen.py): worked example.
- [Test Example](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/primitives/test_example.py): worked example.

## API
- [Base Module](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/primitives/base_module.py): docs page.
- [Module](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/primitives/module.py): docs page.
- [Test Base Module](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/primitives/test_base_module.py): docs page.
- [Test Module](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/primitives/test_module.py): docs page.

## Optional
- [Contributing](https://raw.githubusercontent.com/stanfordnlp/dspy/main/CONTRIBUTING.md): docs page.
- [Build And Release](https://raw.githubusercontent.com/stanfordnlp/dspy/main/.github/.internal_dspyai/internals/build-and-release.md): version history.
- [Release Checklist](https://raw.githubusercontent.com/stanfordnlp/dspy/main/.github/.internal_dspyai/internals/release-checklist.md): version history.
- [README](https://raw.githubusercontent.com/stanfordnlp/dspy/main/README.md): docs page.

## .Github
- [Pull Request Template](https://raw.githubusercontent.com/stanfordnlp/dspy/main/.github/PULL_REQUEST_TEMPLATE/pull_request_template.md): docs page.
- [Test Version](https://raw.githubusercontent.com/stanfordnlp/dspy/main/.github/workflows/build_utils/test_version.py): docs page.

## Dspy
- [Init](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/__init__.py): docs page.
- [Metadata](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/__metadata__.py): docs page.
- [Aggregation](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/predict/aggregation.py): docs page.
- [Annotation](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/utils/annotation.py): docs page.
- [Asyncify](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/utils/asyncify.py): docs page.
- [Auto Evaluation](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/evaluate/auto_evaluation.py): docs page.
- [Avatar Optimizer](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/teleprompt/avatar_optimizer.py): docs page.
- [Baml Adapter](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/adapters/baml_adapter.py): docs page.
- [Base](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/adapters/base.py): docs page.
- [Base Lm](https://raw.githubusercontent.com/stanfordnlp/dspy/main/dspy/clients/base_lm.py): docs page.

## Tests
- [README](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/reliability/README.md): install & quickstart.
- [Conftest](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/conftest.py): docs page.
- [Init](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/__init__.py): docs page.
- [Conftest](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/reliability/conftest.py): docs page.
- [Init](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/reliability/__init__.py): docs page.
- [Init](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/test_utils/__init__.py): docs page.
- [Init](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/utils/__init__.py): docs page.
- [Test Adapter Image](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/signatures/test_adapter_image.py): docs page.
- [Test Adapter Utils](https://raw.githubusercontent.com/stanfordnlp/dspy/main/tests/adapters/test_adapter_utils.py): docs page.


--- artifacts/stanfordnlp/dspy/dspy-llms-full.txt ---
# llms-full (private-aware)
> Built by authenticated GitHub API fetches. Large files may be truncated.

--- docs/README.md ---
**If you're looking to understand the framework, please go to the [DSPy Docs at dspy.ai](https://dspy.ai)**

&nbsp;

--------

&nbsp;

The content below is focused on how to modify the documentation site.

&nbsp;

# Modifying the DSPy Documentation


This website is built using [Material for MKDocs](https://squidfunk.github.io/mkdocs-material/), a Material UI inspired theme for MKDocs.

## Building docs locally

To build and test the documentation locally:

1. Navigate to the `docs` directory:
   ```bash
   cd docs
   ```

2. Install the necessary dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. In docs/ directory, run the command below to generate the API docs and index them:
   ```bash
   python scripts/generate_api_docs.py
   python scripts/generate_api_summary.py
   ```

4. (Optional) On MacOS you may also need to install libraries for building the site
   ```bash
   brew install cairo freetype libffi libjpeg libpng zlib
   export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib
   ```

5. Run the build command:
   ```bash
   mkdocs build
   ```

This will generate a static build of the documentation site in the `site` directory. You can then serve this directory to view the site locally using:

```bash
mkdocs serve
```

If you see the build failing make sure to fix it before pushing.

## Continuous Integration (CI) Build Checks

We have automated build checks set up in our CI pipeline to ensure the documentation builds successfully before merging changes. These checks:

1. Run the `mkdocs build` command
2. Verify that the build completes without errors
3. Help catch potential issues early in the development process

If the CI build check fails, please review your changes and ensure the documentation builds correctly locally before pushing updates.

## Contributing to the `docs` Folder

This guide is for contributors looking to make changes to the documentation in the `dspy/docs` folder. 

1. **Pull the up-to-date version of the website**: Please pull the latest version of the live documentation site via cloning the dspy repo.  The current docs are in the `dspy/docs` folder.

2. **Push your new changes on a new branch**: Feel free to add or edit existing documentation and open a PR for your changes. Once your PR is reviewed and approved, the changes will be ready to merge into main. 

3. **Updating the website**: Once your changes are merged to main, the changes would be reflected on live websites usually in 5-15 mins.

## LLMs.txt

The build process generates an `/llms.txt` file for LLM consumption using [mkdocs-llmstxt](https://github.com/pawamoy/mkdocs-llmstxt). Configure sections in `mkdocs.yml` under the `llmstxt` plugin.



--- docs/docs/tutorials/index.md ---
Welcome to DSPy tutorials! We've organized our tutorials into three main categories to help you get started:

- **Build AI Programs with DSPy**: These hands-on tutorials guide you through building production-ready AI
  applications. From implementing RAG systems to creating intelligent agents, each tutorial demonstrates
  practical use cases. You'll also learn how to leverage DSPy optimizers to enhance your program's performance.

- **Optimize AI Programs with DSPy Optimizers**: These tutorials deep dive into DSPy's optimization capabilities. While
  lighter on programming concepts, they focus on how to systematically improve your AI programs using DSPy
  optimizers, and showcase how DSPy optimizers help improve the quality automatically.

- **DSPy Core Development**: These tutorials cover essential DSPy features and best practices. Learn how to implement
  key functionalities like streaming, caching, deployment, and monitoring in your DSPy applications.


- Build AI Programs with DSPy
    - [Managing Conversation History](conversation_history/index.md)
    - [Building AI Agents with DSPy](customer_service_agent/index.ipynb)
    - [Building AI Applications by Customizing DSPy Modules](custom_module/index.ipynb)
    - [Retrieval-Augmented Generation (RAG)](rag/index.ipynb)
    - [Building RAG as Agent](agents/index.ipynb)
    - [Entity Extraction](entity_extraction/index.ipynb)
    - [Classification](classification/index.md)
    - [Multi-Hop RAG](multihop_search/index.ipynb)
    - [Privacy-Conscious Delegation](papillon/index.md)
    - [Program Of Thought](program_of_thought/index.ipynb)
    - [Image Generation Prompt iteration](image_generation_prompting/index.ipynb)
    - [Audio](audio/index.ipynb)


- Optimize AI Programs with DSPy
    - [Math Reasoning](math/index.ipynb)
    - [Classification Finetuning](classification_finetuning/index.ipynb)
    - [Advanced Tool Use](tool_use/index.ipynb)
    - [Finetuning Agents](games/index.ipynb)


- Reflective Prompt Evolution with dspy.GEPA:
    - [Overview](gepa_ai_program/index.md)
    - [GEPA for AIME](gepa_aime/index.ipynb)
    - [GEPA for PAPILLON](gepa_papillon/index.ipynb)
    - [GEPA for Enterprise classification task](gepa_facilitysupportanalyzer/index.ipynb)


- Tools, Development, and Deployment
    - [Use MCP in DSPy](mcp/index.md)
    - [Output Refinement](output_refinement/best-of-n-and-refine.md)
    - [Saving and Loading](saving/index.md)
    - [Cache](cache/index.md)
    - [Deployment](deployment/index.md)
    - [Debugging & Observability](observability/index.md)
    - [Tracking DSPy Optimizers](optimizer_tracking/index.md)
    - [Streaming](streaming/index.md)
    - [Async](async/index.md)




--- docs/docs/tutorials/ai_text_game/index.md ---
# Building a Creative Text-Based AI Game with DSPy

This tutorial demonstrates how to create an interactive text-based adventure game using DSPy's modular programming approach. You'll build a dynamic game where AI handles narrative generation, character interactions, and adaptive gameplay.

## What You'll Build

An intelligent text-based adventure game featuring:

- Dynamic story generation and branching narratives
- AI-powered character interactions and dialogue
- Adaptive gameplay that responds to player choices
- Inventory and character progression systems
- Save/load game state functionality

## Setup

```bash
pip install dspy rich typer
```

## Step 1: Core Game Framework

```python
import dspy
import json
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import random
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
import typer

# Configure DSPy
lm = dspy.LM(model='openai/gpt-4o-mini')
dspy.configure(lm=lm)

console = Console()

class GameState(Enum):
    MENU = "menu"
    PLAYING = "playing"
    INVENTORY = "inventory"
    CHARACTER = "character"
    GAME_OVER = "game_over"

@dataclass
class Player:
    name: str
    health: int = 100
    level: int = 1
    experience: int = 0
    inventory: list[str] = field(default_factory=list)
    skills: dict[str, int] = field(default_factory=lambda: {
        "strength": 10,
        "intelligence": 10,
        "charisma": 10,
        "stealth": 10
    })
    
    def add_item(self, item: str):
        self.inventory.append(item)
        console.print(f"[green]Added {item} to inventory![/green]")
    
    def remove_item(self, item: str) -> bool:
        if item in self.inventory:
            self.inventory.remove(item)
            return True
        return False
    
    def gain_experience(self, amount: int):
        self.experience += amount
        old_level = self.level
        self.level = 1 + (self.experience // 100)
        if self.level > old_level:
            console.print(f"[bold yellow]Level up! You are now level {self.level}![/bold yellow]")

@dataclass
class GameContext:
    current_location: str = "Village Square"
    story_progress: int = 0
    visited_locations: list[str] = field(default_factory=list)
    npcs_met: list[str] = field(default_factory=list)
    completed_quests: list[str] = field(default_factory=list)
    game_flags: dict[str, bool] = field(default_factory=dict)
    
    def add_flag(self, flag: str, value: bool = True):
        self.game_flags[flag] = value
    
    def has_flag(self, flag: str) -> bool:
        return self.game_flags.get(flag, False)

class GameEngine:
    def __init__(self):
        self.player = None
        self.context = GameContext()
        self.state = GameState.MENU
        self.running = True
        
    def save_game(self, filename: str = "savegame.json"):
        """Save current game state."""
        save_data = {
            "player": {
                "name": self.player.name,
                "health": self.player.health,
                "level": self.player.level,
                "experience": self.player.experience,
                "inventory": self.player.inventory,
                "skills": self.player.skills
            },
            "context": {
                "current_location": self.context.current_location,
                "story_progress": self.context.story_progress,
                "visited_locations": self.context.visited_locations,
                "npcs_met": self.context.npcs_met,
                "completed_quests": self.context.completed_quests,
                "game_flags": self.context.game_flags
            }
        }
        
        with open(filename, 'w') as f:
            json.dump(save_data, f, indent=2)
        console.print(f"[green]Game saved to {filename}![/green]")
    
    def load_game(self, filename: str = "savegame.json") -> bool:
        """Load game state from file."""
        try:
            with open(filename, 'r') as f:
                save_data = json.load(f)
            
            # Reconstruct player
            player_data = save_data["player"]
            self.player = Player(
                name=player_data["name"],
                health=player_data["health"],
                level=player_data["level"],
                experience=player_data["experience"],
                inventory=player_data["inventory"],
                skills=player_data["skills"]
            )
            
            # Reconstruct context
            context_data = save_data["context"]
            self.context = GameContext(
                current_location=context_data["current_location"],
                story_progress=context_data["story_progress"],
                visited_locations=context_data["visited_locations"],
                npcs_met=context_data["npcs_met"],
                completed_quests=context_data["completed_quests"],
                game_flags=context_data["game_flags"]
            )
            
            console.print(f"[green]Game loaded from {filename}![/green]")
            return True
            
        except FileNotFoundError:
            console.print(f"[red]Save file {filename} not found![/red]")
            return False
        except Exception as e:
            console.print(f"[red]Error loading game: {e}![/red]")
            return False

# Initialize game engine
game = GameEngine()
```

## Step 2: AI-Powered Story Generation

```python
class StoryGenerator(dspy.Signature):
    """Generate dynamic story content based on current game state."""
    location: str = dspy.InputField(desc="Current location")
    player_info: str = dspy.InputField(desc="Player information and stats")
    story_progress: int = dspy.InputField(desc="Current story progress level")
    recent_actions: str = dspy.InputField(desc="Player's recent actions")
    
    scene_description: str = dspy.OutputField(desc="Vivid description of current scene")
    available_actions: list[str] = dspy.OutputField(desc="List of possible player actions")
    npcs_present: list[str] = dspy.OutputField(desc="NPCs present in this location")
    items_available: list[str] = dspy.OutputField(desc="Items that can be found or interacted with")

class DialogueGenerator(dspy.Signature):
    """Generate NPC dialogue and responses."""
    npc_name: str = dspy.InputField(desc="Name and type of NPC")
    npc_personality: str = dspy.InputField(desc="NPC personality and background")
    player_input: str = dspy.InputField(desc="What the player said or did")
    context: str = dspy.InputField(desc="Current game context and history")
    
    npc_response: str = dspy.OutputField(desc="NPC's dialogue response")
    mood_change: str = dspy.OutputField(desc="How NPC's mood changed (positive/negative/neutral)")
    quest_offered: bool = dspy.OutputField(desc="Whether NPC offers a quest")
    information_revealed: str = dspy.OutputField(desc="Any important information shared")

class ActionResolver(dspy.Signature):
    """Resolve player actions and determine outcomes."""
    action: str = dspy.InputField(desc="Player's chosen action")
    player_stats: str = dspy.InputField(desc="Player's current stats and skills")
    context: str = dspy.InputField(desc="Current game context")
    difficulty: str = dspy.InputField(desc="Difficulty level of the action")
    
    success: bool = dspy.OutputField(desc="Whether the action succeeded")
    outcome_description: str = dspy.OutputField(desc="Description of what happened")
    stat_changes: dict[str, int] = dspy.OutputField(desc="Changes to player stats")
    items_gained: list[str] = dspy.OutputField(desc="Items gained from this action")
    experience_gained: int = dspy.OutputField(desc="Experience points gained")

class GameAI(dspy.Module):
    """Main AI module for game logic and narrative."""
    
    def __init__(self):
        super().__init__()
        self.story_gen = dspy.ChainOfThought(StoryGenerator)
        self.dialogue_gen = dspy.ChainOfThought(DialogueGenerator)
        self.action_resolver = dspy.ChainOfThought(ActionResolver)
    
    def generate_scene(self, player: Player, context: GameContext, recent_actions: str = "") -> Dict:
        """Generate current scene description and options."""
        
        player_info = f"Level {player.level} {player.name}, Health: {player.health}, Skills: {player.skills}"
        
        scene = self.story_gen(
            location=context.current_location,
            player_info=player_info,
            story_progress=context.story_progress,
            recent_actions=recent_actions
        )
        
        return {
            "description": scene.scene_description,
            "actions": scene.available_actions,
            "npcs": scene.npcs_present,
            "items": scene.items_available
        }
    
    def handle_dialogue(self, npc_name: str, player_input: str, context: GameContext) -> Dict:
        """Handle conversation with NPCs."""
        
        # Create NPC personality based on name and context
        personality_map = {
            "Village Elder": "Wise, knowledgeable, speaks in riddles, has ancient knowledge",
            "Merchant": "Greedy but fair, loves to bargain, knows about valuable items",
            "Guard": "Dutiful, suspicious of strangers, follows rules strictly",
            "Thief": "Sneaky, untrustworthy, has information about hidden things",
            "Wizard": "Mysterious, powerful, speaks about magic and ancient forces"
        }
        
        personality = personality_map.get(npc_name, "Friendly villager with local knowledge")
        game_context = f"Location: {context.current_location}, Story progress: {context.story_progress}"
        
        response = self.dialogue_gen(
            npc_name=npc_name,
            npc_personality=personality,
            player_input=player_input,
            context=game_context
        )
        
        return {
            "response": response.npc_response,
            "mood": response.mood_change,
            "quest": response.quest_offered,
            "info": response.information_revealed
        }
    
    def resolve_action(self, action: str, player: Player, context: GameContext) -> Dict:
        """Resolve player actions and determine outcomes."""
        
        player_stats = f"Level {player.level}, Health {player.health}, Skills: {player.skills}"
        game_context = f"Location: {context.current_location}, Progress: {context.story_progress}"
        
        # Determine difficulty based on action type
        difficulty = "medium"
        if any(word in action.lower() for word in ["fight", "battle", "attack"]):
            difficulty = "hard"
        elif any(word in action.lower() for word in ["look", "examine", "talk"]):
            difficulty = "easy"
        
        result = self.action_resolver(
            action=action,
            player_stats=player_stats,
            context=game_context,
            difficulty=difficulty
        )
        
        return {
            "success": result.success,
            "description": result.outcome_description,
            "stat_changes": result.stat_changes,
            "items": result.items_gained,
            "experience": result.experience_gained
        }

# Initialize AI
ai = GameAI()
```

## Step 3: Game Interface and Interaction

```python
def display_game_header():
    """Display the game header."""
    header = Text("🏰 MYSTIC REALM ADVENTURE 🏰", style="bold magenta")
    console.print(Panel(header, style="bright_blue"))

def display_player_status(player: Player):
    """Display player status panel."""
    status = f"""
[bold]Name:[/bold] {player.name}
[bold]Level:[/bold] {player.level} (XP: {player.experience})
[bold]Health:[/bold] {player.health}/100
[bold]Skills:[/bold]
  • Strength: {player.skills['strength']}
  • Intelligence: {player.skills['intelligence']}
  • Charisma: {player.skills['charisma']}
  • Stealth: {player.skills['stealth']}
[bold]Inventory:[/bold] {len(player.inventory)} items
    """
    console.print(Panel(status.strip(), title="Player Status", style="green"))

def display_location(context: GameContext, scene: Dict):
    """Display current location and scene."""
    location_panel = f"""
[bold yellow]{context.current_location}[/bold yellow]

{scene['description']}
    """
    
    if scene['npcs']:
        location_panel += f"\n\n[bold]NPCs present:[/bold] {', '.join(scene['npcs'])}"
    
    if scene['items']:
        location_panel += f"\n[bold]Items visible:[/bold] {', '.join(scene['items'])}"
    
    console.print(Panel(location_panel.strip(), title="Current Location", style="cyan"))

def display_actions(actions: list[str]):
    """Display available actions."""
    action_text = "\n".join([f"{i+1}. {action}" for i, action in enumerate(actions)])
    console.print(Panel(action_text, title="Available Actions", style="yellow"))

def get_player_choice(max_choices: int) -> int:
    """Get player's choice with input validation."""
    while True:
        try:
            choice = typer.prompt("Choose an action (number)")
            choice_num = int(choice)
            if 1 <= choice_num <= max_choices:
                return choice_num - 1
            else:
                console.print(f"[red]Please enter a number between 1 and {max_choices}[/red]")
        except ValueError:
            console.print("[red]Please enter a valid number[/red]")

def show_inventory(player: Player):
    """Display player inventory."""
    if not player.inventory:
        console.print(Panel("Your inventory is empty.", title="Inventory", style="red"))
    else:
        items = "\n".join([f"• {item}" for item in player.inventory])
        console.print(Panel(items, title="Inventory", style="green"))

def main_menu():
    """Display main menu and handle selection."""
    console.clear()
    display_game_header()
    
    menu_options = [
        "1. New Game",
        "2. Load Game", 
        "3. How to Play",
        "4. Exit"
    ]
    
    menu_text = "\n".join(menu_options)
    console.print(Panel(menu_text, title="Main Menu", style="bright_blue"))
    
    choice = typer.prompt("Select an option")
    return choice

def show_help():
    """Display help information."""
    help_text = """
[bold]How to Play:[/bold]

• This is a text-based adventure game powered by AI
• Make choices by selecting numbered options
• Talk to NPCs to learn about the world and get quests
• Explore different locations to find items and adventures
• Your choices affect the story and character development
• Use 'inventory' to check your items
• Use 'status' to see your character info
• Type 'save' to save your progress
• Type 'quit' to return to main menu

[bold]Tips:[/bold]
• Different skills affect your success in various actions
• NPCs remember your previous interactions
• Explore thoroughly - there are hidden secrets!
• Your reputation affects how NPCs treat you
    """
    console.print(Panel(help_text.strip(), title="Game Help", style="blue"))
    typer.prompt("Press Enter to continue")
```

## Step 4: Main Game Loop

```python
def create_new_character():
    """Create a new player character."""
    console.clear()
    display_game_header()
    
    name = typer.prompt("Enter your character's name")
    
    # Character creation with skill point allocation
    console.print("\n[bold]Character Creation[/bold]")
    console.print("You have 10 extra skill points to distribute among your skills.")
    console.print("Base skills start at 10 each.\n")
    
    skills = {"strength": 10, "intelligence": 10, "charisma": 10, "stealth": 10}
    points_remaining = 10
    
    for skill in skills.keys():
        if points_remaining > 0:
            console.print(f"Points remaining: {points_remaining}")
            while True:
                try:
                    points = int(typer.prompt(f"Points to add to {skill} (0-{points_remaining})"))
                    if 0 <= points <= points_remaining:
                        skills[skill] += points
                        points_remaining -= points
                        break
                    else:
                        console.print(f"[red]Enter a number between 0 and {points_remaining}[/red]")
                except ValueError:
                    console.print("[red]Please enter a valid number[/red]")
    
    player = Player(name=name, skills=skills)
    console.print(f"\n[green]Welcome to Mystic Realm, {name}![/green]")
    return player

def game_loop():
    """Main game loop."""
    recent_actions = ""
    
    while game.running and game.state == GameState.PLAYING:
        console.clear()
        display_game_header()
        
        # Generate current scene
        scene = ai.generate_scene(game.player, game.context, recent_actions)
        
        # Display game state
        display_player_status(game.player)
        display_location(game.context, scene)
        
        # Add standard actions
        all_actions = scene['actions'] + ["Check inventory", "Character status", "Save game", "Quit to menu"]
        display_actions(all_actions)
        
        # Get player choice
        choice_idx = get_player_choice(len(all_actions))
        chosen_action = all_actions[choice_idx]
        
        # Handle special commands
        if chosen_action == "Check inventory":
            show_inventory(game.player)
            typer.prompt("Press Enter to continue")
            continue
        elif chosen_action == "Character status":
            display_player_status(game.player)
            typer.prompt("Press Enter to continue")
            continue
        elif chosen_action == "Save game":
            game.save_game()
            typer.prompt("Press Enter to continue")
            continue
        elif chosen_action == "Quit to menu":
            game.state = GameState.MENU
            break
        
        # Handle game actions
        if chosen_action in scene['actions']:
            # Check if it's dialogue with an NPC
            npc_target = None
            for npc in scene['npcs']:
                if npc.lower() in chosen_action.lower():
                    npc_target = npc
                    break
            
            if npc_target:
                # Handle NPC interaction
                console.print(f"\n[bold]Talking to {npc_target}...[/bold]")
                dialogue = ai.handle_dialogue(npc_target, chosen_action, game.context)
                
                console.print(f"\n[italic]{npc_target}:[/italic] \"{dialogue['response']}\"")
                
                if dialogue['quest']:
                    console.print(f"[yellow]💼 Quest opportunity detected![/yellow]")
                
                if dialogue['info']:
                    console.print(f"[blue]ℹ️  {dialogue['info']}[/blue]")
                    
                # Add NPC to met list
                if npc_target not in game.context.npcs_met:
                    game.context.npcs_met.append(npc_target)
                
                recent_actions = f"Talked to {npc_target}: {chosen_action}"
            else:
                # Handle general action
                result = ai.resolve_action(chosen_action, game.player, game.context)
                
                console.print(f"\n{result['description']}")
                
                # Apply results
                if result['success']:
                    console.print("[green]✅ Success![/green]")
                    
                    # Apply stat changes
                    for stat, change in result['stat_changes'].items():
                        if stat in game.player.skills:
                            game.player.skills[stat] += change
                            if change > 0:
                                console.print(f"[green]{stat.title()} increased by {change}![/green]")
                        elif stat == "health":
                            game.player.health = max(0, min(100, game.player.health + change))
                            if change > 0:
                                console.print(f"[green]Health restored by {change}![/green]")
                            elif change < 0:
                                console.print(f"[red]Health decreased by {abs(change)}![/red]")
                    
                    # Add items
                    for item in result['items']:
                        game.player.add_item(item)
                    
                    # Give experience
                    if result['experience'] > 0:
                        game.player.gain_experience(result['experience'])
                    
                    # Update story progress
                    game.context.story_progress += 1
                else:
                    console.print("[red]❌ The action didn't go as planned...[/red]")
                
                recent_actions = f"Attempted: {chosen_action}"
            
            # Check for game over conditions
            if game.player.health <= 0:
                console.print("\n[bold red]💀 You have died! Game Over![/bold red]")
                game.state = GameState.GAME_OVER
                break
            
            typer.prompt("\nPress Enter to continue")

def main():
    """Main game function."""
    while game.running:
        if game.state == GameState.MENU:
            choice = main_menu()
            
            if choice == "1":
                game.player = create_new_character()
                game.context = GameContext()
                game.state = GameState.PLAYING
                console.print("\n[italic]Your adventure begins...[/italic]")
                typer.prompt("Press Enter to start")
                
            elif choice == "2":
                if game.load_game():
                    game.state = GameState.PLAYING
                typer.prompt("Press Enter to continue")
                
            elif choice == "3":
                show_help()
                
            elif choice == "4":
                game.running = False
                console.print("[bold]Thanks for playing! Goodbye![/bold]")
            
        elif game.state == GameState.PLAYING:
            game_loop()
            
        elif game.state == GameState.GAME_OVER:
            console.print("\n[bold]Game Over[/bold]")
            restart = typer.confirm("Would you like to return to the main menu?")
            if restart:
                game.state = GameState.MENU
            else:
                game.running = False

if __name__ == "__main__":
    main()
```

## Example Gameplay

When you run the game, you'll experience:

**Character Creation:**
```
🏰 MYSTIC REALM ADVENTURE 🏰

Enter your character's name: Aria

Character Creation
You have 10 extra skill points to distribute among your skills.
Base skills start at 10 each.

Points remaining: 10
Points to add to strength (0-10): 2
Points to add to intelligence (0-8): 4
Points to add to charisma (0-4): 3
Points to add to stealth (0-1): 1

Welcome to Mystic Realm, Aria!
```

**Dynamic Scene Generation:**
```
┌──────────── Current Location ────────────┐
│ Village Square                           │
│                                          │
│ You stand in the bustling heart of       │
│ Willowbrook Village. The ancient stone   │
│ fountain bubbles cheerfully as merchants │
│ hawk their wares and children play. A    │
│ mysterious hooded figure lurks near the  │
│ shadows of the old oak tree.             │
│                                          │
│ NPCs present: Village Elder, Merchant    │
│ Items visible: Strange Medallion, Herbs  │
└──────────────────────────────────────────┘

┌────────── Available Actions ─────────────┐
│ 1. Approach the hooded figure            │
│ 2. Talk to the Village Elder             │
│ 3. Browse the merchant's wares           │
│ 4. Examine the strange medallion         │
│ 5. Gather herbs near the fountain        │
│ 6. Head to the forest path               │
└───────────────────────────────────────────┘
```

**AI-Generated Dialogue:**
```
Talking to Village Elder...

Village Elder: "Ah, young traveler, I sense a great destiny 
surrounds you like morning mist. The ancient prophecy speaks 
of one who would come bearing the mark of courage. Tell me, 
have you noticed anything... unusual in your travels?"

💼 Quest opportunity detected!
ℹ️ The Village Elder knows about an ancient prophecy that might involve you
```

## Next Steps

- **Combat System**: Add turn-based battles with strategy
- **Magic System**: Spellcasting with resource management
- **Multiplayer**: Network support for cooperative adventures
- **Quest System**: Complex multi-step missions with branching outcomes
- **World Building**: Procedurally generated locations and characters
- **Audio**: Add sound effects and background music

This tutorial demonstrates how DSPy's modular approach enables complex, interactive systems where AI handles creative content generation while maintaining consistent game logic and player agency.


--- docs/docs/tutorials/async/index.md ---
# Async DSPy Programming

DSPy provides native support for asynchronous programming, allowing you to build more efficient and
scalable applications. This guide will walk you through how to leverage async capabilities in DSPy,
covering both built-in modules and custom implementations.

## Why Use Async in DSPy?

Asynchronous programming in DSPy offers several benefits:
- Improved performance through concurrent operations
- Better resource utilization
- Reduced waiting time for I/O-bound operations
- Enhanced scalability for handling multiple requests

## When Should I use Sync or Async?

Choosing between synchronous and asynchronous programming in DSPy depends on your specific use case.
Here's a guide to help you make the right choice:

Use Synchronous Programming When

- You're exploring or prototyping new ideas
- You're conducting research or experiments
- You're building small to medium-sized applications
- You need simpler, more straightforward code
- You want easier debugging and error tracking

Use Asynchronous Programming When:

- You're building a high-throughput service (high QPS)
- You're working with tools that only support async operations
- You need to handle multiple concurrent requests efficiently
- You're building a production service that requires high scalability

### Important Considerations

While async programming offers performance benefits, it comes with some trade-offs:

- More complex error handling and debugging
- Potential for subtle, hard-to-track bugs
- More complex code structure
- Different code between ipython (Colab, Jupyter lab, Databricks notebooks, ...) and normal python runtime.

We recommend starting with synchronous programming for most development scenarios and switching to async
only when you have a clear need for its benefits. This approach allows you to focus on the core logic of
your application before dealing with the additional complexity of async programming.

## Using Built-in Modules Asynchronously

Most DSPy built-in modules support asynchronous operations through the `acall()` method. This method
maintains the same interface as the synchronous `__call__` method but operates asynchronously.

Here's a basic example using `dspy.Predict`:

```python
import dspy
import asyncio
import os

os.environ["OPENAI_API_KEY"] = "your_api_key"

dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))
predict = dspy.Predict("question->answer")

async def main():
    # Use acall() for async execution
    output = await predict.acall(question="why did a chicken cross the kitchen?")
    print(output)


asyncio.run(main())
```

### Working with Async Tools

DSPy's `Tool` class seamlessly integrates with async functions. When you provide an async
function to `dspy.Tool`, you can execute it using `acall()`. This is particularly useful
for I/O-bound operations or when working with external services.

```python
import asyncio
import dspy
import os

os.environ["OPENAI_API_KEY"] = "your_api_key"

async def foo(x):
    # Simulate an async operation
    await asyncio.sleep(0.1)
    print(f"I get: {x}")

# Create a tool from the async function
tool = dspy.Tool(foo)

async def main():
    # Execute the tool asynchronously
    await tool.acall(x=2)

asyncio.run(main())
```

Note: When using `dspy.ReAct` with tools, calling `acall()` on the ReAct instance will automatically
execute all tools asynchronously using their `acall()` methods.

## Creating Custom Async DSPy Modules

To create your own async DSPy module, implement the `aforward()` method instead of `forward()`. This method
should contain your module's async logic. Here's an example of a custom module that chains two async operations:

```python
import dspy
import asyncio
import os

os.environ["OPENAI_API_KEY"] = "your_api_key"
dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))

class MyModule(dspy.Module):
    def __init__(self):
        self.predict1 = dspy.ChainOfThought("question->answer")
        self.predict2 = dspy.ChainOfThought("answer->simplified_answer")

    async def aforward(self, question, **kwargs):
        # Execute predictions sequentially but asynchronously
        answer = await self.predict1.acall(question=question)
        return await self.predict2.acall(answer=answer)


async def main():
    mod = MyModule()
    result = await mod.acall(question="Why did a chicken cross the kitchen?")
    print(result)


asyncio.run(main())
```


--- docs/docs/tutorials/build_ai_program/index.md ---
# Build AI Programs with DSPy

This section contains hands-on tutorials that guide you through building production-ready AI applications using DSPy. Each tutorial demonstrates practical use cases and shows you how to leverage DSPy's modular programming approach to create robust, maintainable AI systems.

## Core Applications

### [Managing Conversation History](../conversation_history/index.md)
Learn how to manage conversation history in DSPy applications.

### [Building AI Agents with DSPy](../customer_service_agent/index.ipynb)
Learn to create intelligent agents that can handle complex customer service scenarios. This tutorial shows how to build agents that can understand context, maintain conversation state, and provide helpful responses.

### [Building AI Applications by Customizing DSPy Modules](../custom_module/index.ipynb)
Discover how to create custom DSPy modules tailored to your specific needs. Learn the patterns for building reusable, composable components that can be shared across different applications.

## Retrieval-Augmented Generation (RAG)

### [Retrieval-Augmented Generation (RAG)](../rag/index.ipynb)
Master the fundamentals of RAG systems with DSPy. Learn how to combine retrieval mechanisms with language models to build systems that can answer questions using external knowledge sources.

### [Building RAG as Agent](../agents/index.ipynb)
Take RAG to the next level by building `ReAct` agent-based systems that can reason about when and how to retrieve information, making your RAG systems more intelligent and adaptive.

### [Multi-Hop RAG](../multihop_search/index.ipynb)
Build sophisticated RAG systems that can perform multi-step reasoning across multiple information sources, perfect for complex research and analysis tasks.

## Specialized Use Cases

### [Entity Extraction](../entity_extraction/index.ipynb)
Learn to build systems that can identify and extract specific entities from text, essential for information processing and data analysis applications.

### [Classification](../classification/index.md)
Build robust text classification systems using DSPy's modular approach with a topic classification example.

### [Privacy-Conscious Delegation](../papillon/index.md)
Explore advanced techniques for building AI systems that respect privacy constraints while maintaining high performance by combining a small local model and an advanced external model.

## Advanced Reasoning

### [Program Of Thought](../program_of_thought/index.ipynb)
Learn to build systems that can generate and execute code to solve complex problems, combining the power of language models with programmatic reasoning.

## Multimodal Applications

### [Image Generation Prompt iteration](../image_generation_prompting/index.ipynb)
Discover how to use DSPy to iteratively improve image generation prompts, creating better visual content through systematic optimization.

### [Audio](../audio/index.ipynb)
Explore audio processing applications with DSPy, learning to build systems that can understand, process, and generate audio content.


--- docs/docs/tutorials/cache/index.md ---
# Use and Customize DSPy Cache

In this tutorial, we will explore the design of DSPy's caching mechanism and demonstrate how to effectively use and customize it.

## DSPy Cache Structure

DSPy's caching system is architected in three distinct layers:

1.  **In-memory cache**: Implemented using `cachetools.LRUCache`, this layer provides fast access to frequently used data.
2.  **On-disk cache**: Leveraging `diskcache.FanoutCache`, this layer offers persistent storage for cached items.
3.  **Prompt cache (Server-side cache)**: This layer is managed by the LLM service provider (e.g., OpenAI, Anthropic).

While DSPy does not directly control the server-side prompt cache, it offers users the flexibility to enable, disable, and customize the in-memory and on-disk caches to suit their specific requirements.

## Using DSPy Cache

By default, both in-memory and on-disk caching are automatically enabled in DSPy. No specific action is required to start using the cache. When a cache hit occurs, you will observe a significant reduction in the module call's execution time. Furthermore, if usage tracking is enabled, the usage metrics for a cached call will be `None`.

Consider the following example:

```python
import dspy
import os
import time

os.environ["OPENAI_API_KEY"] = "{your_openai_key}"

dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"), track_usage=True)

predict = dspy.Predict("question->answer")

start = time.time()
result1 = predict(question="Who is the GOAT of basketball?")
print(f"Time elapse: {time.time() - start: 2f}\n\nTotal usage: {result1.get_lm_usage()}")

start = time.time()
result2 = predict(question="Who is the GOAT of basketball?")
print(f"Time elapse: {time.time() - start: 2f}\n\nTotal usage: {result2.get_lm_usage()}")
```

A sample output looks like:

```
Time elapse:  4.384113
Total usage: {'openai/gpt-4o-mini': {'completion_tokens': 97, 'prompt_tokens': 144, 'total_tokens': 241, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0, 'text_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'text_tokens': None, 'image_tokens': None}}}

Time elapse:  0.000529
Total usage: {}
```

## Disabling/Enabling DSPy Cache

There are scenarios where you might need to disable caching, either entirely or selectively for in-memory or on-disk caches. For instance:

- You require different responses for identical LM requests.
- You lack disk write permissions and need to disable the on-disk cache.
- You have limited memory resources and wish to disable the in-memory cache.

DSPy provides the `dspy.configure_cache()` utility function for this purpose. You can use the corresponding flags to control the enabled/disabled state of each cache type:

```python
dspy.configure_cache(
    enable_disk_cache=False,
    enable_memory_cache=False,
)
```

In additions, you can manage the capacity of the in-memory and on-disk caches:

```python
dspy.configure_cache(
    enable_disk_cache=True,
    enable_memory_cache=True,
    disk_size_limit_bytes=YOUR_DESIRED_VALUE,
    memory_max_entries=YOUR_DESIRED_VALUE,
)
```

Please note that `disk_size_limit_bytes` defines the maximum size in bytes for the on-disk cache, while `memory_max_entries` specifies the maximum number of entries for the in-memory cache.

## Understanding and Customizing the Cache

In specific situations, you might want to implement a custom cache, for example, to gain finer control over how cache keys are generated. By default, the cache key is derived from a hash of all request arguments sent to `litellm`, excluding credentials like `api_key`.

To create a custom cache, you need to subclass `dspy.clients.Cache` and override the relevant methods:

```python
class CustomCache(dspy.clients.Cache):
    def __init__(self, **kwargs):
        {write your own constructor}

    def cache_key(self, request: dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> str:
        {write your logic of computing cache key}

    def get(self, request: dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> Any:
        {write your cache read logic}

    def put(
        self,
        request: dict[str, Any],
        value: Any,
        ignored_args_for_cache_key: Optional[list[str]] = None,
        enable_memory_cache: bool = True,
    ) -> None:
        {write your cache write logic}
```

To ensure seamless integration with the rest of DSPy, it is recommended to implement your custom cache using the same method signatures as the base class, or at a minimum, include `**kwargs` in your method definitions to prevent runtime errors during cache read/write operations.

Once your custom cache class is defined, you can instruct DSPy to use it:

```python
dspy.cache = CustomCache()
```

Let's illustrate this with a practical example. Suppose we want the cache key computation to depend solely on the request message content, ignoring other parameters like the specific LM being called. We can create a custom cache as follows:

```python
class CustomCache(dspy.clients.Cache):

    def cache_key(self, request: dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> str:
        messages = request.get("messages", [])
        return sha256(ujson.dumps(messages, sort_keys=True).encode()).hexdigest()

dspy.cache = CustomCache(enable_disk_cache=True, enable_memory_cache=True, disk_cache_dir=dspy.clients.DISK_CACHE_DIR)
```

For comparison, consider executing the code below without the custom cache:

```python
import dspy
import os
import time

os.environ["OPENAI_API_KEY"] = "{your_openai_key}"

dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))

predict = dspy.Predict("question->answer")

start = time.time()
result1 = predict(question="Who is the GOAT of soccer?")
print(f"Time elapse: {time.time() - start: 2f}")

start = time.time()
with dspy.context(lm=dspy.LM("openai/gpt-4.1-mini")):
    result2 = predict(question="Who is the GOAT of soccer?")
print(f"Time elapse: {time.time() - start: 2f}")
```

The time elapsed will indicate that the cache is not hit on the second call. However, when using the custom cache:

```python
import dspy
import os
import time
from typing import Dict, Any, Optional
import ujson
from hashlib import sha256

os.environ["OPENAI_API_KEY"] = "{your_openai_key}"

dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))

class CustomCache(dspy.clients.Cache):

    def cache_key(self, request: dict[str, Any], ignored_args_for_cache_key: Optional[list[str]] = None) -> str:
        messages = request.get("messages", [])
        return sha256(ujson.dumps(messages, sort_keys=True).encode()).hexdigest()

dspy.cache = CustomCache(enable_disk_cache=True, enable_memory_cache=True, disk_cache_dir=dspy.clients.DISK_CACHE_DIR)

predict = dspy.Predict("question->answer")

start = time.time()
result1 = predict(question="Who is the GOAT of volleyball?")
print(f"Time elapse: {time.time() - start: 2f}")

start = time.time()
with dspy.context(lm=dspy.LM("openai/gpt-4.1-mini")):
    result2 = predict(question="Who is the GOAT of volleyball?")
print(f"Time elapse: {time.time() - start: 2f}")
```

You will observe that the cache is hit on the second call, demonstrating the effect of the custom cache key logic.

--- docs/docs/tutorials/classification/index.md ---
Please refer to [this tutorial from Drew Breunig](https://www.dbreunig.com/2024/12/12/pipelines-prompt-optimization-with-dspy.html) using DSPy.

This tutorial demonstrates a few aspects of using DSPy in a highly-accessible, concrete context for categorizing historic events with a tiny LM.

--- docs/docs/tutorials/conversation_history/index.md ---
# Managing Conversation History

Maintaining conversation history is a fundamental feature when building AI applications such as chatbots. While DSPy does not provide automatic conversation history management within `dspy.Module`, it offers the `dspy.History` utility to help you manage conversation history effectively.

## Using `dspy.History` to Manage Conversation History

The `dspy.History` class can be used as an input field type, containing a `messages: list[dict[str, Any]]` attribute that stores the conversation history. Each entry in this list is a dictionary with keys corresponding to the fields defined in your signature. See the example below:

```python
import dspy
import os

os.environ["OPENAI_API_KEY"] = "{your_openai_api_key}"

dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))

class QA(dspy.Signature):
    question: str = dspy.InputField()
    history: dspy.History = dspy.InputField()
    answer: str = dspy.OutputField()

predict = dspy.Predict(QA)
history = dspy.History(messages=[])

while True:
    question = input("Type your question, end conversation by typing 'finish': ")
    if question == "finish":
        break
    outputs = predict(question=question, history=history)
    print(f"\n{outputs.answer}\n")
    history.messages.append({"question": question, **outputs})

dspy.inspect_history()
```

There are two key steps when using the conversation history:

- **Include a field of type `dspy.History` in your Signature.**
- **Maintain a history instance at runtime, appending new conversation turns to it.** Each entry should include all relevant input and output field information.

A sample run might look like this:

```
Type your question, end conversation by typing 'finish': do you know the competition between pytorch and tensorflow?

Yes, there is a notable competition between PyTorch and TensorFlow, which are two of the most popular deep learning frameworks. PyTorch, developed by Facebook, is known for its dynamic computation graph, which allows for more flexibility and ease of use, especially in research settings. TensorFlow, developed by Google, initially used a static computation graph but has since introduced eager execution to improve usability. TensorFlow is often favored in production environments due to its scalability and deployment capabilities. Both frameworks have strong communities and extensive libraries, and the choice between them often depends on specific project requirements and personal preference.

Type your question, end conversation by typing 'finish': which one won the battle? just tell me the result, don't include any reasoning, thanks!

There is no definitive winner; both PyTorch and TensorFlow are widely used and have their own strengths.
Type your question, end conversation by typing 'finish': finish




[2025-07-11T16:35:57.592762]

System message:

Your input fields are:
1. `question` (str): 
2. `history` (History):
Your output fields are:
1. `answer` (str):
All interactions will be structured in the following way, with the appropriate values filled in.

[[ ## question ## ]]
{question}

[[ ## history ## ]]
{history}

[[ ## answer ## ]]
{answer}

[[ ## completed ## ]]
In adhering to this structure, your objective is: 
        Given the fields `question`, `history`, produce the fields `answer`.


User message:

[[ ## question ## ]]
do you know the competition between pytorch and tensorflow?


Assistant message:

[[ ## answer ## ]]
Yes, there is a notable competition between PyTorch and TensorFlow, which are two of the most popular deep learning frameworks. PyTorch, developed by Facebook, is known for its dynamic computation graph, which allows for more flexibility and ease of use, especially in research settings. TensorFlow, developed by Google, initially used a static computation graph but has since introduced eager execution to improve usability. TensorFlow is often favored in production environments due to its scalability and deployment capabilities. Both frameworks have strong communities and extensive libraries, and the choice between them often depends on specific project requirements and personal preference.

[[ ## completed ## ]]


User message:

[[ ## question ## ]]
which one won the battle? just tell me the result, don't include any reasoning, thanks!

Respond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.


Response:

[[ ## answer ## ]]
There is no definitive winner; both PyTorch and TensorFlow are widely used and have their own strengths.

[[ ## completed ## ]]
```

Notice how each user input and assistant response is appended to the history, allowing the model to maintain context across turns.

The actual prompt sent to the language model is a multi-turn message, as shown by the output of `dspy.inspect_history`. Each conversation turn is represented as a user message followed by an assistant message.

## History in Few-shot Examples

You may notice that `history` does not appear in the input fields section of the prompt, even though it is listed as an input field (e.g., "2. `history` (History):" in the system message). This is intentional: when formatting few-shot examples that include conversation history, DSPy does not expand the history into multiple turns. Instead, to remain compatible with the OpenAI standard format, each few-shot example is represented as a single turn.

For example:

```
import dspy

dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))


class QA(dspy.Signature):
    question: str = dspy.InputField()
    history: dspy.History = dspy.InputField()
    answer: str = dspy.OutputField()


predict = dspy.Predict(QA)
history = dspy.History(messages=[])

predict.demos.append(
    dspy.Example(
        question="What is the capital of France?",
        history=dspy.History(
            messages=[{"question": "What is the capital of Germany?", "answer": "The capital of Germany is Berlin."}]
        ),
        answer="The capital of France is Paris.",
    )
)

predict(question="What is the capital of America?", history=dspy.History(messages=[]))
dspy.inspect_history()
```

The resulting history will look like this:

```
[2025-07-11T16:53:10.994111]

System message:

Your input fields are:
1. `question` (str): 
2. `history` (History):
Your output fields are:
1. `answer` (str):
All interactions will be structured in the following way, with the appropriate values filled in.

[[ ## question ## ]]
{question}

[[ ## history ## ]]
{history}

[[ ## answer ## ]]
{answer}

[[ ## completed ## ]]
In adhering to this structure, your objective is: 
        Given the fields `question`, `history`, produce the fields `answer`.


User message:

[[ ## question ## ]]
What is the capital of France?

[[ ## history ## ]]
{"messages": [{"question": "What is the capital of Germany?", "answer": "The capital of Germany is Berlin."}]}


Assistant message:

[[ ## answer ## ]]
The capital of France is Paris.

[[ ## completed ## ]]


User message:

[[ ## question ## ]]
What is the capital of Germany?

Respond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.


Response:

[[ ## answer ## ]]
The capital of Germany is Berlin.

[[ ## completed ## ]]
```

As you can see, the few-shot example does not expand the conversation history into multiple turns. Instead, it represents the history as JSON data within its section:

```
[[ ## history ## ]]
{"messages": [{"question": "What is the capital of Germany?", "answer": "The capital of Germany is Berlin."}]}
```

This approach ensures compatibility with standard prompt formats while still providing the model with relevant conversational context.



--- docs/docs/tutorials/core_development/index.md ---
# Tools, Development, and Deployment

This section covers essential DSPy features and best practices for professional AI development. Learn how to implement key functionalities like streaming, caching, deployment, and monitoring in your DSPy applications. These tutorials focus on the practical aspects of building production-ready systems.

## Integration and Tooling

### [Use MCP in DSPy](../mcp/index.md)
Learn to integrate Model Context Protocol (MCP) with DSPy applications. This tutorial shows how to leverage MCP for enhanced context management and more sophisticated AI interactions.

### [Output Refinement](../output_refinement/best-of-n-and-refine.md)
Master techniques for improving output quality through refinement strategies. Learn how to implement best-of-N sampling and iterative refinement to get higher-quality results from your DSPy programs.

## Data Management and Persistence

### [Saving and Loading](../saving/index.md)
Understand how to persist and restore DSPy programs and their optimized states. Learn best practices for model versioning, checkpoint management, and program serialization.

### [Cache](../cache/index.md)
Implement efficient caching strategies to improve performance and reduce API costs. Learn how to configure and use DSPy's caching mechanisms effectively in different scenarios.

## Production Deployment

### [Deployment](../deployment/index.md)
Learn to deploy DSPy applications in production environments. This tutorial covers multiple deployment strategies such as FastAPI and MLflow.

### [Streaming](../streaming/index.md)
Implement real-time streaming capabilities in your DSPy applications. Learn how to handle streaming responses for better user experience in interactive applications.

### [Async](../async/index.md)
Build asynchronous DSPy applications for improved performance and scalability. Learn async/await patterns and concurrent execution strategies for high-throughput systems.

## Monitoring and Optimization

### [Debugging & Observability](../observability/index.md)
Master debugging and monitoring techniques for DSPy applications. Learn to use comprehensive logging, tracing, and error handling for production systems.

### [Tracking DSPy Optimizers](../optimizer_tracking/index.md)
Learn to track and analyze optimizer performance and behavior. Understand how to monitor optimization processes and enhance the reproducibility of the optimization.


--- docs/docs/tutorials/deployment/index.md ---
# Tutorial: Deploying your DSPy program

This guide demonstrates two potential ways to deploy your DSPy program in production: FastAPI for lightweight deployments and MLflow for more production-grade deployments with program versioning and management.

Below, we'll assume you have the following simple DSPy program that you want to deploy. You can replace this with something more sophisticated.

```python
import dspy

dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini"))
dspy_program = dspy.ChainOfThought("question -> answer")
```

## Deploying with FastAPI

FastAPI offers a straightforward way to serve your DSPy program as a REST API. This is ideal when you have direct access to your program code and need a lightweight deployment solution.

```bash
> pip install fastapi uvicorn
> export OPENAI_API_KEY="your-openai-api-key"
```

Let's create a FastAPI application to serve your `dspy_program` defined above.

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

import dspy

app = FastAPI(
    title="DSPy Program API",
    description="A simple API serving a DSPy Chain of Thought program",
    version="1.0.0"
)

# Define request model for better documentation and validation
class Question(BaseModel):
    text: str

# Configure your language model and 'asyncify' your DSPy program.
lm = dspy.LM("openai/gpt-4o-mini")
dspy.settings.configure(lm=lm, async_max_workers=4) # default is 8
dspy_program = dspy.ChainOfThought("question -> answer")
dspy_program = dspy.asyncify(dspy_program)

@app.post("/predict")
async def predict(question: Question):
    try:
        result = await dspy_program(question=question.text)
        return {
            "status": "success",
            "data": result.toDict()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

In the code above, we call `dspy.asyncify` to convert the dspy program to run in async mode for high-throughput FastAPI
deployments. Currently, this runs the dspy program in a separate thread and awaits its result.

By default, the limit of spawned threads is 8. Think of this like a worker pool.
If you have 8 in-flight programs and call it once more, the 9th call will wait until one of the 8 returns.
You can configure the async capacity using the new `async_max_workers` setting.

??? "Streaming, in DSPy 2.6.0+"

    Streaming is also supported in DSPy 2.6.0+, which can be installed via `pip install -U dspy`.

    We can use `dspy.streamify` to convert the dspy program to a streaming mode. This is useful when you want to stream
    the intermediate outputs (i.e. O1-style reasoning) to the client before the final prediction is ready. This uses
    asyncify under the hood and inherits the execution semantics.

    ```python
    dspy_program = dspy.asyncify(dspy.ChainOfThought("question -> answer"))
    streaming_dspy_program = dspy.streamify(dspy_program)

    @app.post("/predict/stream")
    async def stream(question: Question):
        async def generate():
            async for value in streaming_dspy_program(question=question.text):
                if isinstance(value, dspy.Prediction):
                    data = {"prediction": value.labels().toDict()}
                elif isinstance(value, litellm.ModelResponse):
                    data = {"chunk": value.json()}
                yield f"data: {ujson.dumps(data)}\n\n"
            yield "data: [DONE]\n\n"

        return StreamingResponse(generate(), media_type="text/event-stream")

    # Since you're often going to want to stream the result of a DSPy program as server-sent events,
    # we've included a helper function for that, which is equivalent to the code above.

    from dspy.utils.streaming import streaming_response

    @app.post("/predict/stream")
    async def stream(question: Question):
        stream = streaming_dspy_program(question=question.text)
        return StreamingResponse(streaming_response(stream), media_type="text/event-stream")
    ```

Write your code to a file, e.g., `fastapi_dspy.py`. Then you can serve the app with:

```bash
> uvicorn fastapi_dspy:app --reload
```

It will start a local server at `http://127.0.0.1:8000/`. You can test it with the python code below:

```python
import requests

response = requests.post(
    "http://127.0.0.1:8000/predict",
    json={"text": "What is the capital of France?"}
)
print(response.json())
```

You should see the response like below:

```json
{
  "status": "success",
  "data": {
    "reasoning": "The capital of France is a well-known fact, commonly taught in geography classes and referenced in various contexts. Paris is recognized globally as the capital city, serving as the political, cultural, and economic center of the country.",
    "answer": "The capital of France is Paris."
  }
}
```

## Deploying with MLflow

We recommend deploying with MLflow if you are looking to package your DSPy program and deploy in an isolated environment.
MLflow is a popular platform for managing machine learning workflows, including versioning, tracking, and deployment.

```bash
> pip install mlflow>=2.18.0
```

Let's spin up the MLflow tracking server, where we will store our DSPy program. The command below will start a local server at
`http://127.0.0.1:5000/`.

```bash
> mlflow ui
```

Then we can define the DSPy program and log it to the MLflow server. "log" is an overloaded term in MLflow, basically it means
we store the program information along with environment requirements in the MLflow server. This is done via the `mlflow.dspy.log_model()`
function, please see the code below:

> [!NOTE]
> As of MLflow 2.22.0, there is a caveat that you must wrap your DSPy program in a custom DSPy Module class when deploying with MLflow.
> This is because MLflow requires positional arguments while DSPy pre-built modules disallow positional arguments, e.g., `dspy.Predict`
> or `dspy.ChainOfThought`. To work around this, create a wrapper class that inherits from `dspy.Module` and implement your program's
> logic in the `forward()` method, as shown in the example below.

```python
import dspy
import mlflow

mlflow.set_tracking_uri("http://127.0.0.1:5000/")
mlflow.set_experiment("deploy_dspy_program")

lm = dspy.LM("openai/gpt-4o-mini")
dspy.settings.configure(lm=lm)

class MyProgram(dspy.Module):
    def __init__(self):
        super().__init__()
        self.cot = dspy.ChainOfThought("question -> answer")

    def forward(self, messages):
        return self.cot(question=messages[0]["content"])

dspy_program = MyProgram()

with mlflow.start_run():
    mlflow.dspy.log_model(
        dspy_program,
        "dspy_program",
        input_example={"messages": [{"role": "user", "content": "What is LLM agent?"}]},
        task="llm/v1/chat",
    )
```

We recommend you to set `task="llm/v1/chat"` so that the deployed program automatically takes input and generate output in
the same format as the OpenAI chat API, which is a common interface for LM applications. Write the code above into
a file, e.g. `mlflow_dspy.py`, and run it.

After you logged the program, you can view the saved information in MLflow UI. Open `http://127.0.0.1:5000/` and select
the `deploy_dspy_program` experiment, then select the run your just created, under the `Artifacts` tab, you should see the
logged program information, similar to the following screenshot:

![MLflow UI](./dspy_mlflow_ui.png)

Grab your run id from UI (or the console print when you execute `mlflow_dspy.py`), now you can deploy the logged program
with the following command:

```bash
> mlflow models serve -m runs:/{run_id}/model -p 6000
```

After the program is deployed, you can test it with the following command:

```bash
> curl http://127.0.0.1:6000/invocations -H "Content-Type:application/json"  --data '{"messages": [{"content": "what is 2 + 2?", "role": "user"}]}'
```

You should see the response like below:

```json
{
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "{\"reasoning\": \"The question asks for the sum of 2 and 2. To find the answer, we simply add the two numbers together: 2 + 2 = 4.\", \"answer\": \"4\"}"
      },
      "finish_reason": "stop"
    }
  ]
}
```

For complete guide on how to deploy a DSPy program with MLflow, and how to customize the deployment, please refer to the
[MLflow documentation](https://mlflow.org/docs/latest/llms/dspy/index.html).

### Best Practices for MLflow Deployment

1. **Environment Management**: Always specify your Python dependencies in a `conda.yaml` or `requirements.txt` file.
2. **Versioning**: Use meaningful tags and descriptions for your model versions.
3. **Input Validation**: Define clear input schemas and examples.
4. **Monitoring**: Set up proper logging and monitoring for production deployments.

For production deployments, consider using MLflow with containerization:

```bash
> mlflow models build-docker -m "runs:/{run_id}/model" -n "dspy-program"
> docker run -p 6000:8080 dspy-program
```

For a complete guide on production deployment options and best practices, refer to the
[MLflow documentation](https://mlflow.org/docs/latest/llms/dspy/index.html).


--- dspy/primitives/example.py ---
class Example:
    """A flexible data container for DSPy examples and training data.

    The `Example` class is the standard data format used in DSPy evaluation and optimization.

    Key features:
        - Dictionary-like access patterns (item access, iteration, etc.)
        - Flexible initialization from dictionaries, other `Example` instances, or keyword arguments
        - Input/output field separation for training data
        - Serialization support for saving/loading `Example` instances
        - Immutable operations that return new `Example` instances

    Examples:

        Basic usage with keyword arguments:

        ```python
        import dspy

        # Create an example with input and output fields
        example = dspy.Example(
            question="What is the capital of France?",
            answer="Paris",
        )
        print(example.question)  # "What is the capital of France?"
        print(example.answer)   # "Paris"
        ```

        Initialize from a dictionary:

        ```python
        data = {"question": "What is 2+2?", "answer": "4"}
        example = dspy.Example(data)
        print(example["question"])  # "What is 2+2?"
        ```

        Copy from another Example:

        ```python
        original = dspy.Example(question="Hello", answer="World")
        copy = dspy.Example(original)
        print(copy.question)  # "Hello"
        ```

        Working with input/output separation:

        ```python
        # Mark which fields are inputs for training
        example = dspy.Example(
            question="What is the weather?",
            answer="It's sunny",
        ).with_inputs("question")

        # Get only input fields
        inputs = example.inputs()
        print(inputs.question)  # "What is the weather?"

        # Get only output fields (labels)
        labels = example.labels()
        print(labels.answer)  # "It's sunny"
        ```

        Dictionary-like operations:

        ```python
        example = dspy.Example(name="Alice", age=30)

        # Check if key exists
        if "name" in example:
            print("Name field exists")

        # Get with default value
        city = example.get("city", "Unknown")
        print(city)  # "Unknown"
        ```
    """

    def __init__(self, base=None, **kwargs):
        """Initialize an Example instance.

        Args:
            base: Optional base data source. Can be:
                - Another Example instance (copies its data)
                - A dictionary (copies its key-value pairs)
                - None (creates empty Example)
            **kwargs: Additional key-value pairs to store in the Example.
        """
        # Internal storage and other attributes
        self._store = {}
        self._demos = []
        self._input_keys = None

        # Initialize from a base Example if provided
        if base and isinstance(base, type(self)):
            self._store = base._store.copy()

        # Initialize from a dict if provided
        elif base and isinstance(base, dict):
            self._store = base.copy()

        # Update with provided kwargs
        self._store.update(kwargs)

    def __getattr__(self, key):
        if key.startswith("__") and key.endswith("__"):
            raise AttributeError
        if key in self._store:
            return self._store[key]
        raise AttributeError(f"'{type(self).__name__}' object has no attribute '{key}'")

    def __setattr__(self, key, value):
        if key.startswith("_") or key in dir(self.__class__):
            super().__setattr__(key, value)
        else:
            self._store[key] = value

    def __getitem__(self, key):
        return self._store[key]

    def __setitem__(self, key, value):
        self._store[key] = value

    def __delitem__(self, key):
        del self._store[key]

    def __contains__(self, key):
        return key in self._store

    def __len__(self):
        return len([k for k in self._store if not k.startswith("dspy_")])

    def __repr__(self):
        # return f"Example({self._store})" + f" (input_keys={self._input_keys}, demos={self._demos})"
        d = {k: v for k, v in self._store.items() if not k.startswith("dspy_")}
        return f"Example({d})" + f" (input_keys={self._input_keys})"

    def __str__(self):
        return self.__repr__()

    def __eq__(self, other):
        return isinstance(other, Example) and self._store == other._store

    def __hash__(self):
        return hash(tuple(self._store.items()))

    def keys(self, include_dspy=False):
        return [k for k in self._store.keys() if not k.startswith("dspy_") or include_dspy]

    def values(self, include_dspy=False):
        return [v for k, v in self._store.items() if not k.startswith("dspy_") or include_dspy]

    def items(self, include_dspy=False):
        return [(k, v) for k, v in self._store.items() if not k.startswith("dspy_") or include_dspy]

    def get(self, key, default=None):
        return self._store.get(key, default)

    def with_inputs(self, *keys):
        copied = self.copy()
        copied._input_keys = set(keys)
        return copied

    def inputs(self):
        if self._input_keys is None:
            raise ValueError("Inputs have not been set for this example. Use `example.with_inputs()` to set them.")

        # return items that are in input_keys
        d = {key: self._store[key] for key in self._store if key in self._input_keys}
        # return type(self)(d)
        new_instance = type(self)(base=d)
        new_instance._input_keys = self._input_keys  # Preserve input_keys in new instance
        return new_instance

    def labels(self):
        # return items that are NOT in input_keys
        input_keys = self.inputs().keys()
        d = {key: self._store[key] for key in self._store if key not in input_keys}
        return type(self)(d)

    def __iter__(self):
        return iter(dict(self._store))

    def copy(self, **kwargs):
        return type(self)(base=self, **kwargs)

    def without(self, *keys):
        copied = self.copy()
        for key in keys:
            del copied[key]
        return copied

    def toDict(self):  # noqa: N802
        def convert_to_serializable(value):
            if hasattr(value, "toDict"):
                return value.toDict()
            elif isinstance(value, list):
                return [convert_to_serializable(item) for item in value]
            elif isinstance(value, dict):
                return {k: convert_to_serializable(v) for k, v in value.items()}
            else:
                return value

        serializable_store = {}
        for k, v in self._store.items():
            serializable_store[k] = convert_to_serializable(v)

        return serializable_store


--- tests/examples/test_baleen.py ---
import dspy
import dspy.evaluate
from dspy.datasets import HotPotQA
from dspy.dsp.utils import deduplicate
from dspy.evaluate.evaluate import Evaluate
from dspy.teleprompt.bootstrap import BootstrapFewShot


class GenerateAnswer(dspy.Signature):
    """Answer questions with short factoid answers."""

    context = dspy.InputField(desc="may contain relevant facts")
    question = dspy.InputField()
    answer = dspy.OutputField(desc="often between 1 and 5 words")


class GenerateSearchQuery(dspy.Signature):
    """Write a simple search query that will help answer a complex question."""

    context = dspy.InputField(desc="may contain relevant facts")
    question = dspy.InputField()
    query = dspy.OutputField()


class SimplifiedBaleen(dspy.Module):
    def __init__(self, passages_per_hop=3, max_hops=2):
        super().__init__()

        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]
        self.retrieve = dspy.Retrieve(k=passages_per_hop)
        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)
        self.max_hops = max_hops

    def forward(self, question):
        context = []

        for hop in range(self.max_hops):
            query = self.generate_query[hop](context=context, question=question).query
            passages = self.retrieve(query).passages
            context = deduplicate(context + passages)

        pred = self.generate_answer(context=context, question=question)
        return dspy.Prediction(context=context, answer=pred.answer)


def load_hotpotqa():
    # Load the dataset.
    dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)
    # Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.
    trainset = [x.with_inputs("question") for x in dataset.train]
    devset = [x.with_inputs("question") for x in dataset.dev]
    return trainset, devset


# @pytest.mark.slow_test
# TODO: Find a way to make this test run without openai
def _test_baleen():
    lm = dspy.OpenAI(model="gpt-3.5-turbo")
    rm = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")
    dspy.settings.configure(lm=lm, rm=rm)

    # Ask any question you like to this simple RAG program.
    my_question = "How many storeys are in the castle that David Gregory inherited?"

    # Get the prediction. This contains `pred.context` and `pred.answer`.
    uncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program
    pred = uncompiled_baleen(my_question)

    assert pred.answer == "five"


def validate_context_and_answer_and_hops(example, pred, trace=None):
    if not dspy.evaluate.answer_exact_match(example, pred):
        return False
    if not dspy.evaluate.answer_passage_match(example, pred):
        return False

    hops = [example.question] + [outputs.query for *_, outputs in trace if "query" in outputs]

    if max([len(h) for h in hops]) > 100:
        return False
    if any(dspy.evaluate.answer_exact_match_str(hops[idx], hops[:idx], frac=0.8) for idx in range(2, len(hops))):
        return False

    return True


def gold_passages_retrieved(example, pred, trace=None):
    gold_titles = set(map(dspy.evaluate.normalize_text, example["gold_titles"]))
    found_titles = set(map(dspy.evaluate.normalize_text, [c.split(" | ")[0] for c in pred.context]))

    return gold_titles.issubset(found_titles)


# @pytest.mark.slow_test
# TODO: Find a way to make this test run without the slow hotpotqa dataset
def _test_compiled_baleen():
    trainset, devset = load_hotpotqa()
    lm = dspy.OpenAI(model="gpt-3.5-turbo")
    rm = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")
    dspy.settings.configure(lm=lm, rm=rm)

    uncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program

    teleprompter = BootstrapFewShot(metric=validate_context_and_answer_and_hops)
    compiled_baleen = teleprompter.compile(
        SimplifiedBaleen(),
        teacher=SimplifiedBaleen(passages_per_hop=2),
        trainset=trainset,
    )

    evaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)
    uncompiled_baleen_retrieval_score = evaluate_on_hotpotqa(
        uncompiled_baleen, metric=gold_passages_retrieved, display=False
    )
    # assert uncompiled_baleen_retrieval_score / 100 == 18 / 50

    compiled_baleen_retrieval_score = evaluate_on_hotpotqa(compiled_baleen, metric=gold_passages_retrieved)
    # assert compiled_baleen_retrieval_score / 100 == 27 / 50
    assert uncompiled_baleen_retrieval_score < compiled_baleen_retrieval_score


--- tests/primitives/test_example.py ---
import pytest

import dspy
from dspy import Example


def test_example_initialization():
    example = Example(a=1, b=2)
    assert example.a == 1
    assert example.b == 2


def test_example_initialization_from_base():
    base = Example(a=1, b=2)
    example = Example(base=base, c=3)
    assert example.a == 1
    assert example.b == 2
    assert example.c == 3


def test_example_initialization_from_dict():
    base_dict = {"a": 1, "b": 2}
    example = Example(base=base_dict, c=3)
    assert example.a == 1
    assert example.b == 2
    assert example.c == 3


def test_example_set_get_item():
    example = Example()
    example["a"] = 1
    assert example["a"] == 1


def test_example_attribute_access():
    example = Example(a=1)
    assert example.a == 1
    example.a = 2
    assert example.a == 2


def test_example_deletion():
    example = Example(a=1, b=2)
    del example["a"]
    with pytest.raises(AttributeError):
        _ = example.a


def test_example_len():
    example = Example(a=1, b=2, dspy_hidden=3)
    assert len(example) == 2


def test_example_repr_str_img():
    example = Example(
        img=dspy.Image(url="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7")
    )
    assert (
        repr(example)
        == "Example({'img': Image(url=data:image/gif;base64,<IMAGE_BASE_64_ENCODED(56)>)}) (input_keys=None)"
    )
    assert (
        str(example)
        == "Example({'img': Image(url=data:image/gif;base64,<IMAGE_BASE_64_ENCODED(56)>)}) (input_keys=None)"
    )


def test_example_repr_str():
    example = Example(a=1)
    assert repr(example) == "Example({'a': 1}) (input_keys=None)"
    assert str(example) == "Example({'a': 1}) (input_keys=None)"


def test_example_eq():
    example1 = Example(a=1, b=2)
    example2 = Example(a=1, b=2)
    assert example1 == example2
    assert example1 != ""


def test_example_hash():
    example1 = Example(a=1, b=2)
    example2 = Example(a=1, b=2)
    assert hash(example1) == hash(example2)


def test_example_keys_values_items():
    example = Example(a=1, b=2, dspy_hidden=3)
    assert set(example.keys()) == {"a", "b"}
    assert 1 in example.values()
    assert ("b", 2) in example.items()


def test_example_get():
    example = Example(a=1, b=2)
    assert example.get("a") == 1
    assert example.get("c", "default") == "default"


def test_example_with_inputs():
    example = Example(a=1, b=2).with_inputs("a")
    assert example._input_keys == {"a"}


def test_example_inputs_labels():
    example = Example(a=1, b=2).with_inputs("a")
    inputs = example.inputs()
    assert inputs.toDict() == {"a": 1}
    labels = example.labels()
    assert labels.toDict() == {"b": 2}


def test_example_copy_without():
    example = Example(a=1, b=2)
    copied = example.copy(c=3)
    assert copied.a == 1
    assert copied.c == 3
    without_a = copied.without("a")
    with pytest.raises(AttributeError):
        _ = without_a.a


def test_example_to_dict():
    example = Example(a=1, b=2)
    assert example.toDict() == {"a": 1, "b": 2}


--- dspy/primitives/base_module.py ---
import copy
import logging
from collections import deque
from collections.abc import Generator
from pathlib import Path

import cloudpickle
import orjson

from dspy.utils.saving import get_dependency_versions

# NOTE: Note: It's important (temporary decision) to maintain named_parameters that's different in behavior from
# named_sub_modules for the time being.


logger = logging.getLogger(__name__)


class BaseModule:
    def __init__(self):
        pass

    def named_parameters(self):
        """
        Unlike PyTorch, handles (non-recursive) lists of parameters too.
        """

        import dspy
        from dspy.predict.parameter import Parameter

        visited = set()
        named_parameters = []

        def add_parameter(param_name, param_value):
            if isinstance(param_value, Parameter):
                if id(param_value) not in visited:
                    visited.add(id(param_value))
                    named_parameters.append((param_name, param_value))

            elif isinstance(param_value, dspy.Module):
                # When a sub-module is pre-compiled, keep it frozen.
                if not getattr(param_value, "_compiled", False):
                    for sub_name, param in param_value.named_parameters():
                        add_parameter(f"{param_name}.{sub_name}", param)

        if isinstance(self, Parameter):
            add_parameter("self", self)

        for name, value in self.__dict__.items():
            if isinstance(value, Parameter):
                add_parameter(name, value)

            elif isinstance(value, dspy.Module):
                # When a sub-module is pre-compiled, keep it frozen.
                if not getattr(value, "_compiled", False):
                    for sub_name, param in value.named_parameters():
                        add_parameter(f"{name}.{sub_name}", param)

            elif isinstance(value, (list, tuple)):
                for idx, item in enumerate(value):
                    add_parameter(f"{name}[{idx}]", item)

            elif isinstance(value, dict):
                for key, item in value.items():
                    add_parameter(f"{name}['{key}']", item)

        return named_parameters

    def named_sub_modules(self, type_=None, skip_compiled=False) -> Generator[tuple[str, "BaseModule"], None, None]:
        """Find all sub-modules in the module, as well as their names.

        Say `self.children[4]['key'].sub_module` is a sub-module. Then the name will be
        `children[4]['key'].sub_module`. But if the sub-module is accessible at different
        paths, only one of the paths will be returned.
        """
        if type_ is None:
            type_ = BaseModule

        queue = deque([("self", self)])
        seen = {id(self)}

        def add_to_queue(name, item):
            if id(item) not in seen:
                seen.add(id(item))
                queue.append((name, item))

        while queue:
            name, item = queue.popleft()

            if isinstance(item, type_):
                yield name, item

            if isinstance(item, BaseModule):
                if skip_compiled and getattr(item, "_compiled", False):
                    continue
                for sub_name, sub_item in item.__dict__.items():
                    add_to_queue(f"{name}.{sub_name}", sub_item)

            elif isinstance(item, (list, tuple)):
                for i, sub_item in enumerate(item):
                    add_to_queue(f"{name}[{i}]", sub_item)

            elif isinstance(item, dict):
                for key, sub_item in item.items():
                    add_to_queue(f"{name}[{key}]", sub_item)

    def parameters(self):
        return [param for _, param in self.named_parameters()]

    def deepcopy(self):
        """Deep copy the module.

        This is a tweak to the default python deepcopy that only deep copies `self.parameters()`, and for other
        attributes, we just do the shallow copy.
        """
        try:
            # If the instance itself is copyable, we can just deep copy it.
            # Otherwise we will have to create a new instance and copy over the attributes one by one.
            return copy.deepcopy(self)
        except Exception:
            pass

        # Create an empty instance.
        new_instance = self.__class__.__new__(self.__class__)
        # Set attribuetes of the copied instance.
        for attr, value in self.__dict__.items():
            if isinstance(value, BaseModule):
                setattr(new_instance, attr, value.deepcopy())
            else:
                try:
                    # Try to deep copy the attribute
                    setattr(new_instance, attr, copy.deepcopy(value))
                except Exception:
                    logging.warning(
                        f"Failed to deep copy attribute '{attr}' of {self.__class__.__name__}, "
                        "falling back to shallow copy or reference copy."
                    )
                    try:
                        # Fallback to shallow copy if deep copy fails
                        setattr(new_instance, attr, copy.copy(value))
                    except Exception:
                        # If even the shallow copy fails, we just copy over the reference.
                        setattr(new_instance, attr, value)

        return new_instance

    def reset_copy(self):
        """Deep copy the module and reset all parameters."""
        new_instance = self.deepcopy()

        for param in new_instance.parameters():
            param.reset()

        return new_instance

    def dump_state(self, json_mode=True):
        return {name: param.dump_state(json_mode=json_mode) for name, param in self.named_parameters()}

    def load_state(self, state):
        for name, param in self.named_parameters():
            param.load_state(state[name])

    def save(self, path, save_program=False, modules_to_serialize=None):
        """Save the module.

        Save the module to a directory or a file. There are two modes:
        - `save_program=False`: Save only the state of the module to a json or pickle file, based on the value of
            the file extension.
        - `save_program=True`: Save the whole module to a directory via cloudpickle, which contains both the state and
            architecture of the model.

        If `save_program=True` and `modules_to_serialize` are provided, it will register those modules for serialization
        with cloudpickle's `register_pickle_by_value`. This causes cloudpickle to serialize the module by value rather
        than by reference, ensuring the module is fully preserved along with the saved program. This is useful
        when you have custom modules that need to be serialized alongside your program. If None, then no modules
        will be registered for serialization.

        We also save the dependency versions, so that the loaded model can check if there is a version mismatch on
        critical dependencies or DSPy version.

        Args:
            path (str): Path to the saved state file, which should be a .json or .pkl file when `save_program=False`,
                and a directory when `save_program=True`.
            save_program (bool): If True, save the whole module to a directory via cloudpickle, otherwise only save
                the state.
            modules_to_serialize (list): A list of modules to serialize with cloudpickle's `register_pickle_by_value`.
                If None, then no modules will be registered for serialization.

        """
        metadata = {}
        metadata["dependency_versions"] = get_dependency_versions()
        path = Path(path)

        if save_program:
            if path.suffix:
                raise ValueError(
                    f"`path` must point to a directory without a suffix when `save_program=True`, but received: {path}"
                )
            if path.exists() and not path.is_dir():
                raise NotADirectoryError(f"The path '{path}' exists but is not a directory.")

            if not path.exists():
                # Create the directory (and any parent directories)
                path.mkdir(parents=True)

            try:
                modules_to_serialize = modules_to_serialize or []
                for module in modules_to_serialize:
                    cloudpickle.register_pickle_by_value(module)

                with open(path / "program.pkl", "wb") as f:
                    cloudpickle.dump(self, f)
            except Exception as e:
                raise RuntimeError(
                    f"Saving failed with error: {e}. Please remove the non-picklable attributes from your DSPy program, "
                    "or consider using state-only saving by setting `save_program=False`."
                )
            with open(path / "metadata.json", "wb") as f:
                f.write(orjson.dumps(metadata, option=orjson.OPT_INDENT_2 | orjson.OPT_APPEND_NEWLINE))

            return

        if path.suffix == ".json":
            state = self.dump_state()
            state["metadata"] = metadata
            try:
                with open(path, "wb") as f:
                    f.write(orjson.dumps(state, option=orjson.OPT_INDENT_2 | orjson.OPT_APPEND_NEWLINE))
            except Exception as e:
                raise RuntimeError(
                    f"Failed to save state to {path} with error: {e}. Your DSPy program may contain non "
                    "json-serializable objects, please consider saving the state in .pkl by using `path` ending "
                    "with `.pkl`, or saving the whole program by setting `save_program=True`."
                )
        elif path.suffix == ".pkl":
            state = self.dump_state(json_mode=False)
            state["metadata"] = metadata
            with open(path, "wb") as f:
                cloudpickle.dump(state, f)
        else:
            raise ValueError(f"`path` must end with `.json` or `.pkl` when `save_program=False`, but received: {path}")

    def load(self, path):
        """Load the saved module. You may also want to check out dspy.load, if you want to
        load an entire program, not just the state for an existing program.

        Args:
            path (str): Path to the saved state file, which should be a .json or a .pkl file
        """
        path = Path(path)

        if path.suffix == ".json":
            with open(path, "rb") as f:
                state = orjson.loads(f.read())
        elif path.suffix == ".pkl":
            with open(path, "rb") as f:
                state = cloudpickle.load(f)
        else:
            raise ValueError(f"`path` must end with `.json` or `.pkl`, but received: {path}")

        dependency_versions = get_dependency_versions()
        saved_dependency_versions = state["metadata"]["dependency_versions"]
        for key, saved_version in saved_dependency_versions.items():
            if dependency_versions[key] != saved_version:
                logger.warning(
                    f"There is a mismatch of {key} version between saved model and current environment. "
                    f"You saved with `{key}=={saved_version}`, but now you have "
                    f"`{key}=={dependency_versions[key]}`. This might cause errors or performance downgrade "
                    "on the loaded model, please consider loading the model in the same environment as the "
                    "saving environment."
                )
        self.load_state(state)


--- dspy/primitives/module.py ---
import inspect
import logging
from typing import Any

import magicattr

from dspy.dsp.utils.settings import settings, thread_local_overrides
from dspy.predict.parallel import Parallel
from dspy.primitives.base_module import BaseModule
from dspy.primitives.example import Example
from dspy.primitives.prediction import Prediction
from dspy.utils.callback import with_callbacks
from dspy.utils.inspect_history import pretty_print_history
from dspy.utils.usage_tracker import track_usage

logger = logging.getLogger(__name__)


class ProgramMeta(type):
    """Metaclass ensuring every ``dspy.Module`` instance is properly initialised."""

    def __call__(cls, *args, **kwargs):
        # Create the instance without invoking ``__init__`` so we can inject
        # the base initialization beforehand.
        obj = cls.__new__(cls, *args, **kwargs)
        if isinstance(obj, cls):
            # ``_base_init`` sets attributes that should exist on all modules
            # even when a subclass forgets to call ``super().__init__``.
            Module._base_init(obj)
            cls.__init__(obj, *args, **kwargs)

            # Guarantee existence of critical attributes if ``__init__`` didn't
            # create them.
            if not hasattr(obj, "callbacks"):
                obj.callbacks = []
            if not hasattr(obj, "history"):
                obj.history = []
        return obj


class Module(BaseModule, metaclass=ProgramMeta):
    def _base_init(self):
        self._compiled = False
        self.callbacks = []
        self.history = []

    def __init__(self, callbacks=None):
        self.callbacks = callbacks or []
        self._compiled = False
        # LM calling history of the module.
        self.history = []

    def __getstate__(self):
        state = self.__dict__.copy()
        state.pop("history", None)
        state.pop("callbacks", None)
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        if not hasattr(self, "history"):
            self.history = []
        if not hasattr(self, "callbacks"):
            self.callbacks = []

    @with_callbacks
    def __call__(self, *args, **kwargs) -> Prediction:
        caller_modules = settings.caller_modules or []
        caller_modules = list(caller_modules)
        caller_modules.append(self)

        with settings.context(caller_modules=caller_modules):
            if settings.track_usage and thread_local_overrides.get().get("usage_tracker") is None:
                with track_usage() as usage_tracker:
                    output = self.forward(*args, **kwargs)
                tokens = usage_tracker.get_total_tokens()
                self._set_lm_usage(tokens, output)

                return output

            return self.forward(*args, **kwargs)

    @with_callbacks
    async def acall(self, *args, **kwargs) -> Prediction:
        caller_modules = settings.caller_modules or []
        caller_modules = list(caller_modules)
        caller_modules.append(self)

        with settings.context(caller_modules=caller_modules):
            if settings.track_usage and thread_local_overrides.get().get("usage_tracker") is None:
                with track_usage() as usage_tracker:
                    output = await self.aforward(*args, **kwargs)
                    tokens = usage_tracker.get_total_tokens()
                    self._set_lm_usage(tokens, output)

                    return output

            return await self.aforward(*args, **kwargs)

    def named_predictors(self):
        from dspy.predict.predict import Predict

        return [(name, param) for name, param in self.named_parameters() if isinstance(param, Predict)]

    def predictors(self):
        return [param for _, param in self.named_predictors()]

    def set_lm(self, lm):
        for _, param in self.named_predictors():
            param.lm = lm

    def get_lm(self):
        all_used_lms = [param.lm for _, param in self.named_predictors()]

        if len(set(all_used_lms)) == 1:
            return all_used_lms[0]

        raise ValueError("Multiple LMs are being used in the module. There's no unique LM to return.")

    def __repr__(self):
        s = []

        for name, param in self.named_predictors():
            s.append(f"{name} = {param}")

        return "\n".join(s)

    def map_named_predictors(self, func):
        """Applies a function to all named predictors."""
        for name, predictor in self.named_predictors():
            set_attribute_by_name(self, name, func(predictor))
        return self

    def inspect_history(self, n: int = 1):
        return pretty_print_history(self.history, n)

    def batch(
        self,
        examples: list[Example],
        num_threads: int | None = None,
        max_errors: int | None = None,
        return_failed_examples: bool = False,
        provide_traceback: bool | None = None,
        disable_progress_bar: bool = False,
    ) -> list[Example] | tuple[list[Example], list[Example], list[Exception]]:
        """
        Processes a list of dspy.Example instances in parallel using the Parallel module.

        Args:
            examples: List of dspy.Example instances to process.
            num_threads: Number of threads to use for parallel processing.
            max_errors: Maximum number of errors allowed before stopping execution.
                If ``None``, inherits from ``dspy.settings.max_errors``.
            return_failed_examples: Whether to return failed examples and exceptions.
            provide_traceback: Whether to include traceback information in error logs.
            disable_progress_bar: Whether to display the progress bar.

        Returns:
            List of results, and optionally failed examples and exceptions.
        """
        # Create a list of execution pairs (self, example)
        exec_pairs = [(self, example.inputs()) for example in examples]

        # Create an instance of Parallel
        parallel_executor = Parallel(
            num_threads=num_threads,
            max_errors=max_errors,
            return_failed_examples=return_failed_examples,
            provide_traceback=provide_traceback,
            disable_progress_bar=disable_progress_bar,
        )

        # Execute the forward method of Parallel
        if return_failed_examples:
            results, failed_examples, exceptions = parallel_executor.forward(exec_pairs)
            return results, failed_examples, exceptions
        else:
            results = parallel_executor.forward(exec_pairs)
            return results

    def _set_lm_usage(self, tokens: dict[str, Any], output: Any):
        # Some optimizers (e.g., GEPA bootstrap tracing) temporarily patch
        # module.forward to return a tuple: (prediction, trace).
        # When usage tracking is enabled, ensure we attach usage to the
        # prediction object if present.
        prediction_in_output = None
        if isinstance(output, Prediction):
            prediction_in_output = output
        elif isinstance(output, tuple) and len(output) > 0 and isinstance(output[0], Prediction):
            prediction_in_output = output[0]
        if prediction_in_output:
            prediction_in_output.set_lm_usage(tokens)
        else:
            logger.warning("Failed to set LM usage. Please return `dspy.Prediction` object from dspy.Module to enable usage tracking.")


    def __getattribute__(self, name):
        attr = super().__getattribute__(name)

        if name == "forward" and callable(attr):
            # Check if forward is called through __call__ or directly
            stack = inspect.stack()
            forward_called_directly = len(stack) <= 1 or stack[1].function != "__call__"

            if forward_called_directly:
                logger.warning(
                    f"Calling module.forward(...) on {self.__class__.__name__} directly is discouraged. "
                    f"Please use module(...) instead."
                )

        return attr


def set_attribute_by_name(obj, name, value):
    magicattr.set(obj, name, value)


--- tests/primitives/test_base_module.py ---
import asyncio
import logging
import os
import threading
from unittest.mock import patch

import pytest
from litellm import Choices, Message, ModelResponse
from litellm.types.utils import Usage

import dspy
from dspy.primitives.prediction import Prediction
from dspy.utils.dummies import DummyLM


def test_deepcopy_basic():
    signature = dspy.Signature("q -> a")
    cot = dspy.ChainOfThought(signature)
    cot_copy = cot.deepcopy()
    assert len(cot.parameters()) == len(cot_copy.parameters())
    # Parameters should be different objects with the same values.
    assert id(cot.parameters()[0]) != id(cot_copy.parameters()[0])
    assert cot.parameters()[0].__dict__ == cot_copy.parameters()[0].__dict__


def test_deepcopy_with_uncopyable_modules():
    class CustomClass(dspy.Module):
        def __init__(self):
            self.lock = threading.Lock()  # Non-copyable object.
            self.cot = dspy.ChainOfThought(dspy.Signature("q -> a"))

    model = CustomClass()
    model_copy = model.deepcopy()
    assert len(model.parameters()) == len(model_copy.parameters())
    # The lock should be refer to the same object (shallow copy).
    assert id(model.lock) == id(model_copy.lock)
    # Parameters should be different objects with the same values.
    assert id(model.parameters()[0]) != id(model_copy.parameters()[0])
    assert model.parameters()[0].__dict__ == model_copy.parameters()[0].__dict__


def test_deepcopy_with_nested_modules():
    class CustomClass1(dspy.Module):
        def __init__(self):
            self.lock = threading.Lock()  # Non-copyable object.
            self.cot = dspy.ChainOfThought(dspy.Signature("q -> a"))

    class CustomClass2(dspy.Module):
        def __init__(self):
            self.submodel = CustomClass1()

    model = CustomClass2()
    model_copy = model.deepcopy()
    assert len(model.parameters()) == len(model_copy.parameters())
    # The lock should be refer to the same object (shallow copy).
    assert id(model.submodel.lock) == id(model_copy.submodel.lock)
    # Parameters should be different objects with the same values.
    assert id(model.parameters()[0]) != id(model_copy.parameters()[0])
    assert model.parameters()[0].__dict__ == model_copy.parameters()[0].__dict__


def test_save_and_load_with_json(tmp_path):
    model = dspy.ChainOfThought(dspy.Signature("q -> a"))
    model.predict.signature = model.predict.signature.with_instructions("You are a helpful assistant.")
    model.predict.demos = [
        dspy.Example(q="What is the capital of France?", a="Paris", reasoning="n/a").with_inputs("q"),
        # Nested example
        dspy.Example(
            q=[
                dspy.Example(q="What is the capital of France?"),
                dspy.Example(q="What is actually the capital of France?"),
            ],
            a="Paris",
            reasoning="n/a",
        ).with_inputs("q"),
    ]
    save_path = tmp_path / "model.json"
    model.save(save_path)
    new_model = dspy.ChainOfThought(dspy.Signature("q -> a"))
    new_model.load(save_path)

    assert str(new_model.predict.signature) == str(model.predict.signature)
    assert new_model.predict.demos[0] == model.predict.demos[0].toDict()
    assert new_model.predict.demos[1] == model.predict.demos[1].toDict()


@pytest.mark.extra
def test_save_and_load_with_pkl(tmp_path):
    import datetime

    # `datetime.date` is not json serializable, so we need to save with pickle.
    class MySignature(dspy.Signature):
        """Just a custom signature."""

        current_date: datetime.date = dspy.InputField()
        target_date: datetime.date = dspy.InputField()
        date_diff: int = dspy.OutputField(desc="The difference in days between the current_date and the target_date")

    trainset = [
        {"current_date": datetime.date(2024, 1, 1), "target_date": datetime.date(2024, 1, 2), "date_diff": 1},
        {"current_date": datetime.date(2024, 1, 1), "target_date": datetime.date(2024, 1, 3), "date_diff": 2},
        {"current_date": datetime.date(2024, 1, 1), "target_date": datetime.date(2024, 1, 4), "date_diff": 3},
        {"current_date": datetime.date(2024, 1, 1), "target_date": datetime.date(2024, 1, 5), "date_diff": 4},
        {"current_date": datetime.date(2024, 1, 1), "target_date": datetime.date(2024, 1, 6), "date_diff": 5},
    ]
    trainset = [dspy.Example(**example).with_inputs("current_date", "target_date") for example in trainset]

    dspy.settings.configure(
        lm=DummyLM([{"date_diff": "1", "reasoning": "n/a"}, {"date_diff": "2", "reasoning": "n/a"}] * 10)
    )

    cot = dspy.ChainOfThought(MySignature)
    cot(current_date=datetime.date(2024, 1, 1), target_date=datetime.date(2024, 1, 2))

    def dummy_metric(example, pred, trace=None):
        return True

    optimizer = dspy.BootstrapFewShot(max_bootstrapped_demos=4, max_labeled_demos=4, max_rounds=5, metric=dummy_metric)
    compiled_cot = optimizer.compile(cot, trainset=trainset)
    compiled_cot.predict.signature = compiled_cot.predict.signature.with_instructions("You are a helpful assistant.")

    save_path = tmp_path / "program.pkl"
    compiled_cot.save(save_path)

    new_cot = dspy.ChainOfThought(MySignature)
    new_cot.load(save_path)

    assert str(new_cot.predict.signature) == str(compiled_cot.predict.signature)
    assert new_cot.predict.demos == compiled_cot.predict.demos


def test_save_with_extra_modules(tmp_path):
    import sys

    # Create a temporary Python file with our custom module
    custom_module_path = tmp_path / "custom_module.py"
    with open(custom_module_path, "w") as f:
        f.write("""
import dspy

class MyModule(dspy.Module):
    def __init__(self):
        self.cot = dspy.ChainOfThought(dspy.Signature("q -> a"))

    def forward(self, q):
        return self.cot(q=q)
""")

    # Add the tmp_path to Python path so we can import the module
    sys.path.insert(0, str(tmp_path))
    try:
        import custom_module

        cot = custom_module.MyModule()

        cot.save(tmp_path, save_program=True)
        # Remove the custom module from sys.modules to simulate it not being available
        sys.modules.pop("custom_module", None)
        # Also remove it from sys.path
        sys.path.remove(str(tmp_path))
        del custom_module

        # Test the loading fails without using `modules_to_serialize`
        with pytest.raises(ModuleNotFoundError):
            dspy.load(tmp_path)

        sys.path.insert(0, str(tmp_path))
        import custom_module

        cot.save(
            tmp_path,
            modules_to_serialize=[custom_module],
            save_program=True,
        )

        # Remove the custom module from sys.modules to simulate it not being available
        sys.modules.pop("custom_module", None)
        # Also remove it from sys.path
        sys.path.remove(str(tmp_path))
        del custom_module

        loaded_module = dspy.load(tmp_path)
        assert loaded_module.cot.predict.signature == cot.cot.predict.signature

    finally:
        # Only need to clean up sys.path
        if str(tmp_path) in sys.path:
            sys.path.remove(str(tmp_path))


def test_load_with_version_mismatch(tmp_path):
    from dspy.primitives.base_module import logger

    # Mock versions during save
    save_versions = {"python": "3.9", "dspy": "2.4.0", "cloudpickle": "2.0"}

    # Mock versions during load
    load_versions = {"python": "3.10", "dspy": "2.5.0", "cloudpickle": "2.1"}

    predict = dspy.Predict("question->answer")

    # Create a custom handler to capture log messages
    class ListHandler(logging.Handler):
        def __init__(self):
            super().__init__()
            self.messages = []

        def emit(self, record):
            self.messages.append(record.getMessage())

    # Add handler and set level
    handler = ListHandler()
    original_level = logger.level
    logger.addHandler(handler)
    logger.setLevel(logging.WARNING)

    try:
        save_path = tmp_path / "program.pkl"
        # Mock version during save
        with patch("dspy.primitives.base_module.get_dependency_versions", return_value=save_versions):
            predict.save(save_path)

        # Mock version during load
        with patch("dspy.primitives.base_module.get_dependency_versions", return_value=load_versions):
            loaded_predict = dspy.Predict("question->answer")
            loaded_predict.load(save_path)

        # Assert warnings were logged, and one warning for each mismatched dependency.
        assert len(handler.messages) == 3

        for msg in handler.messages:
            assert "There is a mismatch of" in msg

        # Verify the model still loads correctly despite version mismatches
        assert isinstance(loaded_predict, dspy.Predict)
        assert str(predict.signature) == str(loaded_predict.signature)

    finally:
        # Clean up: restore original level and remove handler
        logger.setLevel(original_level)
        logger.removeHandler(handler)


@pytest.mark.llm_call
def test_single_module_call_with_usage_tracker(lm_for_test):
    dspy.settings.configure(lm=dspy.LM(lm_for_test, cache=False), track_usage=True)

    predict = dspy.ChainOfThought("question -> answer")
    output = predict(question="What is the capital of France?")

    lm_usage = output.get_lm_usage()
    assert len(lm_usage) == 1
    assert lm_usage[lm_for_test]["prompt_tokens"] > 0
    assert lm_usage[lm_for_test]["completion_tokens"] > 0
    assert lm_usage[lm_for_test]["total_tokens"] > 0

    # Test no usage being tracked when cache is enabled
    dspy.settings.configure(lm=dspy.LM(lm_for_test, cache=True), track_usage=True)
    for _ in range(2):
        output = predict(question="What is the capital of France?")

    assert len(output.get_lm_usage()) == 0


@pytest.mark.llm_call
def test_multi_module_call_with_usage_tracker(lm_for_test):
    dspy.settings.configure(lm=dspy.LM(lm_for_test, cache=False), track_usage=True)

    class MyProgram(dspy.Module):
        def __init__(self):
            self.predict1 = dspy.ChainOfThought("question -> answer")
            self.predict2 = dspy.ChainOfThought("question, answer -> score")

        def __call__(self, question: str) -> Prediction:
            answer = self.predict1(question=question)
            score = self.predict2(question=question, answer=answer)
            return score

    program = MyProgram()
    output = program(question="What is the capital of France?")

    lm_usage = output.get_lm_usage()
    assert len(lm_usage) == 1
    assert lm_usage[lm_for_test]["prompt_tokens"] > 0
    assert lm_usage[lm_for_test]["prompt_tokens"] > 0
    assert lm_usage[lm_for_test]["completion_tokens"] > 0
    assert lm_usage[lm_for_test]["total_tokens"] > 0


# TODO: prepare second model for testing this unit test in ci
@pytest.mark.skipif(not os.getenv("OPENAI_API_KEY"), reason="Skip the test if OPENAI_API_KEY is not set.")
def test_usage_tracker_in_parallel():
    class MyProgram(dspy.Module):
        def __init__(self, lm):
            self.lm = lm
            self.predict1 = dspy.ChainOfThought("question -> answer")
            self.predict2 = dspy.ChainOfThought("question, answer -> score")

        def __call__(self, question: str) -> Prediction:
            with dspy.settings.context(lm=self.lm):
                answer = self.predict1(question=question)
                score = self.predict2(question=question, answer=answer)
                return score

    dspy.settings.configure(track_usage=True)
    program1 = MyProgram(lm=dspy.LM("openai/gpt-4o-mini", cache=False))
    program2 = MyProgram(lm=dspy.LM("openai/gpt-3.5-turbo", cache=False))

    parallelizer = dspy.Parallel()

    results = parallelizer(
        [
            (program1, {"question": "What is the meaning of life?"}),
            (program2, {"question": "why did a chicken cross the kitchen?"}),
        ]
    )

    assert results[0].get_lm_usage() is not None
    assert results[1].get_lm_usage() is not None

    assert results[0].get_lm_usage().keys() == set(["openai/gpt-4o-mini"])
    assert results[1].get_lm_usage().keys() == set(["openai/gpt-3.5-turbo"])


@pytest.mark.asyncio
async def test_usage_tracker_async_parallel():
    program = dspy.Predict("question -> answer")

    with patch("litellm.acompletion") as mock_completion:
        mock_completion.return_value = ModelResponse(
            choices=[Choices(message=Message(content="{'answer': 'Paris'}"))],
            usage=Usage(
                **{
                    "prompt_tokens": 1117,
                    "completion_tokens": 46,
                    "total_tokens": 1163,
                    "prompt_tokens_details": {"cached_tokens": 0, "audio_tokens": 0},
                    "completion_tokens_details": {
                        "reasoning_tokens": 0,
                        "audio_tokens": 0,
                        "accepted_prediction_tokens": 0,
                        "rejected_prediction_tokens": 0,
                    },
                },
            ),
            model="openai/gpt-4o-mini",
        )

        coroutines = [
            program.acall(question="What is the capital of France?"),
            program.acall(question="What is the capital of France?"),
            program.acall(question="What is the capital of France?"),
            program.acall(question="What is the capital of France?"),
        ]
        with dspy.settings.context(
            lm=dspy.LM("openai/gpt-4o-mini", cache=False), track_usage=True, adapter=dspy.JSONAdapter()
        ):
            results = await asyncio.gather(*coroutines)

        assert results[0].get_lm_usage() is not None
        assert results[1].get_lm_usage() is not None

        lm_usage0 = results[0].get_lm_usage()["openai/gpt-4o-mini"]
        lm_usage1 = results[1].get_lm_usage()["openai/gpt-4o-mini"]
        assert lm_usage0["prompt_tokens"] == 1117
        assert lm_usage1["prompt_tokens"] == 1117
        assert lm_usage0["completion_tokens"] == 46
        assert lm_usage1["completion_tokens"] == 46
        assert lm_usage0["total_tokens"] == 1163
        assert lm_usage1["total_tokens"] == 1163


def test_usage_tracker_no_side_effect():
    class MyProgram(dspy.Module):
        def __init__(self):
            self.predict = dspy.Predict("question -> answer")

        def forward(self, question: str, **kwargs) -> str:
            return self.predict(question=question).answer

    program = MyProgram()
    with dspy.context(lm=DummyLM([{"answer": "Paris"}]), track_usage=True):
        result = program(question="What is the capital of France?")
    assert result == "Paris"


def test_module_history():
    class MyProgram(dspy.Module):
        def __init__(self, **kwargs):
            super().__init__(**kwargs)
            self.cot = dspy.ChainOfThought("question -> answer")

        def forward(self, question: str, **kwargs) -> Prediction:
            return self.cot(question=question)

    with patch("litellm.completion") as mock_completion:
        mock_completion.return_value = ModelResponse(
            choices=[
                Choices(message=Message(content="{'reasoning': 'Paris is the capital of France', 'answer': 'Paris'}"))
            ],
            model="openai/gpt-4o-mini",
        )
        dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.JSONAdapter())
        program = MyProgram()
        program(question="What is the capital of France?")

        # Second call only call the submodule.
        program.cot(question="What is the capital of France?")

        # The LM history entity exists in all the ancestor callers.
        assert len(program.history) == 1
        assert len(program.cot.history) == 2
        assert len(program.cot.predict.history) == 2

        # The same history entity is shared across all the ancestor callers to reduce memory usage.
        assert id(program.history[0]) == id(program.cot.history[0])

        assert program.history[0]["outputs"] == ["{'reasoning': 'Paris is the capital of France', 'answer': 'Paris'}"]

        dspy.settings.configure(disable_history=True)

        program(question="What is the capital of France?")
        # No history is recorded when history is disabled.
        assert len(program.history) == 1
        assert len(program.cot.history) == 2
        assert len(program.cot.predict.history) == 2

        dspy.settings.configure(disable_history=False)

        program(question="What is the capital of France?")
        # History is recorded again when history is enabled.
        assert len(program.history) == 2
        assert len(program.cot.history) == 3
        assert len(program.cot.predict.history) == 3


def test_module_history_with_concurrency():
    class MyProgram(dspy.Module):
        def __init__(self):
            super().__init__()
            self.cot = dspy.ChainOfThought("question -> answer")

        def forward(self, question: str, **kwargs) -> Prediction:
            return self.cot(question=question)

    with patch("litellm.completion") as mock_completion:
        mock_completion.return_value = ModelResponse(
            choices=[Choices(message=Message(content="{'reasoning': 'N/A', 'answer': 'Holy crab!'}"))],
            model="openai/gpt-4o-mini",
        )
        dspy.settings.configure(lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.JSONAdapter())
        program = MyProgram()

        parallelizer = dspy.Parallel()

        parallelizer(
            [
                (program, {"question": "What is the meaning of life?"}),
                (program, {"question": "why did a chicken cross the kitchen?"}),
            ]
        )
        assert len(program.history) == 2
        assert len(program.cot.history) == 2
        assert len(program.cot.predict.history) == 2


@pytest.mark.asyncio
async def test_module_history_async():
    class MyProgram(dspy.Module):
        def __init__(self, **kwargs):
            super().__init__(**kwargs)
            self.cot = dspy.ChainOfThought("question -> answer")

        async def aforward(self, question: str, **kwargs) -> Prediction:
            return await self.cot.acall(question=question)

    with patch("litellm.acompletion") as mock_completion:
        mock_completion.return_value = ModelResponse(
            choices=[
                Choices(message=Message(content="{'reasoning': 'Paris is the capital of France', 'answer': 'Paris'}"))
            ],
            model="openai/gpt-4o-mini",
        )
        program = MyProgram()
        with dspy.context(lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.JSONAdapter()):
            await program.acall(question="What is the capital of France?")

            # Second call only call the submodule.
            await program.cot.acall(question="What is the capital of France?")

        # The LM history entity exists in all the ancestor callers.
        assert len(program.history) == 1
        assert len(program.cot.history) == 2
        assert len(program.cot.predict.history) == 2

        # The same history entity is shared across all the ancestor callers to reduce memory usage.
        assert id(program.history[0]) == id(program.cot.history[0])

        assert program.history[0]["outputs"] == ["{'reasoning': 'Paris is the capital of France', 'answer': 'Paris'}"]

        with dspy.context(
            disable_history=True, lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.JSONAdapter()
        ):
            await program.acall(question="What is the capital of France?")

        # No history is recorded when history is disabled.
        assert len(program.history) == 1
        assert len(program.cot.history) == 2
        assert len(program.cot.predict.history) == 2

        with dspy.context(
            disable_history=False, lm=dspy.LM("openai/gpt-4o-mini", cache=False), adapter=dspy.JSONAdapter()
        ):
            await program.acall(question="What is the capital of France?")
        # History is recorded again when history is enabled.
        assert len(program.history) == 2
        assert len(program.cot.history) == 3
        assert len(program.cot.predict.history) == 3


def test_forward_direct_call_warning(capsys):
    class TestModule(dspy.Module):
        def forward(self, x):
            return x

    module = TestModule()
    module.forward("test")
    captured = capsys.readouterr()
    assert "directly is discouraged" in captured.err


def test_forward_through_call_no_warning(capsys):
    class TestModule(dspy.Module):
        def forward(self, x):
            return x

    module = TestModule()
    module(x="test")
    captured = capsys.readouterr()
    assert "directly is discouraged" not in captured.err


--- tests/primitives/test_module.py ---
from pathlib import Path

import dspy
from dspy.primitives.module import Module, set_attribute_by_name  # Adjust the import based on your file structure
from dspy.utils import DummyLM


class HopModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.predict1 = dspy.Predict("question -> query")
        self.predict2 = dspy.Predict("query -> answer")

    def forward(self, question):
        query = self.predict1(question=question).query
        return self.predict2(query=query)


def test_module_initialization():
    module = Module()
    assert module._compiled is False, "Module _compiled attribute should be False upon initialization"


def test_named_predictors():
    module = HopModule()
    named_preds = module.named_predictors()
    assert len(named_preds) == 2, "Should identify correct number of Predict instances"
    names, preds = zip(*named_preds, strict=False)
    assert "predict1" in names and "predict2" in names, "Named predictors should include 'predict1' and 'predict2'"


def test_predictors():
    module = HopModule()
    preds = module.predictors()
    assert len(preds) == 2, "Should return correct number of Predict instances"
    assert all(isinstance(p, dspy.Predict) for p in preds), "All returned items should be instances of PredictMock"


def test_forward():
    program = HopModule()
    dspy.settings.configure(
        lm=DummyLM(
            {
                "What is 1+1?": {"query": "let me check"},
                "let me check": {"answer": "2"},
            }
        )
    )
    result = program(question="What is 1+1?").answer
    assert result == "2"


def test_nested_named_predictors():
    class Hop2Module(dspy.Module):
        def __init__(self):
            super().__init__()
            self.hop = HopModule()

    module = Hop2Module()
    named_preds = module.named_predictors()
    assert len(named_preds) == 2
    names, _preds = zip(*named_preds, strict=False)
    assert "hop.predict1" in names
    assert "hop.predict2" in names


def test_empty_module():
    module = Module()
    assert list(module.named_sub_modules()) == [("self", module)]


def test_single_level():
    module = Module()
    module.sub = Module()
    expected = [("self", module), ("self.sub", module.sub)]
    assert list(module.named_sub_modules()) == expected


def test_multiple_levels():
    module = Module()
    module.sub = Module()
    module.sub.subsub = Module()
    expected = [("self", module), ("self.sub", module.sub), ("self.sub.subsub", module.sub.subsub)]
    assert list(module.named_sub_modules()) == expected


def test_multiple_sub_modules():
    module = Module()
    module.sub1 = Module()
    module.sub2 = Module()
    expected = [("self", module), ("self.sub1", module.sub1), ("self.sub2", module.sub2)]
    assert sorted(module.named_sub_modules()) == sorted(expected)


def test_non_base_module_attributes():
    module = Module()
    module.sub = Module()
    module.not_a_sub = "Not a self"
    expected = [("self", module), ("self.sub", module.sub)]
    assert list(module.named_sub_modules()) == expected


def test_complex_module_traversal():
    root = Module()
    root.sub_module = Module()
    root.sub_module.nested_list = [Module(), {"key": Module()}]
    root.sub_module.nested_tuple = (Module(), [Module(), Module()])
    expected_names = {
        "self",
        "self.sub_module",
        "self.sub_module.nested_list[0]",
        "self.sub_module.nested_list[1][key]",
        "self.sub_module.nested_tuple[0]",
        "self.sub_module.nested_tuple[1][0]",
        "self.sub_module.nested_tuple[1][1]",
    }
    found_names = {name for name, _ in root.named_sub_modules()}

    assert found_names == expected_names, (
        f"Missing or extra modules found. Missing: {expected_names - found_names}, Extra: {found_names - expected_names}"
    )


def test_complex_module_traversal_with_same_module():
    root = Module()
    root.sub_module = Module()
    root.sub_module.nested_list = [Module(), {"key": Module()}]
    same_module = Module()
    root.sub_module.nested_tuple = (Module(), [same_module, same_module])
    expected_names = {
        "self",
        "self.sub_module",
        "self.sub_module.nested_list[0]",
        "self.sub_module.nested_list[1][key]",  # NOTE: named_sub_modules allows recursive structures
        "self.sub_module.nested_tuple[0]",
        "self.sub_module.nested_tuple[1][0]",  # NEW: named_sub_modules allows recursive structures, but named_parameters does not
    }
    found_names = {name for name, _ in root.named_sub_modules()}

    assert found_names == expected_names, (
        f"Missing or extra modules found. Missing: {expected_names - found_names}, Extra: {found_names - expected_names}"
    )


def test_complex_module_set_attribute_by_name():
    root = Module()
    root.sub_module = Module()
    root.sub_module.nested_list = [Module(), {"key": Module()}]
    same_module = Module()
    root.sub_module.nested_tuple = (Module(), [same_module, same_module])

    set_attribute_by_name(root, "test_attrib", True)
    assert root.test_attrib is True
    set_attribute_by_name(root, "sub_module.test_attrib", True)
    assert root.sub_module.test_attrib is True
    set_attribute_by_name(root, "sub_module.nested_list[0].test_attrib", True)
    assert root.sub_module.nested_list[0].test_attrib is True
    set_attribute_by_name(root, "sub_module.nested_list[1]['key'].test_attrib", True)
    assert root.sub_module.nested_list[1]["key"].test_attrib is True
    set_attribute_by_name(root, "sub_module.nested_tuple[0].test_attrib", True)
    assert root.sub_module.nested_tuple[0].test_attrib is True
    set_attribute_by_name(root, "sub_module.nested_tuple[1][0].test_attrib", True)
    assert root.sub_module.nested_tuple[1][0].test_attrib is True
    assert root.sub_module.nested_tuple[1][1].test_attrib is True


class DuplicateModule(Module):
    def __init__(self):
        super().__init__()
        self.p0 = dspy.Predict("question -> answer")
        self.p1 = self.p0


def test_named_parameters_duplicate_references():
    module = DuplicateModule()
    # Only testing for whether exceptions are thrown or not
    # As Module.named_parameters() is recursive, this is mainly for catching infinite recursion
    module.named_parameters()


def test_load_dspy_program_cross_version():
    """
    Test backward compatibility for loading a saved DSPy program.

    This test verifies that DSPy can load a program saved in version 3.0.1, ensuring compatibility with older versions.
    The saved state is located in 'test/primitives/resources/saved_program.json' and represents an optimized
    `dspy.ReAct` program.
    """
    path = Path(__file__).parent / "resources" / "saved_program.json"
    loaded_react = dspy.ReAct("question->answer", tools=[])
    loaded_react.load(path)
    assert (
        "Imagine you are a detective racing against time to solve a high-profile"
        in loaded_react.react.signature.instructions
    )
    assert "Given the very verbose fields `question`" in loaded_react.extract.predict.signature.instructions

    assert len(loaded_react.react.demos) == 2
    assert len(loaded_react.extract.predict.demos) == 2


--- CONTRIBUTING.md ---
# Contribution Guide

DSPy is an actively growing project and community! We welcome your contributions and involvement. Below are instructions for how to contribute to DSPy.

## Finding an Issue

The fastest way to contribute is to find open issues that need an assignee. We maintain two lists of GitHub tags for contributors:

- [good first issue](https://github.com/stanfordnlp/dspy/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22):
  a list of small, well-defined issues for newcomers to the project.
- [help wanted](https://github.com/stanfordnlp/dspy/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22help%20wanted%22):
  a list of issues that welcome community contributions. These issues have a wide range of complexity.

We also welcome new ideas! If you would like to propose a new feature, please open a feature request to
discuss. If you already have a design in mind, please include a notebook/code example to demonstrate
your idea. Keep in mind that designing a new feature or use case may take longer than contributing to
an open issue.

## Contributing Code

Follow these steps to submit your code contribution.

### Step 1. Open an Issue

Before making any changes, we recommend opening an issue (if one doesn't already exist) and discussing your
proposed changes. This way, we can give you feedback and validate the proposed changes.

If your code change involves fixing a bug, please include a code snippet or notebook
to show how to reproduce the broken behavior.

For minor changes (simple bug fixes or documentation fixes), feel free to open a PR without discussion.

### Step 2. Make Code Changes

To make code changes, fork the repository and set up your local development environment following the
instructions in the "Environment Setup" section below.

### Step 3 Commit Your Code and Run Autoformatting

We follow the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html) and use `ruff` for both linting and formatting. To ensure consistent code quality, we use pre-commit hooks that automatically check and fix common issues.


First you need to set up the pre-commit hooks (do this once after cloning the repository):

```shell
pre-commit install
```

Then stage and commit your changes. When you run `git commit`, the pre-commit hook will be
automatically run.

```shell
git add .
git commit -m "your commit message"
```

If the hooks make any changes, you'll need to stage and commit those changes as well.

You can also run the hooks manually:

- Check staged files only:

  ```shell
  pre-commit run
  ```

- Check specific files:

  ```shell
  pre-commit run --files path/to/file1.py path/to/file2.py
  ```

Please ensure all pre-commit checks pass before creating your pull request. If you're unsure about any
formatting issues, feel free to commit your changes and let the pre-commit hooks fix them automatically.

### Step 4. Create a Pull Request

Once your changes are ready, open a pull request from your branch in your fork to the main branch in the
[DSPy repo](https://github.com/stanfordnlp/dspy).

### Step 5. Code Review

Once your PR is up and passes all CI tests, we will assign reviewers to review the code. There may be
several rounds of comments and code changes before the pull request gets approved by the reviewer.

### Step 6. Merging

Once the pull request is approved, a team member will take care of merging.

## Environment Setup

Python 3.10 or later is required.

Setting up your DSPy development environment requires you to fork the DSPy repository and clone it locally.
If you are not familiar with the GitHub fork process, please refer to [Fork a repository](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo). After creating the fork, clone
it to your local development device:

```shell
git clone {url-to-your-fork}
cd dspy
```

Next, we must set up a Python environment with the correct dependencies. There are two recommended ways to set up the
dev environment.

### [Recommended] Set Up Environment Using uv

[uv](https://github.com/astral-sh/uv) is a rust-based Python package and project manager that provides a fast
way to set up the development environment. First, install uv by following the
[installation guide](https://docs.astral.sh/uv/getting-started/installation/).

After uv is installed, in your working directory (`dspy/`), run:

```shell
uv sync --extra dev
```

Then you are all set!

To verify that your environment is set up successfully, run some unit tests:

```shell
uv run pytest tests/predict
```

Note: You need to use the `uv run` prefix for every Python command, as uv creates a Python virtual
environment and `uv run` points the command to that environment. For example, to execute a Python script you will need
`uv run python script.py`.

### Set Up Environment Using conda + pip

You can also set up the virtual environment via conda + pip, which takes a few extra steps but offers more flexibility. Before starting,
make sure you have conda installed. If not, please follow the instructions
[here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).

To set up the environment, run:

```shell
conda create -n dspy-dev python=3.11
conda activate dspy-dev
pip install -e ".[dev]"
```

Then verify the installation by running some unit tests:

```shell
pytest tests/predict
```



--- .github/.internal_dspyai/internals/build-and-release.md ---
# Build & Release Workflow Implementation

The [build_and_release](https://github.com/stanfordnlp/dspy/blob/main/.github/workflows/build_and_release.yml) workflow automates deployments of dspy-ai to pypi. For a guide to triggering a release using the workflow, refer to [release checklist](release-checklist.md).

## Overview

At a high level, the workflow works as follows: 

1. Maintainer of the repo pushes a tag following [semver](https://semver.org/) versioning for the new release.
2. This triggers the github action which extracts the tag (the version)
3. Builds and publishes a release on [test-pypi](https://test.pypi.org/project/dspy-ai-test/)
4. Uses the test-pypi release to run build_utils/tests/intro.py with the new release as an integration test. Note intro.py is a copy of the intro notebook.
5. Assuming the test runs successfully, it pushes a release to [pypi](https://pypi.org/project/dspy-ai/). If not, the user can delete the tag, make the fixes and then push the tag again. Versioning for multiple releases to test-pypi with the same tag version is taken care of by the workflow by appending a pre-release identifier, so the user only needs to consider the version for pypi. 
6. (Currently manual) the user creates a release and includes release notes, as described in docs/docs/release-checklist.md

## Implementation Details

The workflow executes a series of jobs in sequence: 
- extract-tag
- build-and-publish-test-pypi
- test-intro-script
- build-and-publish-pypi

#### extract-tag
Extracts the tag pushed to the commit. This tag is expected to be the version of the new deployment. 

#### build-and-publish-test-pypi
Builds and publishes the package to test-pypi.
1. Determines the version that should be deployed to test-pypi. There may be an existing deployment with the version specified by the tag in the case that a deployment failed and the maintainer made some changes and pushed the same tag again (which is the intended usage). The following logic is implemented [test_version.py](https://github.com/stanfordnlp/dspy/blob/main/build_utils/test_version.py)
    1. Load the releases on test-pypi
    1. Check if there is a release matching our current tag
        1. If not, create a release with the current tag
        1. If it exists, oad the latest published version (this will either be the version with the tag itself, or the tag + a pre-release version). In either case, increment the pre-release version.
1. Updates the version placeholder in [setup.py](https://github.com/stanfordnlp/dspy/blob/main/setup.py) to the version obtained in step 1.
1. Updates the version placeholder in [pyproject.toml](https://github.com/stanfordnlp/dspy/blob/main/pyproject.toml) to the version obtained in step 1.
1. Updates the package name placeholder in [setup.py](https://github.com/stanfordnlp/dspy/blob/main/setup.py) to  `dspy-ai-test`*
1. Updates the package name placeholder in [pyproject.toml](https://github.com/stanfordnlp/dspy/blob/main/pyproject.toml) to `dspy-ai-test`*
1. Builds the binary wheel
1. Publishes the package to test-pypi. 


#### test-intro-script
Runs the pytest containing the intro script as an integration test using the package published to test-pypi. This is a validation step before publishing to pypi.
1. Uses a loop to install the version just published to test-pypi as sometimes there is a race condition between the package becoming available for installation and this job executing.
2. Runs the test to ensure the package is working as expected. 
3. If this fails, the workflow fails and the maintainer needs to make a fix and delete and then recreate the tag.

#### build-and-publish-pypi
Builds and publishes the package to pypi.

1. Updates the version placeholder in [setup.py](https://github.com/stanfordnlp/dspy/blob/main/setup.py) to the version obtained in step 1.
1. Updates the version placeholder in [pyproject.toml](https://github.com/stanfordnlp/dspy/blob/main/pyproject.toml) to the version obtained in step 1.
1. Updates the package name placeholder in [setup.py](https://github.com/stanfordnlp/dspy/blob/main/setup.py) to  `dspy-ai`*
1. Updates the package name placeholder in [pyproject.toml](https://github.com/stanfordnlp/dspy/blob/main/pyproject.toml) to `dspy-ai`*
1. Builds the binary wheel
1. Publishes the package to pypi.


\* The package name is updated by the workflow to allow the same files to be used to build both the pypi and test-pypi packages.

--- .github/.internal_dspyai/internals/release-checklist.md ---
# Release Checklist

* [ ] On `main` Create a git tag with pattern X.Y.Z where X, Y, and Z follow the [semver pattern](https://semver.org/). Then push the tag to the origin git repo (github).
    * ```bash
      git tag X.Y.Z
      git push origin --tags
      ```
    * This will trigger the github action to build and release the package.
* [ ] Confirm the tests pass and the package has been published to pypi.
    * If the tests fail, you can remove the tag from your local and github repo using:
    ```bash
    git push origin --delete X.Y.Z # Delete on GitHub
    git tag -d X.Y.Z # Delete locally
    ```
    * Fix the errors and then repeat the steps above to recreate the tag locally and push to GitHub to restart the process.
    * Note that the github action takes care of incrementing the release version on test-pypi automatically by adding a pre-release identifier in the scenario where the tests fail and you need to delete and push the same tag again. 
* [ ] [Create a release](https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository) 
* [ ] Add release notes. You can make use of [automatically generated release notes](https://docs.github.com/en/repositories/releasing-projects-on-github/automatically-generated-release-notes)
* If creating a new release for major or minor version:
    * [ ] Create a new release branch with the last commit and name it 'release/X.Y`
    * [ ] [Update the default branch](https://docs.github.com/en/organizations/managing-organization-settings/managing-the-default-branch-name-for-repositories-in-your-organization) on the github rep to the new release branch.

### Prerequisites

The automation requires a [trusted publisher](https://docs.pypi.org/trusted-publishers/) to be set up on both the pypi and test-pypi packages. If the package is migrated to a new project, please follow the [steps](https://docs.pypi.org/trusted-publishers/adding-a-publisher/) to create a trusted publisher. If you have no releases on the new project, you may have to create a [pending trusted publisher](https://docs.pypi.org/trusted-publishers/creating-a-project-through-oidc/) to allow the first automated deployment. 

--- README.md ---
<p align="center">
  <img align="center" src="docs/docs/static/img/dspy_logo.png" width="460px" />
</p>
<p align="left">


## DSPy: _Programming_—not prompting—Foundation Models

**Documentation:** [DSPy Docs](https://dspy.ai/)

[![PyPI Downloads](https://static.pepy.tech/badge/dspy/month)](https://pepy.tech/projects/dspy)


----

DSPy is the framework for _programming—rather than prompting—language models_. It allows you to iterate fast on **building modular AI systems** and offers algorithms for **optimizing their prompts and weights**, whether you're building simple classifiers, sophisticated RAG pipelines, or Agent loops.

DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional _Python code_ and use DSPy to **teach your LM to deliver high-quality outputs**. Learn more via our [official documentation site](https://dspy.ai/) or meet the community, seek help, or start contributing via this GitHub repo and our [Discord server](https://discord.gg/XCGy2WDCQB).


## Documentation: [dspy.ai](https://dspy.ai)


**Please go to the [DSPy Docs at dspy.ai](https://dspy.ai)**


## Installation


```bash
pip install dspy
```

To install the very latest from `main`:

```bash
pip install git+https://github.com/stanfordnlp/dspy.git
````




## 📜 Citation & Reading More

If you're looking to understand the framework, please go to the [DSPy Docs at dspy.ai](https://dspy.ai).

If you're looking to understand the underlying research, this is a set of our papers:

**[Jul'25] [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457)**       
**[Jun'24] [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695)**       
**[Oct'23] [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)**     
[Jul'24] [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/abs/2407.10930)     
[Jun'24] [Prompts as Auto-Optimized Training Hyperparameters](https://arxiv.org/abs/2406.11706)    
[Feb'24] [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207)         
[Jan'24] [In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/abs/2401.12178)       
[Dec'23] [DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines](https://arxiv.org/abs/2312.13382)   
[Dec'22] [Demonstrate-Search-Predict: Composing Retrieval & Language Models for Knowledge-Intensive NLP](https://arxiv.org/abs/2212.14024.pdf)

To stay up to date or learn more, follow [@DSPyOSS](https://twitter.com/DSPyOSS) on Twitter or the DSPy page on LinkedIn.

The **DSPy** logo is designed by **Chuyi Zhang**.

If you use DSPy or DSP in a research paper, please cite our work as follows:

```
@inproceedings{khattab2024dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}
```

<!-- You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:

* [**DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines**](https://arxiv.org/abs/2312.13382)   (Academic Paper, Dec 2023) 
* [**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**](https://arxiv.org/abs/2310.03714) (Academic Paper, Oct 2023) 
* [**Releasing DSPy, the latest iteration of the framework**](https://twitter.com/lateinteraction/status/1694748401374490946) (Twitter Thread, Aug 2023)
* [**Releasing the DSP Compiler (v0.1)**](https://twitter.com/lateinteraction/status/1625231662849073160)  (Twitter Thread, Feb 2023)
* [**Introducing DSP**](https://twitter.com/lateinteraction/status/1617953413576425472)  (Twitter Thread, Jan 2023)
* [**Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP**](https://arxiv.org/abs/2212.14024.pdf) (Academic Paper, Dec 2022) -->



--- .github/PULL_REQUEST_TEMPLATE/pull_request_template.md ---
## 📝 Changes Description

This MR/PR contains the following changes:
...

## ✅ Contributor Checklist

- [] Pre-Commit checks are passing (locally and remotely)
- [] Title of your PR / MR corresponds to the required format
- [] Commit message follows required format {label}(dspy): {message}

## ⚠️ Warnings

Anything we should be aware of ?


--- .github/workflows/build_utils/test_version.py ---
import sys
from datetime import datetime

import requests
import semver
from packaging.version import Version as PyPIVersion


def get_latest_version(package_name, tag_version):  
    # Returns latest version, and T/F as to whether it needs to be incremented
    response = requests.get(f"https://test.pypi.org/pypi/{package_name}/json")  
    if response.status_code == 200:  
        data = response.json()  
        # Flatten the list of files for all releases and get the latest upload  
        all_uploads = [  
            (release['upload_time'], release['filename'], version)  
            for version, releases in data['releases'].items()  
            for release in releases  
        ] 
        # If a release with tag_version does not exist, that is the latest version
        # Then increment is False, as no need to increment the version
        tag_release_exists = any(upload for upload in all_uploads if upload[2] == tag_version)
        if not(tag_release_exists):
            return tag_version, False  
        # Else, get the latest release version, and set increment to True
        else:
            # Sort all uploads by upload time in descending order
            latest_upload = max(all_uploads, key=lambda x: datetime.fromisoformat(x[0].rstrip('Z')))  
            return latest_upload[2], True  
    
    elif response.status_code == 404:
        # If no existing releases can get a 404
        return tag_version, False
    return None, None  
    
def increment_version(curr_version):
    pypi_v = PyPIVersion(curr_version)
    if pypi_v.pre:
        pre = "".join([str(i) for i in pypi_v.pre])
        parsed_v = semver.Version(*pypi_v.release, pre)
    else:
        parsed_v = semver.Version(*pypi_v.release)
    new_v = str(parsed_v.bump_prerelease())
    return new_v
  
if __name__ == "__main__":  
    if len(sys.argv) != 3:  
        raise ValueError("Usage: python get_latest_testpypi_version.py <package_name> <tag_version>")  
      
    package_name = sys.argv[1]
    tag_v = sys.argv[2]

    latest_version, increment = get_latest_version(package_name, tag_v)  
    if increment:
        new_version = increment_version(latest_version)
    else: 
        new_version = latest_version

    # Output new version
    print(new_version)  


--- dspy/__init__.py ---
from dspy.predict import *
from dspy.primitives import *
from dspy.retrievers import *
from dspy.signatures import *
from dspy.teleprompt import *

from dspy.evaluate import Evaluate  # isort: skip
from dspy.clients import *  # isort: skip
from dspy.adapters import Adapter, ChatAdapter, JSONAdapter, XMLAdapter, TwoStepAdapter, Image, Audio, History, Type, Tool, ToolCalls, Code  # isort: skip
from dspy.utils.logging_utils import configure_dspy_loggers, disable_logging, enable_logging
from dspy.utils.asyncify import asyncify
from dspy.utils.syncify import syncify
from dspy.utils.saving import load
from dspy.streaming.streamify import streamify
from dspy.utils.usage_tracker import track_usage

from dspy.dsp.utils.settings import settings
from dspy.dsp.colbertv2 import ColBERTv2
from dspy.clients import DSPY_CACHE
from dspy.__metadata__ import __name__, __version__, __description__, __url__, __author__, __author_email__

configure_dspy_loggers(__name__)

# Singleton definitions and aliasing
configure = settings.configure
context = settings.context

BootstrapRS = BootstrapFewShotWithRandomSearch

cache = DSPY_CACHE


--- dspy/__metadata__.py ---
#replace_package_name_marker
__name__="dspy"
#replace_package_version_marker
__version__="3.0.4b1"
__description__="DSPy"
__url__="https://github.com/stanfordnlp/dspy"
__author__="Omar Khattab"
__author_email__="okhattab@stanford.edu"

--- dspy/predict/aggregation.py ---
from dspy.evaluate import normalize_text
from dspy.primitives.prediction import Completions, Prediction


def default_normalize(s):
    return normalize_text(s) or None


def majority(prediction_or_completions, normalize=default_normalize, field=None):
    """
    Returns the most common completion for the target field (or the last field) in the signature.
    When normalize returns None, that completion is ignored.
    In case of a tie, earlier completion are prioritized.
    """

    assert any(isinstance(prediction_or_completions, t) for t in [Prediction, Completions, list])
    type(prediction_or_completions)

    # Get the completions
    if isinstance(prediction_or_completions, Prediction):
        completions = prediction_or_completions.completions
    else:
        completions = prediction_or_completions

    try:
        signature = completions.signature
    except Exception:
        signature = None

    if not field:
        if signature:
            field = list(signature.output_fields.keys())[-1]
        else:
            field = list(completions[0].keys())[-1]

    # Normalize
    normalize = normalize if normalize else lambda x: x
    normalized_values = [normalize(completion[field]) for completion in completions]
    normalized_values_ = [x for x in normalized_values if x is not None]

    # Count
    value_counts = {}
    for value in normalized_values_ or normalized_values:
        value_counts[value] = value_counts.get(value, 0) + 1

    majority_value = max(value_counts, key=value_counts.get)

    # Return the first completion with the majority value in the field
    for completion in completions:
        if normalize(completion[field]) == majority_value:
            break

    # if input_type == Prediction:
    return Prediction.from_completions([completion], signature=signature)


--- dspy/utils/annotation.py ---
import inspect
import re
import types
from typing import Callable, ParamSpec, TypeVar, overload

P = ParamSpec("P")
R = TypeVar("R")

@overload
def experimental(f: Callable[P, R], version: str | None = None) -> Callable[P, R]: ...

@overload
def experimental(f: None = None, version: str | None = None) -> Callable[[Callable[P, R]], Callable[P, R]]: ...


def experimental(
    f: Callable[P, R] | None = None,
    version: str | None = None,
) -> Callable[[Callable[P, R]], Callable[P, R]]:
    """Decorator / decorator creator for marking APIs experimental in the docstring.

    Args:
        f: The function to be decorated.
        version: The version in which the API was introduced as experimental.
            The version is used to determine whether the API should be considered
            as stable or not when releasing a new version of DSPy.

    Returns:
        A decorator that adds a note to the docstring of the decorated API.
    """
    if f:
        return _experimental(f, version)
    else:
        def decorator(f: Callable[P, R]) -> Callable[P, R]:
            return _experimental(f, version)
        return decorator


def _experimental(api: Callable[P, R], version: str | None = None) -> Callable[P, R]:
    """Add experimental notice to the API's docstring."""
    if inspect.isclass(api):
        api_type = "class"
    elif inspect.isfunction(api):
        api_type = "function"
    elif isinstance(api, property):
        api_type = "property"
    elif isinstance(api, types.MethodType):
        api_type = "method"
    else:
        api_type = str(type(api))

    indent = _get_min_indent_of_docstring(api.__doc__) if api.__doc__ else ""

    version_text = f" (introduced in v{version})" if version else ""
    notice = (
        indent + f"Experimental: This {api_type} may change or "
        f"be removed in a future release without warning{version_text}."
    )

    if api_type == "property":
        api.__doc__ = api.__doc__ + "\n\n" + notice if api.__doc__ else notice
    else:
        if api.__doc__:
            api.__doc__ = notice + "\n\n" + api.__doc__
        else:
            api.__doc__ = notice
    return api


def _get_min_indent_of_docstring(docstring_str: str) -> str:
    """
    Get the minimum indentation string of a docstring, based on the assumption
    that the closing triple quote for multiline comments must be on a new line.
    Note that based on ruff rule D209, the closing triple quote for multiline
    comments must be on a new line.

    Args:
        docstring_str: string with docstring

    Returns:
        Whitespace corresponding to the indent of a docstring.
    """

    if not docstring_str or "\n" not in docstring_str:
        return ""

    match = re.match(r"^\s*", docstring_str.rsplit("\n", 1)[-1])
    return match.group() if match else ""


--- dspy/utils/asyncify.py ---
from typing import TYPE_CHECKING, Any, Awaitable, Callable

import asyncer
from anyio import CapacityLimiter

if TYPE_CHECKING:
    from dspy.primitives.module import Module

_limiter = None


def get_async_max_workers():
    import dspy

    return dspy.settings.async_max_workers


def get_limiter():
    async_max_workers = get_async_max_workers()

    global _limiter
    if _limiter is None:
        _limiter = CapacityLimiter(async_max_workers)
    elif _limiter.total_tokens != async_max_workers:
        _limiter.total_tokens = async_max_workers

    return _limiter


def asyncify(program: "Module") -> Callable[[Any, Any], Awaitable[Any]]:
    """
    Wraps a DSPy program so that it can be called asynchronously. This is useful for running a
    program in parallel with another task (e.g., another DSPy program).

    This implementation propagates the current thread's configuration context to the worker thread.

    Args:
        program: The DSPy program to be wrapped for asynchronous execution.

    Returns:
        An async function: An async function that, when awaited, runs the program in a worker thread.
            The current thread's configuration context is inherited for each call.
    """

    async def async_program(*args, **kwargs) -> Any:
        # Capture the current overrides at call-time.
        from dspy.dsp.utils.settings import thread_local_overrides

        parent_overrides = thread_local_overrides.get().copy()

        def wrapped_program(*a, **kw):
            from dspy.dsp.utils.settings import thread_local_overrides

            original_overrides = thread_local_overrides.get()
            token = thread_local_overrides.set({**original_overrides, **parent_overrides.copy()})
            try:
                return program(*a, **kw)
            finally:
                thread_local_overrides.reset(token)

        # Create a fresh asyncified callable each time, ensuring the latest context is used.
        call_async = asyncer.asyncify(wrapped_program, abandon_on_cancel=True, limiter=get_limiter())
        return await call_async(*args, **kwargs)

    return async_program


--- dspy/evaluate/auto_evaluation.py ---
from dspy.predict.chain_of_thought import ChainOfThought
from dspy.primitives import Module
from dspy.signatures import InputField, OutputField, Signature


class SemanticRecallPrecision(Signature):
    """
    Compare a system's response to the ground truth to compute its recall and precision.
    If asked to reason, enumerate key ideas in each response, and whether they are present in the other response.
    """

    question: str = InputField()
    ground_truth: str = InputField()
    system_response: str = InputField()
    recall: float = OutputField(desc="fraction (out of 1.0) of ground truth covered by the system response")
    precision: float = OutputField(desc="fraction (out of 1.0) of system response covered by the ground truth")


class DecompositionalSemanticRecallPrecision(Signature):
    """
    Compare a system's response to the ground truth to compute recall and precision of key ideas.
    You will first enumerate key ideas in each response, discuss their overlap, and then report recall and precision.
    """

    question: str = InputField()
    ground_truth: str = InputField()
    system_response: str = InputField()
    ground_truth_key_ideas: str = OutputField(desc="enumeration of key ideas in the ground truth")
    system_response_key_ideas: str = OutputField(desc="enumeration of key ideas in the system response")
    discussion: str = OutputField(desc="discussion of the overlap between ground truth and system response")
    recall: float = OutputField(desc="fraction (out of 1.0) of ground truth covered by the system response")
    precision: float = OutputField(desc="fraction (out of 1.0) of system response covered by the ground truth")


def f1_score(precision, recall):
    precision, recall = max(0.0, min(1.0, precision)), max(0.0, min(1.0, recall))
    return 0.0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)


class SemanticF1(Module):
    def __init__(self, threshold=0.66, decompositional=False):
        self.threshold = threshold

        if decompositional:
            self.module = ChainOfThought(DecompositionalSemanticRecallPrecision)
        else:
            self.module = ChainOfThought(SemanticRecallPrecision)

    def forward(self, example, pred, trace=None):
        scores = self.module(question=example.question, ground_truth=example.response, system_response=pred.response)
        score = f1_score(scores.precision, scores.recall)

        return score if trace is None else score >= self.threshold



###########


class AnswerCompleteness(Signature):
    """
    Estimate the completeness of a system's responses, against the ground truth.
    You will first enumerate key ideas in each response, discuss their overlap, and then report completeness.
    """

    question: str = InputField()
    ground_truth: str = InputField()
    system_response: str = InputField()
    ground_truth_key_ideas: str = OutputField(desc="enumeration of key ideas in the ground truth")
    system_response_key_ideas: str = OutputField(desc="enumeration of key ideas in the system response")
    discussion: str = OutputField(desc="discussion of the overlap between ground truth and system response")
    completeness: float = OutputField(desc="fraction (out of 1.0) of ground truth covered by the system response")



class AnswerGroundedness(Signature):
    """
    Estimate the groundedness of a system's responses, against real retrieved documents written by people.
    You will first enumerate whatever non-trivial or check-worthy claims are made in the system response, and then
    discuss the extent to which some or all of them can be deduced from the retrieved context and basic commonsense.
    """

    question: str = InputField()
    retrieved_context: str = InputField()
    system_response: str = InputField()
    system_response_claims: str = OutputField(desc="enumeration of non-trivial or check-worthy claims in the system response")
    discussion: str = OutputField(desc="discussion of how supported the claims are by the retrieved context")
    groundedness: float = OutputField(desc="fraction (out of 1.0) of system response supported by the retrieved context")


class CompleteAndGrounded(Module):
    def __init__(self, threshold=0.66):
        self.threshold = threshold
        self.completeness_module = ChainOfThought(AnswerCompleteness)
        self.groundedness_module = ChainOfThought(AnswerGroundedness)

    def forward(self, example, pred, trace=None):
        completeness = self.completeness_module(question=example.question, ground_truth=example.response, system_response=pred.response)
        groundedness = self.groundedness_module(question=example.question, retrieved_context=pred.context, system_response=pred.response)
        score = f1_score(groundedness.groundedness, completeness.completeness)

        return score if trace is None else score >= self.threshold


--- dspy/teleprompt/avatar_optimizer.py ---
from concurrent.futures import ThreadPoolExecutor
from copy import deepcopy
from random import sample
from typing import Callable

from pydantic import BaseModel
from tqdm import tqdm

import dspy
from dspy.predict.avatar import ActionOutput
from dspy.teleprompt.teleprompt import Teleprompter

DEFAULT_MAX_EXAMPLES = 10


class EvalResult(BaseModel):
    example: dict
    score: float
    actions: list[ActionOutput] | None = None


class Comparator(dspy.Signature):
    """After executing the given actions on user inputs using the given instruction, some inputs have yielded good, results, while others have not. I'll provide you the inputs along with their, corresponding evaluation metrics:

Task:
(1) Firstly, identify and contrast the patterns of inputs that have achieved good results with those that have not.
(2) Then, review the computational logic for any inconsistencies in the previous actions.
(3) Lastly, specify the modification in tools used that can lead to improved performance on the negative inputs."""

    instruction: str = dspy.InputField(
        prefix="Instruction: ",
        desc="Instruction for the actor to execute the task",
    )
    actions: list[str] = dspy.InputField(
        prefix="Actions: ",
        desc="Actions actor can take to complete the task",
    )
    pos_input_with_metrics: list[EvalResult] = dspy.InputField(
        prefix="Positive Inputs: ",
        desc="Positive inputs along with their score on a evaluation metric and actions taken",
    )
    neg_input_with_metrics: list[EvalResult] = dspy.InputField(
        prefix="Negative Inputs: ",
        desc="Negative inputs along with their score on a evaluation metric and actions taken",
    )
    feedback: str = dspy.OutputField(
        prefix="Feedback: ",
        desc="Feedback for the actor to improve the performance of negative inputs",
    )


class FeedbackBasedInstruction(dspy.Signature):
    """There is a task that needs to be completed for which one can use multiple tools to achieve the desired outcome. A group's performance was evaluated on a dataset of inputs, the inputs that did well are positive inputs, and the inputs that did not do well are negative inputs.

You received feedback on how they can better use the tools to improve your performance on the negative inputs. You have been provided with the previous instruction, that they followed to use tools to complete the task, and the feedback on your performance.

Your task is to incorporate the feedback and generate a detailed instruction for the group to follow to improve their performance on the task.

Make sure that the new instruction talks about how to use the tools effectively and should be no more than 3 paragraphs long. The previous instruction contains general guidelines that you must retain in the new instruction."""

    previous_instruction: str = dspy.InputField(
        prefix="Previous Instruction: ",
        desc="Previous instruction for the actor to execute the task",
    )
    feedback: str = dspy.InputField(
        prefix="Feedback: ",
        desc="Feedback for the actor to improve the performance of negative inputs",
    )
    new_instruction: str = dspy.OutputField(
        prefix="New Instruction: ",
        desc="New instruction for the actor to execute the task",
    )


class AvatarOptimizer(Teleprompter):
    def __init__(
        self,
        metric: Callable,
        max_iters: int = 10,
        lower_bound: int = 0,
        upper_bound: int = 1,
        max_positive_inputs: int | None = None,
        max_negative_inputs: int | None = None,
        optimize_for: str = "max",
    ):
        assert metric is not None, "`metric` argument cannot be None. Please provide a metric function."
        self.metric = metric
        self.optimize_for = optimize_for

        self.max_iters = max_iters

        self.lower_bound = lower_bound
        self.upper_bound = upper_bound

        self.max_positive_inputs = max_positive_inputs or DEFAULT_MAX_EXAMPLES
        self.max_negative_inputs = max_negative_inputs or DEFAULT_MAX_EXAMPLES

        self.comparator = dspy.TypedPredictor(Comparator)
        self.feedback_instruction = dspy.Predict(FeedbackBasedInstruction)

    def process_example(self, actor, example, return_outputs):
        actor = deepcopy(actor)

        try:
            prediction = actor(**example.inputs().toDict())
            score = self.metric(example, prediction)

            if return_outputs:
                return example, prediction, score
            else:
                return score

        except Exception as e:
            print(e)

            if return_outputs:
                return example, None, 0
            else:
                return 0


    def thread_safe_evaluator(self, devset, actor, return_outputs=False, num_threads=None):
        total_score = 0
        total_examples = len(devset)
        results = []
        num_threads = num_threads or dspy.settings.num_threads

        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(self.process_example, actor, example, return_outputs) for example in devset]

            for future in tqdm(futures, total=total_examples, desc="Processing examples"):
                result = future.result()
                if return_outputs:
                    example, prediction, score = result
                    total_score += score
                    results.append((example, prediction, score))
                else:
                    total_score += result

        avg_metric = total_score / total_examples

        if return_outputs:
            return avg_metric, results
        else:
            return avg_metric


    def _get_pos_neg_results(
        self,
        actor: dspy.Module,
        trainset: list[dspy.Example]
    ) -> tuple[float, list[EvalResult], list[EvalResult]]:
        pos_inputs = []
        neg_inputs = []

        avg_score, results = self.thread_safe_evaluator(trainset, actor, return_outputs=True)
        print(f"Average Score: {avg_score}")

        for example, prediction, score in results:
            if score >= self.upper_bound:
                pos_inputs.append(
                    EvalResult(
                        example=example.inputs().toDict(),
                        score=score,
                        actions=prediction.actions if prediction else None
                    )
                )
            elif score <= self.lower_bound:
                neg_inputs.append(
                    EvalResult(
                        example=example.inputs().toDict(),
                        score=score,
                        actions=prediction.actions if prediction else None
                    )
                )

        if len(pos_inputs) == 0:
            raise ValueError("No positive examples found, try lowering the upper_bound or providing more training data")
        if len(neg_inputs) == 0:
            raise ValueError("No negative examples found, try raising the lower_bound or providing more training data")

        return (avg_score, pos_inputs, neg_inputs)


    def compile(self, student, *, trainset):
        best_actor = deepcopy(student)
        best_score = -999 if self.optimize_for == "max" else 999

        for i in range(self.max_iters):
            print(20*"=")
            print(f"Iteration {i+1}/{self.max_iters}")

            score, pos_inputs, neg_inputs = self._get_pos_neg_results(best_actor, trainset)
            print(f"Positive examples: {len(pos_inputs)}")
            print(f"Negative examples: {len(neg_inputs)}")
            print(f"Sampling {self.max_positive_inputs} positive examples and {self.max_negative_inputs} negative examples")

            if self.max_positive_inputs and len(pos_inputs) > self.max_positive_inputs:
                pos_inputs = sample(pos_inputs, self.max_positive_inputs)

            if self.max_negative_inputs and len(neg_inputs) > self.max_negative_inputs:
                neg_inputs = sample(neg_inputs, self.max_negative_inputs)

            feedback = self.comparator(
                instruction=best_actor.actor.signature.instructions,
                actions=[str(tool) for tool in best_actor.tools],
                pos_input_with_metrics=pos_inputs,
                neg_input_with_metrics=neg_inputs
            ).feedback

            new_instruction = self.feedback_instruction(
                previous_instruction=best_actor.actor.signature.instructions,
                feedback=feedback
            ).new_instruction

            print(f"Generated new instruction: {new_instruction}")

            if (self.optimize_for == "max" and best_score < score) or (self.optimize_for == "min" and best_score > score):
                best_actor.actor.signature = best_actor.actor.signature.with_instructions(new_instruction)
                best_actor.actor_clone = deepcopy(best_actor.actor)
                best_score = score

        print(f"Best Actor: {best_actor}")

        return best_actor


--- dspy/adapters/baml_adapter.py ---
"""
Custom adapter for improving structured outputs using the information from Pydantic models.
Based on the format used by BAML: https://github.com/BoundaryML/baml
"""

import inspect
import types
from typing import Any, Literal, Union, get_args, get_origin

from pydantic import BaseModel

from dspy.adapters.json_adapter import JSONAdapter
from dspy.adapters.utils import format_field_value as original_format_field_value
from dspy.signatures.signature import Signature

# Changing the comment symbol to Python's # rather than other languages' // seems to help
COMMENT_SYMBOL = "#"


def _render_type_str(
    annotation: Any,
    depth: int = 0,
    indent: int = 0,
    seen_models: set[type] | None = None,
) -> str:
    """Recursively renders a type annotation into a simplified string.

    Args:
        annotation: The type annotation to render
        depth: Current recursion depth (prevents infinite recursion)
        indent: Current indentation level for nested structures
    """
    # Non-nested types
    if annotation is str:
        return "string"
    if annotation is int:
        return "int"
    if annotation is float:
        return "float"
    if annotation is bool:
        return "boolean"
    if inspect.isclass(annotation) and issubclass(annotation, BaseModel):
        return _build_simplified_schema(annotation, indent, seen_models)

    try:
        origin = get_origin(annotation)
        args = get_args(annotation)
    except Exception:
        return str(annotation)

    # Optional[T] or T | None
    if origin in (types.UnionType, Union):
        non_none_args = [arg for arg in args if arg is not type(None)]
        # Render the non-None part of the union
        type_render = " or ".join([_render_type_str(arg, depth + 1, indent) for arg in non_none_args])
        # Add "or null" if None was part of the union
        if len(non_none_args) < len(args):
            return f"{type_render} or null"
        return type_render

    # Literal[T1, T2, ...]
    if origin is Literal:
        return " or ".join(f'"{arg}"' for arg in args)

    # list[T]
    if origin is list:
        # For Pydantic models in lists, use bracket notation
        inner_type = args[0]
        if inspect.isclass(inner_type) and issubclass(inner_type, BaseModel):
            # Build inner schema - the Pydantic model inside should use indent level for array contents
            inner_schema = _build_simplified_schema(inner_type, indent + 1, seen_models)
            # Format with proper bracket notation and indentation
            current_indent = "  " * indent
            return f"[\n{inner_schema}\n{current_indent}]"
        else:
            return f"{_render_type_str(inner_type, depth + 1, indent)}[]"

    # dict[T1, T2]
    if origin is dict:
        return f"dict[{_render_type_str(args[0], depth + 1, indent)}, {_render_type_str(args[1], depth + 1, indent)}]"

    # fallback
    if hasattr(annotation, "__name__"):
        return annotation.__name__
    return str(annotation)


def _build_simplified_schema(
    pydantic_model: type[BaseModel],
    indent: int = 0,
    seen_models: set[type] | None = None,
) -> str:
    """Builds a simplified, human-readable schema from a Pydantic model.

    Args:
        pydantic_model: The Pydantic model to build schema for
        indent: Current indentation level
        seen_models: Set to track visited pydantic models (prevents infinite recursion)
    """
    seen_models = seen_models or set()

    if pydantic_model in seen_models:
        raise ValueError("BAMLAdapter cannot handle recursive pydantic models, please use a different adapter.")

    # Add `pydantic_model` to `seen_models` with a placeholder value to avoid infinite recursion.
    seen_models.add(pydantic_model)

    lines = []
    current_indent = "  " * indent
    next_indent = "  " * (indent + 1)

    lines.append(f"{current_indent}{{")

    fields = pydantic_model.model_fields
    if not fields:
        lines.append(f"{next_indent}{COMMENT_SYMBOL} No fields defined")
    for name, field in fields.items():
        if field.description:
            lines.append(f"{next_indent}{COMMENT_SYMBOL} {field.description}")
        elif field.alias and field.alias != name:
            # If there's an alias but no description, show the alias as a comment
            lines.append(f"{next_indent}{COMMENT_SYMBOL} alias: {field.alias}")

        rendered_type = _render_type_str(field.annotation, indent=indent + 1, seen_models=seen_models)
        line = f"{next_indent}{name}: {rendered_type},"

        lines.append(line)

    lines.append(f"{current_indent}}}")
    return "\n".join(lines)


class BAMLAdapter(JSONAdapter):
    """
    A DSPy adapter that improves the rendering of complex/nested Pydantic models to help LMs.

    This adapter generates a compact, human-readable schema representation for nested Pydantic output
    fields, inspired by the BAML project's JSON formatter (https://github.com/BoundaryML/baml).
    The resulting rendered schema is more token-efficient and easier for smaller LMs to follow than a
    raw JSON schema. It also includes Pydantic field descriptions as comments in the schema, which
    provide valuable additional context for the LM to understand the expected output.

    Example Usage:
    ```python
    import dspy
    from pydantic import BaseModel, Field
    from typing import Literal
    from baml_adapter import BAMLAdapter  # Import from your module

    # 1. Define your Pydantic models
    class PatientAddress(BaseModel):
        street: str
        city: str
        country: Literal["US", "CA"]

    class PatientDetails(BaseModel):
        name: str = Field(description="Full name of the patient.")
        age: int
        address: PatientAddress | None

    # 2. Define a signature using the Pydantic model as an output field
    class ExtractPatientInfo(dspy.Signature):
        '''Extract patient information from the clinical note.'''
        clinical_note: str = dspy.InputField()
        patient_info: PatientDetails = dspy.OutputField()

    # 3. Configure dspy to use the new adapter
    llm = dspy.OpenAI(model="gpt-4.1-mini")
    dspy.configure(lm=llm, adapter=BAMLAdapter())

    # 4. Run your program
    extractor = dspy.Predict(ExtractPatientInfo)
    note = "John Doe, 45 years old, lives at 123 Main St, Anytown. Resident of the US."
    result = extractor(clinical_note=note)
    print(result.patient_info)

    # Expected output:
    # PatientDetails(name='John Doe', age=45, address=PatientAddress(street='123 Main St', city='Anytown', country='US'))
    ```
    """

    def format_field_description(self, signature: type[Signature]) -> str:
        """Format the field description for the system message."""
        sections = []

        # Add input field descriptions
        if signature.input_fields:
            sections.append("Your input fields are:")
            for i, (name, field) in enumerate(signature.input_fields.items(), 1):
                type_name = getattr(field.annotation, "__name__", str(field.annotation))
                description = f": {field.description}" if field.description else ":"
                sections.append(f"{i}. `{name}` ({type_name}){description}")

        # Add output field descriptions
        if signature.output_fields:
            sections.append("Your output fields are:")
            for i, (name, field) in enumerate(signature.output_fields.items(), 1):
                type_name = getattr(field.annotation, "__name__", str(field.annotation))
                description = f": {field.description}" if field.description else ":"
                sections.append(f"{i}. `{name}` ({type_name}){description}")

        return "\n".join(sections)

    def format_field_structure(self, signature: type[Signature]) -> str:
        """Overrides the base method to generate a simplified schema for Pydantic models."""

        sections = []

        # Add structural explanation
        sections.append(
            "All interactions will be structured in the following way, with the appropriate values filled in.\n"
        )

        # Add input structure section
        if signature.input_fields:
            for name in signature.input_fields.keys():
                sections.append(f"[[ ## {name} ## ]]")
                sections.append(f"{{{name}}}")
                sections.append("")  # Empty line after each input

        # Add output structure section
        if signature.output_fields:
            for name, field in signature.output_fields.items():
                field_type = field.annotation
                sections.append(f"[[ ## {name} ## ]]")
                sections.append(f"Output field `{name}` should be of type: {_render_type_str(field_type, indent=0)}\n")

        # Add completed section
        sections.append("[[ ## completed ## ]]")

        return "\n".join(sections)

    def format_user_message_content(
        self,
        signature: type[Signature],
        inputs: dict[str, Any],
        prefix: str = "",
        suffix: str = "",
        main_request: bool = False,
    ) -> str:
        """Overrides the base method to render Pydantic input instances as clean JSON."""
        messages = [prefix]
        for key, field_info in signature.input_fields.items():
            if key in inputs:
                value = inputs.get(key)
                formatted_value = ""
                if isinstance(value, BaseModel):
                    # Use clean, indented JSON for Pydantic instances
                    formatted_value = value.model_dump_json(indent=2, by_alias=True)
                else:
                    # Fallback to the original dspy formatter for other types
                    formatted_value = original_format_field_value(field_info=field_info, value=value)

                messages.append(f"[[ ## {key} ## ]]\n{formatted_value}")

        if main_request:
            output_requirements = self.user_message_output_requirements(signature)
            if output_requirements is not None:
                messages.append(output_requirements)

        messages.append(suffix)
        return "\n\n".join(m for m in messages if m).strip()


--- dspy/adapters/base.py ---
import logging
from typing import TYPE_CHECKING, Any, get_origin

import json_repair
import litellm

from dspy.adapters.types import History, Type
from dspy.adapters.types.base_type import split_message_content_for_custom_types
from dspy.adapters.types.tool import Tool, ToolCalls
from dspy.experimental import Citations
from dspy.signatures.signature import Signature
from dspy.utils.callback import BaseCallback, with_callbacks

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from dspy.clients.lm import LM

_DEFAULT_NATIVE_RESPONSE_TYPES = [Citations]


class Adapter:
    """Base Adapter class.

    The Adapter serves as the interface layer between DSPy module/signature and Language Models (LMs). It handles the
    complete transformation pipeline from DSPy inputs to LM calls and back to structured outputs.

    Key responsibilities:
        - Transform user inputs and signatures into properly formatted LM prompts, which also instructs the LM to format
            the response in a specific format.
        - Parse LM outputs into dictionaries matching the signature's output fields.
        - Enable/disable native LM features (function calling, citations, etc.) based on configuration.
        - Handle conversation history, few-shot examples, and custom type processing.

    The adapter pattern allows DSPy to work with different LM interfaces while maintaining a consistent programming
    model for users.
    """

    def __init__(
        self,
        callbacks: list[BaseCallback] | None = None,
        use_native_function_calling: bool = False,
        native_response_types: list[type[Type]] | None = None,
    ):
        """
        Args:
            callbacks: List of callback functions to execute during `format()` and `parse()` methods. Callbacks can be
                used for logging, monitoring, or custom processing. Defaults to None (empty list).
            use_native_function_calling: Whether to enable native function calling capabilities when the LM supports it.
                If True, the adapter will automatically configure function calling when input fields contain `dspy.Tool`
                or `list[dspy.Tool]` types. Defaults to False.
            native_response_types: List of output field types that should be handled by native LM features rather than
                adapter parsing. For example, `dspy.Citations` can be populated directly by citation APIs
                (e.g., Anthropic's citation feature). Defaults to `[Citations]`.
        """
        self.callbacks = callbacks or []
        self.use_native_function_calling = use_native_function_calling
        self.native_response_types = native_response_types or _DEFAULT_NATIVE_RESPONSE_TYPES

    def __init_subclass__(cls, **kwargs) -> None:
        super().__init_subclass__(**kwargs)

        # Decorate format() and parse() method with with_callbacks
        cls.format = with_callbacks(cls.format)
        cls.parse = with_callbacks(cls.parse)

    def _call_preprocess(
        self,
        lm: "LM",
        lm_kwargs: dict[str, Any],
        signature: type[Signature],
        inputs: dict[str, Any],
    ) -> type[Signature]:
        if self.use_native_function_calling:
            tool_call_input_field_name = self._get_tool_call_input_field_name(signature)
            tool_call_output_field_name = self._get_tool_call_output_field_name(signature)

            if tool_call_output_field_name and tool_call_input_field_name is None:
                raise ValueError(
                    f"You provided an output field {tool_call_output_field_name} to receive the tool calls information, "
                    "but did not provide any tools as the input. Please provide a list of tools as the input by adding an "
                    "input field with type `list[dspy.Tool]`."
                )

            if tool_call_output_field_name and litellm.supports_function_calling(model=lm.model):
                tools = inputs[tool_call_input_field_name]
                tools = tools if isinstance(tools, list) else [tools]

                litellm_tools = []
                for tool in tools:
                    litellm_tools.append(tool.format_as_litellm_function_call())

                lm_kwargs["tools"] = litellm_tools

                signature_for_native_function_calling = signature.delete(tool_call_output_field_name)
                signature_for_native_function_calling = signature_for_native_function_calling.delete(
                    tool_call_input_field_name
                )

                return signature_for_native_function_calling

        # Handle custom types that use native response
        for name, field in signature.output_fields.items():
            if (
                isinstance(field.annotation, type)
                and issubclass(field.annotation, Type)
                and field.annotation in self.native_response_types
            ):
                signature = signature.delete(name)

        return signature

    def _call_postprocess(
        self,
        processed_signature: type[Signature],
        original_signature: type[Signature],
        outputs: list[dict[str, Any]],
        lm: "LM",
    ) -> list[dict[str, Any]]:
        values = []

        tool_call_output_field_name = self._get_tool_call_output_field_name(original_signature)

        for output in outputs:
            output_logprobs = None
            tool_calls = None
            text = output

            if isinstance(output, dict):
                text = output["text"]
                output_logprobs = output.get("logprobs")
                tool_calls = output.get("tool_calls")

            if text:
                value = self.parse(processed_signature, text)
                for field_name in original_signature.output_fields.keys():
                    if field_name not in value:
                        # We need to set the field not present in the processed signature to None for consistency.
                        value[field_name] = None
            else:
                value = {}
                for field_name in original_signature.output_fields.keys():
                    value[field_name] = None

            if tool_calls and tool_call_output_field_name:
                tool_calls = [
                    {
                        "name": v["function"]["name"],
                        "args": json_repair.loads(v["function"]["arguments"]),
                    }
                    for v in tool_calls
                ]
                value[tool_call_output_field_name] = ToolCalls.from_dict_list(tool_calls)

            # Parse custom types that does not rely on the adapter parsing
            for name, field in original_signature.output_fields.items():
                if (
                    isinstance(field.annotation, type)
                    and issubclass(field.annotation, Type)
                    and field.annotation in self.native_response_types
                ):
                    value[name] = field.annotation.parse_lm_response(output)

            if output_logprobs:
                value["logprobs"] = output_logprobs

            values.append(value)

        return values

    def __call__(
        self,
        lm: "LM",
        lm_kwargs: dict[str, Any],
        signature: type[Signature],
        demos: list[dict[str, Any]],
        inputs: dict[str, Any],
    ) -> list[dict[str, Any]]:
        """
        Execute the adapter pipeline: format inputs, call LM, and parse outputs.

        Args:
            lm: The Language Model instance to use for generation. Must be an instance of `dspy.BaseLM`.
            lm_kwargs: Additional keyword arguments to pass to the LM call (e.g., temperature, max_tokens). These are
                passed directly to the LM.
            signature: The DSPy signature associated with this LM call.
            demos: List of few-shot examples to include in the prompt. Each dictionary should contain keys matching the
                signature's input and output field names. Examples are formatted as user/assistant message pairs.
            inputs: The current input values for this call. Keys must match the signature's input field names.

        Returns:
            List of dictionaries representing parsed LM responses. Each dictionary contains keys matching the
            signature's output field names. For multiple generations (n > 1), returns multiple dictionaries.
        """
        processed_signature = self._call_preprocess(lm, lm_kwargs, signature, inputs)
        inputs = self.format(processed_signature, demos, inputs)

        outputs = lm(messages=inputs, **lm_kwargs)
        return self._call_postprocess(processed_signature, signature, outputs, lm)

    async def acall(
        self,
        lm: "LM",
        lm_kwargs: dict[str, Any],
        signature: type[Signature],
        demos: list[dict[str, Any]],
        inputs: dict[str, Any],
    ) -> list[dict[str, Any]]:
        processed_signature = self._call_preprocess(lm, lm_kwargs, signature, inputs)
        inputs = self.format(processed_signature, demos, inputs)

        outputs = await lm.acall(messages=inputs, **lm_kwargs)
        return self._call_postprocess(processed_signature, signature, outputs, lm)

    def format(
        self,
        signature: type[Signature],
        demos: list[dict[str, Any]],
        inputs: dict[str, Any],
    ) -> list[dict[str, Any]]:
        """Format the input messages for the LM call.

        This method converts the DSPy structured input along with few-shot examples and conversation history into
        multiturn messages as expected by the LM. For custom adapters, this method can be overridden to customize
        the formatting of the input messages.

        In general we recommend the messages to have the following structure:
        ```
        [
            {"role": "system", "content": system_message},
            # Begin few-shot examples
            {"role": "user", "content": few_shot_example_1_input},
            {"role": "assistant", "content": few_shot_example_1_output},
            {"role": "user", "content": few_shot_example_2_input},
            {"role": "assistant", "content": few_shot_example_2_output},
            ...
            # End few-shot examples
            # Begin conversation history
            {"role": "user", "content": conversation_history_1_input},
            {"role": "assistant", "content": conversation_history_1_output},
            {"role": "user", "content": conversation_history_2_input},
            {"role": "assistant", "content": conversation_history_2_output},
            ...
            # End conversation history
            {"role": "user", "content": current_input},
        ]

        And system message should contain the field description, field structure, and task description.
        ```


        Args:
            signature: The DSPy signature for which to format the input messages.
            demos: A list of few-shot examples.
            inputs: The input arguments to the DSPy module.

        Returns:
            A list of multiturn messages as expected by the LM.
        """
        inputs_copy = dict(inputs)

        # If the signature and inputs have conversation history, we need to format the conversation history and
        # remove the history field from the signature.
        history_field_name = self._get_history_field_name(signature)
        if history_field_name:
            # In order to format the conversation history, we need to remove the history field from the signature.
            signature_without_history = signature.delete(history_field_name)
            conversation_history = self.format_conversation_history(
                signature_without_history,
                history_field_name,
                inputs_copy,
            )

        messages = []
        system_message = (
            f"{self.format_field_description(signature)}\n"
            f"{self.format_field_structure(signature)}\n"
            f"{self.format_task_description(signature)}"
        )
        messages.append({"role": "system", "content": system_message})
        messages.extend(self.format_demos(signature, demos))
        if history_field_name:
            # Conversation history and current input
            content = self.format_user_message_content(signature_without_history, inputs_copy, main_request=True)
            messages.extend(conversation_history)
            messages.append({"role": "user", "content": content})
        else:
            # Only current input
            content = self.format_user_message_content(signature, inputs_copy, main_request=True)
            messages.append({"role": "user", "content": content})

        messages = split_message_content_for_custom_types(messages)
        return messages

    def format_field_description(self, signature: type[Signature]) -> str:
        """Format the field description for the system message.

        This method formats the field description for the system message. It should return a string that contains
        the field description for the input fields and the output fields.

        Args:
            signature: The DSPy signature for which to format the field description.

        Returns:
            A string that contains the field description for the input fields and the output fields.
        """
        raise NotImplementedError

    def format_field_structure(self, signature: type[Signature]) -> str:
        """Format the field structure for the system message.

        This method formats the field structure for the system message. It should return a string that dictates the
        format the input fields should be provided to the LM, and the format the output fields will be in the response.
        Refer to the ChatAdapter and JsonAdapter for an example.

        Args:
            signature: The DSPy signature for which to format the field structure.
        """
        raise NotImplementedError

    def format_task_description(self, signature: type[Signature]) -> str:
        """Format the task description for the system message.

        This method formats the task description for the system message. In most cases this is just a thin wrapper
        over `signature.instructions`.

        Args:
            signature: The DSPy signature of the DSpy module.

        Returns:
            A string that describes the task.
        """
        raise NotImplementedError

    def format_user_message_content(
        self,
        signature: type[Signature],
        inputs: dict[str, Any],
        prefix: str = "",
        suffix: str = "",
        main_request: bool = False,
    ) -> str:
        """Format the user message content.

        This method formats the user message content, which can be used in formatting few-shot examples, conversation
        history, and the current input.

        Args:
            signature: The DSPy signature for which to format the user message content.
            inputs: The input arguments to the DSPy module.
            prefix: A prefix to the user message content.
            suffix: A suffix to the user message content.

        Returns:
            A string that contains the user message content.
        """
        raise NotImplementedError

    def format_assistant_message_content(
        self,
        signature: type[Signature],
        outputs: dict[str, Any],
        missing_field_message: str | None = None,
    ) -> str:
        """Format the assistant message content.

        This method formats the assistant message content, which can be used in formatting few-shot examples,
        conversation history.

        Args:
            signature: The DSPy signature for which to format the assistant message content.
            outputs: The output fields to be formatted.
            missing_field_message: A message to be used when a field is missing.

        Returns:
            A string that contains the assistant message content.
        """
        raise NotImplementedError

    def format_demos(self, signature: type[Signature], demos: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Format the few-shot examples.

        This method formats the few-shot examples as multiturn messages.

        Args:
            signature: The DSPy signature for which to format the few-shot examples.
            demos: A list of few-shot examples, each element is a dictionary with keys of the input and output fields of
                the signature.

        Returns:
            A list of multiturn messages.
        """
        complete_demos = []
        incomplete_demos = []

        for demo in demos:
            # Check if all fields are present and not None
            is_complete = all(k in demo and demo[k] is not None for k in signature.fields)

            # Check if demo has at least one input and one output field
            has_input = any(k in demo for k in signature.input_fields)
            has_output = any(k in demo for k in signature.output_fields)

            if is_complete:
                complete_demos.append(demo)
            elif has_input and has_output:
                # We only keep incomplete demos that have at least one input and one output field
                incomplete_demos.append(demo)

        messages = []

        incomplete_demo_prefix = "This is an example of the task, though some input or output fields are not supplied."
        for demo in incomplete_demos:
            messages.append(
                {
                    "role": "user",
                    "content": self.format_user_message_content(signature, demo, prefix=incomplete_demo_prefix),
                }
            )
            messages.append(
                {
                    "role": "assistant",
                    "content": self.format_assistant_message_content(
                        signature, demo, missing_field_message="Not supplied for this particular example. "
                    ),
                }
            )

        for demo in complete_demos:
            messages.append({"role": "user", "content": self.format_user_message_content(signature, demo)})
            messages.append(
                {
                    "role": "assistant",
                    "content": self.format_assistant_message_content(
                        signature, demo, missing_field_message="Not supplied for this conversation history message. "
                    ),
                }
            )

        return messages

    def _get_history_field_name(self, signature: type[Signature]) -> bool:
        for name, field in signature.input_fields.items():
            if field.annotation == History:
                return name
        return None

    def _get_tool_call_input_field_name(self, signature: type[Signature]) -> bool:
        for name, field in signature.input_fields.items():
            # Look for annotation `list[dspy.Tool]` or `dspy.Tool`
            origin = get_origin(field.annotation)
            if origin is list and field.annotation.__args__[0] == Tool:
                return name
            if field.annotation == Tool:
                return name
        return None

    def _get_tool_call_output_field_name(self, signature: type[Signature]) -> bool:
        for name, field in signature.output_fields.items():
            if field.annotation == ToolCalls:
                return name
        return None

    def format_conversation_history(
        self,
        signature: type[Signature],
        history_field_name: str,
        inputs: dict[str, Any],
    ) -> list[dict[str, Any]]:
        """Format the conversation history.

        This method formats the conversation history and the current input as multiturn messages.

        Args:
            signature: The DSPy signature for which to format the conversation history.
            history_field_name: The name of the history field in the signature.
            inputs: The input arguments to the DSPy module.

        Returns:
            A list of multiturn messages.
        """
        conversation_history = inputs[history_field_name].messages if history_field_name in inputs else None

        if conversation_history is None:
            return []

        messages = []
        for message in conversation_history:
            messages.append(
                {
                    "role": "user",
                    "content": self.format_user_message_content(signature, message),
                }
            )
            messages.append(
                {
                    "role": "assistant",
                    "content": self.format_assistant_message_content(signature, message),
                }
            )

        # Remove the history field from the inputs
        del inputs[history_field_name]

        return messages

    def parse(self, signature: type[Signature], completion: str) -> dict[str, Any]:
        """Parse the LM output into a dictionary of the output fields.

        This method parses the LM output into a dictionary of the output fields.

        Args:
            signature: The DSPy signature for which to parse the LM output.
            completion: The LM output to be parsed.

        Returns:
            A dictionary of the output fields.
        """
        raise NotImplementedError


--- dspy/clients/base_lm.py ---
import datetime
import uuid

from dspy.dsp.utils import settings
from dspy.utils.callback import with_callbacks
from dspy.utils.inspect_history import pretty_print_history

MAX_HISTORY_SIZE = 10_000
GLOBAL_HISTORY = []


class BaseLM:
    """Base class for handling LLM calls.

    Most users can directly use the `dspy.LM` class, which is a subclass of `BaseLM`. Users can also implement their
    own subclasses of `BaseLM` to support custom LLM providers and inject custom logic. To do so, simply override the
    `forward` method and make sure the return format is identical to the
    [OpenAI response format](https://platform.openai.com/docs/api-reference/responses/object).

    Example:

    ```python
    from openai import OpenAI

    import dspy


    class MyLM(dspy.BaseLM):
        def forward(self, prompt, messages=None, **kwargs):
            client = OpenAI()
            return client.chat.completions.create(
                model=self.model,
                messages=messages or [{"role": "user", "content": prompt}],
                **self.kwargs,
            )


    lm = MyLM(model="gpt-4o-mini")
    dspy.configure(lm=lm)
    print(dspy.Predict("q->a")(q="Why did the chicken cross the kitchen?"))
    ```
    """

    def __init__(self, model, model_type="chat", temperature=0.0, max_tokens=1000, cache=True, **kwargs):
        self.model = model
        self.model_type = model_type
        self.cache = cache
        self.kwargs = dict(temperature=temperature, max_tokens=max_tokens, **kwargs)
        self.history = []

    def _process_lm_response(self, response, prompt, messages, **kwargs):
        merged_kwargs = {**self.kwargs, **kwargs}

        if self.model_type == "responses":
            outputs = self._process_response(response)
        else:
            outputs = self._process_completion(response, merged_kwargs)

        if settings.disable_history:
            return outputs

        # Logging, with removed api key & where `cost` is None on cache hit.
        kwargs = {k: v for k, v in kwargs.items() if not k.startswith("api_")}
        entry = {
            "prompt": prompt,
            "messages": messages,
            "kwargs": kwargs,
            "response": response,
            "outputs": outputs,
            "usage": dict(response.usage),
            "cost": getattr(response, "_hidden_params", {}).get("response_cost"),
            "timestamp": datetime.datetime.now().isoformat(),
            "uuid": str(uuid.uuid4()),
            "model": self.model,
            "response_model": response.model,
            "model_type": self.model_type,
        }

        self.update_history(entry)

        return outputs

    @with_callbacks
    def __call__(self, prompt=None, messages=None, **kwargs):
        response = self.forward(prompt=prompt, messages=messages, **kwargs)
        outputs = self._process_lm_response(response, prompt, messages, **kwargs)

        return outputs

    @with_callbacks
    async def acall(self, prompt=None, messages=None, **kwargs):
        response = await self.aforward(prompt=prompt, messages=messages, **kwargs)
        outputs = self._process_lm_response(response, prompt, messages, **kwargs)
        return outputs

    def forward(self, prompt=None, messages=None, **kwargs):
        """Forward pass for the language model.

        Subclasses must implement this method, and the response should be identical to
        [OpenAI response format](https://platform.openai.com/docs/api-reference/responses/object).
        """
        raise NotImplementedError("Subclasses must implement this method.")

    async def aforward(self, prompt=None, messages=None, **kwargs):
        """Async forward pass for the language model.

        Subclasses that support async should implement this method, and the response should be identical to
        [OpenAI response format](https://platform.openai.com/docs/api-reference/responses/object).
        """
        raise NotImplementedError("Subclasses must implement this method.")

    def copy(self, **kwargs):
        """Returns a copy of the language model with possibly updated parameters.

        Any provided keyword arguments update the corresponding attributes or LM kwargs of
        the copy. For example, ``lm.copy(rollout_id=1, temperature=1.0)`` returns an LM whose
        requests use a different rollout ID at non-zero temperature to bypass cache collisions.
        """

        import copy

        new_instance = copy.deepcopy(self)
        new_instance.history = []

        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(new_instance, key, value)
            if (key in self.kwargs) or (not hasattr(self, key)):
                if value is None:
                    new_instance.kwargs.pop(key, None)
                else:
                    new_instance.kwargs[key] = value
        if hasattr(new_instance, "_warned_zero_temp_rollout"):
            new_instance._warned_zero_temp_rollout = False

        return new_instance

    def inspect_history(self, n: int = 1):
        return pretty_print_history(self.history, n)

    def update_history(self, entry):
        if settings.disable_history:
            return

        # Global LM history
        if len(GLOBAL_HISTORY) >= MAX_HISTORY_SIZE:
            GLOBAL_HISTORY.pop(0)

        GLOBAL_HISTORY.append(entry)

        if settings.max_history_size == 0:
            return

        # dspy.LM.history
        if len(self.history) >= settings.max_history_size:
            self.history.pop(0)

        self.history.append(entry)

        # Per-module history
        caller_modules = settings.caller_modules or []
        for module in caller_modules:
            if len(module.history) >= settings.max_history_size:
                module.history.pop(0)
            module.history.append(entry)

    def _process_completion(self, response, merged_kwargs):
        """Process the response of OpenAI chat completion API and extract outputs.

        Args:
            response: The OpenAI chat completion response
                https://platform.openai.com/docs/api-reference/chat/object
            merged_kwargs: Merged kwargs from self.kwargs and method kwargs

        Returns:
            List of processed outputs
        """
        outputs = []
        for c in response.choices:
            output = {}
            output["text"] = c.message.content if hasattr(c, "message") else c["text"]
            if merged_kwargs.get("logprobs"):
                output["logprobs"] = c.logprobs if hasattr(c, "logprobs") else c["logprobs"]
            if hasattr(c, "message") and getattr(c.message, "tool_calls", None):
                output["tool_calls"] = c.message.tool_calls

            # Extract citations from LiteLLM response if available
            citations = self._extract_citations_from_response(c)
            if citations:
                output["citations"] = citations

            outputs.append(output)

        if all(len(output) == 1 for output in outputs):
            # Return a list if every output only has "text" key
            outputs = [output["text"] for output in outputs]

        return outputs

    def _extract_citations_from_response(self, choice):
        """Extract citations from LiteLLM response if available.
        Reference: https://docs.litellm.ai/docs/providers/anthropic#beta-citations-api

        Args:
            choice: The choice object from response.choices

        Returns:
            A list of citation dictionaries or None if no citations found
        """
        try:
            # Check for citations in LiteLLM provider_specific_fields
            citations_data = choice.message.provider_specific_fields.get("citations")
            if isinstance(citations_data, list):
                return [citation for citations in citations_data for citation in citations]
        except Exception:
            return None

    def _process_response(self, response):
        """Process the response of OpenAI Response API and extract outputs.

        Args:
            response: OpenAI Response API response
                https://platform.openai.com/docs/api-reference/responses/object

        Returns:
            List of processed outputs, which is always of size 1 because the Response API only supports one output.
        """
        text_outputs = []
        tool_calls = []
        reasoning_contents = []

        for output_item in response.output:
            output_item_type = output_item.type
            if output_item_type == "message":
                for content_item in output_item.content:
                    text_outputs.append(content_item.text)
            elif output_item_type == "function_call":
                tool_calls.append(output_item.model_dump())
            elif output_item_type == "reasoning":
                if getattr(output_item, "content", None) and len(output_item.content) > 0:
                    for content_item in output_item.content:
                        reasoning_contents.append(content_item.text)
                elif getattr(output_item, "summary", None) and len(output_item.summary) > 0:
                    for summary_item in output_item.summary:
                        reasoning_contents.append(summary_item.text)

        result = {}
        if len(text_outputs) > 0:
            result["text"] = "".join(text_outputs)
        if len(tool_calls) > 0:
            result["tool_calls"] = tool_calls
        if len(reasoning_contents) > 0:
            result["reasoning_content"] = "".join(reasoning_contents)
        # All `response.output` items map to one answer, so we return a list of size 1.
        return [result]


def inspect_history(n: int = 1):
    """The global history shared across all LMs."""
    return pretty_print_history(GLOBAL_HISTORY, n)


--- tests/README.md ---
The tests in this directory are primarily concerned with code correctness and Adapter reliability.

If you're looking for testing the end-to-end quality of DSPy modules and optimizer, refer to [LangProBe](https://github.com/Shangyint/langProBe).

--- tests/reliability/README.md ---
# DSPy Reliability Tests

This directory contains reliability tests for DSPy programs. The purpose of these tests is to verify that DSPy programs reliably produce expected outputs across multiple large language models (LLMs), regardless of model size or capability. These tests are designed to ensure that DSPy programs maintain robustness and accuracy across diverse LLM configurations.

### Overview

Each test in this directory executes a DSPy program using various LLMs. By running the same tests across different models, these tests help validate that DSPy programs handle a wide range of inputs effectively and produce reliable outputs, even in cases where the model might struggle with the input or task.

### Key Features

- **Diverse LLMs**: Each DSPy program is tested with multiple LLMs, ranging from smaller models to more advanced, high-performance models. This approach allows us to assess the consistency and generality of DSPy program outputs across different model capabilities.
- **Challenging and Adversarial Tests**: Some of the tests are intentionally challenging or adversarial, crafted to push the boundaries of DSPy. These challenging cases allow us to gauge the robustness of DSPy and identify areas for potential improvement.
- **Cross-Model Compatibility**: By testing with different LLMs, we aim to ensure that DSPy programs perform well across model types and configurations, reducing model-specific edge cases and enhancing program versatility.

### Running the Tests

- First, populate the configuration file `reliability_tests_conf.yaml` (located in this directory) with the necessary LiteLLM model/provider names and access credentials for 1. each LLM you want to test and 2. the LLM judge that you want to use for assessing the correctness of outputs in certain test cases. These should be placed in the `litellm_params` section for each model in the defined `model_list`. You can also use `litellm_params` to specify values for LLM hyperparameters like `temperature`. Any model that lacks configured `litellm_params` in the configuration file will be ignored during testing.

  The configuration must also specify a DSPy adapter to use when testing, e.g. `"chat"` (for `dspy.ChatAdapter`) or `"json"` (for `dspy.JSONAdapter`).

  An example of `reliability_tests_conf.yaml`:

      ```yaml
      adapter: chat
      model_list:
        # The model to use for judging the correctness of program
        # outputs throughout reliability test suites. We recommend using
        # a high quality model as the judge, such as OpenAI GPT-4o
        - model_name: "judge"
          litellm_params:
            model: "openai/gpt-4o"
            api_key: "<my_openai_api_key>"
        - model_name: "gpt-4o"
          litellm_params:
            model: "openai/gpt-4o"
            api_key: "<my_openai_api_key>"
        - model_name: "claude-3.5-sonnet"
          litellm_params:
            model: "anthropic/claude-3.5"
            api_key: "<my_anthropic_api_key>"

- Second, to run the tests, run the following command from this directory:

  ```bash
      pytest .
  ```

  This will execute all tests for the configured models and display detailed results for each model configuration. Tests are set up to mark expected failures for known challenging cases where a specific model might struggle, while actual (unexpected) DSPy reliability issues are flagged as failures (see below).

#### Running specific generated tests

You can run specific generated tests by using the `-k` flag with `pytest`. For example, to test the generated program located at `tests/reliability/complex_types/generated/test_nesting_1` against generated test input `input1.json`, you can run the following command from this directory:

```bash
pytest test_generated.py -k "test_nesting_1-input1"
```

### Test generation

You can generate test DSPy programs and test inputs from text descriptions using the `tests.reliability.generate` CLI, or the `tests.reliability.generate.generate_test_cases` API. For example, to generate a test classification program and 3 challenging test inputs in the `tests/reliability/classification/generated` directory, you can run the following command from the DSPy repository root directory:

```bash
python \
    -m tests.reliability.generate \
    -d tests/reliability/classification/generated/test_example \
    -p "Generate a program that performs a classification task involving objects with multiple properties. The task should be realistic" \
    -i "Based on the program description, generate a challenging example" \
    -n 3
```

The test program will be written to `tests/reliability/classification/generated/test_example/program.py`, and the test inputs will be written as JSON files to the `tests/reliability/classification/generated/test_exaple/inputs/` directory.

All generated tests should be located in directories with the structure `tests/reliability/<test_type>/generated/<test_name>`, where `<test_type>` is the type of test (e.g., `classification`, `complex_types`, `chat`, etc.), and `<test_name>` is a descriptive name for the test.

### Known Failing Models

Some tests may be expected to fail with certain models, especially in challenging cases. These known failures are logged but do not affect the overall test result. This setup allows us to keep track of model-specific limitations without obstructing general test outcomes. Models that are known to fail a particular test case are specified using the `@known_failing_models` decorator. For example:

```
@known_failing_models(["llama-3.2-3b-instruct"])
def test_program_with_complex_deeply_nested_output_structure():
    ...
```


--- tests/conftest.py ---
import copy
import os

import pytest

from tests.test_utils.server import litellm_test_server, read_litellm_test_server_request_logs  # noqa: F401

SKIP_DEFAULT_FLAGS = ["reliability", "extra", "llm_call"]


@pytest.fixture(autouse=True)
def clear_settings():
    """Ensures that the settings are cleared after each test."""

    yield

    import dspy
    from dspy.dsp.utils.settings import DEFAULT_CONFIG

    dspy.settings.configure(**copy.deepcopy(DEFAULT_CONFIG), inherit_config=False)


@pytest.fixture
def anyio_backend():
    return "asyncio"


# Taken from: https://gist.github.com/justinmklam/b2aca28cb3a6896678e2e2927c6b6a38
def pytest_addoption(parser):
    for flag in SKIP_DEFAULT_FLAGS:
        parser.addoption(
            f"--{flag}",
            action="store_true",
            default=False,
            help=f"run {flag} tests",
        )


def pytest_configure(config):
    for flag in SKIP_DEFAULT_FLAGS:
        config.addinivalue_line("markers", flag)


def pytest_collection_modifyitems(config, items):
    for flag in SKIP_DEFAULT_FLAGS:
        if config.getoption(f"--{flag}"):
            return

        skip_mark = pytest.mark.skip(reason=f"need --{flag} option to run")
        for item in items:
            if flag in item.keywords:
                item.add_marker(skip_mark)


@pytest.fixture
def lm_for_test():
    model = os.environ.get("LM_FOR_TEST", None)
    if model is None:
        pytest.skip("LM_FOR_TEST is not set in the environment variables")
    return model


--- tests/__init__.py ---


--- tests/reliability/conftest.py ---
import os

import pytest

import dspy
from ..conftest import clear_settings
from ..reliability.utils import get_adapter, parse_reliability_conf_yaml

# Standard list of models that should be used for periodic DSPy reliability testing
MODEL_LIST = [
    "gpt-4o",
    "gpt-4o-mini",
    "gpt-4-turbo",
    "gpt-o1-preview",
    "gpt-o1-mini",
    "claude-3.5-sonnet",
    "claude-3.5-haiku",
    "gemini-1.5-pro",
    "gemini-1.5-flash",
    "llama-3.1-405b-instruct",
    "llama-3.1-70b-instruct",
    "llama-3.1-8b-instruct",
    "llama-3.2-3b-instruct",
    "deepseek-r1",
]


def pytest_generate_tests(metafunc):
    """
    Hook to parameterize reliability test cases with each model defined in the
    reliability tests YAML configuration
    """
    known_failing_models = getattr(metafunc.function, "_known_failing_models", [])

    if "configure_model" in metafunc.fixturenames:
        params = [(model, model in known_failing_models) for model in MODEL_LIST]
        ids = [f"{model}" for model, _ in params]  # Custom IDs for display
        metafunc.parametrize("configure_model", params, indirect=True, ids=ids)


@pytest.fixture(autouse=True)
def configure_model(request):
    """
    Fixture to configure the DSPy library with a particular configured model and adapter
    before executing a test case.
    """
    module_dir = os.path.dirname(os.path.abspath(__file__))
    conf_path = os.path.join(module_dir, "reliability_conf.yaml")
    reliability_conf = parse_reliability_conf_yaml(conf_path)
    adapter = get_adapter(reliability_conf)

    model_name, should_ignore_failure = request.param
    model_params = reliability_conf.models.get(model_name)
    if model_params:
        lm = dspy.LM(**model_params)
        dspy.configure(lm=lm, adapter=adapter)
    else:
        pytest.skip(
            f"Skipping test because no reliability testing YAML configuration was found"
            f" for model {model_name}, or the YAML configuration is missing LiteLLM parameters"
            f" for this model ('litellm_params' section of conf file is missing)."
        )

    # Store `should_ignore_failure` flag on the request node for use in post-test handling
    request.node.should_ignore_failure = should_ignore_failure
    request.node.model_name = model_name


@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    """
    Hook to conditionally ignore failures in a given test case for known failing models.
    """
    outcome = yield
    rep = outcome.get_result()

    should_ignore_failure = getattr(item, "should_ignore_failure", False)

    if should_ignore_failure and rep.failed:
        rep.outcome = "passed"
        rep.wasxfail = "Ignoring failure for known failing model"


--- tests/reliability/__init__.py ---


--- tests/test_utils/__init__.py ---


--- tests/utils/__init__.py ---


--- tests/signatures/test_adapter_image.py ---
import os
import tempfile
from io import BytesIO

import pydantic
import pytest
import requests
from PIL import Image as PILImage

import dspy
from dspy.adapters.types.image import encode_image
from dspy.utils.dummies import DummyLM


@pytest.fixture
def sample_pil_image():
    """Fixture to provide a sample image for testing"""
    url = "https://images.dog.ceo/breeds/dane-great/n02109047_8912.jpg"
    response = requests.get(url)
    response.raise_for_status()
    return PILImage.open(BytesIO(response.content))


@pytest.fixture
def sample_dspy_image_download():
    url = "https://images.dog.ceo/breeds/dane-great/n02109047_8912.jpg"
    return dspy.Image(url, download=True)


@pytest.fixture
def sample_url():
    return "https://images.dog.ceo/breeds/dane-great/n02109047_8912.jpg"


@pytest.fixture
def sample_dspy_image_no_download():
    return dspy.Image("https://images.dog.ceo/breeds/dane-great/n02109047_8912.jpg")


def count_messages_with_image_url_pattern(messages):
    pattern = {"type": "image_url", "image_url": {"url": lambda x: isinstance(x, str)}}

    try:

        def check_pattern(obj, pattern):
            if isinstance(pattern, dict):
                if not isinstance(obj, dict):
                    return False
                return all(k in obj and check_pattern(obj[k], v) for k, v in pattern.items())
            if callable(pattern):
                return pattern(obj)
            return obj == pattern

        def count_patterns(obj, pattern):
            count = 0
            if check_pattern(obj, pattern):
                count += 1
            if isinstance(obj, dict):
                count += sum(count_patterns(v, pattern) for v in obj.values())
            if isinstance(obj, (list, tuple)):
                count += sum(count_patterns(v, pattern) for v in obj)
            return count

        return count_patterns(messages, pattern)
    except Exception:
        return 0


def setup_predictor(signature, expected_output):
    """Helper to set up a predictor with DummyLM"""
    lm = DummyLM([expected_output])
    dspy.settings.configure(lm=lm)
    return dspy.Predict(signature), lm


@pytest.mark.parametrize(
    "test_case",
    [
        {
            "name": "probabilistic_classification",
            "signature": "image: dspy.Image, class_labels: list[str] -> probabilities: dict[str, float]",
            "inputs": {"image": "https://example.com/dog.jpg", "class_labels": ["dog", "cat", "bird"]},
            "key_output": "probabilities",
            "expected": {"probabilities": {"dog": 0.8, "cat": 0.1, "bird": 0.1}},
        },
        {
            "name": "image_to_code",
            "signature": "ui_image: dspy.Image, target_language: str -> generated_code: str",
            "inputs": {"ui_image": "https://example.com/button.png", "target_language": "HTML"},
            "key_output": "generated_code",
            "expected": {"generated_code": "<button>Click me</button>"},
        },
        {
            "name": "bbox_detection",
            "signature": "image: dspy.Image -> bboxes: list[Tuple[int, int, int, int]]",
            "inputs": {"image": "https://example.com/image.jpg"},
            "key_output": "bboxes",
            "expected": {"bboxes": [(10, 20, 30, 40), (50, 60, 70, 80)]},
        },
        {
            "name": "multilingual_caption",
            "signature": "image: dspy.Image, languages: list[str] -> captions: dict[str, str]",
            "inputs": {"image": "https://example.com/dog.jpg", "languages": ["en", "es", "fr"]},
            "key_output": "captions",
            "expected": {
                "captions": {"en": "A golden retriever", "es": "Un golden retriever", "fr": "Un golden retriever"}
            },
        },
    ],
)
def test_basic_image_operations(test_case):
    """Consolidated test for basic image operations"""
    predictor, lm = setup_predictor(test_case["signature"], test_case["expected"])

    # Convert string URLs to dspy.Image objects
    inputs = {
        k: dspy.Image(v) if isinstance(v, str) and k in ["image", "ui_image"] else v
        for k, v in test_case["inputs"].items()
    }

    result = predictor(**inputs)

    # Check result based on output field name
    output_field = next(f for f in ["probabilities", "generated_code", "bboxes", "captions"] if hasattr(result, f))
    assert getattr(result, output_field) == test_case["expected"][test_case["key_output"]]
    assert count_messages_with_image_url_pattern(lm.history[-1]["messages"]) == 1


@pytest.mark.parametrize(
    "image_input,description",
    [
        ("pil_image", "PIL Image"),
        ("encoded_pil_image", "encoded PIL image string"),
        ("dspy_image_download", "dspy.Image with download=True"),
        ("dspy_image_no_download", "dspy.Image without download"),
    ],
)
def test_image_input_formats(
    request, sample_pil_image, sample_dspy_image_download, sample_dspy_image_no_download, image_input, description
):
    """Test different input formats for image fields"""
    signature = "image: dspy.Image, class_labels: list[str] -> probabilities: dict[str, float]"
    expected = {"probabilities": {"dog": 0.8, "cat": 0.1, "bird": 0.1}}
    predictor, lm = setup_predictor(signature, expected)

    input_map = {
        "pil_image": sample_pil_image,
        "encoded_pil_image": encode_image(sample_pil_image),
        "dspy_image_download": sample_dspy_image_download,
        "dspy_image_no_download": sample_dspy_image_no_download,
    }

    actual_input = input_map[image_input]
    # TODO(isaacbmiller): Support the cases without direct dspy.Image coercion
    if image_input in ["pil_image", "encoded_pil_image"]:
        pytest.xfail(f"{description} not fully supported without dspy.Image coercion")

    result = predictor(image=actual_input, class_labels=["dog", "cat", "bird"])
    assert result.probabilities == expected["probabilities"]
    assert count_messages_with_image_url_pattern(lm.history[-1]["messages"]) == 1


def test_predictor_save_load(sample_url, sample_pil_image):
    """Test saving and loading predictors with image fields"""
    signature = "image: dspy.Image -> caption: str"
    examples = [
        dspy.Example(image=dspy.Image(sample_url), caption="Example 1"),
        dspy.Example(image=sample_pil_image, caption="Example 2"),
    ]

    predictor, lm = setup_predictor(signature, {"caption": "A golden retriever"})
    optimizer = dspy.teleprompt.LabeledFewShot(k=1)
    compiled_predictor = optimizer.compile(student=predictor, trainset=examples, sample=False)

    with tempfile.NamedTemporaryFile(mode="w+", delete=True, suffix=".json") as temp_file:
        compiled_predictor.save(temp_file.name)
        loaded_predictor = dspy.Predict(signature)
        loaded_predictor.load(temp_file.name)

    loaded_predictor(image=dspy.Image("https://example.com/dog.jpg"))
    assert count_messages_with_image_url_pattern(lm.history[-1]["messages"]) == 2
    assert "<DSPY_IMAGE_START>" not in str(lm.history[-1]["messages"])


def test_save_load_complex_default_types():
    """Test saving and loading predictors with complex default types (lists of images)"""
    examples = [
        dspy.Example(
            image_list=[
                dspy.Image("https://example.com/dog.jpg"),
                dspy.Image("https://example.com/cat.jpg"),
            ],
            caption="Example 1",
        ).with_inputs("image_list"),
    ]

    class ComplexTypeSignature(dspy.Signature):
        image_list: list[dspy.Image] = dspy.InputField(desc="A list of images")
        caption: str = dspy.OutputField(desc="A caption for the image list")

    predictor, lm = setup_predictor(ComplexTypeSignature, {"caption": "A list of images"})
    optimizer = dspy.teleprompt.LabeledFewShot(k=1)
    compiled_predictor = optimizer.compile(student=predictor, trainset=examples, sample=False)

    with tempfile.NamedTemporaryFile(mode="w+", delete=True, suffix=".json") as temp_file:
        compiled_predictor.save(temp_file.name)
        loaded_predictor = dspy.Predict(ComplexTypeSignature)
        loaded_predictor.load(temp_file.name)

    result = loaded_predictor(**examples[0].inputs())
    assert result.caption == "A list of images"
    assert str(lm.history[-1]["messages"]).count("'url'") == 4
    assert "<DSPY_IMAGE_START>" not in str(lm.history[-1]["messages"])


class BasicImageSignature(dspy.Signature):
    """Basic signature with a single image input"""

    image: dspy.Image = dspy.InputField()
    output: str = dspy.OutputField()


class ImageListSignature(dspy.Signature):
    """Signature with a list of images input"""

    image_list: list[dspy.Image] = dspy.InputField()
    output: str = dspy.OutputField()


@pytest.mark.parametrize(
    "test_case",
    [
        {
            "name": "basic_dspy_signature",
            "signature_class": BasicImageSignature,
            "inputs": {"image": "https://example.com/dog.jpg"},
            "expected": {"output": "A dog photo"},
            "expected_image_urls": 2,
        },
        {
            "name": "list_dspy_signature",
            "signature_class": ImageListSignature,
            "inputs": {"image_list": ["https://example.com/dog.jpg", "https://example.com/cat.jpg"]},
            "expected": {"output": "Multiple photos"},
            "expected_image_urls": 4,
        },
    ],
)
def test_save_load_complex_types(test_case):
    """Test saving and loading predictors with complex types"""
    signature_cls = test_case["signature_class"]

    # Convert string URLs to dspy.Image objects in input
    processed_input = {}
    for key, value in test_case["inputs"].items():
        if isinstance(value, str) and "http" in value:
            processed_input[key] = dspy.Image(value)
        elif isinstance(value, list) and value and isinstance(value[0], str):
            processed_input[key] = [dspy.Image(url) for url in value]
        else:
            processed_input[key] = value

    # Create example and predictor
    examples = [dspy.Example(**processed_input, **test_case["expected"]).with_inputs(*processed_input.keys())]

    predictor, lm = setup_predictor(signature_cls, test_case["expected"])
    optimizer = dspy.teleprompt.LabeledFewShot(k=1)
    compiled_predictor = optimizer.compile(student=predictor, trainset=examples, sample=False)

    # Test save and load
    with tempfile.NamedTemporaryFile(mode="w+", delete=True, suffix=".json") as temp_file:
        compiled_predictor.save(temp_file.name)
        loaded_predictor = dspy.Predict(signature_cls)
        loaded_predictor.load(temp_file.name)

    # Run prediction
    result = loaded_predictor(**processed_input)

    # Verify output matches expected
    for key, value in test_case["expected"].items():
        assert getattr(result, key) == value

    # Verify correct number of image URLs in messages
    assert count_messages_with_image_url_pattern(lm.history[-1]["messages"]) == test_case["expected_image_urls"]
    assert "<DSPY_IMAGE_START>" not in str(lm.history[-1]["messages"])


def test_save_load_pydantic_model():
    """Test saving and loading predictors with pydantic models"""

    class ImageModel(pydantic.BaseModel):
        image: dspy.Image
        image_list: list[dspy.Image] | None = None
        output: str

    class PydanticSignature(dspy.Signature):
        model_input: ImageModel = dspy.InputField()
        output: str = dspy.OutputField()

    # Create model instance
    model_input = ImageModel(
        image=dspy.Image("https://example.com/dog.jpg"),
        image_list=[dspy.Image("https://example.com/cat.jpg")],
        output="Multiple photos",
    )

    # Create example and predictor
    examples = [dspy.Example(model_input=model_input, output="Multiple photos").with_inputs("model_input")]

    predictor, lm = setup_predictor(PydanticSignature, {"output": "Multiple photos"})
    optimizer = dspy.teleprompt.LabeledFewShot(k=1)
    compiled_predictor = optimizer.compile(student=predictor, trainset=examples, sample=False)

    # Test save and load
    with tempfile.NamedTemporaryFile(mode="w+", delete=True, suffix=".json") as temp_file:
        compiled_predictor.save(temp_file.name)
        loaded_predictor = dspy.Predict(PydanticSignature)
        loaded_predictor.load(temp_file.name)

    # Run prediction
    result = loaded_predictor(model_input=model_input)

    # Verify output matches expected
    assert result.output == "Multiple photos"
    assert count_messages_with_image_url_pattern(lm.history[-1]["messages"]) == 4
    assert "<DSPY_IMAGE_START>" not in str(lm.history[-1]["messages"])


def test_optional_image_field():
    """Test that optional image fields are not required"""

    class OptionalImageSignature(dspy.Signature):
        image: dspy.Image | None = dspy.InputField()
        output: str = dspy.OutputField()

    predictor, lm = setup_predictor(OptionalImageSignature, {"output": "Hello"})
    result = predictor(image=None)
    assert result.output == "Hello"
    assert count_messages_with_image_url_pattern(lm.history[-1]["messages"]) == 0


def test_pdf_url_support():
    """Test support for PDF files from URLs"""
    pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"

    # Create a dspy.Image object from the PDF URL with download=True
    pdf_image = dspy.Image(pdf_url, download=True)

    # The data URI should contain application/pdf in the MIME type
    assert "data:application/pdf" in pdf_image.url
    assert ";base64," in pdf_image.url

    # Test using it in a predictor
    class PDFSignature(dspy.Signature):
        document: dspy.Image = dspy.InputField(desc="A PDF document")
        summary: str = dspy.OutputField(desc="A summary of the PDF")

    predictor, lm = setup_predictor(PDFSignature, {"summary": "This is a dummy PDF"})
    result = predictor(document=pdf_image)

    assert result.summary == "This is a dummy PDF"
    assert count_messages_with_image_url_pattern(lm.history[-1]["messages"]) == 1

    # Ensure the URL was properly expanded in messages
    messages_str = str(lm.history[-1]["messages"])
    assert "application/pdf" in messages_str


def test_different_mime_types():
    """Test support for different file types and MIME type detection"""
    # Test with various file types
    file_urls = {
        "pdf": "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf",
        "image": "https://images.dog.ceo/breeds/dane-great/n02109047_8912.jpg",
    }

    expected_mime_types = {
        "pdf": "application/pdf",
        "image": "image/jpeg",
    }

    for file_type, url in file_urls.items():
        # Download and encode
        encoded = encode_image(url, download_images=True)

        # Check for correct MIME type in the encoded data - using 'in' instead of startswith
        # to account for possible parameters in the MIME type
        assert f"data:{expected_mime_types[file_type]}" in encoded
        assert ";base64," in encoded


def test_mime_type_from_response_headers():
    """Test that MIME types from response headers are correctly used"""
    # This URL returns proper Content-Type header
    pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"

    # Make an actual request to get the content type from headers
    response = requests.get(pdf_url)
    expected_mime_type = response.headers.get("Content-Type", "")

    # Should be application/pdf or similar
    assert "pdf" in expected_mime_type.lower()

    # Encode with download to test MIME type from headers
    encoded = encode_image(pdf_url, download_images=True)

    # The encoded data should contain the correct MIME type
    assert "application/pdf" in encoded
    assert ";base64," in encoded


def test_pdf_from_file():
    """Test handling a PDF file from disk"""
    # Download a PDF to a temporary file
    pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
    response = requests.get(pdf_url)
    response.raise_for_status()

    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_file:
        tmp_file.write(response.content)
        tmp_file_path = tmp_file.name

    try:
        # Create a dspy.Image from the file
        pdf_image = dspy.Image(tmp_file_path)

        # The constructor encodes the file into a data URI we can inspect directly
        assert "data:application/pdf" in pdf_image.url
        assert ";base64," in pdf_image.url

        # Test the image in a predictor
        class FilePDFSignature(dspy.Signature):
            document: dspy.Image = dspy.InputField(desc="A PDF document from file")
            summary: str = dspy.OutputField(desc="A summary of the PDF")

        predictor, lm = setup_predictor(FilePDFSignature, {"summary": "This is a PDF from file"})
        result = predictor(document=pdf_image)

        assert result.summary == "This is a PDF from file"
        assert count_messages_with_image_url_pattern(lm.history[-1]["messages"]) == 1
    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_file_path)
        except Exception:
            pass


def test_image_repr():
    """Test string representation of Image objects"""
    url_image = dspy.Image("https://example.com/dog.jpg")
    assert str(url_image) == (
        "<<CUSTOM-TYPE-START-IDENTIFIER>>"
        '[{"type": "image_url", "image_url": {"url": "https://example.com/dog.jpg"}}]'
        "<<CUSTOM-TYPE-END-IDENTIFIER>>"
    )
    assert repr(url_image) == "Image(url='https://example.com/dog.jpg')"

    sample_pil = PILImage.new("RGB", (60, 30), color="red")
    pil_image = dspy.Image(sample_pil)
    assert str(pil_image).startswith('<<CUSTOM-TYPE-START-IDENTIFIER>>[{"type": "image_url",')
    assert str(pil_image).endswith("<<CUSTOM-TYPE-END-IDENTIFIER>>")
    assert "base64" in str(pil_image)


def test_from_methods_warn(tmp_path):
    """Deprecated from_* methods emit warnings"""
    tmp_file = tmp_path / "test.png"
    tmp_file.write_bytes(b"pngdata")

    with pytest.warns(DeprecationWarning):
        dspy.Image.from_url("https://example.com/dog.jpg")
    with pytest.warns(DeprecationWarning):
        dspy.Image.from_file(str(tmp_file))
    sample_pil = PILImage.new("RGB", (10, 10), color="blue")
    with pytest.warns(DeprecationWarning):
        dspy.Image.from_PIL(sample_pil)


def test_invalid_string_format():
    """Test that invalid string formats raise a ValueError"""
    invalid_string = "this_is_not_a_url_or_file"

    # Should raise a ValueError and not pass the string through
    with pytest.raises(ValueError, match="Unrecognized") as warning_info:
        image = dspy.Image(invalid_string)

def test_pil_image_with_download_parameter():
    """Test behavior when PIL image is passed with download=True"""
    sample_pil = PILImage.new("RGB", (60, 30), color="red")

    # PIL image should be encoded regardless of download parameter
    image_no_download = dspy.Image(sample_pil)
    image_with_download = dspy.Image(sample_pil, download=True)

    # Both should result in base64 encoded data URIs
    assert image_no_download.url.startswith("data:")
    assert image_with_download.url.startswith("data:")
    assert "base64," in image_no_download.url
    assert "base64," in image_with_download.url

    # They should be identical since PIL images are always encoded
    assert image_no_download.url == image_with_download.url


--- tests/adapters/test_adapter_utils.py ---
# ruff: noqa: UP007

from typing import Literal, Optional, Union

import pytest
from pydantic import BaseModel

from dspy.adapters.utils import parse_value


class Profile(BaseModel):
    name: str
    age: int


def test_parse_value_str_annotation():
    # Test basic string conversion
    assert parse_value(123, str) == "123"
    assert parse_value(True, str) == "True"
    assert parse_value("hello", str) == "hello"
    assert parse_value(None, str) == "None"
    assert parse_value([1, 2, 3], str) == "[1, 2, 3]"


def test_parse_value_pydantic_types():
    # Test with pydantic BaseModel - JSON string input
    json_str = '{"name": "John", "age": 30}'
    result = parse_value(json_str, Profile)
    assert isinstance(result, Profile)
    assert result.name == "John"
    assert result.age == 30

    # Test with pydantic BaseModel - dict input
    dict_input = {"name": "Jane", "age": 25}
    result = parse_value(dict_input, Profile)
    assert isinstance(result, Profile)
    assert result.name == "Jane"
    assert result.age == 25

    # Test with invalid pydantic data
    with pytest.raises(Exception):
        parse_value('{"name": "John"}', Profile)  # missing required age field


def test_parse_value_basic_types():
    # Test int
    assert parse_value("42", int) == 42
    assert parse_value(42, int) == 42

    # Test float
    assert parse_value("3.14", float) == 3.14
    assert parse_value(3.14, float) == 3.14

    # Test bool
    assert parse_value("true", bool) is True
    assert parse_value(True, bool) is True
    assert parse_value("false", bool) is False

    # Test list
    assert parse_value("[1, 2, 3]", list[int]) == [1, 2, 3]
    assert parse_value([1, 2, 3], list[int]) == [1, 2, 3]


def test_parse_value_literal():
    # Test Literal type
    assert parse_value("option1", Literal["option1", "option2"]) == "option1"
    assert parse_value("option2", Literal["option1", "option2"]) == "option2"

    # Test Literal with quotes and prefixes
    assert parse_value("'option1'", Literal["option1", "option2"]) == "option1"
    assert parse_value('"option1"', Literal["option1", "option2"]) == "option1"
    assert parse_value("Literal[option1]", Literal["option1", "option2"]) == "option1"
    assert parse_value("str[option1]", Literal["option1", "option2"]) == "option1"

    # Test invalid literal
    with pytest.raises(ValueError):
        parse_value("invalid", Literal["option1", "option2"])


def test_parse_value_union():
    # Test Union with None (Optional)
    assert parse_value("test", Optional[str]) == "test"
    assert parse_value("test", str | None) == "test"
    assert parse_value("5", int | None) == 5
    assert parse_value(None, Optional[str]) is None
    assert parse_value("text with [placeholder]", Optional[str]) == "text with [placeholder]"
    assert parse_value("text with [placeholder]", str | None) == "text with [placeholder]"

    # Test Union fallback to str
    assert parse_value("fallback", Union[int, str, None]) == "fallback"
    assert parse_value(5, Union[int, str, None]) == 5
    assert parse_value("fallback", int | str | None) == "fallback"
    assert parse_value(5, int | str | None) == 5
    assert parse_value("text with [placeholder]", Union[int, str, None]) == "text with [placeholder]"


def test_parse_value_json_repair():
    # Test cases where json_repair is needed
    assert parse_value('{"key": "value"}', dict) == {"key": "value"}

    # Test ast.literal_eval fallback
    assert parse_value("{'key': 'value'}", dict) == {"key": "value"}

    # Test fallback to original value when parsing fails
    malformed = "not json or literal"
    with pytest.raises(Exception):
        parse_value(malformed, dict)


--- artifacts/mlflow/mlflow/mlflow-llms.txt ---
# Mlflow

> MLflow is an open-source platform to build AI/LLM applications and models with confidence, providing end-to-end experiment tracking, observability, evaluation, and model management in one integrated solution.

**Remember:**
- Experiment Tracking
- Model Registry
- Deployment Tools
- LLM Evaluation
- Tracing / Observability
- Prompt Management

## Docs
- [Mlflow Tracking](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/quickstart/mlflow_tracking.py): install & quickstart.
- [Pythonmodel Type Hints Quickstart](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/demos/pythonmodel_type_hints_quickstart.ipynb): install & quickstart.
- [Sentence Transformers Quickstart](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/classic-ml/deep-learning/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb): install & quickstart.
- [Quickstart Drilldown](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/quickstart_drilldown/index.mdx): install & quickstart.
- [Getting Started](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/classic-ml/getting-started/index.mdx): install & quickstart.
- [Getting Started](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/genai/getting-started/index.mdx): install & quickstart.
- [Databricks Trial](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/genai/getting-started/databricks-trial/index.mdx): install & quickstart.
- [Hyperparameter Tuning](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/classic-ml/getting-started/hyperparameter-tuning/index.mdx): install & quickstart.
- [Logging First Model](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/classic-ml/getting-started/logging-first-model/index.mdx): install & quickstart.
- [Quickstart](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/classic-ml/tracking/quickstart/index.mdx): install & quickstart.

## Tutorials
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/auth/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/deployments/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/evaluation/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/gateway/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/lightgbm/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/llms/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/mlflow_artifacts/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/pyfunc/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/pyspark_ml_autologging/README.md): install & quickstart.

## API
- [Api Request Parallel Processor](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/langchain/api_request_parallel_processor.py): API reference.
- [Api Request Parallel Processor](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/openai/api_request_parallel_processor.py): API reference.
- [Capture Modules](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/utils/_capture_modules.py): docs page.
- [Capture Transformers Modules](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/utils/_capture_transformers_modules.py): docs page.
- [Class Utils](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/utils/class_utils.py): docs page.
- [Fastapi App](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/server/fastapi_app.py): docs page.
- [Fastapi Security](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/server/fastapi_security.py): security policy.
- [Iris Data Module](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/pytorch/iris_data_module.py): docs page.
- [Job Api](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/server/job_api.py): docs page.
- [Otel Api](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/server/otel_api.py): docs page.

## Concepts
- [Design System](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/server/js/.storybook/decorators/design-system.js): docs page.

## Optional
- [Changelog](https://raw.githubusercontent.com/mlflow/mlflow/master/CHANGELOG.md): version history.
- [Contributing](https://raw.githubusercontent.com/mlflow/mlflow/master/CONTRIBUTING.md): docs page.
- [License](https://raw.githubusercontent.com/mlflow/mlflow/master/LICENSE.txt): usage terms.
- [Security](https://raw.githubusercontent.com/mlflow/mlflow/master/SECURITY.md): security policy.
- [Create Release Branch](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/create_release_branch.py): version history.
- [Create Release Tag](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/create_release_tag.py): version history.
- [Show Package Release Dates](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/show_package_release_dates.py): version history.
- [Update Changelog](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/update_changelog.py): version history.
- [Validate Release Version](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/validate_release_version.py): version history.
- [License](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/skinny/LICENSE.txt): usage terms.
- [Claude](https://raw.githubusercontent.com/mlflow/mlflow/master/CLAUDE.md): docs page.
- [Code Of Conduct](https://raw.githubusercontent.com/mlflow/mlflow/master/CODE_OF_CONDUCT.rst): docs page.
- [Committer](https://raw.githubusercontent.com/mlflow/mlflow/master/COMMITTER.md): docs page.
- [Extra Dependencies](https://raw.githubusercontent.com/mlflow/mlflow/master/EXTRA_DEPENDENCIES.rst): docs page.
- [Issue Policy](https://raw.githubusercontent.com/mlflow/mlflow/master/ISSUE_POLICY.md): docs page.
- [Issue Triage](https://raw.githubusercontent.com/mlflow/mlflow/master/ISSUE_TRIAGE.rst): docs page.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/README.md): docs page.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/bin/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/docker-compose/README.md): install & quickstart.
- [Prettier.Config](https://raw.githubusercontent.com/mlflow/mlflow/master/prettier.config.js): docs page.

## .Github
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/workflows/README.md): install & quickstart.
- [Pull Request Template](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/pull_request_template.md): docs page.
- [Advice](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/workflows/advice.js): docs page.
- [Autoformat](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/workflows/autoformat.js): docs page.
- [Autoformat](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/workflows/autoformat.md): docs page.
- [Cancel](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/workflows/cancel.js): docs page.
- [Closing Pr](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/workflows/closing-pr.js): docs page.
- [Cross Version Test Runner](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/workflows/cross-version-test-runner.js): docs page.
- [Cross Version Testing](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/workflows/cross-version-testing.md): docs page.
- [Delete Artifact](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/workflows/delete-artifact.js): docs page.

## Dev
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/clint/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/proto_to_graphql/README.md): install & quickstart.
- [Build](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/build.py): docs page.
- [Check Function Signatures](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/check_function_signatures.py): docs page.
- [Check Init Py](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/check_init_py.py): docs page.
- [Check Patch Prs](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/check_patch_prs.py): docs page.
- [Extract Deps](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/extract_deps.py): docs page.
- [Format](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/format.py): docs page.
- [Generate Protos](https://raw.githubusercontent.com/mlflow/mlflow/master/dev/generate_protos.py): docs page.

## Libs
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/skinny/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/tracing/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/typescript/README.md): install & quickstart.
- [Readme Skinny](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/skinny/README_SKINNY.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/typescript/core/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/typescript/integrations/openai/README.md): install & quickstart.
- [Jest.Global Server Setup](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/typescript/jest.global-server-setup.ts): docs page.
- [Jest.Global Server Teardown](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/typescript/jest.global-server-teardown.ts): docs page.
- [Jest.Config](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/typescript/core/jest.config.js): docs page.
- [Helper](https://raw.githubusercontent.com/mlflow/mlflow/master/libs/typescript/core/tests/helper.ts): docs page.

## Mlflow
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/claude_code/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/R/mlflow/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/java/client/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/store/db_migrations/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/server/js/src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/README.md): install & quickstart.
- [Public](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/server/js/public/index.html): docs page.
- [Oss Notebook Renderer](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/server/js/src/shared/web-shared/model-trace-explorer/oss-notebook-renderer/index.html): docs page.
- [Client](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/client.py): docs page.
- [Db](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/db.py): docs page.
- [Environment Variables](https://raw.githubusercontent.com/mlflow/mlflow/master/mlflow/environment_variables.py): docs page.

## Requirements
- [Constraints](https://raw.githubusercontent.com/mlflow/mlflow/master/requirements/constraints.txt): docs page.
- [Dev Requirements](https://raw.githubusercontent.com/mlflow/mlflow/master/requirements/dev-requirements.txt): docs page.
- [Doc Min Requirements](https://raw.githubusercontent.com/mlflow/mlflow/master/requirements/doc-min-requirements.txt): docs page.
- [Doc Requirements](https://raw.githubusercontent.com/mlflow/mlflow/master/requirements/doc-requirements.txt): docs page.
- [Extra Ml Requirements](https://raw.githubusercontent.com/mlflow/mlflow/master/requirements/extra-ml-requirements.txt): docs page.
- [Lint Requirements](https://raw.githubusercontent.com/mlflow/mlflow/master/requirements/lint-requirements.txt): docs page.
- [Skinny Test Requirements](https://raw.githubusercontent.com/mlflow/mlflow/master/requirements/skinny-test-requirements.txt): docs page.
- [Test Requirements](https://raw.githubusercontent.com/mlflow/mlflow/master/requirements/test-requirements.txt): docs page.

## Tests
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/db/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/resources/pyfunc_models/README.md): install & quickstart.
- [Check Mlflow Lazily Imports Ml Packages](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/check_mlflow_lazily_imports_ml_packages.py): docs page.
- [Conftest](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/conftest.py): docs page.
- [Generate Ui Test Data](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/generate_ui_test_data.py): docs page.
- [Helper Functions](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/helper_functions.py): docs page.
- [Init](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/__init__.py): docs page.
- [Test Cli](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/test_cli.py): docs page.
- [Test Environment Variables](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/test_environment_variables.py): docs page.
- [Test Exceptions](https://raw.githubusercontent.com/mlflow/mlflow/master/tests/test_exceptions.py): docs page.


--- artifacts/mlflow/mlflow/mlflow-llms-full.txt ---
# llms-full (private-aware)
> Built by GitHub fetches (raw or authenticated). Large files may be truncated.

--- examples/quickstart/mlflow_tracking.py ---
import os
from random import randint, random

from mlflow import log_artifacts, log_metric, log_param

if __name__ == "__main__":
    print("Running mlflow_tracking.py")

    log_param("param1", randint(0, 100))

    log_metric("foo", random())
    log_metric("foo", random() + 1)
    log_metric("foo", random() + 2)

    if not os.path.exists("outputs"):
        os.makedirs("outputs")
    with open("outputs/test.txt", "w") as f:
        f.write("hello world!")

    log_artifacts("outputs")


--- examples/demos/pythonmodel_type_hints_quickstart.ipynb ---
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66132127-faa2-43d7-a23d-94035f0dc6a4",
   "metadata": {},
   "source": [
    "# PythonModel with type hints Quickstart\n",
    "This notebook will demonstrates how to use type hints for data validation in MLflow PythonModel.\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Install required packages: `pip install mlflow==2.20.0 openai==1.65.4`\n",
    "\n",
    "Set your OpenAI API key with `os.environ[\"OPENAI_API_KEY\"]=\"<YOUR_KEY>\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce58023-e1ab-4bf9-8325-0e706e327c54",
   "metadata": {},
   "source": [
    "## Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30503174-848c-4017-85f6-87f1b90fce70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-B83LveI6Wc2RHEdbMwNGkFh8ZoehY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='MLflow is an open-source platform designed to manage the machine learning (ML) lifecycle, which includes components such as experimentation, reproducibility, and deployment. It provides a suite of tools to help data scientists and machine learning practitioners track experiments, package and share their code, and deploy models. Here are the key components of MLflow:\\n\\n1. **MLflow Tracking**: This component allows users to log and query experiments. You can track metrics, parameters, and artifacts (such as model files) associated with different runs of your machine learning models.\\n\\n2. **MLflow Projects**: This functionality enables users to package their data science code in a reusable and reproducible way. Projects are defined with a standard format that specifies their dependencies and how to run them.\\n\\n3. **MLflow Models**: This part of MLflow provides a way to manage and deploy machine learning models in various formats. It supports multiple model types, enabling users to deploy models in different environments, such as REST APIs, cloud services, or on-premises servers.\\n\\n4. **MLflow Registry**: This is a centralized model store that manages the lifecycle of machine learning models, including versioning, stage transitions (like staging, production, archived), and annotations. It allows teams to track model changes and collaborate more effectively.\\n\\nMLflow is designed to be flexible and integrates well with existing machine learning frameworks like TensorFlow, PyTorch, Scikit-Learn, and many others. This flexibility makes it widely adopted in production ML workflows and among research communities. \\n\\nOverall, MLflow aims to streamline the process of developing, tracking, and deploying machine learning models, reducing friction and enhancing collaboration among data science teams.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741259211, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_06737a9306', usage=CompletionUsage(completion_tokens=339, prompt_tokens=13, total_tokens=352, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import pydantic\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Use OpenAI model\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the MLflow?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1522dd-089e-4908-be5d-08de29deb683",
   "metadata": {},
   "source": [
    "## Use MLflow PythonModel with type hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a960b6b-3dc7-47e3-ac4e-a50ef296cae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DSPy is a Python library designed for creating and managing decision systems, particularly in the context of data-driven applications. It provides tools for building models that can make decisions based on inputs from various data sources. The library aims to simplify the process of developing and deploying decision logic, which is often a complex task in machine learning and artificial intelligence projects.\\n\\nKey features of DSPy may include:\\n\\n1. **Declarative Syntax**: Allowing users to express decision logic in a clear and concise manner.\\n2. **Integration with Data Sources**: Facilitating easy integration with various data workflows, making it simpler to utilize datasets for decision-making.\\n3. **Evaluation and Testing**: Providing tools for evaluating and testing decision-making models, ensuring their accuracy and reliability.\\n\\nBy leveraging DSPy, data scientists and developers can focus on building effective decision systems without getting bogged down by the complexities usually associated with programming these systems from scratch.\\n\\nFor the latest updates and more specific functionalities, it's a good idea to refer to the official DSPy documentation or repository, as libraries are frequently updated and improved.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define your input schema\n",
    "class Message(pydantic.BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "# inherit mlflow PythonModel\n",
    "class MyModel(mlflow.pyfunc.PythonModel):\n",
    "    # add type hint to model_input\n",
    "    def predict(self, model_input: list[Message]) -> str:\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=model_input)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "model.predict([{\"role\": \"user\", \"content\": \"What is DSPy?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d8e83c-3afa-4b75-80bf-1a8efa51a415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Failed to validate data against type hint `list[Message]`, invalid elements: [('What is DSPy?', \"Expecting example to be a dictionary or pydantic model instance for Pydantic type hint, got <class 'str'>\")]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# An incorrect input will trigger validation error\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is DSPy?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/repos/mlflow/mlflow/pyfunc/utils/data_validation.py:68\u001b[0m, in \u001b[0;36m_wrap_predict_with_pyfunc.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, MlflowException):\n\u001b[0;32m---> 68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to validate the input data against the type hint \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_info\u001b[38;5;241m.\u001b[39minput_type_hint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/repos/mlflow/mlflow/pyfunc/utils/data_validation.py:59\u001b[0m, in \u001b[0;36m_wrap_predict_with_pyfunc.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m         args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_model_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_input_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_type_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_param_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, MlflowException):\n",
      "File \u001b[0;32m~/Documents/repos/mlflow/mlflow/pyfunc/utils/data_validation.py:200\u001b[0m, in \u001b[0;36m_validate_model_input\u001b[0;34m(args, kwargs, model_input_index_in_sig, type_hint, model_input_param_name)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     data \u001b[38;5;241m=\u001b[39m _convert_data_to_type_hint(model_input, type_hint)\n\u001b[0;32m--> 200\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_data_against_type_hint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_hint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_pos \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    202\u001b[0m         kwargs[model_input_param_name] \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/Documents/repos/mlflow/mlflow/types/type_hints.py:452\u001b[0m, in \u001b[0;36m_validate_data_against_type_hint\u001b[0;34m(data, type_hint)\u001b[0m\n\u001b[1;32m    450\u001b[0m args \u001b[38;5;241m=\u001b[39m get_args(type_hint)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_list_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m origin_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _validate_dict_elements(element_type\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;241m1\u001b[39m], data\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/Documents/repos/mlflow/mlflow/types/type_hints.py:528\u001b[0m, in \u001b[0;36m_validate_list_elements\u001b[0;34m(element_type, data)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m invalid_elems:\n\u001b[1;32m    525\u001b[0m     invalid_elems_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_elems[:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ... (truncated)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(invalid_elems) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m invalid_elems\n\u001b[1;32m    527\u001b[0m     )\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException\u001b[38;5;241m.\u001b[39minvalid_parameter_value(\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to validate data against type hint `list[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_type_hint_repr(element_type)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid elements: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_elems_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mMlflowException\u001b[0m: Failed to validate data against type hint `list[Message]`, invalid elements: [('What is DSPy?', \"Expecting example to be a dictionary or pydantic model instance for Pydantic type hint, got <class 'str'>\")]"
     ]
    }
   ],
   "source": [
    "# An incorrect input will trigger validation error\n",
    "model.predict([\"What is DSPy?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b9c51-d840-435b-9923-c4088087020b",
   "metadata": {},
   "source": [
    "## Model logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bde4ec-992c-432d-aefa-3087e4a59861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 19:07:07 INFO mlflow.models.signature: Inferring model signature from type hints\n",
      "2025/03/06 19:07:07 INFO mlflow.models.signature: Running the predict function to generate output based on input example\n"
     ]
    }
   ],
   "source": [
    "# log the model\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(name=\"model\", python_model=model, input_example=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805d1dd8-b63a-4236-9f7e-1ab845c7a39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [{content: string (required), role: string (required)} (required)]\n",
       "outputs: \n",
       "  [string (required)]\n",
       "params: \n",
       "  None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4e0a8a-924d-4b84-88d3-4a1faa2c01a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLflow is an open-source platform designed to manage the machine learning lifecycle. It provides tools for various stages of the machine learning process, including:\\n\\n1. **Experiment Tracking**: MLflow allows you to log and track experiments, enabling you to compare different runs and their performance metrics easily. You can log parameters, metrics, tags, and artifacts related to your models.\\n\\n2. **Projects**: MLflow Projects facilitate packaging and sharing code in a reusable format. This makes it easier to reproduce experiments and share your work with others.\\n\\n3. **Models**: MLflow Models provides a standard format for packaging machine learning models. It supports various flavors of models (e.g., TensorFlow, PyTorch, Scikit-learn) and allows you to deploy them to various environments (like Docker, cloud-based services, or local servers).\\n\\n4. **Registry**: The MLflow Model Registry provides a centralized repository to manage models, including versioning, annotation, and lifecycle management (staging, production, and archived statuses).\\n\\n5. **Integration**: MLflow integrates well with popular machine learning frameworks and libraries, making it a versatile choice for data scientists and machine learning engineers.\\n\\nBy using MLflow, teams can streamline their machine learning workflows, enhance collaboration, and ensure reproducibility in their experiments, leading to more efficient model development and deployment.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pyfunc model\n",
    "pyfunc_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "# the same validation works for pyfunc model predict\n",
    "pyfunc_model.predict(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920f0d1-4c89-43fb-bdc9-e69702b059b3",
   "metadata": {},
   "source": [
    "## Verify model before deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c702c097-19aa-4530-a075-fd5c676a0d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41f48f443de4b5c810b46704358b815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 19:10:16 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "2025/03/06 19:10:16 INFO mlflow.utils.virtualenv: Creating a new environment in /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094 with python version 3.9.18 using uv\n",
      "Using CPython 3.9.18 interpreter at: \u001b[36m/Users/serena.ruan/miniconda3/envs/mlflow/bin/python3.9\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m/var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094\u001b[39m\n",
      "Activate with: \u001b[32msource /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094/bin/activate\u001b[39m\n",
      "2025/03/06 19:10:16 INFO mlflow.utils.virtualenv: Installing dependencies\n",
      "\u001b[2mUsing Python 3.9.18 environment at: /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m3 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==23.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==68.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwheel\u001b[0m\u001b[2m==0.41.2\u001b[0m\n",
      "\u001b[2mUsing Python 3.9.18 environment at: /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m84 packages\u001b[0m \u001b[2min 4.91s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 423ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m83 packages\u001b[0m \u001b[2min 849ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1malembic\u001b[0m\u001b[2m==1.15.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==23.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblinker\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbrotli\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==5.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.1.31\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.1.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcloudpickle\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcython\u001b[0m\u001b[2m==3.0.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatabricks-sdk\u001b[0m\u001b[2m==0.44.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdeprecated\u001b[0m\u001b[2m==1.2.18\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocker\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mflask\u001b[0m\u001b[2m==3.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.56.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.44\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.38.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgraphene\u001b[0m\u001b[2m==3.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgraphql-core\u001b[0m\u001b[2m==3.2.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgraphql-relay\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgunicorn\u001b[0m\u001b[2m==23.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh2\u001b[0m\u001b[2m==4.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhpack\u001b[0m\u001b[2m==4.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhyperframe\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-resources\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mitsdangerous\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.8.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmako\u001b[0m\u001b[2m==1.3.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown\u001b[0m\u001b[2m==3.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmlflow\u001b[0m\u001b[2m==2.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmlflow-skinny\u001b[0m\u001b[2m==2.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.63.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.37b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==24.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==19.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.10.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.27.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrsa\u001b[0m\u001b[2m==4.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msqlalchemy\u001b[0m\u001b[2m==2.0.38\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msqlparse\u001b[0m\u001b[2m==0.5.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.12.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwerkzeug\u001b[0m\u001b[2m==3.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwrapt\u001b[0m\u001b[2m==1.17.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.21.0\u001b[0m\n",
      "2025/03/06 19:10:22 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094/bin/activate && python -c \"\"']'\n",
      "2025/03/06 19:10:22 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpp18g94f9/envs/virtualenv_envs/mlflow-9d81fff15053e6e06e2edaefcc9e075d6c04a094/bin/activate && python /Users/serena.ruan/Documents/repos/mlflow/mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py --model-uri file:///Users/serena.ruan/Documents/test/mlruns/0/33b7da4d1693490b97934a5781964766/artifacts/model --content-type json --input-path /var/folders/9g/psrbbvm92t712cy09d7_00d00000gp/T/tmpz3dlhg3n/input.json']'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": \"New York is a state located in the northeastern region of the United States. It is bordered by Vermont to the northeast, Massachusetts to the east, Connecticut to the southeast, and New Jersey and Pennsylvania to the south. The state also has access to the Atlantic Ocean to the southeast. The city of New York, often referred to simply as NYC, is the largest city in the state and is known for its significant cultural, financial, and historical influence.\"}"
     ]
    }
   ],
   "source": [
    "# verify model before serving\n",
    "mlflow.models.predict(\n",
    "    model_uri=model_info.model_uri,\n",
    "    input_data=[{\"role\": \"user\", \"content\": \"Where is New York?\"}],\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386566db-841a-41cf-a46f-d75bd060b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs:/33b7da4d1693490b97934a5781964766/model\n"
     ]
    }
   ],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f72697-57c9-4a02-9c11-da35c277bab8",
   "metadata": {},
   "source": [
    "## Serve the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0e4e4-372b-4176-85e5-6f624b728c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run below command to serve the model locally\n",
    "# mlflow models serve -m runs:/33b7da4d1693490b97934a5781964766/model -p 6666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85013e9c-fb42-4d8a-a8f1-5fb69f695b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"predictions\": \"British Shorthairs are generally considered to be intelligent cats, though their intelligence may manifest differently compared to some other breeds. They are known for their calm and laid-back demeanor, which can sometimes be mistaken for a lack of intelligence. In reality, they are capable of problem-solving and can be trained to perform basic commands or tricks, though they may not be as eager to please as some more active breeds.\\\\n\\\\nTheir intelligence is often reflected in their ability to adapt to their environment and their understanding of routines. British Shorthairs tend to be independent and may not seek out interaction as much as other, more playful breeds, but they can still form strong bonds with their owners. Essentially, while they might not be the most overtly intelligent cats, they possess a subtle understanding of their surroundings that reflects their adaptability and awareness.\"}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from mlflow.models.utils import convert_input_example_to_serving_input\n",
    "\n",
    "payload = convert_input_example_to_serving_input(\n",
    "    [{\"role\": \"user\", \"content\": \"Is British shorthair smart?\"}]\n",
    ")\n",
    "resp = requests.post(\n",
    "    \"http://127.0.0.1:6666/invocations\", data=payload, headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "resp.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


--- docs/docs/classic-ml/deep-learning/sentence-transformers/tutorials/quickstart/sentence-transformers-quickstart.ipynb ---
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Sentence Transformers and MLflow\n",
    "\n",
    "Welcome to our tutorial on leveraging **Sentence Transformers** with **MLflow** for advanced natural language processing and model management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Set up a pipeline for sentence embeddings with `sentence-transformers`.\n",
    "- Log models and configurations using MLflow.\n",
    "- Understand and apply model signatures in MLflow to `sentence-transformers`.\n",
    "- Deploy and use models for inference with MLflow's features.\n",
    "\n",
    "#### What are Sentence Transformers?\n",
    "Sentence Transformers, an extension of the Hugging Face Transformers library, are designed for generating semantically rich sentence embeddings. They utilize models like BERT and RoBERTa, fine-tuned for tasks such as semantic search and text clustering, producing high-quality sentence-level embeddings.\n",
    "\n",
    "#### Benefits of Integrating MLflow with Sentence Transformers\n",
    "Combining MLflow with Sentence Transformers enhances NLP projects by:\n",
    "\n",
    "- Streamlining experiment management and logging.\n",
    "- Offering better control over model versions and configurations.\n",
    "- Ensuring reproducibility of results and model predictions.\n",
    "- Simplifying the deployment process in production environments.\n",
    "\n",
    "This integration empowers efficient tracking, management, and deployment of NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# Disable tokenizers warnings when constructing pipelines\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Disable a few less-than-useful UserWarnings from setuptools and pydantic\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Environment for Sentence Embedding\n",
    "\n",
    "Begin your journey with Sentence Transformers and MLflow by establishing the core working environment.\n",
    "\n",
    "#### Key Steps for Initialization\n",
    "\n",
    "- Import necessary libraries: `SentenceTransformer` and `mlflow`.\n",
    "- Initialize the `\"all-MiniLM-L6-v2\"` Sentence Transformer model.\n",
    "    \n",
    "#### Model Initialization\n",
    "The compact and efficient `\"all-MiniLM-L6-v2\"` model is chosen for its effectiveness in generating meaningful sentence embeddings. Explore more models at the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=trending).\n",
    "\n",
    "#### Purpose of the Model\n",
    "This model excels in transforming sentences into semantically rich embeddings, applicable in various NLP tasks like semantic search and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import mlflow\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Signature with MLflow\n",
    "Defining the model signature is a crucial step in setting up our Sentence Transformer model for consistent and expected behavior during inference.\n",
    "\n",
    "#### Steps for Signature Definition\n",
    "\n",
    "- **Prepare Example Sentences**: Define example sentences to demonstrate the model's input and output formats.\n",
    "- **Generate Model Signature**: Use the `mlflow.models.infer_signature` function with the model's input and output to automatically define the signature.\n",
    "\n",
    "#### Importance of the Model Signature\n",
    "\n",
    "- **Clarity in Data Formats**: Ensures clear documentation of the data types and structures the model expects and produces.\n",
    "- **Model Deployment and Usage**: Crucial for deploying models to production, ensuring the model receives inputs in the correct format and produces expected outputs.\n",
    "- **Error Prevention**: Helps in preventing errors during model inference by enforcing consistent data formats.\n",
    "\n",
    "**NOTE**: The `List[str]` input type is equivalent at inference time to `str`. The MLflow flavor uses a `ColSpec[str]` definition for the input type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [string]\n",
       "outputs: \n",
       "  [Tensor('float32', (-1, 384))]\n",
       "params: \n",
       "  None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentences = [\"A sentence to encode.\", \"Another sentence to encode.\"]\n",
    "\n",
    "# Infer the signature of the custom model by providing an input example and the resultant prediction output.\n",
    "# We're not including any custom inference parameters in this example, but you can include them as a third argument\n",
    "# to infer_signature(), as you will see in the advanced tutorials for Sentence Transformers.\n",
    "signature = mlflow.models.infer_signature(\n",
    "    model_input=example_sentences,\n",
    "    model_output=model.encode(example_sentences),\n",
    ")\n",
    "\n",
    "# Visualize the signature\n",
    "signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an experiment\n",
    "\n",
    "We create a new MLflow Experiment so that the run we're going to log our model to does not log to the default experiment and instead has its own contextually relevant entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/sentence-transformers/tutorials/quickstart/mlruns/469990615226680434', creation_time=1701280211449, experiment_id='469990615226680434', last_update_time=1701280211449, lifecycle_stage='active', name='Introduction to Sentence Transformers', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you are running this tutorial in local mode, leave the next line commented out.\n",
    "# Otherwise, uncomment the following line and set your tracking uri to your local or remote tracking server.\n",
    "\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "mlflow.set_experiment(\"Introduction to Sentence Transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the Sentence Transformer Model with MLflow\n",
    "\n",
    "Logging the model in MLflow is essential for tracking, version control, and deployment, following the initialization and signature definition of our Sentence Transformer model.\n",
    "\n",
    "#### Steps for Logging the Model\n",
    "\n",
    "- **Start an MLflow Run**: Initiate a new run with `mlflow.start_run()`, grouping all logging operations.\n",
    "- **Log the Model**: Use `mlflow.sentence_transformers.log_model` to log the model, providing the model object, artifact path, signature, and an input example.\n",
    "\n",
    "#### Importance of Model Logging\n",
    "\n",
    "- **Model Management**: Facilitates the model's lifecycle management from training to deployment.\n",
    "- **Reproducibility and Tracking**: Enables tracking of model versions and ensures reproducibility.\n",
    "- **Ease of Deployment**: Simplifies deployment by allowing models to be easily deployed for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    logged_model = mlflow.sentence_transformers.log_model(\n",
    "        model=model,\n",
    "        name=\"sbert_model\",\n",
    "        signature=signature,\n",
    "        input_example=example_sentences,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model and Testing Inference\n",
    "\n",
    "After logging the Sentence Transformer model in MLflow, we demonstrate how to load and test it for real-time inference.\n",
    "    \n",
    "#### Loading the Model as a PyFunc\n",
    "\n",
    "- **Why PyFunc**: Load the logged model using `mlflow.pyfunc.load_model` for seamless integration into Python-based services or applications.\n",
    "- **Model URI**: Use the `logged_model.model_uri` to accurately locate and load the model from MLflow.\n",
    "\n",
    "#### Conducting Inference Tests\n",
    "\n",
    "- **Test Sentences**: Define sentences to test the model's embedding generation capabilities.\n",
    "- **Performing Predictions**: Use the model's `predict` method with test sentences to obtain embeddings.\n",
    "- **Printing Embedding Lengths**: Verify embedding generation by checking the length of embedding arrays, corresponding to the dimensionality of each sentence representation.\n",
    "\n",
    "#### Importance of Inference Testing\n",
    "\n",
    "- **Model Validation**: Confirm the model's expected behavior and data processing capability upon loading.\n",
    "- **Deployment Readiness**: Validate the model's readiness for real-time integration into application services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The return structure length is: 2\n",
      "The size of embedding 1 is: 384\n",
      "The size of embedding 2 is: 384\n"
     ]
    }
   ],
   "source": [
    "inference_test = [\"I enjoy pies of both apple and cherry.\", \"I prefer cookies.\"]\n",
    "\n",
    "# Load our custom model by providing the uri for where the model was logged.\n",
    "loaded_model_pyfunc = mlflow.pyfunc.load_model(logged_model.model_uri)\n",
    "\n",
    "# Perform a quick test to ensure that our loaded model generates the correct output\n",
    "embeddings_test = loaded_model_pyfunc.predict(inference_test)\n",
    "\n",
    "# Verify that the output is a list of lists of floats (our expected output format)\n",
    "print(f\"The return structure length is: {len(embeddings_test)}\")\n",
    "\n",
    "for i, embedding in enumerate(embeddings_test):\n",
    "    print(f\"The size of embedding {i + 1} is: {len(embeddings_test[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Samples of Generated Embeddings\n",
    "Examine the content of embeddings to verify their quality and understand the model's output.\n",
    "    \n",
    "#### Inspecting the Embedding Samples\n",
    "\n",
    "- **Purpose of Sampling**: Inspect a sample of the entries in each embedding to understand the vector representations generated by the model.\n",
    "- **Printing Embedding Samples**: Print the first 10 entries of each embedding vector using `embedding[:10]` to get a glimpse into the model's output.\n",
    "\n",
    "#### Why Sampling is Important\n",
    "\n",
    "- **Quality Check**: Sampling provides a quick way to verify the embeddings' quality and ensures they are meaningful and non-degenerate.\n",
    "- **Understanding Model Output**: Seeing parts of the embedding vectors offers an intuitive understanding of the model's output, beneficial for debugging and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample of the first 10 entries in embedding 1 is: [ 0.04866192 -0.03687946  0.02408808  0.03534171 -0.12739632  0.00999414\n",
      "  0.07135344 -0.01433522  0.04296691 -0.00654414]\n",
      "The sample of the first 10 entries in embedding 2 is: [-0.03879027 -0.02373698  0.01314073  0.03589077 -0.01641303 -0.0857707\n",
      "  0.08282158 -0.03173266  0.04507608  0.02777079]\n"
     ]
    }
   ],
   "source": [
    "for i, embedding in enumerate(embeddings_test):\n",
    "    print(f\"The sample of the first 10 entries in embedding {i + 1} is: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Model Loading in MLflow for Extended Functionality\n",
    "Explore the full range of Sentence Transformer functionalities with MLflow's support for native model loading.\n",
    "    \n",
    "#### Why Support Native Loading?\n",
    "\n",
    "- **Access to Native Functionalities**: Native loading unlocks all the features of the Sentence Transformer model, essential for advanced NLP tasks.\n",
    "- **Loading the Model Natively**: Use `mlflow.sentence_transformers.load_model` to load the model with its full capabilities, enhancing flexibility and efficiency.\n",
    "\n",
    "#### Generating Embeddings Using Native Model\n",
    "\n",
    "- **Model Encoding**: Employ the model's native `encode` method to generate embeddings, taking advantage of optimized functionality.\n",
    "- **Importance of Native Encoding**: Native encoding ensures the utilization of the model's full embedding generation capabilities, suitable for large-scale or complex NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/30 15:50:24 INFO mlflow.sentence_transformers: 'runs:/eeab3c1b13594fdea13e07585b1c0596/sbert_model' resolved as 'file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/sentence-transformers/tutorials/quickstart/mlruns/469990615226680434/eeab3c1b13594fdea13e07585b1c0596/artifacts/sbert_model'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample of the native library encoding call for embedding 1 is: [ 0.04866192 -0.03687946  0.02408808  0.03534171 -0.12739632  0.00999414\n",
      "  0.07135344 -0.01433522  0.04296691 -0.00654414]\n",
      "The sample of the native library encoding call for embedding 2 is: [-0.03879027 -0.02373698  0.01314073  0.03589077 -0.01641303 -0.0857707\n",
      "  0.08282158 -0.03173266  0.04507608  0.02777079]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model as a native Sentence Transformers model (unlike above, where we loaded as a generic python function)\n",
    "loaded_model_native = mlflow.sentence_transformers.load_model(logged_model.model_uri)\n",
    "\n",
    "# Use the native model to generate embeddings by calling encode() (unlike for the generic python function which uses the single entrypoint of `predict`)\n",
    "native_embeddings = loaded_model_native.encode(inference_test)\n",
    "\n",
    "for i, embedding in enumerate(native_embeddings):\n",
    "    print(\n",
    "        f\"The sample of the native library encoding call for embedding {i + 1} is: {embedding[:10]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Embracing the Power of Sentence Transformers with MLflow\n",
    "\n",
    "As we reach the end of our Introduction to Sentence Transformers tutorial, we have successfully navigated the basics of integrating the Sentence Transformers library with MLflow. This foundational knowledge sets the stage for more advanced and specialized applications in the field of Natural Language Processing (NLP).\n",
    "\n",
    "#### Recap of Key Learnings\n",
    "\n",
    "1. **Integration Basics**: We covered the essential steps of loading and logging a Sentence Transformer model using MLflow. This process demonstrated the simplicity and effectiveness of integrating cutting-edge NLP tools within MLflow's ecosystem.\n",
    "\n",
    "2. **Signature and Inference**: Through the creation of a model signature and the execution of inference tasks, we showcased how to operationalize the Sentence Transformer model, ensuring that it's ready for real-world applications.\n",
    "\n",
    "3. **Model Loading and Prediction**: We explored two ways of loading the model - as a PyFunc model and using the native Sentence Transformers loading mechanism. This dual approach highlighted the versatility of MLflow in accommodating different model interaction methods.\n",
    "\n",
    "4. **Embeddings Exploration**: By generating and examining sentence embeddings, we glimpsed the transformative potential of transformer models in capturing semantic information from text.\n",
    "\n",
    "#### Looking Ahead\n",
    "\n",
    "- **Expanding Horizons**: While this tutorial focused on the foundational aspects of Sentence Transformers and MLflow, there's a whole world of advanced applications waiting to be explored. From semantic similarity analysis to paraphrase mining, the potential use cases are vast and varied.\n",
    "\n",
    "- **Continued Learning**: We strongly encourage you to delve into the other tutorials in this series, which dive deeper into more intriguing use cases like similarity analysis, semantic search, and paraphrase mining. These tutorials will provide you with a broader understanding and more practical applications of Sentence Transformers in various NLP tasks.\n",
    "\n",
    "#### Final Thoughts\n",
    "\n",
    "The journey into NLP with Sentence Transformers and MLflow is just beginning. With the skills and insights gained from this tutorial, you are well-equipped to explore more complex and exciting applications. The integration of advanced NLP models with MLflow's robust management and deployment capabilities opens up new avenues for innovation and exploration in the field of language understanding and beyond.\n",
    "\n",
    "Thank you for joining us on this introductory journey, and we look forward to seeing how you apply these tools and concepts in your NLP endeavors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

--- docs/docs/quickstart_drilldown/index.mdx ---
---
sidebar_custom_props:
  hide: true
displayed_sidebar: docsSidebar
---

import Link from "@docusaurus/Link";
import { Table } from "@site/src/components/Table";

# Quickstart options and troubleshooting

{/** Eventually, these H2s will probably all be separate articles. For now, I'm
avoiding that so as not to create a bunch of super-skinny pages. **/}

## Customize and troubleshoot MLflow installation \{#quickstart_drilldown_install}

### Python library options

Rather than the default MLflow library, you can install the following variations:

<Table>
  <thead>
    <tr>
      <th>**Name**</th>
      <th>**`pip install` command**</th>
      <th>**Description**</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mlflow-skinny</td>
      <td>`pip install mlflow-skinny`</td>
      <td>Lightweight MLflow package without SQL storage, server, UI, or data science dependencies.</td>
    </tr>
    <tr>
      <td>mlflow[extras]</td>
      <td>`pip install mlflow[extras]`</td>
      <td>MLflow package with all dependencies needed to run various MLflow flavors. These dependencies are listed in [this document](https://github.com/mlflow/mlflow/blob/master/requirements/extra-ml-requirements.txt).</td>
    </tr>
    <tr>
      <td>In-development version</td>
      <td>`pip install git+https://github.com/mlflow/mlflow.git@master`</td>
      <td>This is the latest version of MLflow, which may be useful for getting hot-fixes or new features.</td>
    </tr>
  </tbody>
</Table>

### Python and Mac OS X

We strongly recommend using a virtual environment manager on Macs. We always recommend
using virtual environments, but they are especially important on Mac OS X because the system
`python` version varies depending on the installation and whether you've installed the Xcode
command line tools. The default environment manager for MLflow is `virtualenv`.
Other popular options are `conda` and `venv`.

### Python

We release MLflow on:

- PyPI (`pip install mlflow`)
- conda-forge (`conda install -c conda-forge mlflow`)

### R and Java

We release MLflow on:

- CRAN (`install.packages("mlflow")`)
- Maven Central (`mlflow-client`, `mlflow-parent`, `mlflow-spark`)

For R, see <APILink fn="mlflow.r" hash="">installing MLflow for R</APILink>.
For Java, see <APILink fn="mlflow.java" hash="">Java API</APILink>.

## Save and serve models \{#quickstart_drilldown_log_and_load_model}

MLflow includes a generic `MLmodel` format for saving **models** from a variety of tools in diverse
**flavors**. For example, many models can be served as Python functions, so an `MLmodel` file can
declare how each model should be interpreted as a Python function in order to let various tools
serve it. MLflow also includes tools for running such models locally and exporting them to Docker
containers or commercial serving platforms.

To illustrate this functionality, the `mlflow.sklearn` flavor can log scikit-learn models as
MLflow artifacts and then load them again for serving. There is an example training application in
[sklearn_logistic_regression/train.py](https://github.com/mlflow/mlflow/tree/master/examples/sklearn_logistic_regression).
To run it, switch to the MLflow repository root and run:

```bash
python examples/sklearn_logistic_regression/train.py
```

When you run the example, it outputs an MLflow run ID for that experiment. If you look at the
`mlflow ui`, you will also see that the run saved a **model** folder containing an `MLmodel`
description file and a pickled scikit-learn model. You can pass the run ID and the path of the model
within the artifacts directory (here **model/**) to various tools. For example, MLflow includes a
simple REST server for python-based models:

```bash
mlflow models serve -m --env-manager local runs:/<RUN_ID>/model
```

:::note
By default the server runs on port 5000. If that port is already in use, use the `--port` option to
specify a different port. For example: `mlflow models serve -m runs:/<RUN_ID>/model --port 1234`
:::

Once you have started the server, you can pass it some sample data and see the
predictions.

The following example uses `curl` to send a JSON-serialized pandas DataFrame with the `split`
orientation to the model server. For more information about the input data formats accepted by
the pyfunc model server, see the [MLflow deployment tools documentation](/deployment/deploy-model-locally).

```bash
curl -d '{"dataframe_split": {"columns": ["x"], "data": [[1], [-1]]}}' -H 'Content-Type: application/json' -X POST localhost:5000/invocations
```

which returns:

```bash
[1, 0]
```

For more information, see [MLflow Models](/model).


--- docs/docs/classic-ml/getting-started/index.mdx ---
---
sidebar_position: 3
---

import { CardGroup, PageCard } from "@site/src/components/Card";
import Link from "@docusaurus/Link";

# Getting Started with MLflow

If you're new to MLflow or seeking a refresher on its core functionalities, the
quickstart tutorials here are the perfect starting point. They will guide you
step-by-step through fundamental concepts, focusing purely on a task that will maximize your understanding of
how to use MLflow to solve a particular task.

:::tip
If you'd like to try a free trial of a fully-managed MLflow experience on Databricks, you can quickly sign up and start using MLflow for your GenAI and ML project needs without having to configure, setup, and run your own tracking server. You can learn more about the

<Link to="/ml/getting-started/databricks-trial" target="_blank">Databricks Free Trial</Link> here. This trial offers full access to a personal Databricks account that includes MLflow and other tightly integrated AI services and features.
:::

## Guidance on Running Tutorials

If you have never interfaced with the [MLflow Tracking Server](/self-hosting/architecture/tracking-server), we highly encourage you to head on over to quickly **read the guide below**. It
will help you get started as quickly as possible with tutorial content throughout the documentation.

<CardGroup>
  <PageCard
    link="/ml/getting-started/running-notebooks/"
    headerText="How to Run Tutorials"
    text="Learn about your options for running a MLflow Tracking Server for executing any of the guides and tutorials in the MLflow documentation"
  />
</CardGroup>
<br />

## Getting Started Guides

### MLflow Tracking

[MLflow Tracking](/ml/tracking) is one of the primary service components of MLflow. In these guides, you will gain an understanding of what MLflow Tracking can do to
enhance your MLOps related activities while building ML models.

![The basics of MLflow tracking](/images/tutorials/introductory/tracking-basics.png 'The basics of MLflow tracking')

In these introductory guides to MLflow Tracking, you will learn how to leverage MLflow to:

- **Log** training statistics (loss, accuracy, etc.) and hyperparameters for a model
- **Log** (save) a model for later retrieval
- **Register** a model using the [MLflow Model Registry](/ml/model-registry) to enable deployment
- **Load** the model and use it for inference

In the process of learning these key concepts, you will be exposed to the [MLflow Tracking APIs](/ml/tracking/tracking-api), the MLflow Tracking UI, and learn how to add metadata associated with
a model training event to an MLflow run.

<CardGroup>
  <PageCard link="/ml/tracking/quickstart" headerText="MLFlow Tracking Quickstart Guide" text="Learn the basics of MLflow Tracking in a fast-paced guide with a focus on seeing your first model in the MLflow UI" />
  <PageCard link="/ml/getting-started/logging-first-model" headerText="In-depth Tutorial for MLflow Tracking" text="Learn the nuances of interfacing with the MLflow Tracking Server in an in-depth tutorial" />
</CardGroup>

### Autologging Basics

A great way to get started with MLflow is to use the [autologging](/ml/tracking/autolog) feature. Autologging automatically logs your model, metrics, examples, signature, and parameters
with only a single line of code for many of the most popular ML libraries in the Python ecosystem.

<div class="center-div" style={{ width: "80%" }}>
  ![The basics of MLflow tracking](/images/tutorials/introductory/autologging-intro.png "The basics of MLflow tracking")
</div>

In this brief tutorial, you'll learn how to leverage MLflow's autologging feature to simplify your model logging activities.

<CardGroup>
  <PageCard link="/ml/tracking/autolog" headerText="MLflow Autologging Quickstart" text="Get started with logging to MLflow with the high-level autologging API in a fast-paced guide" />
</CardGroup>
<br />

### Run Comparison Basics

This quickstart tutorial focuses on the MLflow UI's run comparison feature and provides a step-by-step walkthrough of registering the best model found from a
hyperparameter tuning execution sweep. After locally serving the registered model, a brief example of preparing a model for remote [deployment](/ml/deployment)
by containerizing the model using Docker is covered.

![The basics of MLflow run comparison](/images/tutorials/introductory/intro-run-comparison.png 'The basics of MLflow run comparison')

<CardGroup>
  <PageCard link="/ml/getting-started/hyperparameter-tuning" headerText="MLflow Run Comparison Quickstart" text="Get started with using the MLflow UI to compare runs and register a model for deployment" />
</CardGroup>
<br />

### Tracking Server Quickstart

This quickstart tutorial walks through different types of [MLflow Tracking Servers](/self-hosting/architecture/tracking-server) and how to use them to log
your MLflow experiments.

<CardGroup>
  <PageCard link="/ml/getting-started/tracking-server-overview" headerText="5 Minute Tracking Server Overview" text="Learn how to log MLflow experiments with different tracking servers" />
</CardGroup>
<br />

### Model Registry Quickstart

This quickstart tutorial walks through registering a model in the MLflow model registry and how to
retrieve registered models.

<CardGroup>
  <PageCard link="/ml/getting-started/registering-first-model/" headerText="5 Minute Model Registry Overview" text="Learn how to log MLflow models to the model registry" />
</CardGroup>

## Further Learning - What's Next?

Now that you have the essentials under your belt, below are some recommended collections of tutorial and guide content that will help to broaden your
understanding of MLflow and its APIs.

- **Tracking** - Learn more about the MLflow tracking APIs by [reading the tracking guide](/ml/tracking).
- **MLflow Deployment** - Follow the comprehensive [guide on model deployment](/ml/deployment) to learn how to deploy your MLflow models to a variety of deployment targets.
- **Model Registry** - Learn about the [MLflow Model Registry](/ml/model-registry) and how it can help you manage the lifecycle of your ML models.
- **Deep Learning Library Integrations** - From PyTorch to TensorFlow and more, learn about the integrated deep learning capabilities in MLflow by [reading the deep learning guide](/ml/deep-learning).
- **Traditional ML** - Learn about the [traditional ML capabilities](/ml/traditional-ml) in MLflow and how they can help you manage your traditional ML workflows.


--- docs/docs/genai/getting-started/index.mdx ---
---
description: "Build, evaluate, and deploy production-ready GenAI applications with MLflow's comprehensive LLMOps platform"
sidebar_position: 1
---

import FeatureHighlights from "@site/src/components/FeatureHighlights";
import ConceptOverview from "@site/src/components/ConceptOverview";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import useBaseUrl from '@docusaurus/useBaseUrl';
import { Code2, TestTube, Rocket, Eye, Database, Shield, Zap, Users, TrendingUp, BookOpen, PlayCircle, Target, Settings } from "lucide-react";

# Getting Started with MLflow for GenAI

## The Complete Open Source LLMOps Platform for Production GenAI

MLflow transforms how software engineers build, evaluate, and deploy GenAI applications. Get complete observability, systematic evaluation, and deployment confidence—all while maintaining the flexibility to use any framework or model provider.

<div style={{margin: '2rem 0', textAlign: 'center'}}>
  <img
    src={useBaseUrl('/images/llms/tracing/tracing-top.gif')}
    alt="MLflow Tracing UI showing detailed GenAI observability"
    style={{maxWidth: '100%', borderRadius: '8px', boxShadow: '0 4px 12px rgba(0, 0, 0, 0.15)'}}
  />
</div>

## The GenAI Development Lifecycle

MLflow provides a complete platform that supports every stage of GenAI application development. From initial prototyping to production monitoring, these integrated capabilities ensure you can build, test, and deploy with confidence.

<ConceptOverview concepts={[
  {
    icon: Code2,
    title: "Develop & Debug",
    description: "Trace every LLM call, prompt interaction, and tool invocation. Debug complex AI workflows with complete visibility into execution paths, token usage, and decision points."
  },
  {
    icon: TestTube,
    title: "Evaluate & Improve",
    description: "Systematically test with LLM judges, human feedback, and custom metrics. Compare versions objectively and catch regressions before they reach production."
  },
  {
    icon: Rocket,
    title: "Deploy & Monitor",
    description: "Serve models with confidence using built-in deployment targets. Monitor production performance and iterate based on real-world usage patterns."
  }
]} />

## Why Open Source MLflow for GenAI?

As the original open source ML platform, MLflow brings battle-tested reliability and community-driven innovation to GenAI development. No vendor lock-in, no proprietary formats—just powerful tools that work with your stack.

<FeatureHighlights features={[
  {
    icon: Eye,
    title: "Production-Grade Observability",
    description: "Automatically instrument 15+ frameworks including OpenAI, LangChain, and LlamaIndex. Get detailed traces showing token usage, latency, and execution paths for every request—no black boxes."
  },
  {
    icon: Database,
    title: "Intelligent Prompt Management",
    description: "Version, compare, and deploy prompts with MLflow's prompt registry. Track performance across prompt variations and maintain audit trails for production systems."
  },
  {
    icon: Shield,
    title: "Automated Quality Assurance",
    description: "Build confidence with LLM judges and automated evaluation. Run systematic tests on every change and track quality metrics over time to prevent regressions."
  },
  {
    icon: Zap,
    title: "Framework-Agnostic Integration",
    description: "Use any LLM framework or provider without vendor lock-in. MLflow works with your existing tools while providing unified tracking, evaluation, and deployment."
  }
]} />

## Start Building Production GenAI Applications

MLflow transforms GenAI development from complex instrumentation to simple, one-line integrations. See how easy it is to add comprehensive observability, evaluation, and deployment to your AI applications. Visit the [Tracing guide](/genai/tracing) for more information.

### Add Complete Observability in One Line

Transform any GenAI application into a fully observable system:

```python
import mlflow

# Enable automatic tracing for your framework
mlflow.openai.autolog()  # For OpenAI
mlflow.langchain.autolog()  # For LangChain
mlflow.llama_index.autolog()  # For LlamaIndex
mlflow.dspy.autolog()  # For DSPy

# Your existing code now generates detailed traces
from openai import OpenAI

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Explain quantum computing"}],
)
# ✅ Automatically traced: tokens, latency, cost, full request/response
```

No code changes required. Every LLM call, tool interaction, and prompt execution is automatically captured with detailed metrics.

### Manage and Optimize Prompts Systematically

Register prompts and automatically optimize them with data-driven techniques. See the [Prompt Registry](/genai/prompt-registry/create-and-edit-prompts) guide for comprehensive prompt management:

```python
import mlflow
import openai
from mlflow.genai.optimize import GepaPromptOptimizer
from mlflow.genai.scorers import Correctness

# Register an initial prompt
prompt = mlflow.genai.register_prompt(
    name="math_tutor",
    template="Answer this math question: {{question}}. Provide a clear explanation.",
)


# Define prediction function that includes prompt.format() call for your target prompt(s)
def predict_fn(question: str) -> str:
    prompt = mlflow.genai.load_prompt("prompts:/math_tutor/latest")
    completion = openai.OpenAI().chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt.format(question=question)}],
    )
    return completion.choices[0].message.content


# Prepare training data with inputs and expectations
train_data = [
    {
        "inputs": {"question": "What is 15 + 27?"},
        "expectations": {"expected_response": "42"},
    },
    {
        "inputs": {"question": "Calculate 8 × 9"},
        "expectations": {"expected_response": "72"},
    },
    {
        "inputs": {"question": "What is 100 - 37?"},
        "expectations": {"expected_response": "63"},
    },
    # ... more examples
]

# Automatically optimize the prompt using MLflow + GEPA
result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=train_data,
    prompt_uris=[prompt.uri],
    optimizer=GepaPromptOptimizer(reflection_model="openai:/gpt-4o-mini"),
    scorers=[Correctness(model="openai:/gpt-4o-mini")],
)

# The optimized prompt is automatically registered as a new version
optimized_prompt = result.optimized_prompts[0]
print(f"Optimized prompt registered as version {optimized_prompt.version}")
print(f"Template: {optimized_prompt.template}")
print(f"Score: {result.final_eval_score}")
```

Transform manual prompt engineering into systematic, data-driven optimization with automatic performance tracking. Learn more in the [Optimize Prompts](/genai/prompt-registry/optimize-prompts) guide.

### Prerequisites

Ready to get started? You'll need:

- Python 3.10+ installed
- MLflow 3.5+ (`pip install --upgrade mlflow`)
- API access to an LLM provider (OpenAI, Anthropic, etc.)

---

## Essential Learning Path

Master these core capabilities to build robust GenAI applications with MLflow. Start with observability, then add systematic evaluation and deployment.

<TilesGrid>
  <TileCard
    icon={PlayCircle}
    iconSize={48}
    title="Environment Setup"
    description="Configure MLflow tracking, connect to registries, and set up your development environment for GenAI workflows"
    href="/genai/getting-started/connect-environment"
    linkText="Start setup →"
    containerHeight={64}
  />
  <TileCard
    icon={Eye}
    iconSize={48}
    title="Observability with Tracing"
    description="Auto-instrument your GenAI application to capture every LLM call, prompt, and tool interaction for complete visibility"
    href="/genai/tracing/quickstart"
    linkText="Learn tracing →"
    containerHeight={64}
  />
  <TileCard
    icon={TestTube}
    iconSize={48}
    title="Systematic Evaluation"
    description="Build confidence with LLM judges and automated testing to catch quality issues before production"
    href="/genai/eval-monitor"
    linkText="Start evaluating →"
    containerHeight={64}
  />
</TilesGrid>

These three foundations will give you the observability and quality confidence needed for production GenAI development. Each tutorial includes real code examples and best practices from production deployments.

---

## Advanced GenAI Capabilities

Once you've mastered the essentials, explore these advanced features to build sophisticated GenAI applications with enterprise-grade reliability.

<TilesGrid>
  <TileCard
    icon={Database}
    iconSize={48}
    title="Prompt Registry & Management"
    description="Version prompts, A/B test variations, and maintain audit trails for production prompt management"
    href="/genai/prompt-registry/prompt-engineering"
    linkText="Manage prompts →"
    containerHeight={64}
  />
  <TileCard
    icon={Target}
    iconSize={48}
    title="Automated Prompt Optimization"
    description="Automatically improve prompts using DSPy's MIPROv2 algorithm with data-driven optimization and performance tracking"
    href="/genai/prompt-registry/optimize-prompts"
    linkText="Optimize prompts →"
    containerHeight={64}
  />
  <TileCard
    icon={Rocket}
    iconSize={48}
    title="Model Deployment"
    description="Deploy GenAI models to production with built-in serving, scaling, and monitoring capabilities"
    href="/genai/serving"
    linkText="Deploy models →"
    containerHeight={64}
  />
</TilesGrid>

These capabilities enable you to build production-ready GenAI applications with systematic quality management and robust deployment infrastructure.

---

## Framework-Specific Integration Guides

MLflow provides deep integrations with popular GenAI frameworks. Choose your framework to get started with optimized instrumentation and best practices.

<TilesGrid>
  <TileCard
    image="/images/logos/langchain-logo.png"
    iconSize={48}
    title="LangChain Integration"
    description="Auto-trace chains, agents, and tools with comprehensive LangChain instrumentation"
    href="/genai/flavors/langchain"
    linkText="Use LangChain →"
    containerHeight={64}
  />
  <TileCard
    image="/images/logos/llamaindex-logo.svg"
    iconSize={48}
    title="LlamaIndex Integration"
    description="Instrument RAG pipelines and document processing workflows with LlamaIndex support"
    href="/genai/flavors/llama-index"
    linkText="Use LlamaIndex →"
    containerHeight={64}
  />
  <TileCard
    image="/images/logos/openai-logo.svg"
    iconSize={48}
    title="OpenAI Integration"
    description="Track completions, embeddings, and function calls with native OpenAI instrumentation"
    href="/genai/flavors/openai"
    linkText="Use OpenAI →"
    containerHeight={64}
  />
  <TileCard
    icon={Code2}
    iconSize={48}
    title="DSPy Integration"
    description="Build systematic prompt optimization workflows with DSPy modules and MLflow prompt registry"
    href="/genai/flavors/dspy"
    linkText="Use DSPy →"
    containerHeight={64}
  />
  <TileCard
    icon={Code2}
    iconSize={48}
    title="Custom Framework Support"
    description="Instrument any LLM framework or build custom integrations with MLflow's flexible APIs"
    href="/genai/flavors/chat-model-intro"
    linkText="Build custom →"
    containerHeight={64}
  />
</TilesGrid>

Each integration guide includes framework-specific examples, best practices, and optimization techniques for production deployments.

---

## Start Your GenAI Journey with MLflow

Ready to build production-ready GenAI applications? Start with the Environment Setup guide above, then explore tracing for complete observability into your AI systems. Join thousands of engineers who trust MLflow's open source platform for their GenAI development.


--- docs/docs/genai/getting-started/databricks-trial/index.mdx ---
---
sidebar_position: 2
---

import { APILink } from "@site/src/components/APILink";

# Try Managed MLflow

The [Databricks Free Trial](https://docs.databricks.com/en/getting-started/free-trial.html) offers an opportunity to experience the Databricks platform without prior cloud provider access.
Most Databricks features, including full MLflow functionality are available during the trial period, allowing you to explore the platform with trial credits.
You create account with your email only and won't get charged unless you decide to upgrade to a paid plan and register your payment information.

## Start your trial

To get started with Databricks Free Trial, visit the [Databricks Trial Signup Page](https://signup.databricks.com/?destination_url=/ml/experiments-signup?source=OSS_DOCS&dbx_source=TRY_MLFLOW&signup_experience_step=EXPRESS&provider=MLFLOW&utm_source=OSS_DOCS)
and follow the instructions outlined there. It takes about 5 minutes to set up, after which you'll have access to a nearly fully-functional Databricks Workspace for logging your tutorial experiments, traces, models, and artifacts.

:::tip
Do you already have a Databricks trial account? [Click here](https://login.databricks.com/?destination_url=/ml/experiments&dbx_source=MLFLOW_DOCS&source=MLFLOW_DOCS) if you'd like to login and get back to the MLflow UI.
:::

## First Steps

When you login for the first time, you will be directed to the MLflow Tracing tutorial, giving you an opportunity to try out one of the most powerful GenAI features that MLflow has to offer.

Simply click on either of the two tutorials and you will be able to test out MLflow's instrumentation capabilities within minutes.

<figure>
  ![MLflow Tracing Tutorial](/images/tutorials/introductory/lighthouse/tracing-tutorial.png)
  <figcaption style={{ textAlign: "center" }}>Learn Tracing within Databricks MLflow UI</figcaption>
</figure>

## Navigating the Databricks UI

Otherwise, once you log in to the Databricks Workspace on subsequent visits, you will see a landing page like this:

<figure>
  ![Databricks Trial Landing Page](/images/tutorials/introductory/lighthouse/landing-page.png)
  <figcaption style={{ textAlign: "center" }}>Databricks Landing Page</figcaption>
</figure>

In order to get to the MLflow UI, you can navigate to it by clicking on the "Experiments" link on the left-hand side (denoted by the laboratory beaker icon).
When you get to the MLflow UI on Databricks for the first time, you'll see this:

<figure>
  ![Databricks Trial MLflow UI](/images/tutorials/introductory/lighthouse/experiments-page.png)
    <figcaption style={{ textAlign: "center" }}>Databricks MLflow UI</figcaption>
</figure>

## Decisions about where to run your Notebook

With a Databricks managed instance of MLflow, you have two options for running the tutorial notebooks: importing notebooks directly into Databricks Workspace or running notebooks locally and using Databricks Workspace as a remote tracking server.

### Importing Notebooks directly into Databricks Workspace

Once you're at the main page of the Databricks Workspace, you can import any of the notebooks within this tutorial.
Firstly, click "Download this Notebook" button in a tutorial page to download the tutorial notebook.
Then navigate to the "Workspace" tab on the left and click that link to open the workspace page.
From there, navigate to `Home` and you can right click to bring up the "Import" option.
The below image shows what the import dialog should look like if you're going to directly import a notebook from the MLflow documentation website:

![Databricks Workspace import Notebook from MLflow docs website](/images/tutorials/introductory/lighthouse/import-notebook.png)

At this point, you can simply just run the tutorial.
Any calls to MLflow for creating experiments, initiating runs, logging metadata, and saving artifacts will be fully managed for you and your logging history will appear within the MLflow UI.

:::note
On the Databricks platform, an MLflow experiment is automatically created for each notebook and you can skip `mlflow.set_tracking_uri()` and `mlflow.set_experiment()` calls in tutorials.
:::

### Running Notebooks locally and using Databricks Workspace as a remote tracking server

In order to stay within the comfortable confines of your local machine and still have the use of the managed MLflow Tracking Server, you need to:

- Generate a Personal Access Token (PAT)
- Set up Databricks workspace authentication in your dev environment.
- Connect to your Databricks Workspace in your MLflow experiment session.

#### Generate a PAT

If you are following along in the Tracing Tutorial, these steps are handled for you in both tutorials within the product. You can generate a remote access token directly within the tutorial.

Otherwise, follow the steps in [this guide](https://docs.databricks.com/aws/en/dev-tools/auth/pat) to create a PAT for remotely accessing your Databricks Workspace.

#### Install Dependencies

Run the following command in your dev environment to install dependencies.

```bash
%pip install -q mlflow
```

#### Set Up Authentication to a Databricks Workspace

To set up Databricks Workspace authentication, we can use the API <APILink fn="mlflow.login" />, which will prompt you for required information:

- **Databricks Host**: Use "https://\<your workspace host\>.cloud.databricks.com/
- **Token**: Your personal access token for your Databricks Workspace.

If the authentication succeeds, you should see a message "Successfully signed in to Databricks!".

```python
import mlflow

mlflow.login()
```

```
2025/02/19 12:25:04 INFO mlflow.utils.credentials: No valid Databricks credentials found, please enter your credentials...
Databricks Host (should begin with https://):  https://<your workspace host>.cloud.databricks.com/
Token:  ········
2025/02/19 12:26:24 INFO mlflow.utils.credentials: Successfully connected to MLflow hosted tracking server! Host: https://<your workspace host>.cloud.databricks.com.
```

#### Connect MLflow Session to Databricks Workspace

We have set up the credentials, now we need to tell MLflow to send the data into Databricks Workspace.
To do so, we will use `mlflow.set_tracking_uri("databricks")` to port MLflow to Databricks Workspace. Basically
it is the command below. Please note that you need to always use _"databricks"_ as the keyword.

```python
mlflow.set_tracking_uri("databricks")
```

Now you are ready to go! Let's try starting an MLflow experiment and log some dummy metrics and view it in the UI.

#### Log Artifacts to Unity Catalog (Optional)

In order to keep all of your artifacts within a single place, you can opt to use Unity Catalog's Volumes feature.
Firstly, you need to create a Unity Catalog Volume `test.mlflow.check-databricks-connection` by following [this guide](https://docs.databricks.com/aws/en/volumes/utility-commands#create-a-volume).
Then, you can run the following code to start an experiment with the Unity Catalog Volume and log metrics to it.
Note that your experiment name must follow the `/Users/<your email>/<experiment_name>` format when using a Databricks Workspace.

```python
mlflow.create_experiment(
    "/Users/<your email>/check-databricks-connection",
    artifact_location="dbfs:/Volumes/test/mlflow/check-databricks-connection",
)
mlflow.set_experiment("/Users/<your email>/check-databricks-connection")

with mlflow.start_run():
    mlflow.log_metric("foo", 1)
    mlflow.log_metric("bar", 2)
```

```
2025/02/19 12:26:33 INFO mlflow.tracking.fluent: Experiment with name '/Users/<your email>/check-databricks-connection' does not exist. Creating a new experiment.
```

#### View Your Experiment on your Databricks Workspace

Now let's navigate to your Databricks Workspace to view the experiment result. Log in to your
Databricks Workspace, and click on top left to select machine learning
in the drop down list. Then click on the experiment icon. See the screenshot below:

<div className="center-div" style={{ width: 800, maxWidth: "100%" }}>
  ![Landing page of Databricks MLflow server](/images/quickstart/tracking-server-overview/databricks-lighthouse-landing-page.png)
</div>

In the "Experiments" view, you should be able to find the experiment "check-databricks-connection", similar to

<div className="center-div" style={{ width: 800, maxWidth: "100%" }}>
  ![Experiment view of Databricks MLflow server](/images/quickstart/tracking-server-overview/databricks-lighthouse-experiment-view.png)
</div>

Clicking on the run name, in our example it is "skillful-jay-111" (it's a randomly generated name, you will see
a different name in your Databricks console), will bring you to the run view, similar to

<div className="center-div" style={{ width: 800, maxWidth: "100%" }}>
  ![Run view of Databricks MLflow server](/images/quickstart/tracking-server-overview/databricks-lighthouse-run-view.png)
</div>

In the run view, you will see your dummy metrics _"foo"_ and _"bar"_ are logged successfully.

At this point, you're ready to go! You can run any of the tutorials locally and they will log to the managed MLflow Tracking Server.


--- docs/docs/classic-ml/getting-started/hyperparameter-tuning/index.mdx ---
# Hyperparameter Tuning & Deployment Quickstart

Master the complete MLOps workflow with MLflow's hyperparameter optimization capabilities. In this hands-on quickstart, you'll learn how to systematically find the best model parameters, track experiments, and deploy production-ready models.

## What You'll Learn

By the end of this tutorial, you'll know how to:

- 🔍 **Run intelligent hyperparameter optimization** with Hyperopt and MLflow tracking
- 📊 **Compare experiment results** using MLflow's powerful visualization tools
- 🏆 **Identify and register your best model** for production use
- 🚀 **Deploy models to REST APIs** for real-time inference
- 📦 **Build production containers** ready for cloud deployment

<div className="center-div" style={{ width: 800, maxWidth: "100%" }}>
  ![Diagram showing Data Science and MLOps workflow with MLflow](/images/quickstart/quickstart_tracking_overview.png)
</div>

## Prerequisites & Setup

### Quick Setup

For this quickstart, we'll use a local MLflow tracking server. Start it with:

```bash
mlflow ui --port 5000
```

Keep this running in a separate terminal. Your MLflow UI will be available at [http://localhost:5000](http://localhost:5000).

### Install Dependencies

```bash
pip install mlflow[extras] hyperopt tensorflow scikit-learn pandas numpy
```

### Set Environment Variable

```bash
export MLFLOW_TRACKING_URI=http://localhost:5000
```

:::tip Team Collaboration and Managed Setup
For production environments or team collaboration, consider using [MLflow Tracking Server configurations](/ml/getting-started/running-notebooks/). For a fully-managed solution, get started with Databricks Free Trial by visiting the [Databricks Trial Signup Page](https://signup.databricks.com/?destination_url=/ml/experiments-signup?source=OSS_DOCS&dbx_source=TRY_MLFLOW&signup_experience_step=EXPRESS&provider=MLFLOW&utm_source=OSS_DOCS) and follow the instructions outlined there. It takes about 5 minutes to set up, after which you'll have access to a nearly fully-functional Databricks Workspace for logging your tutorial experiments, traces, models, and artifacts.
:::

## The Challenge: Wine Quality Prediction

We'll optimize a neural network that predicts wine quality from chemical properties. Our goal is to minimize **Root Mean Square Error (RMSE)** by finding the optimal combination of:

- **Learning Rate**: How aggressively the model learns
- **Momentum**: How much the optimizer considers previous updates

## Step 1: Prepare Your Data

First, let's load and explore our dataset:

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow import keras
import mlflow
from mlflow.models import infer_signature
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials

# Load the wine quality dataset
data = pd.read_csv(
    "https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv",
    sep=";",
)

# Create train/validation/test splits
train, test = train_test_split(data, test_size=0.25, random_state=42)
train_x = train.drop(["quality"], axis=1).values
train_y = train[["quality"]].values.ravel()
test_x = test.drop(["quality"], axis=1).values
test_y = test[["quality"]].values.ravel()

# Further split training data for validation
train_x, valid_x, train_y, valid_y = train_test_split(
    train_x, train_y, test_size=0.2, random_state=42
)

# Create model signature for deployment
signature = infer_signature(train_x, train_y)
```

## Step 2: Define Your Model Architecture

Create a reusable function to build and train models:

```python
def create_and_train_model(learning_rate, momentum, epochs=10):
    """
    Create and train a neural network with specified hyperparameters.

    Returns:
        dict: Training results including model and metrics
    """
    # Normalize input features for better training stability
    mean = np.mean(train_x, axis=0)
    var = np.var(train_x, axis=0)

    # Define model architecture
    model = keras.Sequential(
        [
            keras.Input([train_x.shape[1]]),
            keras.layers.Normalization(mean=mean, variance=var),
            keras.layers.Dense(64, activation="relu"),
            keras.layers.Dropout(0.2),  # Add regularization
            keras.layers.Dense(32, activation="relu"),
            keras.layers.Dense(1),
        ]
    )

    # Compile with specified hyperparameters
    model.compile(
        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),
        loss="mean_squared_error",
        metrics=[keras.metrics.RootMeanSquaredError()],
    )

    # Train with early stopping for efficiency
    early_stopping = keras.callbacks.EarlyStopping(
        patience=3, restore_best_weights=True
    )

    # Train the model
    history = model.fit(
        train_x,
        train_y,
        validation_data=(valid_x, valid_y),
        epochs=epochs,
        batch_size=64,
        callbacks=[early_stopping],
        verbose=0,  # Reduce output for cleaner logs
    )

    # Evaluate on validation set
    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)

    return {
        "model": model,
        "val_rmse": val_rmse,
        "val_loss": val_loss,
        "history": history,
        "epochs_trained": len(history.history["loss"]),
    }
```

## Step 3: Set Up Hyperparameter Optimization

Now let's create the optimization framework:

```python
def objective(params):
    """
    Objective function for hyperparameter optimization.
    This function will be called by Hyperopt for each trial.
    """
    with mlflow.start_run(nested=True):
        # Log hyperparameters being tested
        mlflow.log_params(
            {
                "learning_rate": params["learning_rate"],
                "momentum": params["momentum"],
                "optimizer": "SGD",
                "architecture": "64-32-1",
            }
        )

        # Train model with current hyperparameters
        result = create_and_train_model(
            learning_rate=params["learning_rate"],
            momentum=params["momentum"],
            epochs=15,
        )

        # Log training results
        mlflow.log_metrics(
            {
                "val_rmse": result["val_rmse"],
                "val_loss": result["val_loss"],
                "epochs_trained": result["epochs_trained"],
            }
        )

        # Log the trained model
        mlflow.tensorflow.log_model(result["model"], name="model", signature=signature)

        # Log training curves as artifacts
        import matplotlib.pyplot as plt

        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.plot(result["history"].history["loss"], label="Training Loss")
        plt.plot(result["history"].history["val_loss"], label="Validation Loss")
        plt.title("Model Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(
            result["history"].history["root_mean_squared_error"], label="Training RMSE"
        )
        plt.plot(
            result["history"].history["val_root_mean_squared_error"],
            label="Validation RMSE",
        )
        plt.title("Model RMSE")
        plt.xlabel("Epoch")
        plt.ylabel("RMSE")
        plt.legend()

        plt.tight_layout()
        plt.savefig("training_curves.png")
        mlflow.log_artifact("training_curves.png")
        plt.close()

        # Return loss for Hyperopt (it minimizes)
        return {"loss": result["val_rmse"], "status": STATUS_OK}


# Define search space for hyperparameters
search_space = {
    "learning_rate": hp.loguniform("learning_rate", np.log(1e-5), np.log(1e-1)),
    "momentum": hp.uniform("momentum", 0.0, 0.9),
}

print("Search space defined:")
print("- Learning rate: 1e-5 to 1e-1 (log-uniform)")
print("- Momentum: 0.0 to 0.9 (uniform)")
```

## Step 4: Run the Hyperparameter Optimization

Execute the optimization experiment:

```python
# Create or set experiment
experiment_name = "wine-quality-optimization"
mlflow.set_experiment(experiment_name)

print(f"Starting hyperparameter optimization experiment: {experiment_name}")
print("This will run 15 trials to find optimal hyperparameters...")

with mlflow.start_run(run_name="hyperparameter-sweep"):
    # Log experiment metadata
    mlflow.log_params(
        {
            "optimization_method": "Tree-structured Parzen Estimator (TPE)",
            "max_evaluations": 15,
            "objective_metric": "validation_rmse",
            "dataset": "wine-quality",
            "model_type": "neural_network",
        }
    )

    # Run optimization
    trials = Trials()
    best_params = fmin(
        fn=objective,
        space=search_space,
        algo=tpe.suggest,
        max_evals=15,
        trials=trials,
        verbose=True,
    )

    # Find and log best results
    best_trial = min(trials.results, key=lambda x: x["loss"])
    best_rmse = best_trial["loss"]

    # Log optimization results
    mlflow.log_params(
        {
            "best_learning_rate": best_params["learning_rate"],
            "best_momentum": best_params["momentum"],
        }
    )
    mlflow.log_metrics(
        {
            "best_val_rmse": best_rmse,
            "total_trials": len(trials.trials),
            "optimization_completed": 1,
        }
    )
```

## Step 5: Analyze Results in MLflow UI

Open [http://localhost:5000](http://localhost:5000) in your browser to explore your results:

### Table View Analysis

1. **Navigate to your experiment**: Click on "wine-quality-optimization"
2. **Add key columns**: Click "Columns" and add:
   - `Metrics | val_rmse`
   - `Parameters | learning_rate`
   - `Parameters | momentum`
3. **Sort by performance**: Click the `val_rmse` column header to sort by best performance

### Visual Analysis

1. **Switch to Chart view**: Click the "Chart" tab
2. **Create parallel coordinates plot**:
   - Select "Parallel coordinates"
   - Add `learning_rate` and `momentum` as coordinates
   - Set `val_rmse` as the metric
3. **Interpret the visualization**:
   - Blue lines = better performing runs
   - Red lines = worse performing runs
   - Look for patterns in successful parameter combinations

### Key Insights to Look For

- **Learning rate patterns**: Too high causes instability, too low causes slow convergence
- **Momentum effects**: Moderate momentum (0.3-0.7) often works best
- **Training curves**: Check artifacts to see if models converged properly

## Step 6: Register Your Best Model

Time to promote your best model to production:

1. **Find the best run**: In the Table view, click on the run with the lowest `val_rmse`
2. **Navigate to model artifacts**: Scroll to the "Artifacts" section
3. **Register the model**:
   - Click "Register Model" next to the model folder
   - Enter model name: `wine-quality-predictor`
   - Add description: "Optimized neural network for wine quality prediction"
   - Click "Register"

4. **Manage model lifecycle**:
   - Go to "Models" tab in MLflow UI
   - Click on your registered model
   - Transition to "Staging" stage for testing
   - Add tags and descriptions as needed

## Step 7: Deploy Your Model Locally

Test your model with a REST API deployment:

```bash
# Serve the model (choose the version number you registered)
mlflow models serve -m "models:/wine-quality-predictor/1" --port 5002
```

:::note Port Configuration
We use port 5002 to avoid conflicts with the MLflow UI running on port 5000. In production, you'd typically use port 80 or 443.
:::

### Test Your Deployment

```bash
# Test with a sample wine
curl -X POST http://localhost:5002/invocations \
  -H "Content-Type: application/json" \
  -d '{
    "dataframe_split": {
      "columns": [
        "fixed acidity", "volatile acidity", "citric acid", "residual sugar",
        "chlorides", "free sulfur dioxide", "total sulfur dioxide", "density",
        "pH", "sulphates", "alcohol"
      ],
      "data": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]
    }
  }'
```

**Expected Response:**

```json
{
  "predictions": [5.31]
}
```

This predicts a wine quality score of approximately 5.31 on the 3-8 scale.

### Test with Python

```python
import requests
import json

# Prepare test data
test_wine = {
    "dataframe_split": {
        "columns": [
            "fixed acidity",
            "volatile acidity",
            "citric acid",
            "residual sugar",
            "chlorides",
            "free sulfur dioxide",
            "total sulfur dioxide",
            "density",
            "pH",
            "sulphates",
            "alcohol",
        ],
        "data": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]],
    }
}

# Make prediction request
response = requests.post(
    "http://localhost:5002/invocations",
    headers={"Content-Type": "application/json"},
    data=json.dumps(test_wine),
)

prediction = response.json()
print(f"Predicted wine quality: {prediction['predictions'][0]:.2f}")
```

## Step 8: Build Production Container

Create a Docker container for cloud deployment:

```bash
# Build Docker image
mlflow models build-docker \
  --model-uri "models:/wine-quality-predictor/1" \
  --name "wine-quality-api"
```

:::info Build Time
The Docker build process typically takes 3-5 minutes as it installs all dependencies and configures the runtime environment.
:::

### Test Your Container

```bash
# Run the container
docker run -p 5003:8080 wine-quality-api

# Test in another terminal
curl -X POST http://localhost:5003/invocations \
  -H "Content-Type: application/json" \
  -d '{
    "dataframe_split": {
      "columns": ["fixed acidity","volatile acidity","citric acid","residual sugar","chlorides","free sulfur dioxide","total sulfur dioxide","density","pH","sulphates","alcohol"],
      "data": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]
    }
  }'
```

## Step 9: Deploy to Cloud (Optional)

Your Docker container is now ready for cloud deployment:

### Popular Cloud Options

**AWS**: Deploy to ECS, EKS, or SageMaker

```bash
# Example: Push to ECR and deploy to ECS
aws ecr create-repository --repository-name wine-quality-api
docker tag wine-quality-api:latest <your-account>.dkr.ecr.us-east-1.amazonaws.com/wine-quality-api:latest
docker push <your-account>.dkr.ecr.us-east-1.amazonaws.com/wine-quality-api:latest
```

**Azure**: Deploy to Container Instances or AKS

```bash
# Example: Deploy to Azure Container Instances
az container create \
  --resource-group myResourceGroup \
  --name wine-quality-api \
  --image wine-quality-api:latest \
  --ports 8080
```

**Google Cloud**: Deploy to Cloud Run or GKE

```bash
# Example: Deploy to Cloud Run
gcloud run deploy wine-quality-api \
  --image gcr.io/your-project/wine-quality-api \
  --platform managed \
  --port 8080
```

**Databricks**: Deploy with Mosaic AI Model Serving

```python
# First, register your model in Unity Catalog
import mlflow

mlflow.set_registry_uri("databricks-uc")

with mlflow.start_run():
    # Log your model to Unity Catalog
    mlflow.tensorflow.log_model(
        model,
        name="wine-quality-model",
        registered_model_name="main.default.wine_quality_predictor",
    )

# Then create a serving endpoint using the Databricks UI:
# 1. Navigate to "Serving" in the Databricks workspace
# 2. Click "Create serving endpoint"
# 3. Select your registered model from Unity Catalog
# 4. Configure compute and traffic settings
# 5. Deploy and test your endpoint
```

Or use the Databricks deployment client programmatically:

```python
from mlflow.deployments import get_deploy_client

# Create deployment client
client = get_deploy_client("databricks")

# Create serving endpoint
endpoint = client.create_endpoint(
    config={
        "name": "wine-quality-endpoint",
        "config": {
            "served_entities": [
                {
                    "entity_name": "main.default.wine_quality_predictor",
                    "entity_version": "1",
                    "workload_size": "Small",
                    "scale_to_zero_enabled": True,
                }
            ]
        },
    }
)

# Query the endpoint
response = client.predict(
    endpoint="wine-quality-endpoint",
    inputs={
        "dataframe_split": {
            "columns": [
                "fixed acidity",
                "volatile acidity",
                "citric acid",
                "residual sugar",
                "chlorides",
                "free sulfur dioxide",
                "total sulfur dioxide",
                "density",
                "pH",
                "sulphates",
                "alcohol",
            ],
            "data": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]],
        }
    },
)
```

## What You've Accomplished

🎉 **Congratulations!** You've completed a full MLOps workflow:

- ✅ **Optimized hyperparameters** using systematic search instead of guesswork
- ✅ **Tracked 15+ experiments** with complete reproducibility
- ✅ **Visualized results** to understand parameter relationships
- ✅ **Registered your best model** with proper versioning
- ✅ **Deployed to REST API** for real-time predictions
- ✅ **Containerized for production** deployment

## Next Steps

### Enhance Your MLOps Skills

- **Advanced Optimization**: Try [Optuna](https://optuna.org/) or [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for more sophisticated hyperparameter optimization. Both work seamlessly with MLflow.
- **Model Monitoring**: Implement drift detection and performance monitoring in production
- **A/B Testing**: Compare model versions in production using MLflow's model registry
- **CI/CD Integration**: Automate model training and deployment with GitHub Actions or similar

### Scale Your Infrastructure with a [Tracking Server](/ml/getting-started/tracking-server-overview)

- **MLflow on Kubernetes**: Deploy MLflow tracking server on K8s for team collaboration
- **Database Backend**: Use PostgreSQL or MySQL instead of file-based storage
- **Artifact Storage**: Configure S3, Azure Blob, or GCS for model artifacts
- **Authentication**: Add user management and access controls with built-in [Authentication](/self-hosting/security/basic-http-auth)

The foundation you've built here scales to any machine learning problem. The key principles—systematic experimentation, comprehensive tracking, and automated deployment—remain constant across domains and complexity levels.


--- docs/docs/classic-ml/getting-started/logging-first-model/index.mdx ---
---
sidebar-position: 3
---

import { NotebookDownloadButton } from "@site/src/components/NotebookDownloadButton";

# Your First MLflow Model: Complete Tutorial

Master the fundamentals of MLflow by building your first end-to-end machine learning workflow. This hands-on tutorial takes you from setup to deployment, covering all the essential MLflow concepts you need to succeed.

## What You'll Build

By the end of this tutorial, you'll have created a complete ML pipeline that:

- 🎯 **Predicts apple quality** using a synthetic dataset you'll generate
- 📊 **Tracks experiments** with parameters, metrics, and model artifacts
- 🔍 **Compares model performance** using the MLflow UI
- 📦 **Registers your best model** for production use
- 🚀 **Deploys a working API** for real-time predictions

:::info Perfect for Beginners
🎓 No prior MLflow experience required. We'll guide you through every concept with clear explanations and practical examples.

⏱️ Complete the full tutorial at your own pace in 30-45 minutes, with each step building naturally on the previous one.
:::

## Learning Path

This tutorial is designed as a progressive learning experience:

### **Phase 1: Setup & Foundations** (10 minutes)

- [🖥️ Start Your MLflow Tracking Server](/ml/getting-started/logging-first-model/step1-tracking-server) - Get your local environment running
- [🔌 Master the MLflow Client API](/ml/getting-started/logging-first-model/step2-mlflow-client) - Learn the programmatic interface
- [📁 Understand MLflow Experiments](/ml/getting-started/logging-first-model/step3-create-experiment) - Organize your ML work

### **Phase 2: Data & Experimentation** (15 minutes)

- [🔍 Search and Filter Experiments](/ml/getting-started/logging-first-model/step4-experiment-search) - Navigate your work efficiently
- [🍎 Generate Your Apple Dataset](/ml/getting-started/logging-first-model/step5-synthetic-data) - Create realistic training data
- [📈 Log Your First ML Runs](/ml/getting-started/logging-first-model/step6-logging-a-run) - Track parameters, metrics, and models

## What Makes This Tutorial Special

### **Real-World Focused**

Instead of toy examples, you'll work with a realistic apple quality prediction problem that demonstrates practical ML workflows.

### **Hands-On Learning**

Every concept is immediately applied through code examples that you can run and modify.

### **Complete Workflow**

Experience the full ML lifecycle from data creation to model deployment, not just isolated features.

### **Visual Learning**

Extensive use of the MLflow UI helps you understand how tracking data appears in practice.

## Prerequisites

- **Python 3.8+** installed on your system
- **Basic Python knowledge** (variables, functions, loops)
- **10 minutes** for initial setup

No machine learning expertise required - we'll explain the ML concepts as we go!

## Two Ways to Follow Along

### **Interactive Web Tutorial** (Recommended)

Follow the step-by-step guide in your browser with detailed explanations and screenshots. Perfect for understanding concepts deeply.

[▶️ **Start the Interactive Tutorial**](/ml/getting-started/logging-first-model/step1-tracking-server)

### **Jupyter Notebook**

Download and run the complete tutorial locally. Great for experimentation and customization.

<NotebookDownloadButton href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/docs/classic-ml/getting-started/logging-first-model/notebooks/logging-first-model.ipynb">📓 Download the Complete Notebook</NotebookDownloadButton>

## Key Concepts You'll Master

**🖥️ MLflow Tracking Server**
Set up and connect to the central hub that stores all your ML experiments and artifacts.

**🔬 Experiments & Runs**
Organize your ML work into logical groups and track individual training sessions with complete reproducibility.

**📊 Metrics & Parameters**
Log training performance, hyperparameters, and model configuration for easy comparison and optimization.

**🤖 Model Artifacts**
Save trained models with proper versioning and metadata for consistent deployment and sharing.

**🏷️ Tags & Organization**
Use tags and descriptions to keep your experiments organized and searchable as your projects grow.

**🔍 Search & Discovery**
Find and compare experiments efficiently using MLflow's powerful search and filtering capabilities.

## What Happens Next

After completing this tutorial, you'll be ready to:

- **Apply MLflow to your own projects** with confidence in the core concepts
- **Explore advanced features** like hyperparameter tuning and A/B testing
- **Scale to team workflows** with shared tracking servers and model registries
- **Deploy production models** using MLflow's serving capabilities

## Ready to Begin?

Choose your preferred learning style and dive in! The tutorial is designed to be completed in one session, but you can also bookmark your progress and return anytime.

:::tip Get Started Now
**Interactive Tutorial**: [🚀 Start Step 1 - Tracking Server](/ml/getting-started/logging-first-model/step1-tracking-server)

**Notebook Version**: Use the download button above to get the complete Jupyter notebook
:::

---

**Questions or feedback?** This tutorial is continuously improved based on user input. Let us know how we can make your learning experience even better!


--- docs/docs/classic-ml/tracking/quickstart/index.mdx ---
---
sidebar_position: 2
---

import { CardGroup, PageCard } from "@site/src/components/Card";
import Link from "@docusaurus/Link";
import { NotebookDownloadButton } from "@site/src/components/NotebookDownloadButton";
import { Table } from "@site/src/components/Table";

# MLflow Tracking Quickstart

Welcome to MLflow!

The purpose of this quickstart is to provide a quick guide to the most essential core APIs of MLflow Tracking.
Specifically, those that enable the logging, registering, and loading of a model for inference.

:::note
For a more in-depth and tutorial-based approach (if that is your style), please see the
[Getting Started with MLflow](/ml/getting-started/logging-first-model) tutorial. We recommend that you start here first, though, as this quickstart
uses the most common and frequently-used APIs for MLflow Tracking and serves as a good foundation for the other tutorials in the documentation.
:::

## What you will learn

In just a few minutes of following along with this quickstart, you will learn:

- How to **log** parameters, metrics, and a model
- The basics of the **MLflow fluent API**
- How to **register** a model during logging
- How to navigate to a model in the **MLflow UI**
- How to **load** a logged model for inference

:::note
If you would prefer to view a Jupyter Notebook version of this tutorial, click the following link:

<Link className="button button--primary" to="notebooks/tracking_quickstart" target="_blank">
  <span>View the Notebook</span>
</Link>
:::

## Step 1 - Get MLflow

MLflow is available on PyPI.

### Installing Stable Release

If you don't already have it installed on your system, you can install it with:

```bash
pip install mlflow
```

### Installing a Release Candidate (RC)

If you are eager to test out new features and validate that an upcoming release of MLflow will work well in your infrastructure, installing the latest
release candidate may be of interest to you.

:::note
Release Candidate builds are not recommended for actual use, rather they are intended only for testing validation.
:::

To install the latest version of MLflow's release candidates for a given version, see the example below that uses MLflow 2.14.0 as an example:

```bash
# install the latest release candidate
pip install --pre mlflow

# or install a specific rc version
pip install mlflow==3.1.0rc0
```

## Step 2 - Start a Tracking Server

### Using a Managed MLflow Tracking Server

For details on options for using a managed MLflow Tracking Server, including how to create a Databricks Free Trial account with
managed MLflow, [see the guide for tracking server options](/ml/getting-started/running-notebooks/).

### Run a local Tracking Server

We're going to start a local MLflow Tracking Server, which we will connect to for logging our data for this quickstart.
From a terminal, run:

```bash
mlflow server --host 127.0.0.1 --port 8080
```

:::note
You can choose any port that you would like, provided that it's not already in use.
:::

### Set the Tracking Server URI (if not using a Databricks Managed MLflow Tracking Server)

If you're using a managed MLflow Tracking Server that is not provided by Databricks, or if you're running a local tracking server,
ensure that you set the tracking server's uri using:

```python
import mlflow

mlflow.set_tracking_uri(uri="http://<host>:<port>")
```

If this is not set within your notebook or runtime environment, the runs will be logged to your local file system.

## Step 3 - Train a model and prepare metadata for logging

In this section, we're going to log a model with MLflow. A quick overview of the steps are:

- Load and prepare the Iris dataset for modeling.
- Train a Logistic Regression model and evaluate its performance.
- Prepare the model hyperparameters and calculate metrics for logging.

```python
import mlflow
from mlflow.models import infer_signature

import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


# Load the Iris dataset
X, y = datasets.load_iris(return_X_y=True)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Define the model hyperparameters
params = {
    "solver": "lbfgs",
    "max_iter": 1000,
    "random_state": 8888,
}

# Train the model
lr = LogisticRegression(**params)
lr.fit(X_train, y_train)

# Predict on the test set
y_pred = lr.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
```

## Step 4 - Log the model and its metadata to MLflow

In this next step, we're going to use the model that we trained, the hyperparameters that we specified for the model's fit, and the
loss metrics that were calculated by evaluating the model's performance on the test data to log to MLflow.

The steps that we will take are:

- Initiate an MLflow **run** context to start a new run that we will log the model and metadata to.
- **Log** model **parameters** and performance **metrics**.
- **Tag** the run for easy retrieval.
- **Register** the model in the MLflow Model Registry while **logging** (saving) the model.

:::note
While it can be valid to wrap the entire code within the `start_run` block, this is **not recommended**. If there as in issue with the
training of the model or any other portion of code that is unrelated to MLflow-related actions, an empty or partially-logged run will be
created, which will necessitate manual cleanup of the invalid run. It is best to keep the training execution outside of the run context block
to ensure that the loggable content (parameters, metrics, artifacts, and the model) are fully materialized prior to logging.
:::

```python
# Set our tracking server uri for logging
mlflow.set_tracking_uri(uri="http://127.0.0.1:8080")

# Create a new MLflow Experiment
mlflow.set_experiment("MLflow Quickstart")

# Start an MLflow run
with mlflow.start_run():
    # Log the hyperparameters
    mlflow.log_params(params)

    # Log the loss metric
    mlflow.log_metric("accuracy", accuracy)

    # Infer the model signature
    signature = infer_signature(X_train, lr.predict(X_train))

    # Log the model, which inherits the parameters and metric
    model_info = mlflow.sklearn.log_model(
        sk_model=lr,
        name="iris_model",
        signature=signature,
        input_example=X_train,
        registered_model_name="tracking-quickstart",
    )

    # Set a tag that we can use to remind ourselves what this model was for
    mlflow.set_logged_model_tags(
        model_info.model_id, {"Training Info": "Basic LR model for iris data"}
    )
```

:::note Console Output
When you run the above code, you will see console output that looks something like this:

```text
2025/09/09 17:22:20 ERROR mlflow.webhooks.delivery: Failed to deliver webhook for event registered_model.created: FileStore does not support list_webhooks_by_event
Traceback (most recent call last):
  ...
NotImplementedError: FileStore does not support list_webhooks_by_event
```

You can ignore these errors. They will go away in [MLflow 4](https://github.com/mlflow/mlflow/issues/17562#issuecomment-3259679453) and will not be fixed until then.
:::

## Step 5 - Load the model as a Python Function (pyfunc) and use it for inference

After logging the model, we can perform inference by:

- **Loading** the model using MLflow's `pyfunc` flavor.
- Running **Predict** on new data using the loaded model.

:::note
The iris training data that we used was a numpy array structure. However, we can submit a Pandas DataFrame as well to the `predict` method, as shown
below.
:::

```python
# Load the model back for predictions as a generic Python Function model
loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)

predictions = loaded_model.predict(X_test)

iris_feature_names = datasets.load_iris().feature_names

result = pd.DataFrame(X_test, columns=iris_feature_names)
result["actual_class"] = y_test
result["predicted_class"] = predictions

result[:4]
```

The output of this code will look something like this:

<Table>
  <thead>
    <tr>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>actual_class</th>
      <th>predicted_class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>6.1</td>
      <td>2.8</td>
      <td>4.7</td>
      <td>1.2</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>5.7</td>
      <td>3.8</td>
      <td>1.7</td>
      <td>0.3</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>7.7</td>
      <td>2.6</td>
      <td>6.9</td>
      <td>2.3</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>6.0</td>
      <td>2.9</td>
      <td>4.5</td>
      <td>1.5</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</Table>

## Step 6 - View the Run and Model in the MLflow UI

In order to see the results of our run, we can navigate to the MLflow UI. Since we have already started the Tracking Server at
_http://localhost:8080_, we can simply navigate to that URL in our browser.

When opening the site, you will see a screen similar to the following:

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Experiment view page](/images/tutorials/introductory/quickstart-tracking/quickstart-our-experiment.png)
  <figcaption>The main MLflow Tracking page, showing Experiments that have been created</figcaption>
</figure>

Clicking on the name of the Experiment that we created ("MLflow Quickstart") will give us a list of runs associated with the
Experiment. You should see a random name that has been generated for the run and nothing else show up in the `Table` list view to the right.

Clicking on the name of the run will take you to the Run page, where the details of what we've logged will be shown. The elements have
been highlighted below to show how and where this data is recorded within the UI.

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Run view page](/images/tutorials/introductory/quickstart-tracking/quickstart-our-run.png)
  <figcaption>The run view page for our run</figcaption>
</figure>

Switch to the Models tab in the experiments page to view all the logged models under the Experiment, where you can see an entry for the logged model we just created ("tracking-quickstart").

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Experiment view page models tab](/images/tutorials/introductory/quickstart-tracking/quickstart-our-experiment-models-tab.png)
  <figcaption>The models tab of the MLflow Tracking page, showing a list of all models created</figcaption>
</figure>

Clicking on the name of the model will take you to the Logged Model page, with details on the logged model and its metadata.

<figure className="center-div" style={{ width: 1024, maxWidth: "100%", textAlign: "center" }}>
  ![MLflow UI Model view page](/images/tutorials/introductory/quickstart-tracking/quickstart-our-model.png)
  <figcaption>The model view page for our logged model</figcaption>
</figure>

## Conclusion

Congratulations on working through the MLflow Tracking Quickstart! You should now have a basic understanding of how to use the MLflow Tracking API to log
models.

If you are interested in a more in-depth tutorial, please see the [Getting Started with MLflow](/ml/getting-started/logging-first-model) tutorial as a
good next step in increasing your knowledge about MLflow!


--- examples/README.md ---
## MLflow examples

### Quick Start example

- `quickstart/mlflow_tracking.py` is a basic example to introduce MLflow concepts.

## Tutorials

Various examples that depict MLflow tracking, project, and serving use cases.

- `h2o` depicts how MLflow can be use to track various random forest architectures to train models
  for predicting wine quality.
- `hyperparam` shows how to do hyperparameter tuning with MLflow and some popular optimization libraries.
- `keras` modifies
  [a Keras classification example](https://github.com/keras-team/keras/blob/ed07472bc5fc985982db355135d37059a1f887a9/examples/reuters_mlp.py)
  and uses MLflow's `mlflow.tensorflow.autolog()` API to automatically log metrics and parameters
  to MLflow during training.
- `multistep_workflow` is an end-to-end of a data ETL and ML training pipeline built as an MLflow
  project. The example shows how parts of the workflow can leverage from previously run steps.
- `pytorch` uses CNN on MNIST dataset for character recognition. The example logs TensorBoard events
  and stores (logs) them as MLflow artifacts.
- `remote_store` has a usage example of REST based backed store for tracking.
- `r_wine` demonstrates how to log parameters, metrics, and models from R.
- `sklearn_elasticnet_diabetes` uses the sklearn diabetes dataset to predict diabetes progression
  using ElasticNet.
- `sklearn_elasticnet_wine_quality` is an example for MLflow projects. This uses the Wine
  Quality dataset and Elastic Net to predict quality. The example uses `MLproject` to set up a
  Conda environment, define parameter types and defaults, entry point for training, etc.
- `sklearn_logistic_regression` is a simple MLflow example with hooks to log training data to MLflow
  tracking server.
- `supply_chain_security` shows how to strengthen the security of ML projects against supply-chain attacks by enforcing hash checks on Python packages.
- `tensorflow` contains end-to-end one run examples from train to predict for TensorFlow 2.8+ It includes usage of MLflow's
  `mlflow.tensorflow.autolog()` API, which captures TensorBoard data and logs to MLflow with no code change.
- `docker` demonstrates how to create and run an MLflow project using docker (rather than conda)
  to manage project dependencies
- `johnsnowlabs` gives you access to [20.000+ state-of-the-art enterprise NLP models in 200+ languages](https://nlp.johnsnowlabs.com/models) for medical, finance, legal and many more domains.

## Demos

- `demos` folder contains notebooks used during MLflow presentations.


--- examples/auth/README.md ---
# Basic authentication example

This example demonstrates the authentication and authorization feature of MLflow.

To run this example,

1. Start the tracking server
   ```shell
   mlflow ui --app-name=basic-auth
   ```
2. Go to `http://localhost:5000/signup` and register two users:
   - `(user_a, password_a)`
   - `(user_b, password_b)`
3. Run the script
   ```shell
   python auth.py
   ```
   Expected output:
   ```
   2023/05/02 14:03:58 INFO mlflow.tracking.fluent: Experiment with name 'experiment_a' does not exist. Creating a new experiment.
   {}
   API request to endpoint /api/2.0/mlflow/runs/create failed with error code 403 != 200. Response body: 'Permission denied'
   ```


--- examples/deployments/README.md ---
# MLflow Deployments

The examples provided within this directory show how to get started with MLflow Deployments using:

- Databricks (see the `databricks` subdirectory)


--- examples/evaluation/README.md ---
### MLflow evaluation Examples

The examples in this directory demonstrate how to use the `mlflow.evaluate()` API. Specifically,
they show how to evaluate a PyFunc model on a specified dataset using the builtin default evaluator
and specified extra metrics, where the resulting metrics & artifacts are logged to MLflow Tracking.
They also show how to specify validation thresholds for the resulting metrics to validate the quality
of your model. See full list of examples below:

- Example `evaluate_on_binary_classifier.py` evaluates an xgboost `XGBClassifier` model on dataset loaded by
  `shap.datasets.adult`.
- Example `evaluate_on_multiclass_classifier.py` evaluates a scikit-learn `LogisticRegression` model on dataset
  generated by `sklearn.datasets.make_classification`.
- Example `evaluate_on_regressor.py` evaluate as scikit-learn `LinearRegression` model on dataset loaded by
  `sklearn.datasets.fetch_california_housing`
- Example `evaluate_with_custom_metrics.py` evaluates a scikit-learn `LinearRegression`
  model with a custom metric function on dataset loaded by `sklearn.datasets.fetch_california_housing`
- Example `evaluate_with_custom_metrics_comprehensive.py` evaluates a scikit-learn `LinearRegression` model
  with a comprehensive list of custom metric functions on dataset loaded by `sklearn.datasets.fetch_california_housing`
- Example `evaluate_with_model_validation.py` trains both a candidate xgboost `XGBClassifier` model
  and a baseline `DummyClassifier` model on dataset loaded by `shap.datasets.adult`. Then, it validates
  the candidate model against specified thresholds on both builtin and extra metrics and the dummy model.

#### Prerequisites

```
pip install scikit-learn xgboost shap>=0.40 matplotlib
```

#### How to run the examples

Run in this directory with Python.

```sh
python evaluate_on_binary_classifier.py
python evaluate_on_multiclass_classifier.py
python evaluate_on_regressor.py
python evaluate_with_custom_metrics.py
python evaluate_with_custom_metrics_comprehensive.py
python evaluate_with_model_vaidation.py
```


--- examples/gateway/README.md ---
# MLflow AI Gateway

The examples provided within this directory show how to get started with individual providers and at least
one of the supported endpoint types. When configuring an instance of the MLflow AI Gateway, multiple providers,
instances of endpoint types, and model versions can be specified for each query endpoint on the server.

## Example configuration files

Within this directory are example config files for each of the supported providers. If using these as a guide
for configuring a large number of endpoints, ensure that the placeholder names (i.e., "completions", "chat", "embeddings")
are modified to prevent collisions. These names are provided for clarity only for the examples and real-world
use cases should define a relevant and meaningful endpoint name to eliminate ambiguity and minimize the chances of name collisions.

# Getting Started with MLflow AI Gateway for OpenAI

This guide will walk you through the installation and basic setup of the MLflow AI Gateway.
Within sub directories of this examples section, you can find specific executable examples
that can be used to validate a given provider's configuration through the MLflow AI Gateway.
Let's get started.

## Step 1: Installing the MLflow AI Gateway

The MLflow AI Gateway is best installed from PyPI. Open your terminal and use the following pip command:

```sh
# Installation from PyPI
pip install 'mlflow[genai]'
```

For those interested in development or in using the most recent build of the MLflow AI Gateway, you may choose to install from the fork of the repository:

```sh
# Installation from the repository
pip install -e '.[genai]'
```

## Step 2: Configuring Endpoints

Each provider has a distinct set of allowable endpoint types (i.e., chat, completions, etc) and
specific requirements for the initialization of the endpoints to interface with their services.
For full examples of configurations and supported endpoint types, see:

- [OpenAI](openai/config.yaml)
- [MosaicML](mosaicml/config.yaml)
- [Anthropic](anthropic/config.yaml)
- [Cohere](cohere/config.yaml)
- [AI21 Labs](ai21labs/config.yaml)
- [PaLM](palm/config.yaml)
- [AzureOpenAI](azure_openai/config.yaml)
- [Mistral](mistral/config.yaml)
- [TogetherAI](togetherai/config.yaml)

## Step 3: Setting Access Keys

See information on specific methods of obtaining and setting the access keys within the provider-specific documentation within this directory.

## Step 4: Starting the MLflow AI Gateway

With the MLflow configuration file in place and access key(s) set, you can now start the MLflow AI Gateway.
Replace `<provider>` with the actual path to the MLflow configuration file for the provider of your choice:

```sh
mlflow gateway start --config-path examples/gateway/<provider>/config.yaml --port 7000

# For example:
mlflow gateway start --config-path examples/gateway/openai/config.yaml --port 7000
```

## Step 5: Accessing the Interactive API Documentation

With the MLflow AI Gateway up and running, access its interactive API documentation by navigating to the following URL:

http://127.0.0.1:7000/docs

## Step 6: Sending Test Requests

After successfully setting up the MLflow AI Gateway, you can send a test request using the provided Python script.
Replace <provider> with the name of the provider example test script that you'd like to use:

```sh
python examples/gateway/<provider>/example.py
```


--- examples/lightgbm/README.md ---
# Examples for LightGBM Autologging

LightGBM autologging functionalities are demonstrated through two examples. The first example in the `lightgbm_native` folder logs a Booster model trained by `lightgbm.train()`. The second example in the `lightgbm_sklearn` folder shows how autologging works for LightGBM scikit-learn models. The autologging for all LightGBM models is enabled via `mlflow.lightgbm.autolog()`.


--- examples/llms/README.md ---
# MLflow examples for LLM use cases

This directory includes several examples for tracking, evaluating, and scoring models with LLMs.

## Summarization

The `summarization/summarization.py` script uses prompt engineering to build two summarization models for news articles with LangChain. It leverages the `mlflow.langchain` flavor to package and log the models to MLflow, `mlflow.evaluate()` to evaluate each model's performance on a small example dataset, and `mlflow.pyfunc.load_model()` to load and score the best packaged model on a new example article.

To run the example as an MLflow Project, simply execute the following command from this directory:

```
$ cd summarization && mlflow run .
```

To run the example as a Python script, simply execute the following command from this directory:

```
$ cd summarization && python summarization.py
```

Note that this example requires MLflow 2.4.0 or greater to run. Additionally, you must have [LangChain](https://python.langchain.com/en/latest/index.html) and the [OpenAI Python client](https://pypi.org/project/openai/) installed in order to run the example. We also recommend installing the [Hugging Face Evaluate library](https://huggingface.co/docs/evaluate/index) to compute [ROUGE metrics](<https://en.wikipedia.org/wiki/ROUGE_(metric)>) for summary quality. Finally, you must specify a valid OpenAI API key in the `OPENAI_API_KEY` environment variable.

## Question answering

The `question_answering/question_answering.py` script uses prompt engineering to build two models that answer questions about MLflow.

It leverages the `mlflow.openai` flavor to package and log the models to MLflow, `mlflow.evaluate()` to evaluate each model's performance on some example questions, and `mlflow.pyfunc.load_model()` to load and score the best packaged model on a new example question.

To run the example as an MLflow Project, simply execute the following command from this directory:

```
$ cd question_answering && mlflow run .
```

To run the example as a Python script, simply execute the following command from this directory:

```
$ cd question_answering && python question_answering.py
```

Note that this example requires MLflow 2.4.0 or greater to run. Additionally, you must have the [OpenAI Python client](https://pypi.org/project/openai/), [tiktoken](https://pypi.org/project/tiktoken/), and [tenacity](https://pypi.org/project/tenacity/) installed in order to run the example. Finally, you must specify a valid OpenAI API key in the `OPENAI_API_KEY` environment variable.


--- examples/mlflow_artifacts/README.md ---
# MLflow Artifacts Example

This directory contains a set of files for demonstrating the MLflow Artifacts Service.

## What does the MLflow Artifacts Service do?

The MLflow Artifacts Service serves as a proxy between the client and artifact storage (e.g. S3)
and allows the client to upload, download, and list artifacts via REST API without configuring
a set of credentials required to access resources in the artifact storage (e.g. `AWS_ACCESS_KEY_ID`
and `AWS_SECRET_ACCESS_KEY` for S3).

## Quick start

First, launch the tracking server with the artifacts service via `mlflow server`:

```sh
# Launch a tracking server with the artifacts service
$ mlflow server \
    --backend-store-uri=mlruns \
    --artifacts-destination ./mlartifacts \
    --default-artifact-root http://localhost:5000/api/2.0/mlflow-artifacts/artifacts/experiments \
    --gunicorn-opts "--log-level debug"
```

Notes:

- `--artifacts-destination` specifies the base artifact location from which to resolve artifact upload/download/list requests. In this examples, we're using a local directory `./mlartifacts`, but it can be changed to a s3 bucket or
- `--default-artifact-root` points to the `experiments` directory of the artifacts service. Therefore, the default artifact location of a newly-created experiment is set to `./mlartifacts/experiments/<experiment_id>`.
- `--gunicorn-opts "--log-level debug"` is specified to print out request logs but can be omitted if unnecessary.
- `--artifacts-only` disables all other endpoints for the tracking server apart from those involved in listing, uploading, and downloading artifacts. This makes the MLflow server a single-purpose proxy for artifact handling only.

Then, run `example.py` that performs upload, download, and list operations for artifacts:

```
$ MLFLOW_TRACKING_URI=http://localhost:5000 python example.py
```

After running the command above, the server should print out request logs for artifact operations:

```diff
...
[2021-11-05 19:13:34 +0900] [92800] [DEBUG] POST /api/2.0/mlflow/runs/create
[2021-11-05 19:13:34 +0900] [92800] [DEBUG] GET /api/2.0/mlflow/runs/get
[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/a.txt
[2021-11-05 19:13:34 +0900] [92802] [DEBUG] PUT /api/2.0/mlflow-artifacts/artifacts/0/a1b2c3d4/artifacts/dir/b.txt
[2021-11-05 19:13:34 +0900] [92802] [DEBUG] POST /api/2.0/mlflow/runs/update
[2021-11-05 19:13:34 +0900] [92802] [DEBUG] GET /api/2.0/mlflow-artifacts/artifacts
...
```

The contents of the `mlartifacts` directory should look like this:

```sh
$ tree mlartifacts
mlartifacts
└── experiments
    └── 0  # experiment ID
        └── a1b2c3d4  # run ID
            └── artifacts
                ├── a.txt
                └── dir
                    └── b.txt

5 directories, 2 files
```

To delete the logged artifacts, run the following command:

```bash
mlflow gc --backend-store-uri=mlruns --run-ids <run_id>
```

### Clean up

```sh
# Remove experiment and run data
$ rm -rf mlruns

# Remove artifacts
$ rm -rf mlartifacts
```

## Advanced example using `docker-compose`

[`docker-compose.yml`](./docker-compose.yml) provides a more advanced setup than the quick-start example above:

- Tracking service uses PostgreSQL as a backend store.
- Artifact service uses MinIO as a artifact store.
- Tracking and artifacts services are running on different servers.

```sh
# Build services
$ docker-compose build

# Launch tracking and artifacts servers in the background
$ docker-compose up -d

# Run `example.py` in the client container
$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py
```

You can view the logged artifacts on MinIO Console served at http://localhost:9001. The login username and password are `user` and `password`.

### Clean up

```sh
# Remove containers, networks, volumes, and images
$ docker-compose down --rmi all --volumes --remove-orphans
```

### Development

```sh
# Build services using the dev version of mlflow
$ ./build.sh
$ docker-compose run -v ${PWD}/example.py:/app/example.py client python example.py
```


--- examples/pyfunc/README.md ---
# Pyfunc model example

This example demonstrates the use of a pyfunc model with custom inference logic.
More specifically:

- train a simple classification model
- create a _pyfunc_ model that encapsulates the classification model with an attached module for custom inference logic

## Structure of this example

This examples contains a `train.py` file that trains a scikit-learn model with iris dataset and uses MLflow Tracking APIs to log the model. The nested **mlflow run** delivers the packaging of `pyfunc` model and `custom_code` module is attached
to act as a custom inference logic layer in inference time.

```
├── train.py
├── infer_model_code_path.py
└── custom_code.py
```

## Running this example

1. Train and log the model

```
$ python train.py
```

or train and log the model using inferred code paths

```
$ python infer_model_code_paths.py
```

2. Serve the pyfunc model

```bash
# Replace <pyfunc_run_id> with the run ID obtained in the previous step
$ mlflow models serve -m "runs:/<pyfunc_run_id>/model" -p 5001
```

3. Send a request

```
$ curl http://127.0.0.1:5001/invocations -H 'Content-Type: application/json' -d '{
  "dataframe_records": [[1, 1, 1, 1]]
}'
```

The response should look like this:

```
[0]
```


--- examples/pyspark_ml_autologging/README.md ---
# PySpark ML Autologging Examples

This directory contains examples for demonstrating how PySpark ML autologging works.

| File                     | Description                        |
| :----------------------- | :--------------------------------- |
| `logistic_regression.py` | Train a `LogisticRegression` model |
| `one_vs_rest.py`         | Train a `OneVsRest` model          |


--- mlflow/langchain/api_request_parallel_processor.py ---
# Based ons: https://github.com/openai/openai-cookbook/blob/6df6ceff470eeba26a56de131254e775292eac22/examples/api_request_parallel_processor.py
# Several changes were made to make it work with MLflow.
# Currently, only chat completion is supported.

"""
API REQUEST PARALLEL PROCESSOR

Using the LangChain API to process lots of text quickly takes some care.
If you trickle in a million API requests one by one, they'll take days to complete.
This script parallelizes requests using LangChain API.

Features:
- Streams requests from file, to avoid running out of memory for giant jobs
- Makes requests concurrently, to maximize throughput
- Logs errors, to diagnose problems with requests
"""

from __future__ import annotations

import logging
import queue
import threading
import time
import traceback
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Any

import langchain.chains
from langchain.callbacks.base import BaseCallbackHandler

import mlflow
from mlflow.exceptions import MlflowException
from mlflow.langchain.utils.chat import (
    transform_request_json_for_chat_if_necessary,
    try_transform_response_iter_to_chat_format,
    try_transform_response_to_chat_format,
)
from mlflow.langchain.utils.serialization import convert_to_serializable
from mlflow.pyfunc.context import Context, get_prediction_context
from mlflow.tracing.utils import maybe_set_prediction_context

_logger = logging.getLogger(__name__)


@dataclass
class StatusTracker:
    """
    Stores metadata about the script's progress. Only one instance is created.
    """

    num_tasks_started: int = 0
    num_tasks_in_progress: int = 0  # script ends when this reaches 0
    num_tasks_succeeded: int = 0
    num_tasks_failed: int = 0
    num_api_errors: int = 0  # excluding rate limit errors, counted above
    lock: threading.Lock = threading.Lock()

    def start_task(self):
        with self.lock:
            self.num_tasks_started += 1
            self.num_tasks_in_progress += 1

    def complete_task(self, *, success: bool):
        with self.lock:
            self.num_tasks_in_progress -= 1
            if success:
                self.num_tasks_succeeded += 1
            else:
                self.num_tasks_failed += 1

    def increment_num_api_errors(self):
        with self.lock:
            self.num_api_errors += 1


@dataclass
class APIRequest:
    """
    Stores an API request's inputs, outputs, and other metadata. Contains a method to make an API
    call.

    Args:
        index: The request's index in the tasks list
        lc_model: The LangChain model to call
        request_json: The request's input data
        results: The list to append the request's output data to, it's a list of tuples
            (index, response)
        errors: A dictionary to store any errors that occur
        convert_chat_responses: Whether to convert the model's responses to chat format
        did_perform_chat_conversion: Whether the input data was converted to chat format
            based on the model's type and input data.
        stream: Whether the request is a stream request
        prediction_context: The prediction context to use for the request
    """

    index: int
    lc_model: langchain.chains.base.Chain
    request_json: dict[str, Any]
    results: list[tuple[int, str]]
    errors: dict[int, str]
    convert_chat_responses: bool
    did_perform_chat_conversion: bool
    stream: bool
    params: dict[str, Any]
    prediction_context: Context | None = None

    def _predict_single_input(self, single_input, callback_handlers, **kwargs):
        config = kwargs.pop("config", {})
        config["callbacks"] = config.get("callbacks", []) + (callback_handlers or [])
        if self.stream:
            return self.lc_model.stream(single_input, config=config, **kwargs)
        if hasattr(self.lc_model, "invoke"):
            return self.lc_model.invoke(single_input, config=config, **kwargs)
        else:
            # for backwards compatibility, __call__ is deprecated and will be removed in 0.3.0
            # kwargs shouldn't have config field if invoking with __call__
            return self.lc_model(single_input, callbacks=callback_handlers, **kwargs)

    def _try_convert_response(self, response):
        if self.stream:
            return try_transform_response_iter_to_chat_format(response)
        else:
            return try_transform_response_to_chat_format(response)

    def single_call_api(self, callback_handlers: list[BaseCallbackHandler] | None):
        from langchain.schema import BaseRetriever

        from mlflow.langchain.utils.logging import langgraph_types, lc_runnables_types

        if isinstance(self.lc_model, BaseRetriever):
            # Retrievers are invoked differently than Chains
            response = self.lc_model.get_relevant_documents(
                **self.request_json, callbacks=callback_handlers, **self.params
            )
        elif isinstance(self.lc_model, lc_runnables_types() + langgraph_types()):
            if isinstance(self.request_json, dict):
                # This is a temporary fix for the case when spark_udf converts
                # input into pandas dataframe with column name, while the model
                # does not accept dictionaries as input, it leads to errors like
                # Expected Scalar value for String field 'query_text'
                try:
                    response = self._predict_single_input(
                        self.request_json, callback_handlers, **self.params
                    )
                except TypeError as e:
                    _logger.debug(
                        f"Failed to invoke {self.lc_model.__class__.__name__} "
                        f"with {self.request_json}. Error: {e!r}. Trying to "
                        "invoke with the first value of the dictionary."
                    )
                    self.request_json = next(iter(self.request_json.values()))
                    (
                        prepared_request_json,
                        did_perform_chat_conversion,
                    ) = transform_request_json_for_chat_if_necessary(
                        self.request_json, self.lc_model
                    )
                    self.did_perform_chat_conversion = did_perform_chat_conversion

                    response = self._predict_single_input(
                        prepared_request_json, callback_handlers, **self.params
                    )
            else:
                response = self._predict_single_input(
                    self.request_json, callback_handlers, **self.params
                )

            if self.did_perform_chat_conversion or self.convert_chat_responses:
                response = self._try_convert_response(response)
        else:
            # return_only_outputs is invalid for stream call
            if isinstance(self.lc_model, langchain.chains.base.Chain) and not self.stream:
                kwargs = {"return_only_outputs": True}
            else:
                kwargs = {}
            kwargs.update(**self.params)
            response = self._predict_single_input(self.request_json, callback_handlers, **kwargs)

            if self.did_perform_chat_conversion or self.convert_chat_responses:
                response = self._try_convert_response(response)
            elif isinstance(response, dict) and len(response) == 1:
                # to maintain existing code, single output chains will still return
                # only the result
                response = response.popitem()[1]

        return convert_to_serializable(response)

    def call_api(
        self, status_tracker: StatusTracker, callback_handlers: list[BaseCallbackHandler] | None
    ):
        """
        Calls the LangChain API and stores results.
        """
        _logger.debug(f"Request #{self.index} started with payload: {self.request_json}")

        try:
            with maybe_set_prediction_context(self.prediction_context):
                response = self.single_call_api(callback_handlers)
            _logger.debug(f"Request #{self.index} succeeded with response: {response}")
            self.results.append((self.index, response))
            status_tracker.complete_task(success=True)
        except Exception as e:
            self.errors[self.index] = (
                f"error: {e!r} {traceback.format_exc()}\n request payload: {self.request_json}"
            )
            status_tracker.increment_num_api_errors()
            status_tracker.complete_task(success=False)


def process_api_requests(
    lc_model,
    requests: list[Any | dict[str, Any]] | None = None,
    max_workers: int = 10,
    callback_handlers: list[BaseCallbackHandler] | None = None,
    convert_chat_responses: bool = False,
    params: dict[str, Any] | None = None,
    context: Context | None = None,
):
    """
    Processes API requests in parallel.
    """

    # initialize trackers
    retry_queue = queue.Queue()
    status_tracker = StatusTracker()  # single instance to track a collection of variables
    next_request = None  # variable to hold the next request to call
    context = context or get_prediction_context()

    results = []
    errors = {}

    # Note: we should call `transform_request_json_for_chat_if_necessary`
    # for the whole batch data, because the conversion should obey the rule
    # that if any record in the batch can't be converted, then all the record
    # in this batch can't be converted.
    (
        converted_chat_requests,
        did_perform_chat_conversion,
    ) = transform_request_json_for_chat_if_necessary(requests, lc_model)

    requests_iter = enumerate(converted_chat_requests)
    with ThreadPoolExecutor(
        max_workers=max_workers, thread_name_prefix="MlflowLangChainApi"
    ) as executor:
        while True:
            # get next request (if one is not already waiting for capacity)
            if not retry_queue.empty():
                next_request = retry_queue.get_nowait()
                _logger.warning(f"Retrying request {next_request.index}: {next_request}")
            elif req := next(requests_iter, None):
                # get new request
                index, converted_chat_request_json = req
                next_request = APIRequest(
                    index=index,
                    lc_model=lc_model,
                    request_json=converted_chat_request_json,
                    results=results,
                    errors=errors,
                    convert_chat_responses=convert_chat_responses,
                    did_perform_chat_conversion=did_perform_chat_conversion,
                    stream=False,
                    prediction_context=context,
                    params=params,
                )
                status_tracker.start_task()
            else:
                next_request = None

            # if enough capacity available, call API
            if next_request:
                # call API
                executor.submit(
                    next_request.call_api,
                    status_tracker=status_tracker,
                    callback_handlers=callback_handlers,
                )

            # if all tasks are finished, break
            # check next_request to avoid terminating the process
            # before extra requests need to be processed
            if status_tracker.num_tasks_in_progress == 0 and next_request is None:
                break

            time.sleep(0.001)  # avoid busy waiting

        # after finishing, log final status
        if status_tracker.num_tasks_failed > 0:
            raise mlflow.MlflowException(
                f"{status_tracker.num_tasks_failed} tasks failed. Errors: {errors}"
            )

        return [res for _, res in sorted(results)]


def process_stream_request(
    lc_model,
    request_json: Any | dict[str, Any],
    callback_handlers: list[BaseCallbackHandler] | None = None,
    convert_chat_responses: bool = False,
    params: dict[str, Any] | None = None,
):
    """
    Process single stream request.
    """
    if not hasattr(lc_model, "stream"):
        raise MlflowException(
            f"Model {lc_model.__class__.__name__} does not support streaming prediction output. "
            "No `stream` method found."
        )

    (
        converted_chat_requests,
        did_perform_chat_conversion,
    ) = transform_request_json_for_chat_if_necessary(request_json, lc_model)

    api_request = APIRequest(
        index=0,
        lc_model=lc_model,
        request_json=converted_chat_requests,
        results=None,
        errors=None,
        convert_chat_responses=convert_chat_responses,
        did_perform_chat_conversion=did_perform_chat_conversion,
        stream=True,
        prediction_context=get_prediction_context(),
        params=params,
    )
    with maybe_set_prediction_context(api_request.prediction_context):
        return api_request.single_call_api(callback_handlers)


--- mlflow/openai/api_request_parallel_processor.py ---
# Based ons: https://github.com/openai/openai-cookbook/blob/6df6ceff470eeba26a56de131254e775292eac22/examples/api_request_parallel_processor.py
# Several changes were made to make it work with MLflow.

"""
API REQUEST PARALLEL PROCESSOR

Using the OpenAI API to process lots of text quickly takes some care.
If you trickle in a million API requests one by one, they'll take days to complete.
If you flood a million API requests in parallel, they'll exceed the rate limits and fail with
errors. To maximize throughput, parallel requests need to be throttled to stay under rate limits.

This script parallelizes requests to the OpenAI API

Features:
- Makes requests concurrently, to maximize throughput
- Retries failed requests up to {max_attempts} times, to avoid missing data
- Logs errors, to diagnose problems with requests
"""

from __future__ import annotations

import logging
import threading
from concurrent.futures import FIRST_EXCEPTION, ThreadPoolExecutor, wait
from dataclasses import dataclass
from typing import Any, Callable

import mlflow

_logger = logging.getLogger(__name__)


@dataclass
class StatusTracker:
    """Stores metadata about the script's progress. Only one instance is created."""

    num_tasks_started: int = 0
    num_tasks_in_progress: int = 0  # script ends when this reaches 0
    num_tasks_succeeded: int = 0
    num_tasks_failed: int = 0
    num_rate_limit_errors: int = 0
    lock: threading.Lock = threading.Lock()
    error = None

    def start_task(self):
        with self.lock:
            self.num_tasks_started += 1
            self.num_tasks_in_progress += 1

    def complete_task(self, *, success: bool):
        with self.lock:
            self.num_tasks_in_progress -= 1
            if success:
                self.num_tasks_succeeded += 1
            else:
                self.num_tasks_failed += 1

    def increment_num_rate_limit_errors(self):
        with self.lock:
            self.num_rate_limit_errors += 1


def call_api(
    index: int,
    results: list[tuple[int, Any]],
    task: Callable[[], Any],
    status_tracker: StatusTracker,
):
    import openai

    status_tracker.start_task()
    try:
        result = task()
        _logger.debug(f"Request #{index} succeeded")
        status_tracker.complete_task(success=True)
        results.append((index, result))
    except openai.RateLimitError as e:
        status_tracker.complete_task(success=False)
        _logger.debug(f"Request #{index} failed with: {e}")
        status_tracker.increment_num_rate_limit_errors()
        status_tracker.error = mlflow.MlflowException(
            f"Request #{index} failed with rate limit: {e}."
        )
    except Exception as e:
        status_tracker.complete_task(success=False)
        _logger.debug(f"Request #{index} failed with: {e}")
        status_tracker.error = mlflow.MlflowException(
            f"Request #{index} failed with: {e.__cause__}"
        )


def process_api_requests(
    request_tasks: list[Callable[[], Any]],
    max_workers: int = 10,
):
    """Processes API requests in parallel"""
    # initialize trackers
    status_tracker = StatusTracker()  # single instance to track a collection of variables

    results: list[tuple[int, Any]] = []
    request_tasks_iter = enumerate(request_tasks)
    _logger.debug(f"Request pool executor will run {len(request_tasks)} requests")
    with ThreadPoolExecutor(
        max_workers=max_workers, thread_name_prefix="MlflowOpenAiApi"
    ) as executor:
        futures = [
            executor.submit(
                call_api,
                index=index,
                task=task,
                results=results,
                status_tracker=status_tracker,
            )
            for index, task in request_tasks_iter
        ]
        wait(futures, return_when=FIRST_EXCEPTION)

    # after finishing, log final status
    if status_tracker.num_tasks_failed > 0:
        if status_tracker.num_tasks_failed == 1:
            raise status_tracker.error
        raise mlflow.MlflowException(
            f"{status_tracker.num_tasks_failed} tasks failed. See logs for details."
        )
    if status_tracker.num_rate_limit_errors > 0:
        _logger.debug(
            f"{status_tracker.num_rate_limit_errors} rate limit errors received. "
            "Consider running at a lower rate."
        )

    return [res for _, res in sorted(results)]


--- mlflow/utils/_capture_modules.py ---
"""
This script should be executed in a fresh python interpreter process using `subprocess`.
"""

import argparse
import builtins
import functools
import importlib
import json
import os
import sys

import mlflow
from mlflow.models.model import MLMODEL_FILE_NAME, Model
from mlflow.pyfunc import MAIN
from mlflow.utils._spark_utils import _prepare_subprocess_environ_for_creating_local_spark_session
from mlflow.utils.exception_utils import get_stacktrace
from mlflow.utils.file_utils import write_to
from mlflow.utils.requirements_utils import (
    DATABRICKS_MODULES_TO_PACKAGES,
    MLFLOW_MODULES_TO_PACKAGES,
)


def _get_top_level_module(full_module_name):
    return full_module_name.split(".")[0]


def _get_second_level_module(full_module_name):
    return ".".join(full_module_name.split(".")[:2])


class _CaptureImportedModules:
    """
    A context manager to capture imported modules by temporarily applying a patch to
    `builtins.__import__` and `importlib.import_module`.

    If `record_full_module` is set to `False`, it only captures top level modules
    for inferring python package purpose.
    If `record_full_module` is set to `True`, it captures full module name for all
    imported modules and sub-modules. This is used in automatic model code path inference.
    """

    def __init__(self, record_full_module=False):
        self.imported_modules = set()
        self.original_import = None
        self.original_import_module = None
        self.record_full_module = record_full_module

    def _wrap_import(self, original):
        @functools.wraps(original)
        def wrapper(name, globals=None, locals=None, fromlist=(), level=0):
            is_absolute_import = level == 0
            if not self.record_full_module and is_absolute_import:
                self._record_imported_module(name)

            result = original(name, globals, locals, fromlist, level)

            if self.record_full_module:
                if is_absolute_import:
                    parent_modules = name.split(".")
                else:
                    parent_modules = globals["__name__"].split(".")
                    if level > 1:
                        parent_modules = parent_modules[: -(level - 1)]

                if fromlist:
                    for from_name in fromlist:
                        full_modules = parent_modules + [from_name]
                        full_module_name = ".".join(full_modules)
                        if full_module_name in sys.modules:
                            self._record_imported_module(full_module_name)
                else:
                    full_module_name = ".".join(parent_modules)
                    self._record_imported_module(full_module_name)

            return result

        return wrapper

    def _wrap_import_module(self, original):
        @functools.wraps(original)
        def wrapper(name, *args, **kwargs):
            self._record_imported_module(name)
            return original(name, *args, **kwargs)

        return wrapper

    def _record_imported_module(self, full_module_name):
        if self.record_full_module:
            self.imported_modules.add(full_module_name)
            return

        # If the module is an internal module (prefixed by "_") or is the "databricks"
        # module, which is populated by many different packages, don't record it (specific
        # module imports within the databricks namespace are still recorded and mapped to
        # their corresponding packages)
        if full_module_name.startswith("_") or full_module_name == "databricks":
            return

        top_level_module = _get_top_level_module(full_module_name)
        second_level_module = _get_second_level_module(full_module_name)

        if top_level_module == "databricks":
            # Multiple packages populate the `databricks` module namespace on Databricks;
            # to avoid bundling extraneous Databricks packages into model dependencies, we
            # scope each module to its relevant package
            if second_level_module in DATABRICKS_MODULES_TO_PACKAGES:
                self.imported_modules.add(second_level_module)
                return

            for databricks_module in DATABRICKS_MODULES_TO_PACKAGES:
                if full_module_name.startswith(databricks_module):
                    self.imported_modules.add(databricks_module)
                    return

        # special casing for mlflow extras since they may not be required by default
        if top_level_module == "mlflow":
            if second_level_module in MLFLOW_MODULES_TO_PACKAGES:
                self.imported_modules.add(second_level_module)
                return

        self.imported_modules.add(top_level_module)

    def __enter__(self):
        # Patch `builtins.__import__` and `importlib.import_module`
        self.original_import = builtins.__import__
        self.original_import_module = importlib.import_module
        builtins.__import__ = self._wrap_import(self.original_import)
        importlib.import_module = self._wrap_import_module(self.original_import_module)
        return self

    def __exit__(self, *_, **__):
        # Revert the patches
        builtins.__import__ = self.original_import
        importlib.import_module = self.original_import_module


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", required=True)
    parser.add_argument("--flavor", required=True)
    parser.add_argument("--output-file", required=True)
    parser.add_argument("--sys-path", required=True)
    parser.add_argument("--module-to-throw", required=False)
    parser.add_argument("--error-file", required=False)
    parser.add_argument("--record-full-module", default=False, action="store_true")
    return parser.parse_args()


def store_imported_modules(
    cap_cm, model_path, flavor, output_file, error_file=None, record_full_module=False
):
    # If `model_path` refers to an MLflow model directory, load the model using
    # `mlflow.pyfunc.load_model`
    if os.path.isdir(model_path) and MLMODEL_FILE_NAME in os.listdir(model_path):
        mlflow_model = Model.load(model_path)
        pyfunc_conf = mlflow_model.flavors.get(mlflow.pyfunc.FLAVOR_NAME)
        input_example = mlflow_model.load_input_example(model_path)
        params = mlflow_model.load_input_example_params(model_path)

        def load_model_and_predict(original_load_fn, *args, **kwargs):
            model = original_load_fn(*args, **kwargs)
            if input_example is not None:
                try:
                    model.predict(input_example, params=params)
                except Exception as e:
                    if error_file:
                        stack_trace = get_stacktrace(e)
                        write_to(
                            error_file,
                            "Failed to run predict on input_example, dependencies "
                            "introduced in predict are not captured.\n" + stack_trace,
                        )
                    else:
                        raise e
            return model

        if record_full_module:
            # Note: if we want to record all imported modules
            # (for inferring code_paths purpose),
            # The `importlib.import_module(pyfunc_conf[MAIN])` invocation
            # must be wrapped with `cap_cm` context manager,
            # because `pyfunc_conf[MAIN]` might also be a module loaded from
            # code_paths.
            with cap_cm:
                # `mlflow.pyfunc.load_model` internally invokes
                # `importlib.import_module(pyfunc_conf[MAIN])`
                mlflow.pyfunc.load_model(model_path)
        else:
            loader_module = importlib.import_module(pyfunc_conf[MAIN])
            original = loader_module._load_pyfunc

            @functools.wraps(original)
            def _load_pyfunc_patch(*args, **kwargs):
                with cap_cm:
                    return load_model_and_predict(original, *args, **kwargs)

            loader_module._load_pyfunc = _load_pyfunc_patch
            try:
                mlflow.pyfunc.load_model(model_path)
            finally:
                loader_module._load_pyfunc = original
    # Otherwise, load the model using `mlflow.<flavor>._load_pyfunc`.
    # For models that don't contain pyfunc flavor (e.g. scikit-learn estimator
    # that doesn't implement a `predict` method),
    # we need to directly pass a model data path to this script.
    else:
        with cap_cm:
            importlib.import_module(f"mlflow.{flavor}")._load_pyfunc(model_path)

    # Store the imported modules in `output_file`
    write_to(output_file, "\n".join(cap_cm.imported_modules))


def main():
    args = parse_args()
    model_path = args.model_path
    flavor = args.flavor
    output_file = args.output_file
    error_file = args.error_file
    # Mirror `sys.path` of the parent process
    sys.path = json.loads(args.sys_path)

    if flavor == mlflow.spark.FLAVOR_NAME:
        # Create a local spark environment within the subprocess
        from mlflow.utils._spark_utils import _create_local_spark_session_for_loading_spark_model

        _prepare_subprocess_environ_for_creating_local_spark_session()
        _create_local_spark_session_for_loading_spark_model()

    cap_cm = _CaptureImportedModules(record_full_module=args.record_full_module)
    store_imported_modules(
        cap_cm,
        model_path,
        flavor,
        output_file,
        error_file,
        record_full_module=args.record_full_module,
    )

    # Clean up a spark session created by `mlflow.spark._load_pyfunc`
    if flavor == mlflow.spark.FLAVOR_NAME:
        from mlflow.utils._spark_utils import _get_active_spark_session

        spark = _get_active_spark_session()
        if spark:
            try:
                spark.stop()
            except Exception:
                # Swallow unexpected exceptions
                pass


if __name__ == "__main__":
    main()


--- mlflow/utils/_capture_transformers_modules.py ---
"""
This script should be executed in a fresh python interpreter process using `subprocess`.
"""

import json
import os
import sys

import mlflow
from mlflow.exceptions import MlflowException
from mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE
from mlflow.utils._capture_modules import (
    _CaptureImportedModules,
    parse_args,
    store_imported_modules,
)


class _CaptureImportedModulesForHF(_CaptureImportedModules):
    """
    A context manager to capture imported modules by temporarily applying a patch to
    `builtins.__import__` and `importlib.import_module`.
    Used for 'transformers' flavor only.
    """

    def __init__(self, module_to_throw, record_full_module=False):
        super().__init__(record_full_module=record_full_module)
        self.module_to_throw = module_to_throw

    def _record_imported_module(self, full_module_name):
        if full_module_name == self.module_to_throw or full_module_name.startswith(
            f"{self.module_to_throw}."
        ):
            raise ImportError(f"Disabled package {full_module_name}")
        return super()._record_imported_module(full_module_name)


def main():
    args = parse_args()
    model_path = args.model_path
    flavor = args.flavor
    output_file = args.output_file
    module_to_throw = args.module_to_throw
    # Mirror `sys.path` of the parent process
    sys.path = json.loads(args.sys_path)

    if flavor != mlflow.transformers.FLAVOR_NAME:
        raise MlflowException(
            f"This script is only applicable to '{mlflow.transformers.FLAVOR_NAME}' flavor, "
            "if you're applying other flavors, please use _capture_modules script.",
        )

    if module_to_throw == "":
        raise MlflowException("Please specify the module to throw.")
    elif module_to_throw == "tensorflow":
        if os.environ.get("USE_TORCH", None) != "TRUE":
            raise MlflowException(
                "The environment variable USE_TORCH has to be set to TRUE to disable Tensorflow.",
                error_code=INVALID_PARAMETER_VALUE,
            )
    elif module_to_throw == "torch":
        if os.environ.get("USE_TF", None) != "TRUE":
            raise MlflowException(
                "The environment variable USE_TF has to be set to TRUE to disable Pytorch.",
                error_code=INVALID_PARAMETER_VALUE,
            )

    cap_cm = _CaptureImportedModulesForHF(
        module_to_throw, record_full_module=args.record_full_module
    )
    store_imported_modules(cap_cm, model_path, flavor, output_file)


if __name__ == "__main__":
    main()


--- mlflow/utils/class_utils.py ---
import importlib


def _get_class_from_string(fully_qualified_class_name):
    module, class_name = fully_qualified_class_name.rsplit(".", maxsplit=1)
    return getattr(importlib.import_module(module), class_name)


--- mlflow/server/fastapi_app.py ---
"""
FastAPI application wrapper for MLflow server.

This module provides a FastAPI application that wraps the existing Flask application
using WSGIMiddleware to maintain 100% API compatibility while enabling future migration
to FastAPI endpoints.
"""

from fastapi import FastAPI
from fastapi.middleware.wsgi import WSGIMiddleware
from flask import Flask

from mlflow.server import app as flask_app
from mlflow.server.fastapi_security import init_fastapi_security
from mlflow.server.job_api import job_api_router
from mlflow.server.otel_api import otel_router
from mlflow.version import VERSION


def create_fastapi_app(flask_app: Flask = flask_app):
    """
    Create a FastAPI application that wraps the existing Flask app.

    Returns:
        FastAPI application instance with the Flask app mounted via WSGIMiddleware.
    """
    # Create FastAPI app with metadata
    fastapi_app = FastAPI(
        title="MLflow Tracking Server",
        description="MLflow Tracking Server API",
        version=VERSION,
        # TODO: Enable API documentation when we have native FastAPI endpoints
        # For now, disable docs since we only have Flask routes via WSGI
        docs_url=None,
        redoc_url=None,
        openapi_url=None,
    )

    # Initialize security middleware BEFORE adding routes
    init_fastapi_security(fastapi_app)

    # Include OpenTelemetry API router BEFORE mounting Flask app
    # This ensures FastAPI routes take precedence over the catch-all Flask mount
    fastapi_app.include_router(otel_router)

    fastapi_app.include_router(job_api_router)

    # Mount the entire Flask application at the root path
    # This ensures compatibility with existing APIs
    # NOTE: This must come AFTER include_router to avoid Flask catching all requests
    fastapi_app.mount("/", WSGIMiddleware(flask_app))

    return fastapi_app


# Create the app instance that can be used by ASGI servers
app = create_fastapi_app()


--- mlflow/server/fastapi_security.py ---
import logging
from http import HTTPStatus

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from starlette.types import ASGIApp

from mlflow.environment_variables import (
    MLFLOW_SERVER_DISABLE_SECURITY_MIDDLEWARE,
    MLFLOW_SERVER_X_FRAME_OPTIONS,
)
from mlflow.server.security_utils import (
    CORS_BLOCKED_MSG,
    HEALTH_ENDPOINTS,
    INVALID_HOST_MSG,
    get_allowed_hosts_from_env,
    get_allowed_origins_from_env,
    get_default_allowed_hosts,
    is_allowed_host_header,
    is_api_endpoint,
    should_block_cors_request,
)

_logger = logging.getLogger(__name__)


class HostValidationMiddleware:
    """Middleware to validate Host headers using fnmatch patterns."""

    def __init__(self, app: ASGIApp, allowed_hosts: list[str]):
        self.app = app
        self.allowed_hosts = allowed_hosts

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            return await self.app(scope, receive, send)

        if scope["path"] in HEALTH_ENDPOINTS:
            return await self.app(scope, receive, send)

        headers = dict(scope.get("headers", []))
        host = headers.get(b"host", b"").decode("utf-8")

        if not is_allowed_host_header(self.allowed_hosts, host):
            _logger.warning(f"Rejected request with invalid Host header: {host}")

            async def send_403(message):
                if message["type"] == "http.response.start":
                    message["status"] = 403
                    message["headers"] = [(b"content-type", b"text/plain")]
                await send(message)

            await send_403({"type": "http.response.start", "status": 403, "headers": []})
            await send({"type": "http.response.body", "body": INVALID_HOST_MSG.encode()})
            return

        return await self.app(scope, receive, send)


class SecurityHeadersMiddleware:
    """Middleware to add security headers to all responses."""

    def __init__(self, app: ASGIApp):
        self.app = app
        self.x_frame_options = MLFLOW_SERVER_X_FRAME_OPTIONS.get()

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            return await self.app(scope, receive, send)

        async def send_wrapper(message):
            if message["type"] == "http.response.start":
                headers = dict(message.get("headers", []))
                headers[b"x-content-type-options"] = b"nosniff"

                if self.x_frame_options and self.x_frame_options.upper() != "NONE":
                    headers[b"x-frame-options"] = self.x_frame_options.upper().encode()

                if (
                    scope["method"] == "OPTIONS"
                    and message.get("status") == 200
                    and is_api_endpoint(scope["path"])
                ):
                    message["status"] = HTTPStatus.NO_CONTENT

                message["headers"] = list(headers.items())
            await send(message)

        await self.app(scope, receive, send_wrapper)


class CORSBlockingMiddleware:
    """Middleware to actively block cross-origin state-changing requests."""

    def __init__(self, app: ASGIApp, allowed_origins: list[str]):
        self.app = app
        self.allowed_origins = allowed_origins

    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            return await self.app(scope, receive, send)

        if not is_api_endpoint(scope["path"]):
            return await self.app(scope, receive, send)

        method = scope["method"]
        headers = dict(scope["headers"])
        origin = headers.get(b"origin", b"").decode("utf-8")

        if should_block_cors_request(origin, method, self.allowed_origins):
            _logger.warning(f"Blocked cross-origin request from {origin}")
            await send(
                {
                    "type": "http.response.start",
                    "status": HTTPStatus.FORBIDDEN,
                    "headers": [[b"content-type", b"text/plain"]],
                }
            )
            await send(
                {
                    "type": "http.response.body",
                    "body": CORS_BLOCKED_MSG.encode(),
                }
            )
            return

        await self.app(scope, receive, send)


def get_allowed_hosts() -> list[str]:
    """Get list of allowed hosts from environment or defaults."""
    return get_allowed_hosts_from_env() or get_default_allowed_hosts()


def get_allowed_origins() -> list[str]:
    """Get list of allowed CORS origins from environment or defaults."""
    return get_allowed_origins_from_env() or []


def init_fastapi_security(app: FastAPI) -> None:
    """
    Initialize security middleware for FastAPI application.

    This configures:
    - Host header validation (DNS rebinding protection) via TrustedHostMiddleware
    - CORS protection via CORSMiddleware
    - Security headers via custom middleware

    Args:
        app: FastAPI application instance.
    """
    if MLFLOW_SERVER_DISABLE_SECURITY_MIDDLEWARE.get() == "true":
        return

    app.add_middleware(SecurityHeadersMiddleware)

    allowed_origins = get_allowed_origins()

    if allowed_origins and "*" in allowed_origins:
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
            expose_headers=["*"],
        )
    else:
        app.add_middleware(CORSBlockingMiddleware, allowed_origins=allowed_origins)
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],
            allow_headers=["*"],
            expose_headers=["*"],
        )

    allowed_hosts = get_allowed_hosts()

    if allowed_hosts and "*" not in allowed_hosts:
        app.add_middleware(HostValidationMiddleware, allowed_hosts=allowed_hosts)


--- tests/pytorch/iris_data_module.py ---
import pytorch_lightning as pl
import torch
from sklearn.datasets import load_iris
from torch.utils.data import DataLoader, TensorDataset, random_split


class IrisDataModuleBase(pl.LightningDataModule):
    def __init__(self):
        super().__init__()
        self.columns = None

    def _get_iris_as_tensor_dataset(self):
        iris = load_iris()
        df = iris.data
        self.columns = iris.feature_names
        target = iris["target"]
        data = torch.Tensor(df).float()
        labels = torch.Tensor(target).long()
        return TensorDataset(data, labels)

    def setup(self, stage=None):
        # Assign train/val datasets for use in dataloaders
        if stage == "fit" or stage is None:
            iris_full = self._get_iris_as_tensor_dataset()
            self.train_set, self.val_set = random_split(iris_full, [130, 20])

        # Assign test dataset for use in dataloader(s)
        if stage == "test" or stage is None:
            self.train_set, self.test_set = random_split(self.train_set, [110, 20])


class IrisDataModule(IrisDataModuleBase):
    def train_dataloader(self):
        return DataLoader(self.train_set, batch_size=4)

    def val_dataloader(self):
        return DataLoader(self.val_set, batch_size=4)

    def test_dataloader(self):
        return DataLoader(self.test_set, batch_size=4)


class IrisDataModuleWithoutValidation(IrisDataModuleBase):
    def train_dataloader(self):
        return DataLoader(self.train_set, batch_size=4)

    def test_dataloader(self):
        return DataLoader(self.test_set, batch_size=4)


if __name__ == "__main__":
    pass


--- mlflow/server/job_api.py ---
"""
Internal job APIs for UI invocation
"""

import json
from typing import Any

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from mlflow.entities._job import Job as JobEntity
from mlflow.entities._job_status import JobStatus
from mlflow.exceptions import MlflowException

job_api_router = APIRouter(prefix="/ajax-api/3.0/jobs", tags=["Job"])


class Job(BaseModel):
    """
    Pydantic model for job query response.
    """

    job_id: str
    creation_time: int
    function_fullname: str
    params: dict[str, Any]
    timeout: float | None
    status: JobStatus
    result: Any
    retry_count: int
    last_update_time: int

    @classmethod
    def from_job_entity(cls, job: JobEntity) -> "Job":
        return cls(
            job_id=job.job_id,
            creation_time=job.creation_time,
            function_fullname=job.function_fullname,
            params=json.loads(job.params),
            timeout=job.timeout,
            status=job.status,
            result=job.parsed_result,
            retry_count=job.retry_count,
            last_update_time=job.last_update_time,
        )


@job_api_router.get("/{job_id}", response_model=Job)
def get_job(job_id: str) -> Job:
    from mlflow.server.jobs import get_job

    try:
        job = get_job(job_id)
        return Job.from_job_entity(job)
    except MlflowException as e:
        raise HTTPException(
            status_code=e.get_http_status_code(),
            detail=e.message,
        )


class SubmitJobPayload(BaseModel):
    function_fullname: str
    params: dict[str, Any]
    timeout: float | None = None


@job_api_router.post("/", response_model=Job)
def submit_job(payload: SubmitJobPayload) -> Job:
    from mlflow.server.jobs import submit_job
    from mlflow.server.jobs.utils import _load_function

    function_fullname = payload.function_fullname
    try:
        function = _load_function(function_fullname)
        job = submit_job(function, payload.params, payload.timeout)
        return Job.from_job_entity(job)
    except MlflowException as e:
        raise HTTPException(
            status_code=e.get_http_status_code(),
            detail=e.message,
        )


class SearchJobPayload(BaseModel):
    function_fullname: str | None = None
    params: dict[str, Any] | None = None
    statuses: list[JobStatus] | None = None


class SearchJobsResponse(BaseModel):
    """
    Pydantic model for job searching response.
    """

    jobs: list[Job]


@job_api_router.post("/search", response_model=SearchJobsResponse)
def search_jobs(payload: SearchJobPayload) -> SearchJobsResponse:
    from mlflow.server.handlers import _get_job_store

    try:
        store = _get_job_store()
        job_results = [
            Job.from_job_entity(job)
            for job in store.list_jobs(
                function_fullname=payload.function_fullname,
                statuses=payload.statuses,
                params=payload.params,
            )
        ]
        return SearchJobsResponse(jobs=job_results)
    except MlflowException as e:
        raise HTTPException(
            status_code=e.get_http_status_code(),
            detail=e.message,
        )


--- mlflow/server/otel_api.py ---
"""
OpenTelemetry REST API endpoints for MLflow FastAPI server.

This module implements the OpenTelemetry Protocol (OTLP) REST API for ingesting spans
according to the OTel specification:
https://opentelemetry.io/docs/specs/otlp/#otlphttp

Note: This is a minimal implementation that serves as a placeholder for the OTel endpoint.
The actual span ingestion logic would need to properly convert incoming OTel format spans
to MLflow spans, which requires more complex conversion logic.
"""

from typing import Any

from fastapi import APIRouter, Header, HTTPException, Request, Response, status
from google.protobuf.message import DecodeError
from opentelemetry.proto.collector.trace.v1.trace_service_pb2 import ExportTraceServiceRequest
from pydantic import BaseModel, Field

from mlflow.entities.span import Span
from mlflow.server.handlers import _get_tracking_store
from mlflow.tracing.utils.otlp import MLFLOW_EXPERIMENT_ID_HEADER, OTLP_TRACES_PATH

# Create FastAPI router for OTel endpoints
otel_router = APIRouter(prefix=OTLP_TRACES_PATH, tags=["OpenTelemetry"])


class OTelExportTraceServiceResponse(BaseModel):
    """
    Pydantic model for the OTLP/HTTP ExportTraceServiceResponse.

    This matches the OpenTelemetry protocol specification for trace export responses.
    Reference: https://opentelemetry.io/docs/specs/otlp/
    """

    partialSuccess: dict[str, Any] | None = Field(
        None, description="Details about partial success of the export operation"
    )


@otel_router.post("", response_model=OTelExportTraceServiceResponse, status_code=200)
async def export_traces(
    request: Request,
    response: Response,
    x_mlflow_experiment_id: str = Header(..., alias=MLFLOW_EXPERIMENT_ID_HEADER),
    content_type: str = Header(None),
) -> OTelExportTraceServiceResponse:
    """
    Export trace spans to MLflow via the OpenTelemetry protocol.

    This endpoint accepts OTLP/HTTP protobuf trace export requests.
    Protobuf format reference: https://opentelemetry.io/docs/specs/otlp/#binary-protobuf-encoding

    Args:
        request: OTel ExportTraceServiceRequest in protobuf format
        response: FastAPI Response object for setting headers
        x_mlflow_experiment_id: Required header containing the experiment ID
        content_type: Content-Type header from the request

    Returns:
        OTel ExportTraceServiceResponse indicating success

    Raises:
        HTTPException: If the request is invalid or span logging fails
    """
    # Validate Content-Type header
    if content_type != "application/x-protobuf":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid Content-Type: {content_type}. Expected: application/x-protobuf",
        )

    # Set response Content-Type header
    response.headers["Content-Type"] = "application/x-protobuf"

    body = await request.body()
    parsed_request = ExportTraceServiceRequest()

    try:
        # In Python protobuf library 5.x, ParseFromString may not raise DecodeError on invalid data
        parsed_request.ParseFromString(body)

        # Check if we actually parsed any data
        # If no resource_spans were parsed, the data was likely invalid
        if not parsed_request.resource_spans:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid OpenTelemetry protobuf format - no spans found",
            )

    except DecodeError:
        # This will catch errors in Python protobuf library 3.x
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid OpenTelemetry protobuf format",
        )

    mlflow_spans = []
    for resource_span in parsed_request.resource_spans:
        for scope_span in resource_span.scope_spans:
            for otel_proto_span in scope_span.spans:
                try:
                    mlflow_span = Span.from_otel_proto(otel_proto_span)
                    mlflow_spans.append(mlflow_span)
                except Exception:
                    raise HTTPException(
                        status_code=422,
                        detail="Cannot convert OpenTelemetry span to MLflow span",
                    )

    if mlflow_spans:
        store = _get_tracking_store()

        try:
            store.log_spans(x_mlflow_experiment_id, mlflow_spans)
        except NotImplementedError:
            raise HTTPException(
                status_code=status.HTTP_501_NOT_IMPLEMENTED,
                detail=f"REST OTLP span logging is not supported by {store.__class__.__name__}",
            )
        except Exception as e:
            raise HTTPException(
                status_code=422,
                detail=f"Cannot store OpenTelemetry spans: {e}",
            )

    return OTelExportTraceServiceResponse()


--- mlflow/server/js/.storybook/decorators/design-system.js ---
import React from 'react';
import { Global } from '@emotion/react';
import { useRef } from 'react';
import { DesignSystemContainer } from '../../src/common/components/DesignSystemContainer';

export const designSystemDecorator = (Story) => {
  const modalContainerRef = useRef(null);

  return (
    <DesignSystemContainer isCompact getPopupContainer={() => modalContainerRef.current}>
      <>
        <Global
          styles={{
            'html, body': {
              fontSize: 13,
              fontFamily:
                '-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji',
              height: '100%',
            },
            '#root': { height: '100%' },
            '*': {
              boxSizing: 'border-box',
            },
          }}
        />
        <Story />
        <div ref={modalContainerRef} />
      </>
    </DesignSystemContainer>
  );
};


--- CHANGELOG.md ---
# CHANGELOG

## 3.5.0 (2025-10-16)

MLflow 3.5.0 includes several major features and improvements!

### Major Features

- ⚙️ **Job Execution Backend**: Introduced a new job execution backend infrastructure for running asynchronous tasks with individual execution pools, job search capabilities, and transient error handling. (#17676, #18012, #18070, #18071, #18112, #18049, @WeichenXu123)
- 🎯 **Flexible Prompt Optimization API**: Introduced a new flexible API for prompt optimization with support for model switching and the GEPA algorithm, enabling more efficient prompt tuning with fewer rollouts. See the [documentation](https://mlflow.org/docs/latest/genai/prompt-registry/optimize-prompts/) to get started. (#18183, #18031, @TomeHirata)
- 🎨 **Enhanced UI Onboarding**: Improved in-product onboarding experience with trace quickstart drawer and updated homepage guidance to help users discover MLflow's latest features. (#18098, #18187, @B-Step62)
- 🔐 **Security Middleware for Tracking Server**: Added a security middleware layer to protect against DNS rebinding, CORS attacks, and other security threats. Read the [documentation](https://mlflow.org/docs/latest/ml/tracking/server/security/) for configuration details. (#17910, @BenWilson2)

### Features

- [Tracing / Tracking] Add `unlink_traces_from_run` batch operation (#18316, @harupy)
- [Tracing] Add batch trace link/unlink operations to DatabricksTracingRestStore (#18295, @harupy)
- [Tracking] Claude Code SDK autologging support (#18022, @smoorjani)
- [Tracing] Add support for reading trace configuration from environment variables (#17792, @joelrobin18)
- [Tracking] Mistral tracing improvements (#16370, @joelrobin18)
- [Tracking] Gemini token count tracking (#16248, @joelrobin18)
- [Tracking] Gemini streaming support (#16249, @joelrobin18)
- [Tracking] CrewAI token count tracking with documentation updates (#16373, @joelrobin18)
- [Evaluation] Allow passing empty scorer list for manual result comparison (#18265, @B-Step62)
- [Evaluation] Log assessments to DSPy evaluation traces (#18136, @B-Step62)
- [Evaluation] Add support for trace inputs to built-in scorers (#17943, @BenWilson2)
- [Evaluation] Add synonym handling for built-in scorers (#17980, @BenWilson2)
- [Evaluation] Add span timing tool for Agent Judges (#17948, @BenWilson2)
- [Evaluation] Allow disabling evaluation sample check (#18032, @B-Step62)
- [Evaluation] Reduce verbosity of SIMBA optimizer logs when aligning judges (#17795, @BenWilson2)
- [Evaluation] Add `__repr__` method for Judges (#17794, @BenWilson2)
- [Prompts] Add prompt registry support to MLflow webhooks (#17640, @harupy)
- [Prompts] Prompt Registry Chat UI (#17334, @joelrobin18)
- [UI] Delete parent and child runs together (#18052, @joelrobin18)
- [UI] Added move to top, move to bottom for charts (#17742, @joelrobin18)
- [Tracking] Use sampling data for run comparison to improve performance (#17645, @lkuo)
- [Tracking] Add optional 'outputs' column for evaluation dataset records (#17735, @WeichenXu123)

### Bug Fixes

- [Tracing] Fix parent run resolution mechanism for LangChain (#17273, @B-Step62)
- [Tracing] Add client-side retry for `get_trace` to improve reliability (#18224, @B-Step62)
- [Tracing] Fix OpenTelemetry dual export (#18163, @B-Step62)
- [Tracing] Suppress false warnings from span logging (#18092, #18276, @B-Step62)
- [Tracing] Fix OpenTelemetry resource attributes not propagating correctly (#18019, @xiaosha007)
- [Tracing] Fix DSPy prompt display (#17988, @B-Step62)
- [Tracing] Fix usage aggregation to avoid ancestor duplication (#17921, @TomeHirata)
- [Tracing] Fix double counting in Strands tracing (#17855, @joelrobin18)
- [Tracing] Fix `to_predict_fn` to handle traces without tags field (#17784, @harupy)
- [Tracing] URL-encode trace tag keys in `delete_trace_tag` to prevent 404 errors (#18232, @copilot-swe-agent)
- [Tracking] Fix Claude Code autologging inputs not displaying (#17858, @smoorjani)
- [Tracking] Fix runs with 0-valued metrics not appearing in experiment list contour plots (#17916, @WeichenXu123)
- [Tracking] Fix DSPy run display (#18137, @B-Step62)
- [Tracking] Allow list of types in tools JSON Schema for OpenAI autolog (#17908, @fedem96)
- [Tracking] Set tracking URI environment variable for job runner (#18073, @WeichenXu123)
- [Evaluation] Add atomicity to `job_start` API (#18226, @BenWilson2)
- [Evaluation] Fix trace ingest for outputs in `merge_records()` API (#18047, @BenWilson2)
- [Evaluation] Fix judge regression (#18039, @B-Step62)
- [Evaluation] Fix judges to use non-empty user messages for Anthropic model compatibility (#17935, @dbczumar)
- [Evaluation] Fix endpoints error in judge (#18048, @joelrobin18)
- [Model Registry] Fix creating model versions from non-Databricks tracking to Databricks Unity Catalog registry (#18244, @austinwarner-8451)
- [Model Registry] Fix registry URI instantiation for artifact download (#17982, @arpitjasa-db)
- [Model Registry] Include original error details in Unity Catalog model copy failure messages (#17997, @harupy)
- [Model Registry] Fix webhook delivery to exit early for FileStore instances (#18015, @copilot-swe-agent)
- [Prompts] Fix error suppression during prompt alias resolution when `allow_missing` is set (#17541, @mr-brobot)
- [UI] General UI improvements (#18281, @joelrobin18)
- [Models] Fix dataset issue (#18081, @joelrobin18)
- [Models] Forward dataset name and digest to PolarsDataset's `to_evaluation_dataset` method (#17886, @sadelcarpio)
- [Build] Fix `mlflow server` exiting immediately when optional `huey` package is missing (#18016, @harupy)
- [Scoring] Fix chat completion arguments (#18248, @aravind-segu)

### Documentation Updates

- [Docs] Add self-hosted documentation support (#17986, @B-Step62)
- [Docs] Add GitHub feature requests section to GenAI documentation (#18342, @TomeHirata)
- [Docs] Update Claude Code SDK tracing documentation (#18026, @smoorjani)
- [Docs] Add documentation for Analyze Experiment MCP/CLI command (#17978, @nsthorat)
- [Docs] Add deprecation notice for custom prompt judge (#18287, @smoorjani)
- [Docs] Overhaul scorer documentation (#17930, @B-Step62)
- [Docs] Add default optimizer documentation (#17814, @BenWilson2)
- [Docs] Update TypeScript SDK contribution documentation (#17995, @joelrobin18)
- [Docs] Fix Postgres 18+ mount path in documentation (#18192, @soyun11)
- [Docs] Fix typo: correct variable name from `max_few_show_examples` to `max_few_shot_examples` (#18246, @srinathmkce)
- [Docs] Replace single quotes with double quotes for Windows compatibility (#18266, @PavithraNelluri)
- [Docs] Fix typo in model registry documentation (#18038, @EddieMG)

Small bug fixes and documentation updates:

#18349, #18338, #18241, #18319, #18309, #18292, #18280, #18239, #18236, #17786, #18003, #17970, #17898, #17765, #17667, @serena-ruan; #18346, #17882, @dbrx-euirim; #18306, #18208, #18165, #18110, #18109, #18108, #18107, #18105, #18104, #18100, #18099, #18155, #18079, #18082, #18078, #18077, #18083, #18030, #18001, #17999, #17712, #17785, #17756, #17729, #17731, #17733, @daniellok-db; #18339, #18291, #18222, #18210, #18124, #18101, #18054, #18053, #18007, #17922, #17823, #17822, #17805, #17789, #17750, #17752, #17760, #17758, #17688, #17689, #17693, #17675, #17673, #17656, #17674, @harupy; #18331, #18308, #18303, #18146, @smoorjani; #18315, #18279, #18310, #18187, #18225, #18277, #18193, #18223, #18209, #18200, #18178, #17574, #18021, #18006, #17944, @B-Step62; #18290, #17946, #17627, @bbqiu; #18274, @Ninja3047; #18204, #17868, #17866, #17833, #17826, #17835, @TomeHirata; #18273, #18043, #17928, #17931, #17936, #17937, @dbczumar; #18185, #18180, #18174, #18170, #18167, #18164, #18168, #18166, #18162, #18160, #18159, #18157, #18156, #18154, #18148, #18145, #18135, #18143, #18142, #18139, #18132, #18130, #18119, #18117, #18115, #18102, #18075, #18046, #18062, #18042, #18051, #18036, #18027, #18014, #18011, #18009, #18004, #17903, #18000, #18002, #17973, #17993, #17989, #17984, #17968, #17966, #17967, #17962, #17977, #17976, #17972, #17965, #17964, #17963, #17969, #17971, #17939, #17926, #17924, #17915, #17911, #17912, #17904, #17902, #17900, #17897, #17892, #17889, #17888, #17885, #17884, #17878, #17874, #17873, #17871, #17870, #17865, #17860, #17861, #17859, #17857, #17856, #17854, #17853, #17851, #17849, #17850, #17847, #17845, #17846, #17844, #17843, #17842, #17838, #17836, #17834, #17831, #17824, #17828, #17819, #17825, #17817, #17821, #17809, #17807, #17808, #17803, #17800, #17799, #17797, #17793, #17790, #17772, #17771, #17769, #17770, #17753, #17762, #17747, #17749, #17745, #17740, #17734, #17732, #17726, #17723, #17722, #17721, #17719, #17720, #17718, #17716, #17713, #17715, #17710, #17709, #17708, #17707, #17705, #17697, #17701, #17698, #17696, #17695, @copilot-swe-agent; #18151, #18153, #17983, #18040, #17981, #17841, #17818, #17776, #17781, @BenWilson2; #18068, @alkispoly-db; #18133, @kevin-lyn; #17105, #17717, @joelrobin18; #17879, @lkuo; #17996, #17945, #17913, @WeichenXu123

## 3.5.0rc0 (2025-10-08)

MLflow 3.5.0rc0 includes several major features and improvements

Major new features:

- 🤖 **Tracing support for Claude Code SDK**: MLflow now provides a tracing integration for both the Claude Code CLI and SDK! Configure the autologging integration to track your prompts, Claude's responses, tool calls, and more. Check out this [doc page](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/claude_code/) to get started. (#18022, @smoorjani)
- ✨ **Improved UI homepage**: The MLflow UI's homepage has been updated to help you get started with more of our latest features. This page will be updated regularly moving forward, allowing you to get more in-product guidance.
- 🗂️ **Evaluation datasets UI integration**: In MLflow 3.4.0, we released backend support for creating evaluation datasets for GenAI applications. In this release, we've added a new tab to the MLflow Experiment UI, allowing you to create, manage, and export traces to your datasets without having to write a line of code.
- 🧮 **GEPA support for prompt optimization**: MLflow's prompt optimization feature now supports the [GEPA algorithm](https://dspy.ai/api/optimizers/GEPA/overview/), allowing you to achieve higher performing prompts with less rollouts. For instructions on how to get started with prompt optimization, visit this [doc page](https://mlflow.org/docs/latest/genai/prompt-registry/optimize-prompts/)!
- 🔐 **Security middleware layer for tracking server**: MLflow now ships with a security middleware layer by default, allowing you to protect against DNS rebinding, CORS attacks, and more. Read the documentation [here](https://mlflow.org/docs/latest/ml/tracking/server/security/) to learn how to configure these options.

Stay tuned for the full release, which will be packed with more features and bugfixes.

To try out this release candidate, please run:

`pip install mlflow==3.5.0rc0`

## 3.4.0rc0 (2025-09-11)

MLflow 3.4.0rc0 includes several major features and improvements

### Major New Features

- 📊 **OpenTelemetry Metrics Export**: MLflow now exports span-level statistics as OpenTelemetry metrics, providing enhanced observability and monitoring capabilities for traced applications. (#17325, @dbczumar)
- 🤖 **MCP Server Integration**: Introducing the Model Context Protocol (MCP) server for MLflow, enabling AI assistants and LLMs to interact with MLflow programmatically. (#17122, @harupy)
- 🧑‍⚖️ **Custom Judges API**: New `make_judge` API enables creation of custom evaluation judges for assessing LLM outputs with domain-specific criteria. (#17647, @BenWilson2, @dbczumar, @alkispoly-db, @smoorjani)
- 📈 **Correlations Backend**: Implemented backend infrastructure for storing and computing correlations between experiment metrics using NPMI (Normalized Pointwise Mutual Information). (#17309, #17368, @BenWilson2)
- 🗂️ **Evaluation Datasets**: MLflow now supports storing and versioning evaluation datasets directly within experiments for reproducible model assessment. (#17447, @BenWilson2)
- 🔗 **Databricks Backend for MLflow Server**: MLflow server can now use Databricks as a backend, enabling seamless integration with Databricks workspaces. (#17411, @nsthorat)
- 🤖 **Claude Autologging**: Automatic tracing support for Claude AI interactions, capturing conversations and model responses. (#17305, @smoorjani)
- 🌊 **Strands Agent Tracing**: Added comprehensive tracing support for Strands agents, including automatic instrumentation for agent workflows and interactions. (#17151, @joelrobin18)
- 🧪 **Experiment Types in UI**: MLflow now introduces experiment types, helping reduce clutter between classic ML/DL and GenAI features. MLflow auto-detects the type, but you can easily adjust it via a selector next to the experiment name. (#17605, @daniellok-db)

Features:

- [Evaluation] Add ability to pass tags via dataframe in mlflow.genai.evaluate (#17549, @smoorjani)
- [Evaluation] Add custom judge model support for Safety and RetrievalRelevance builtin scorers (#17526, @dbrx-euirim)
- [Tracing] Add AI commands as MCP prompts for LLM interaction (#17608, @nsthorat)
- [Tracing] Add MLFLOW_ENABLE_OTLP_EXPORTER environment variable (#17505, @dbczumar)
- [Tracing] Support OTel and MLflow dual export (#17187, @dbczumar)
- [Tracing] Make set_destination use ContextVar for thread safety (#17219, @B-Step62)
- [CLI] Add MLflow commands CLI for exposing prompt commands to LLMs (#17530, @nsthorat)
- [CLI] Add 'mlflow runs link-traces' command (#17444, @nsthorat)
- [CLI] Add 'mlflow runs create' command for programmatic run creation (#17417, @nsthorat)
- [CLI] Add MLflow traces CLI command with comprehensive search and management capabilities (#17302, @nsthorat)
- [CLI] Add --env-file flag to all MLflow CLI commands (#17509, @nsthorat)
- [Tracking] Backend for storing scorers in MLflow experiments (#17090, @WeichenXu123)
- [Model Registry] Allow cross-workspace copying of model versions between WMR and UC (#17458, @arpitjasa-db)
- [Models] Add automatic Git-based model versioning for GenAI applications (#17076, @harupy)
- [Models] Improve WheeledModel.\_download_wheels safety (#17004, @serena-ruan)
- [Projects] Support resume run for Optuna hyperparameter optimization (#17191, @lu-wang-dl)
- [Scoring] Add MLFLOW_DEPLOYMENT_CLIENT_HTTP_REQUEST_TIMEOUT environment variable (#17252, @dbczumar)
- [UI] Add ability to hide/unhide all finished runs in Chart view (#17143, @joelrobin18)
- [Telemetry] Add MLflow OSS telemetry for invoke_custom_judge_model (#17585, @dbrx-euirim)

Bug fixes:

- [Evaluation] Implement DSPy LM interface for default Databricks model serving (#17672, @smoorjani)
- [Evaluation] Fix aggregations incorrectly applied to legacy scorer interface (#17596, @BenWilson2)
- [Evaluation] Add Unity Catalog table source support for mlflow.evaluate (#17546, @BenWilson2)
- [Evaluation] Fix custom prompt judge encoding issues with custom judge models (#17584, @dbrx-euirim)
- [Tracking] Fix OpenAI autolog to properly reconstruct Response objects from streaming events (#17535, @WeichenXu123)
- [Tracking] Add basic authentication support in TypeScript SDK (#17436, @kevin-lyn)
- [Tracking] Update scorer endpoints to v3.0 API specification (#17409, @WeichenXu123)
- [Tracking] Fix scorer status handling in MLflow tracking backend (#17379, @WeichenXu123)
- [Tracking] Fix missing source-run information in UI (#16682, @WeichenXu123)
- [Scoring] Fix spark_udf to always use stdin_serve for model serving (#17580, @WeichenXu123)
- [Scoring] Fix a bug with Spark UDF usage of uv as an environment manager (#17489, @WeichenXu123)
- [Model Registry] Extract source workspace ID from run_link during model version migration (#17600, @arpitjasa-db)
- [Models] Improve security by reducing write permissions in temporary directory creation (#17544, @BenWilson2)
- [Server-infra] Fix --env-file flag compatibility with --dev mode (#17615, @nsthorat)
- [Server-infra] Fix basic authentication with Uvicorn server (#17523, @kevin-lyn)
- [UI] Fix experiment comparison functionality in UI (#17550, @Flametaa)
- [UI] Fix compareExperimentsSearch route definitions (#17459, @WeichenXu123)

Documentation updates:

- [Docs] Add clarification for trace requirements in scorers documentation (#17542, @BenWilson2)
- [Docs] Add documentation for Claude code autotracing (#17521, @smoorjani)
- [Docs] Remove experimental status message for MPU/MPD features (#17486, @BenWilson2)
- [Docs] Remove problematic pages from documentation (#17453, @BenWilson2)
- [Docs] Add documentation for updating signatures on Databricks registered models (#17450, @arpitjasa-db)
- [Docs] Update Scorers API documentation (#17298, @WeichenXu123)
- [Docs] Add comprehensive documentation for scorers (#17258, @B-Step62)

Small bug fixes and documentation updates:

#17655, #17657, #17597, #17545, #17547, @BenWilson2; #17671, @smoorjani; #17668, #17665, #17662, #17661, #17659, #17658, #17653, #17643, #17642, #17636, #17634, #17631, #17628, #17611, #17607, #17588, #17570, #17575, #17564, #17557, #17556, #17555, #17536, #17531, #17524, #17510, #17511, #17499, #17500, #17494, #17493, #17490, #17488, #17478, #17479, #17425, #17471, #17457, #17440, #17403, #17405, #17404, #17402, #17366, #17346, #17344, #17337, #17316, #17313, #17284, #17276, #17235, #17226, #17229, @copilot-swe-agent; #17664, #17654, #17613, #17637, #17633, #17612, #17630, #17616, #17626, #17617, #17610, #17614, #17602, #17538, #17522, #17512, #17508, #17492, #17462, #17475, #17468, #17455, #17338, #17257, #17231, #17214, #17223, #17218, #17216, @harupy; #17635, #17663, #17426, #16870, #17428, #17427, #17441, #17377, @serena-ruan; #17605, #17306, @daniellok-db; #17624, #17578, #17369, #17391, #17072, #17326, #17115, @dbczumar; #17598, #17408, #17353, @nsthorat; #17601, #17553, @dbrx-euirim; #17586, #17587, #17310, #17180, @TomeHirata; #17516, @bbqiu; #17477, #17474, @WeichenXu123; #17449, @raymondzhou-db; #17470, @jacob-danner; #17378, @arpitjasa-db; #17121, @ctaymor; #17351, #17322, @ispoljari; #17292, @dsuhinin; #17287, #17281, #17230, #17245, #17237, @B-Step62

## 3.3.2 (2025-08-27)

MLflow 3.3.2 is a patch release that includes several minor improvements and bugfixes

Features:

- [Evaluation] Add support for dataset name persistence (#17250, @BenWilson2)

Bug fixes:

- [Tracing] Add retry policy support to `_invoke_litellm` for improved reliability (#17394, @dbczumar)
- [UI] fix ui sorting in experiments (#17340, @Flametaa)
- [Serving] Add Databricks Lakebase Resource (#17277, @jennsun)
- [Tracing] Fix set trace tags endpoint (#17362, @daniellok-db)

Documentation updates:

- [Docs] Add docs for package lock (#17395, @BenWilson2)
- [Docs] Fix span processor docs (#17386, @mr-brobot)

Small bug fixes and documentation updates:

#17301, #17299, @B-Step62; #17420, #17421, #17398, #17397, #17349, #17361, #17377, #17359, #17358, #17356, #17261, #17263, #17262, @serena-ruan; #17422, #17310, #17357, @TomeHirata; #17406, @sotagg; #17418, @annzhang-db; #17384, #17376, @daniellok-db

## 3.3.1 (2025-08-20)

MLflow 3.3.1 includes several major features and improvements

Bug fixes:

- [Tracking] Fix `mlflow.genai.datasets` attribute (#17307, @WeichenXu123)
- [UI] Fix tag display as column in experiment overview (#17296, @joelrobin18)
- [Tracing] Fix the slowness of dspy tracing (#17290, @TomeHirata)

Small bug fixes and documentation updates:

#17295, @gunsodo; #17272, @bbqiu

## 3.3.0 (2025-08-19)

MLflow 3.3.0 includes several major features and improvements

### Major new features:

- 🪝 **Model Registry Webhooks**: MLflow now supports [webhooks](https://mlflow.org/docs/latest/ml/webhooks/) for model registry events, enabling automated notifications and integrations with external systems. (#16583, @harupy)
- 🧭 **Agno Tracing Integration**: Added [Agno tracing integration](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/agno/) for enhanced observability of AI agent workflows. (#16995, @joelrobin18)
- 🧪 **GenAI Evaluation in OSS**: MLflow open-sources [the new evaluation capability for LLM applications](https://mlflow.org/docs/latest/genai/eval-monitor/). This suite enables systematic measurement and improvement of LLM application quality, with tight integration into MLflow's observability, feedback collection, and experiment tracking capabilities. (#17161, #17159, @B-Step62)
- 🖥️ **Revamped Trace Table View**: The new trace view in MLflow UI provides a streamlined interface for exploring, filtering, and monitoring traces, with enhanced search capabilities including full-text search across requests.(#17092, @daniellok-db)
- ⚡️ **FastAPI + Uvicorn Server**: MLflow Tracking Server now defaults to FastAPI + Uvicorn for improved performance, while maintaining Flask compatibility. (#17038, @dbczumar)

New features:

- [Tracking] Add a Docker compose file to quickly start a local MLflow server with recommended minimum setup (#17065, @joelrobin18)
- [Tracing] Add `memory` span type for agentic workflows (#17034, @B-Step62)
- [Prompts] Enable custom prompt optimizers in `optimize_prompt` including DSPy support (#17052, @TomeHirata)
- [Model Registry / Prompts] Proper support for the @latest alias (#17146, @B-Step62)
- [Metrics] Allow custom tokenizer encoding in `token_count` function (#16253, @joelrobin18)

Bug fixes:

- [Tracking] Fix Databricks secret scope check to reduce audit log errors (#17166, @harupy)
- [Tracking] Fix Databricks SDK error code mapping in retry logic (#17095, @harupy)
- [Tracking] Fix Databricks secret scope check to reduce error rates (#17166, @harupy)
- [Tracing] Remove API keys from CrewAI traces to prevent credential leakage (#17082, @diy2learn)
- [Tracing] Fix LiteLLM span association issue by making callbacks synchronous (#16982, @B-Step62)
- [Tracing] Fix OpenAI Agents tracing (#17227, @B-Step62)
- [Evaluation] Fix issue with get_label_schema has no attribute (#17163, @smoorjani)
- [Docs] Fix version selector on API Reference page by adding missing CSS class and versions.json generation (#17247, @copilot-swe-agent)

Documentation updates:

- [Docs] Document custom optimizer usage with `optimize_prompt` (#17084, @TomeHirata)
- [Docs] Fix built-in scorer documentation for expectation parameter (#17075, @smoorjani)
- [Docs] Add comprehensive documentation for scorers (#17258, @B-Step62)

Small bug fixes and documentation updates:

#17230, #17264, #17289, #17287, #17265, #17238, #17215, #17224, #17185, #17148, #17193, #17157, #17067, #17033, #17087, #16973, #16875, #16956, #16959, @B-Step62; #17269, @BenWilson2; #17285, #17259, #17260, #17236, #17196, #17169, #17062, #16943, @serena-ruan; #17253, @sotagg; #17212, #17206, #17211, #17207, #17205, #17118, #17177, #17182, #17170, #17153, #17168, #17123, #17136, #17119, #17125, #17088, #17101, #17056, #17077, #17057, #17036, #17018, #17024, #17019, #16883, #16972, #16961, #16968, #16962, #16958, @harupy; #17209, #17202, #17184, #17179, #17174, #17141, #17155, #17145, #17130, #17113, #17110, #17098, #17104, #17100, #17060, #17044, #17032, #17008, #17001, #16994, #16991, #16984, #16976, @copilot-swe-agent; #17069, @hayescode; #17199, #17081, #16928, #16931, @TomeHirata; #17198, @WeichenXu123; #17195, #17192, #17131, #17128, #17124, #17120, #17102, #17093, #16941, @daniellok-db; #17070, #17074, #17073, @dbczumar

## 3.2.0 (2025-08-05)

MLflow 3.2.0 includes several major features and improvements

### Major New Features

- 🧭 **Tracing TypeScript SDK**: MLflow Tracing now supports the [TypeScript SDK](https://github.com/mlflow/mlflow/tree/master/libs/typescript), allowing developers to trace GenAI applications in TypeScript environments. (#16871, @B-Step62)
- 🔗 **Semantic Kernel Tracing**: MLflow now provides [automatic tracing support for Semantic Kernel](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/semantic_kernel/), simplifying trace capture for SK-based workflows. (#16469, @michael-berk)
- 🧪 **Feedback Tracking**: MLflow OSS now natively supports tracking [human feedbacks](https://mlflow.org/docs/latest/genai/assessments/feedback/), [ground truths](https://mlflow.org/docs/latest/genai/assessments/expectations/), LLM judges on traces, providing integrated quality monitoring and feedback management capabilities. (#16743, @BenWilson2)
- 🖥️ **MLflow UI Improvements**: The MLflow UI now features **a redesigned experiment home view** and includes enhancements like pagination on the model page for better usability. (#16464, @frontsideair, #15801, @Flametaa)
- 🔍 **Updated Trace UI**: The Trace UI now has image support when rendering chat messages for OpenAI, Langchain, and Anthropic! Additionally, we're introducing a "summary view" which is a simplified, flat representation of the important spans in a trace. The full detail view is still available in a separate tab.
- 🛡️ **PII Masking in Tracing**: Added support for [masking personally identifiable information (PII) via a custom span post-processor](https://mlflow.org/docs/latest/genai/tracing/observe-with-traces/masking). (#16344, @B-Step62)
- 🐻‍❄️ **Polars Dataset Support**: MLflow now supports [Polars datasets](https://mlflow.org/docs/latest/ml/dataset/#dataset), expanding compatibility with performant DataFrame libraries. (#13006, @AlpAribal)

### 📊 Usage Tracking (New in 3.2.0)

- Starting with version 3.2.0, MLflow will begin collecting anonymized usage data about how core features of the platform are used. This data contains **no sensitive or personally identifiable information**, and users can opt out of data collection at any time. Check [MLflow documentation](https://mlflow.org/docs/latest/community/usage-tracking/) for more details. (#16439, @serena-ruan)

Features:

- [Tracing] Include mlflow-tracing as a dependency of mlflow (#16589, @B-Step62)
- [Tracing] Convert DatabricksRM output to MLflow document format (#16866, @WeichenXu123)
- [Tracing] Add unified token usage tracking for Bedrock LLMs (#16351, @mohammadsubhani)
- [Tracing] Token usage tracking for agent frameworks including Anthropic, Autogen, LlamaIndex etc. (#16251, #16362, #16246, #16258, #16313, #16312, #16340, #16357, #16358, @joelrobin18, #16387, @sanatb187)
- [Tracing] Render multi-modal trace for LangChain (#16799, @B-Step62)
- [Tracing] Support async tracing for Gemini (#16632, @B-Step62)
- [Tracing] Support global sampling for tracing (#16700, @B-Step62)
- [Tracing] ResponsesAgent tracing aggregation (#16787, @bbqiu)
- [Tracing] Add Agent and LLM complete name (#16613, @joelrobin18)
- [Tracking] Allow setting thread-local tracing destination via mlflow.tracing.set_destination (#16859, @WeichenXu123)
- [Tracking] Introduce MLFLOW_DISABLE_SCHEMA_DETAILS environment variable to toggle detailed schema errors (#16631, @NJAHNAVI2907)
- [Tracking] Add support for chat-style prompts with structured output with prompt object (#16341, @harshilprajapati96)
- [Tracking] Add support for responses.parse calls in oai autologger (#16245, @dipakkrishnan)
- [Tracking] Add support for uv as an environment manager in mlflow run (#16274, @isuyyy)
- [Evaluation] Replace guideline_adherence to guidelines (#16856, @smoorjani)
- [Evaluation] Replace Scheduled Scorers API to a Scorer Registration System (#16977, @dbrx-euirim)
- [UI] Add tag filter to the experiments page (#16648, @frontsideair)
- [UI] Add ability to the UI to edit experiment tags (#16614, @frontsideair)
- [UI] Create runs table using selected columns in the experiment view (#16804, @wangh118)
- [Scoring] Make spark_udf support 'uv' env manager (#16292, @WeichenXu123)

Bug fixes:

- [Tracking / UI] Add missing default headers and replace absolute URLs in new browser client requests (GraphQL & logged models) (#16840, @danilopeixoto)
- [Tracking] Fix tracking_uri positional argument bug in artifact repositories (#16878, @copilot-swe-agent)
- [Models] Fix UnionType support for Python 3.10 style union syntax (#16882, @harupy)
- [Tracing / Tracking] Fix OpenAI autolog Pydantic validation for enum values (#16862, @mohammadsubhani)
- [Tracking] Fix tracing for Anthropic and Langchain combination (#15151, @maver1ck)
- [Models] Fix OpenAI multimodal message logging support (#16795, @mohammadsubhani)
- [Tracing] Avoid using nested threading for Azure Databricks trace export (#16733, @TomeHirata)
- [Evaluation] Bug fix: Databricks GenAI evaluation dataset source returns string, instead of DatasetSource instance (#16712, @dbczumar)
- [Models] Fix `get_model_info` to provide logged model info (#16713, @harupy)
- [Evaluation] Fix serialization and deserialization for python scorers (#16688, @connorchenn)
- [UI] Fix GraphQL handler erroring on NaN metric values (#16628, @daniellok-db)
- [UI] Add back video artifact preview (#16620, @daniellok-db)
- [Tracing] Proper chat message reconstruction from OAI streaming response (#16519, @B-Step62)
- [Tracing] Convert trace column in search_traces() response to JSON string (#16523, @B-Step62)
- [Evaluation] Fix mlflow.evaluate crashes in \_get_binary_classifier_metrics due to … (#16485, @mohammadsubhani)
- [Evaluation] Fix trace detection logic for `mlflow.genai.evaluate` (#16932, @B-Step62)
- [Evaluation] Enable to use make_genai_metric_from_prompt for mlflow.evaluate (#16960, @TomeHirata)
- [Models] Add explicit encoding for decoding streaming Responses (#16855, @aravind-segu)
- [Tracking] Prevent from tracing DSPy model API keys (#17021, @czyzby)
- [Tracking] Fix pytorch datetime issue (#17030, @serena-ruan)
- [Tracking] Fix predict with pre-releases (#16998, @serena-ruan)

Documentation updates:

- [Docs] Overhaul of top level version management GenAI docs (#16728, @BenWilson2)
- [Docs] Fix Additional GenAI Docs pages (#16691, @BenWilson2)
- [Docs] Update the docs selector dropdown (#16280, @BenWilson2)
- [Docs] Update docs font sizing and link coloring (#16281, @BenWilson2)
- [Docs] Fix typo in model deployment page (#16999, @premkiran-o7)

Small bug fixes and documentation updates:

#17003, #17049, #17035, #17026, #16981, #16971, #16953, #16930, #16917, #16738, #16717, #16693, #16694, #16684, #16678, #16656, #16513, #16459, #16277, #16276, #16275, #16170, #16217, @serena-ruan; #16927, #16915, #16913, #16911, #16909, #16889, #16727, #16600, #16543, #16551, #16526, #16533, #16535, #16531, #16472, #16392, #16389, #16385, #16376, #16369, #16367, #16321, #16311, #16307, #16273, #16268, #16265, #16112, #16243, #16231, #16226, #16221, #16196, @copilot-swe-agent; #17050, #17048, #16955, #16894, #16885, #16860, #16841, #16835, #16801, #16701, @daniellok-db; #16898, #16881, #16858, #16735, #16823, #16814, #16647, #16750, #16809, #16794, #16793, #16789, #16780, #16770, #16773, #16771, #16772, #16768, #16752, #16754, #16751, #16748, #16730, #16729, #16346, #16709, #16704, #16703, #16702, #16658, #16662, #16645, #16639, #16640, #16626, #16572, #16566, #16565, #16563, #16561, #16559, #16544, #16539, #16520, #16508, #16505, #16494, #16495, #16491, #16487, #16482, #16473, #16465, #16456, #16458, #16394, #16445, #16433, #16434, #16413, #16417, #16416, #16414, #16415, #16378, #16350, #16323, #15788, #16263, #16256, #16237, #16234, #16219, #16216, #16207, #16199, #16192, #16705, @harupy; #17047, #17017, #17005, #16989, #16952, #16951, #16903, #16900, #16755, #16762, #16757, #15860, #16661, #16630, #16657, #16605, #16602, #16568, #16569, #16553, #16345, #16454, #16489, #16486, #16438, #16266, #16382, #16381, #16303, @B-Step62; #17028, #17027, #17020, @he7d3r; #16969, #16957, #16852, #16829, #16816, #16808, #16775, #16807, #16806, #16624, #16524, #16410, #16403, @TomeHirata; #16987, @wangh118; #16760, #16761, #16736, #16737, #16699, #16718, #16663, #16676, #16574, #16477, #16552, #16527, #16515, #16452, #16210, #16204, #16610, @frontsideair; #16723, #16124, @AveshCSingh; #16744, @BenWilson2; #16683, @dsuhinin; #16877, #16502, @bbqiu; #16619, @AchimGaedkeLynker; #16595, @Aiden-Jeon; #16480, #16479, @shushantrishav; #16398, #16331, #16328, #16329, #16293, @WeichenXu123

## 3.1.4 (2025-07-23)

MLflow 3.1.4 includes several major features and improvements

Small bug fixes and documentation updates:

#16835, #16820, @daniellok-db

## 3.1.3 (2025-07-22)

MLflow 3.1.3 includes several major features and improvements

Features:

- [Artifacts / Tracking] Do not copy file permissions when logging artifacts to local artifact repo (#16642, @connortann)
- [Tracking] Add support for OpenAI ChatCompletions parse method (#16493, @harupy)

Bug fixes:

- [Deployments] Propagate `MLFLOW_DEPLOYMENT_PREDICT_TIMEOUT` to databricks-sdk (#16783, @bbqiu)
- [Model Registry] Fix issue with search_registered_models with Databricks UC backend not supporting filter_string (#16766, @BenWilson2)
- [Evaluation] Bug fix: Databricks GenAI evaluation dataset source returns string, instead of DatasetSource instance (#16712, @dbczumar)
- [Tracking] Fix the position of added tracking_uri param to artifact store implementations (#16653, @BenWilson2)

Small bug fixes and documentation updates:

#16786, #16692, @daniellok-db; #16594, @ngoduykhanh; #16475, @harupy

## 3.1.2 (2025-07-08)

MLflow 3.1.2 is a patch release that includes several bug fixes.

Bug fixes:

- [Tracking] Fix `download_artifacts` ignoring `tracking_uri` parameter (#16461, @harupy)
- [Models] Fix event type for ResponsesAgent error (#16427, @bbqiu)
- [Models] Remove falsey chat conversion for LangGraph models (#16601, @B-Step62)
- [Tracing] Use empty Resource when instantiating OTel provider to fix LiteLLM tracing issue (#16590, @B-Step62)

Small fixes and documentation updates:

#16568, #16454, #16617, #16605, #16569, #16553, #16625, @B-Step62; #16571, #16552, #16452, #16395, #16446, #16420, #16447, #16554, #16515, @frontsideair; #16558, #16443, #16457, @16442, #16449, @harupy; #16509, #16512, #16524, #16514, #16607, @TomeHirata; #16541, @copilot-swe-agent; #16427, @bbqiu; #16573, @daniellok-db; #16470, #16281, @BenWilson2

## 3.1.1 (2025-06-25)

MLflow 3.1.1 includes several major features and improvements

Features:

- [Model Registry / Sqlalchemy] Increase prompt text limit from 5K to 100K (#16377, @harupy)
- [Tracking] Support pagination in get-history of FileStore and SqlAlchemyStore (#16325, @TomeHirata)

Bug fixes:

- [Artifacts] Support downloading logged model artifacts (#16356, @TomeHirata)
- [Models] Fix bedrock provider, configured inference profile compatibility (#15604, @lloydhamilton)
- [Tracking] Specify attribute.run_id when search_traces filters by run_id (#16295, @artjen)
- [Tracking] Fix graphql batching attacks (#16227, @serena-ruan)
- [Model Registry] Make the chunk size configurable in DatabricksSDKModelsArtifactRepository (#16247, @TomeHirata)

Documentation updates:

- [Docs] Move the Lighthouse main signup page to GenAI (#16404, @BenWilson2)
- [Docs] [DOC-FIX] Dspy doc fix (#16397, @joelrobin18)
- [Docs] Fix(docs): Resolve self-referencing 'Next' link on GenAI Tracing overview page (#16334, @mohammadsubhani)
- [Docs] Update the docs selector dropdown (#16280, @BenWilson2)
- [Docs] Update utm_source for source tracking to signup URL (#16316, @BenWilson2)
- [Docs] Fix footer rendering in docs for light mode display (#16214, @BenWilson2)

Small bug fixes and documentation updates:

#16261, @rohitarun-db; #16411, #16352, #16327, #16324, #16279, #16193, #16197, @harupy; #16409, #16348, #16347, #16290, #16286, #16283, #16271, #16223, @TomeHirata; #16326, @mohammadsubhani; #16364, @BenWilson2; #16308, #16218, @serena-ruan; #16262, @raymondzhou-db; #16191, @copilot-swe-agent; #16212, @B-Step62; #16208, @frontsideair; #16205, #16200, #16198, @daniellok-db

## 3.0.1 (2025-06-25)

MLflow 3.0.1 includes several major features and improvements

Features:

- [Model Registry / Sqlalchemy] Increase prompt text limit from 5K to 100K (#16377, @harupy)

Bug fixes:

- [Models] Fix bedrock provider, configured inference profile compatibility (#15604, @lloydhamilton)

Small bug fixes and documentation updates:

#16364, @BenWilson2; #16347, @TomeHirata; #16279, #15835, @harupy; #16182, @B-Step62

## 3.1 (2025-06-11)

MLflow 3 includes several major features and improvements

Features:

- [Tracking] MLflow 3.0 (#13211, @harupy)
- [Prompts] Add Custom Prompt Judges to `mlflow[databricks]` (#16097, @dbrx-euirim)
- [Artifacts / Model Registry / Tracking] Package model environment when registering model (#15783, @qyc)
- [Tracking] Add `MlflowSparkStudy` (#15418, @lu-wang-dl)
- [Scoring] Make `spark_udf` support DBConnect + DBR 15.4 / DBR dedicated cluster (#15968, @WeichenXu123)
- [Tracking] Lock model dependencies when logging a model using `uv` (#15875, @harupy)
- [Model Registry] Introduce `mlflow.genai.optimize_prompt` to optimize prompts (#15861, @TomeHirata)
- [Tracing] Support custom request/response preview (#15919, @B-Step62)
- [Tracking] Add integration for AutoGen > 0.4 (#14729, @TomeHirata)
- [Tracking] Support token tracking for OpenAI (#15870, @B-Step62)
- [Tracking] Support tracing `ResponsesAgent.predict_stream` (#15762, @bbqiu)
- [Tracking] Introduce client and fluent APIs for `LogLoggedModelParams` (#15717, @artjen)
- [Models] Support `predict_stream` in DSPy flavor (#15678, @TomeHirata)
- [Tracking] Record notebook and git metadata in trace metadata (#15650, @B-Step62)
- [Model Registry] Added `search_prompts` function to list all the prompts registered (#15445, @joelrobin18)
- [Models] Support compression for pyfunc log model (#14700, @antbbn)
- [Gateway] Add support for Gemini in AI Gateway (#15069, @joelrobin18)
- [Tracing] PydanticAI Autologging (#15553, @joelrobin18)
- [Tracking] Support setting databricks auth profile by `DATABRICKS_CONFIG_PROFILE` environment variable. (#15587, @WeichenXu123)
- [Tracking] create mlflow tracing for `smolagents` (#15574, @y-okt)
- [Artifacts / UI] Support for video artifacts (#15518, @joelrobin18)
- [Model Registry] Add `allow_missing` parameter in `load_prompt` (#15371, @joelrobin18)
- [Tracking] Emit a warning for `mlflow.get_artifact_uri()` usage outside active run (#12902, @Shashank1202)

Bug fixes:

- [GenAI] Add Databricks App resource (#15867, @aravind-segu)
- [Tracking] Support json-string for inputs/expectations column in Spark Dataframe (#16011, @B-Step62)
- [Tracking] Avoid generating traces from scorers during evaluation (#16004, @B-Step62)
- [GenAI] Allow multi inputs module in DSPy (#15859, @TomeHirata)
- [Tracking] Improve error handling if tracking URI is not set when running `mlflow gc` (#11773, @oleg-z)
- [Tracking] Trace search: Avoid spawning threads for span fetching if `include_spans=False` (#15634, @dbczumar)
- [Tracking] Fix `global_guideline_adherence` (#15572, @artjen)
- [Model Registry] Log `Resources` from `SystemAuthPolicy` in `CreateModelVersion` (#15485, @aravind-segu)
- [Models] `ResponsesAgent` interface update (#15601, #15741, @bbqiu)

Breaking changes:

- [Tracking] Move prompt registry APIs under `mlflow.genai.prompts` namespace (#16174, @B-Step62)
- [Model Registry] Default URI to databricks-uc when tracking URI is databricks & registry URI is unspecified (#16135, @dbczumar)
- [Tracking] Do not log SHAP explainer in `mlflow.evaluate` (#15827, @harupy)
- [Tracking] Update DataFrame schema returned from `mlflow.search_trace()` to be V3 format (#15643, @B-Step62)

Documentation updates:

- [Docs] Documentation revamp for MLflow 3.0 (#15954, @harupy)
- [Docs] Add Prompt Optimization Document Page (#15958, @TomeHirata)
- [Docs] Redesign API reference page (#15811, @besirovic)
- [Docs] MLflow 3 breaking changes list (#15716, @WeichenXu123)
- [Docs] Update Lighthouse signup and signin links (#15740, @BenWilson2)
- [Docs] Document models:/ URIs explicitly in OSS MLflow docs (#15727, @WeichenXu123)
- [Docs] Spark UDF Doc update (#15586, @WeichenXu123)

Small bug fixes and documentation updates:

#16193, #16192, #16171, #16119, #16036, #16130, #16081, #16101, #16047, #16086, #16077, #16045, #16065, #16067, #16063, #16061, #16058, #16050, #16043, #16034, #16033, #15966, #16025, #16015, #16002, #15970, #16001, #15999, #15942, #15960, #15955, #15951, #15939, #15885, #15883, #15890, #15887, #15874, #15869, #15846, #15845, #15826, #15834, #15822, #15830, #15796, #15821, #15818, #15817, #15805, #15804, #15798, #15793, #15797, #15782, #15775, #15772, #15790, #15773, #15776, #15756, #15767, #15766, #15765, #15746, #15747, #15748, #15751, #15743, #15731, #15720, #15722, #15670, #15614, #15715, #15677, #15708, #15673, #15680, #15686, #15671, #15657, #15669, #15664, #15675, #15667, #15666, #15668, #15651, #15649, #15647, #15640, #15638, #15630, #15627, #15624, #15622, #15558, #15610, #15577, #15575, #15545, #15576, #15559, #15563, #15555, #15557, #15548, #15551, #15547, #15542, #15536, #15524, #15531, #15525, #15520, #15521, #15502, #15499, #15442, #15426, #15315, #15392, #15397, #15399, #15394, #15358, #15352, #15349, #15328, #15336, #15335, @harupy; #16196, #16191, #16093, #16114, #16080, #16088, #16053, #15856, #16039, #15987, #16009, #16014, #16007, #15996, #15993, #15991, #15989, #15978, #15839, #15953, #15934, #15929, #15926, #15909, #15900, #15893, #15889, #15881, #15879, #15877, #15865, #15863, #15854, #15852, #15848, @copilot-swe-agent; #16178, #16153, #16155, #15823, #15754, #15794, #15800, #15799, #15615, #15777, #15726, #15752, #15745, #15753, #15738, #15681, #15684, #15682, #15702, #15679, #15623, #15645, #15612, #15533, #15607, #15522, @serena-ruan; #16177, #16167, #16168, #16166, #16152, #16144, #15920, #16134, #16128, #16098, #16059, #16024, #15974, #15917, #15676, #15750, @dbczumar; #16162, #16161, #16137, #16126, #16127, #16099, #16074, #16041, #16040, #16010, #15945, #15697, #15588, #15602, #15581, @rohitarun-db; #16150, #15984, #16125, #16102, #16062, #16060, #15986, #15985, #15983, #15982, #15980, #15763, @smoorjani; #16160, #16149, #16103, #15538, #16055, #16054, #16048, #16012, #16029, #16003, #15940, #15956, #15950, #15906, #15922, #15932, #15930, #15905, #15910, #15902, #15901, #15840, #15896, #15898, #15895, #15850, #15833, #15824, #15819, #15816, #15806, #15803, #15795, #15759, #15791, #15792, #15774, #15769, #15768, #15770, #15755, #15771, #15737, #15690, #15733, #15730, #15687, #15660, #15735, #15688, #15705, #15590, #15663, #15665, #15658, #15594, #15620, #15644, #15648, #15605, #15639, #15642, #15619, #15618, #15611, #15597, #15589, #15580, #15593, #15437, #15584, #15582, #15448, #15351, #15317, #15353, #15320, #15319, @B-Step62; #16151, #16142, #16111, #16106, #16051, #16046, #16044, #15971, #15957, #15810, #15749, #15706, #15683, #15728, #15732, #15707, #15621, #15567, #15566, #15523, #15479, #15404, #15400, #15378, @TomeHirata; #16026, #16072, @AveshCSingh; #15967, @euirim; #15884, #15924, #15395, #15393, #15390, @daniellok-db; #15786, @rahuja23; #15734, @lhrotk; #15809, #15739, #15695, #15654, #15694, #15655, #15653, #15608, #15543, #15573, @dhruyads; #15596, @mrharishkumar; #15742, #15723, #15633, #15606, @ShaylanDias; #15703, #15637, #15613, #15473, @joelrobin18; #15636, #15659, #15616, #15617, @raymondzhou-db; #15674, #15598, #15357, #15586, @WeichenXu123; #15691, @artjen; #15698, @prithvikannan; #15631, @hubertzub-db; #15569, @Anand1923; #15578, @y-okt; #14790, @singh-kristian; #14129, @jamblejoe; #15552, @BenWilson2; #14197, @clarachristiansen; #15505, @Conor0Callaghan; #15509, @tr33k; #15507, @vzamboulingame; #15459, @UnMelow; #13991, @abhishekpawar1060; #12161, @zhouyou9505; #15293, @tornikeo

## 2.22.1 (2025-06-06)

MLflow 2.22.1 includes several major features and improvements

Features:

- [Scoring] For DBConnect client, make spark_udf support DBR 15.4 and DBR dedicated cluster (#15938, @WeichenXu123)

Bug fixes:

- [Model Registry] Log Resources from SystemAuthPolicy in CreateModelVersion (#15485, @aravind-segu)
- [Tracking] Trace search: Avoid spawning threads for span fetching if include_spans=False (#15635, @dbczumar)

Documentation updates:

- [Docs] Spark UDF Doc update (#15586, @WeichenXu123)

Small bug fixes and documentation updates:

#15523, #15728, @TomeHirata; #13997, #16025, #15647, #16030, @harupy; #15786, @rahuja23; #15703, @joelrobin18; #15612, @serena-ruan; #16031, @daniellok-db; #15841, @frontsideair; #15807, @B-Step62

## 2.22.0 (2025-04-24)

MLflow 2.22.0 brings important bug fixes and improves the UI and tracking capabilities.

Features:

- [Tracking] Supported tracing for OpenAI Responses API.  
  (#15240, @B-Step62)
- [Tracking] Introduced `get_last_active_trace`, which affects model serving/monitoring logic.  
  (#15233, @B-Step62)
- [Tracking] Introduced async export for Databricks traces (default behavior).  
  (#15163, @B-Step62)
- [AI Gateway] Added Gemini embeddings support with corresponding unit tests.  
  (#15017, @joelrobin18)
- [Tracking / SQLAlchemy] MySQL SSL connections are now supported with client certs.  
  (#14839, @aksylumoed)
- [Models] Added Optuna storage utility for enabling parallel hyperparameter tuning.  
  (#15243, @XiaohanZhangCMU)
- [Artifacts] Added support for Azure Data Lake Storage (ADLS) artifact repositories.  
  (#14723, @serena-ruan)
- [UI] Artifact views for text now auto-refresh in the UI.  
  (#14939, @joelrobin18)

Bug Fixes:

- [Tracking / UI] Fixed serialization for structured output in `langchain_tracer` + added unit tests.  
  (#14971, @joelrobin18)
- [Server-infra] Enforced password validation for authentication (min. 8 characters).  
  (#15287, @WeichenXu123)
- [Deployments] Resolved an issue with the OpenAI Gateway adapter.  
  (#15286, @WeichenXu123)
- [Artifacts / Tracking / Server-infra] Normalized paths by stripping trailing slashes.  
  (#15016, @tarek7669)
- [Tags] Fixed bug where tag values containing `": "` were being truncated.  
  (#14896, @harupy)

Small bug fixes and documentation updates:

#15396, #15379, #15292, #15305, #15078, #15251, #15267, #15208, #15104, #15045, #15084, #15055, #15056, #15048, #14946, #14956, #14903, #14854, #14830, @serena-ruan; #15417, #15256, #15186, #15007, @TomeHirata; #15119, @bbqiu; #15413, #15314, #15311, #15303, #15301, #15288, #15275, #15269, #15272, #15268, #15262, #15266, #15264, #15261, #15252, #15249, #15244, #15236, #15235, #15237, #15140, #14982, #14898, #14893, #14861, #14870, #14853, #14849, #14813, #14822, @harupy; #15333, #15298, #15300, #15156, #15019, #14957, @B-Step62; #15313, #15297, #14880, @daniellok-db; #15066, #15074, #14913, @joelrobin18; #15232, @kbolashev; #15242, @dbczumar; #15210, #15178, @WeichenXu123; #15187, #15177, @hubertzub-db; #15059, #15070, #15050, #15012, #14959, #14918, #15005, #14965, #14858, #14930, #14927, #14786, #14883, #14863, #14852, #14788, @Gumichocopengin8; #14920, #14919, @jaceklaskowski

## 2.21.3 (2025-04-03)

MLflow 2.21.3 includes a few bugi

Bug fixes:

- [Tracking] Fix spark ML save model error in Databricks shared or serverless cluster (#15198, @WeichenXu123)
- [Tracking] Fix Spark model logging / loading in Databricks shared cluster and serverless (#15075, @WeichenXu123)

Documentation updates:

- [Docs] Add document page for DSPy optimizer tracking (#15143, @TomeHirata)

Small bug fixes and documentation updates:

#15205, @mlflow-app[bot]; #15184, #15157, #15137, @TomeHirata; #15085, @B-Step62; #15118, @bbqiu; #15172, @harupy

## 2.21.2 (2025-03-26)

MLflow 2.21.2 is a patch release that introduces minor features and bug fixes.

- Fix connection exhausting when exporting traces to Databricks (#15124, @B-Step62)
- Add logging of result table for DSPy optimizer tracking (#15061, @TomeHirata)

## 2.21.1 (2025-03-25)

MLflow 2.21.1 is a patch release that introduces minor features and addresses some minor bugs.

Features:

- [Tracking] Introduce support for logging evaluations within DSPy (#14962, @TomeHirata)
- [Tracking] Add support for run creation when DSPy compile is executed (#14949, @TomeHirata)
- [Docker / Sagemaker] Add support for building a SageMaker serving container that does not contain Java via the `--install-java option` (#14868, @rgangopadhya)

Bug fixes:

- [Tracing] Fix an issue with trace ordering due to a timestamp conversion timezone bug (#15094, @orm011)
- [Tracking] Fix a typo in the environment variable `OTEL_EXPORTER_OTLP_PROTOCOL` definition (#15008, @gabrielfu)
- [Tracking] Fix an issue in shared and serverless clusters on Databricks when logging Spark Datasources when using the evaluate API (#15077, @WeichenXu123)
- [UI] Fix a rendering issue with displaying images from within the metric tab in the UI (#15034, @TomeHirata)

Documentation updates:

- [Docs] Add additional contextual information within the set_retriever_schema API docs (#15099, @smurching)

Small bug fixes and documentation updates:

#15009, #14995, #15039, #15040, @TomeHirata; #15010, #15053, @B-Step62; #15014, #15025, #15030, #15050, #15070, @Gumichocopengin8; #15035, #15064, @joelrobin18; #15058, @serena-ruan; #14945, @turbotimon

## 2.21.0 (2025-03-14)

We are excited to announce the release of MLflow 2.21.0! This release includes a number of significant features, enhancements, and bug fixes.

### Major New Features

- 📚 **Documentation Redesign**: [MLflow documentation](https://mlflow.org/docs/latest/) is fully revamped with a new MDX-based website that provides better navigation and makes it easier to find the information you need! (#13645, @daniellok-db)
- 🤖 **Prompt Registry**: [MLflow Prompt Registry](https://mlflow.org/docs/latest/prompts/) is a powerful tool that streamlines prompt engineering and management in your GenAI applications. It enables you to version, track, and reuse prompts across your organization. (#14795, #14834, #14936, @B-Step62, #14960, #14984, @daniellok-db, #14909, @hubertzub-db)
- ⚡️ **FastAPI Scoring Server**: The [MLflow inference server](https://mlflow.org/docs/latest/deployment/deploy-model-locally/#serving-frameworks) has been migrated from Flask to FastAPI, enabling ASGI-based scalable inference for improved performance and throughput. (#14307, @TomeHirata)
- 🔍 **Enhanced Tracing Capabilities**: [MLflow Tracing](https://mlflow.org/docs/latest/tracing/) now supports synchronous/asynchronous generators and auto-tracing for Async OpenAI, providing more flexible and comprehensive tracing options. (#14459, #14400, #14793, @14792, @B-Step62)

Features:

- [Tracking] Support OpenAI Agent SDK Auto Tracing (#14987, @B-Step62)
- [Sqlalchemy / Tracking] Support mysql ssl connections with client certs (#14839, @aksylumoed)
- [Artifacts] Supports ADLS artifact repo (#14723, @serena-ruan)
- [Tracking] Add import and docs for txtai integration (#14712, @B-Step62)
- [Models] Introduce User Auth Policy for Pyfunc Models (#14538, @aravind-segu)
- [Tracking] Support new Google GenAI SDK (#14576, @TomeHirata)
- [Tracking] Support generating traces from DSPy built-in compilation and evaluation (#14400, @B-Step62)
- [Tracking] Add mlflow.log_trace API (#14418, @TomeHirata)
- [Models] ChatAgent LangGraph and LangChain Connectors (#14215, @bbqiu)

Bug fixes:

- [Models] Fix infinite recursion error with warning handler module (#14954, @BenWilson2)
- [Model Registry] Fix invalid type issue for ModelRegistry RestStore (#14980, @B-Step62)
- [Tracking] Fix: `ExperimentViewRunsControlsActionsSelectTags` doesn't set loading state to `false` when `set-tag` request fails. (#14907, @harupy)
- [Tracking] Fix a bug in tag creation where tag values containing `": "` get truncated (#14896, @harupy)
- [Tracking] Fix false alert from AMD GPU monitor (#14884, @B-Step62)
- [Tracking] Fix `mlflow.doctor` to fall back to `mlflow-skinny` when `mlflow` is not found (#14782, @harupy)
- [Models] Handle LangGraph breaking change (#14794, @B-Step62)
- [Tracking] Fix DSPy tracing in serving (#14743, @B-Step62)
- [Tracking] Add limit to the length of experiment artifact locations (#14416, @daniellok-db)
- [Build] Fix build.py to restore specific files #14444 (#14448, @arunkothari84)
- [Models] Fix false alert for ChatModel type hint (#14343, @B-Step62)
- [Model Registry] use aes256 to talk to s3 (#14354, @artjen)
- [Tracking] Fix LiteLLM autologging (#14340, @B-Step62)
- [Models] Fix ChatCompletionResponse for model serving Pydantic 1.x (#14332, @BenWilson2)

Documentation updates:

- [Tracking] Add guide about using MLflow tracing across thread (#14881, @B-Step62)
- [Docs] Add guide for tracing deepseek (#14826, @B-Step62)
- [Docs] Update llama Jupyter notebook source (#14754, @emmanuel-ferdman)
- [Docs] Replace Databricks Community Edition with Lighthouse [1] (#14642, @TomeHirata)
- [Docs] Update models from code guide and chat model guide to always recommend models from code (#14370, @smurching)
- [Artifacts] [DOC-FIX #14183] Improve documentation for 'artifact_uri' in 'download_artifacts' (#14225, @vinayakkgarg)

Small bug fixes and documentation updates:

#14994, #14992, #14990, #14979, #14964, #14969, #14944, #14948, #14957, #14958, #14942, #14940, #14935, #14929, #14805, #14876, #14833, #14748, #14744, #14666, #14668, #14664, #14667, #14580, #14475, #14439, #14397, #14363, #14361, #14377, #14378, #14337, #14324, #14339, #14259, @B-Step62; #14981, #14943, #14914, #14930, #14924, #14927, #14786, #14910, #14859, #14891, #14883, #14863, #14852, #14788, @Gumichocopengin8; #14946, #14978, #14956, #14906, #14903, #14854, #14860, #14857, #14824, #14830, #14767, #14772, #14770, #14766, #14651, #14629, #14636, #14572, #14498, #14328, #14265, @serena-ruan; #14989, #14895, #14880, #14878, #14866, #14821, #14817, #14815, #14765, #14803, #14773, #14783, #14784, #14776, #14759, #14541, #14553, #14540, #14499, #14495, #14481, #14479, #14456, #14022, #14411, #14407, #14408, #14315, #14346, #14325, #14322, #14326, #14310, #14309, #14320, #14308, @daniellok-db; #14986, #14904, #14898, #14893, #14861, #14870, #14853, #14849, #14813, #14822, #14818, #14802, #14804, #14814, #14779, #14796, #14735, #14731, #14728, #14734, #14727, #14726, #14721, #14719, #14716, #14692, #14683, #14687, #14684, #14674, #14673, #14662, #14652, #14650, #14648, #14647, #14646, #14639, #14637, #14635, #14634, #14633, #14630, #14628, #14624, #14623, #14621, #14619, #14615, #14613, #14603, #14601, #14600, #14597, #14570, #14564, #14554, #14551, #14550, #14515, #14529, #14528, #14525, #14516, #14514, #14486, #14476, #14472, #14477, #14364, #14431, #14414, #14398, #14412, #14399, #14359, #14369, #14381, #14349, #14350, #14347, #14348, #14342, #14329, #14250, #14318, #14323, #14306, #14280, #14279, #14272, #14270, #14263, #14222, @harupy; #14985, #14850, #14800, #14799, #14671, #14665, #14594, #14506, #14457, #14395, #14371, #14360, #14327, @TomeHirata; #14755, #14567, #14367, @bbqiu; #14892, @brilee; #14941, #14932, @hubertzub-db; #14913, @joelrobin18; #14756, @jiewpeng; #14701, @jaceklaskowski; #14568, #14450, @BenWilson2; #14535, @njbrake; #14507, @arunprd; #14489, @RuchitAgrawal; #14467, @seal07; #14460, @ManzoorAhmedShaikh; #14374, @wasup-yash; #14333, @singh-kristian; #14362, #14353, #14296, #13789, @dsuhinin; #14358, @apoxnen; #14335, @Fresnel-Fabian; #14178, @emmanuel-ferdman

## 2.20.4 (2025-03-13)

MLflow 2.20.4 is a tiny patch release to include a bug fix:

- [Tracking] fix: remove `log_trace` at top level module (#14873, @yxtay)

## 2.20.3 (2025-02-26)

MLflow 2.20.3 is a patch release includes several major features and improvements

Features:

- Implemented GPU metrics for AMD/HIP GPUs (#12694, @evenmn)
- Add txtai tracing integration (#14712, @B-Step62)
- Support new Google GenAI SDK (#14576, @TomeHirata)
- Support the new thinking content block in Anthropic Claude 3.7 models (#14733, @B-Step62)

Bug fixes:

- Resolve LangGraph tracing bug with `astream_event` API (#14598, @B-Step62)

Small bug fixes and documentation updates:

#14640, #14574, #14593, @serena-ruan; #14338, #14693, #14664, #14663, #14377, @B-Step62; #14680, @JulesLandrySimard; #14388, #14685, @harupy; #14704, @brilee; #14698, #14658, @bbqiu; #14660, #14659, #14632, #14616, #14594, @TomeHirata; #14535, @njbrake

## 2.20.2 (2025-02-13)

MLflow 2.20.2 is a patch release includes several bug fixes and features

Features:

- [Tracing] Support tracing sync/async generator function with @mlflow.trace (#14459, @B-Step62)
- [Tracing] Support generating traces from DSPy built-in compilation and evaluation (#14400, @B-Step62)
- [Models] ChatAgent interface enhancements and Langgraph connectors updates (#14368, #14567, @bbqiu)
- [Models] VariantType support in spark_udf (#14317, @serena-ruan)

Bug fixes:

- [Models] DSPy thread issue fix (#14471, @chenmoneygithub)

Documentation updates:

- [Docs] ChatAgent documentation updates (#14367, @bbqiu)

Small bug fixes and documentation updates:

#14410, #14569, #14440, @harupy; #14510, #14544, #14491, #14488, @bbqiu; #14518, @serena-ruan; #14517, #14500, #14461, #14478, @TomeHirata; #14512, @shaikmoeed; #14496, #14473, #14475, @B-Step62; #14467, @seal07; #14022, #14453, #14539, @daniellok-db; #14450, @BenWilson2; #14449, @SaiMadhavanG

## 2.20.1 (2025-01-30)

MLflow 2.20.1 is a patch release includes several bug fixes and features:

Features:

- Spark_udf support for the model signatures based on type hints (#14265, @serena-ruan)
- Helper connectors to use ChatAgent with LangChain and LangGraph (#14215, @bbqiu)
- Update classifier evaluator to draw RUC/Lift curves for CatBoost models by default (#14333, @singh-kristian)

Bug fixes:

- Fix Pydantic 1.x incompatibility issue (#14332, @BenWilson2)
- Apply temporary fix for LiteLLM tracing to workaround https://github.com/BerriAI/litellm/issues/8013 (#14340, @B-Step62)
- Fix false alert from type hint based model signature for ChatModel (#14343, @B-Step62)

Other small updates:

#14337, #14382, @B-Step62; #14356, @daniellok-db, #14354, @artjen, #14360, @TomuHirata,

## 2.20.0 (2025-01-23)

We are excited to announce the release of MLflow 2.20.0! This release includes a number of significant features, enhancements, and bug fixes.

### Major New Features

- **💡Type Hint-Based Model Signature**: Define your model's [signature](https://www.mlflow.org/docs/latest/model/signatures.html) in the most **Pythonic** way. MLflow now supports defining a model signature based on the type hints in your `PythonModel`'s `predict` function, and validating input data payloads against it. (#14182, #14168, #14130, #14100, #14099, @serena-ruan)

- **🧠 Bedrock / Groq Tracing Support**: [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) now offers a one-line auto-tracing experience for **Amazon Bedrock** and **Groq** LLMs. Track LLM invocation within your model by simply adding `mlflow.bedrock.tracing` or `mlflow.groq.tracing` call to the code. (#14018, @B-Step62, #14006, @anumita0203)

- **🗒️ Inline Trace Rendering in Jupyter Notebook**: MLflow now supports rendering a trace UI **within** the notebook where you are running models. This eliminates the need to frequently switch between the notebook and browser, creating a seamless local model debugging experience. Check out [this blog post](https://mlflow.org/blog/mlflow-tracing-in-jupyter) for a quick demo! (#13955, @daniellok-db)

- **⚡️Faster Model Validation with `uv` Package Manager**: MLflow has adopted [uv](https://github.com/astral-sh/uv), a new Rust-based, super-fast Python package manager. This release adds support for the new package manager in the [mlflow.models.predict](https://www.mlflow.org/docs/latest/model/dependencies.html#validating-environment-for-prediction) API, enabling faster model environment validation. Stay tuned for more updates! (#13824, @serena-ruan)

- **🖥️ New Chat Panel in Trace UI**: THe MLflow Trace UI now shows a unified `chat` panel for LLM invocations. The update allows you to view chat messages and function calls in a rich and consistent UI across LLM providers, as well as inspect the raw input and output payloads. (#14211, @TomuHirata)

Other Features:

- Introduced `ChatAgent` base class for defining custom python agent (#13797, @bbqiu)
- Supported Tool Calling in DSPy Tracing (#14196, @B-Step62)
- Applied timeout override to within-request local scoring server for Spark UDF inference (#14202, @BenWilson2)
- Supported dictionary type for inference params (#14091, @serena-ruan)
- Make `context` parameter optional for calling `PythonModel` instance (#14059, @serena-ruan)
- Set default task for `ChatModel` (#14068, @stevenchen-db)

Bug fixes:

- [Tracking] Fix filename encoding issue in `log_image` (#14281, @TomeHirata)
- [Models] Fix the faithfulness metric for custom override parameters supplied to the callable metric implementation (#14220, @BenWilson2)
- [Artifacts] Update presigned URL list_artifacts to return an empty list instead of an exception (#14203, @arpitjasa-db)
- [Tracking] Fix rename permission model registry (#14139, @MohamedKHALILRouissi)
- [Tracking] Fix hard-dependency to langchain package in autologging (#14125, @B-Step62)
- [Tracking] Fix constraint name for MSSQL in migration 0584bdc529eb (#14146, @daniellok-db)
- [Scoring] Fix uninitialized `loaded_model` variable (#14109, @yang-chengg)
- [Model Registry] Return empty array when `DatabricksSDKModelsArtifactRepository.list_artifacts` is called on a file (#14027, @shichengzhou-db)

Documentation updates:

- [Docs] Add a quick guide for how to host MLflow on various platforms (#14289, @B-Step62)
- [Docs] Improve documentation for 'artifact_uri' in 'download_artifacts' (#14225, @vinayakkgarg)
- [Docs] Add a page for search_traces (#14033, @TomeHirata)

Small bug fixes and documentation updates:

#14294, #14252, #14233, #14205, #14217, #14172, #14188, #14167, #14166, #14163, #14162, #14161, #13971, @TomeHirata; #14299, #14280, #14279, #14278, #14272, #14270, #14268, #14269, #14263, #14258, #14222, #14248, #14128, #14112, #14111, #14093, #14096, #14095, #14090, #14089, #14085, #14078, #14074, #14070, #14053, #14060, #14035, #14014, #14002, #14000, #13997, #13996, #13995, @harupy; #14298, #14286, #14249, #14276, #14259, #14242, #14254, #14232, #14207, #14206, #14185, #14196, #14193, #14173, #14164, #14159, #14165, #14152, #14151, #14126, #14069, #13987, @B-Step62; #14295, #14265, #14271, #14262, #14235, #14239, #14234, #14228, #14227, #14229, #14218, #14216, #14213, #14208, #14204, #14198, #14187, #14181, #14177, #14176, #14156, #14169, #14099, #14086, #13983, @serena-ruan; #14155, #14067, #14140, #14132, #14072, @daniellok-db; #14178, @emmanuel-ferdman; #14247, @dbczumar; #13789, #14108, @dsuhinin; #14212, @aravind-segu; #14223, #14191, #14084, @dsmilkov; #13804, @kriscon-db; #14158, @Lodewic; #14148, #14147, #14115, #14079, #14116, @WeichenXu123; #14135, @brilee; #14133, @manos02; #14121, @LeahKorol; #14025, @nojaf; #13948, @benglewis; #13942, @justsomerandomdude264; #14003, @Ajay-Satish-01; #13982, @prithvikannan; #13638, @MaxwellSalmon

## 2.19.0 (2024-12-11)

We are excited to announce the release of MLflow 2.19.0! This release includes a number of significant features, enhancements, and bug fixes.

### Major New Features

- **ChatModel enhancements** - ChatModel now adopts `ChatCompletionRequest` and `ChatCompletionResponse` as its new schema. The `predict_stream` interface uses `ChatCompletionChunk` to deliver true streaming responses. Additionally, the `custom_inputs` and `custom_outputs` fields in ChatModel now utilize `AnyType`, enabling support for a wider variety of data types. **Note:** In a future version of MLflow, `ChatParams` (and by extension, `ChatCompletionRequest`) will have the default values for `n`, `temperature`, and `stream` removed. (#13782, #13857, @stevenchen-db)

- **Tracing improvements** - [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) now supports both automatic and manual tracing for DSPy, LlamaIndex and Langchain flavors. Tracing feature is also auto-enabled for mlflow evaluation for all supported flavors. (#13790, #13793, #13795, #13897, @B-Step62)

- **New Tracing Integrations** - [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) now supports **CrewAI** and **Anthropic**, enabling a one-line, fully automated tracing experience. (#13903, @TomeHirata, #13851, @gabrielfu)

- **Any Type in model signature** - MLflow now supports AnyType in model signature. It can be used to host any data types that were not supported before. (#13766, @serena-ruan)

Other Features:

- [Tracking] Add `update_current_trace` API for adding tags to an active trace. (#13828, @B-Step62)
- [Deployments] Update databricks deployments to support AI gateway & additional update endpoints (#13513, @djliden)
- [Models] Support uv in mlflow.models.predict (#13824, @serena-ruan)
- [Models] Add type hints support including pydantic models (#13924, @serena-ruan)
- [Tracking] Add the `trace.search_spans()` method for searching spans within traces (#13984, @B-Step62)

Bug fixes:

- [Tracking] Allow passing in spark connect dataframes in mlflow evaluate API (#13889, @WeichenXu123)
- [Tracking] Fix `mlflow.end_run` inside a MLflow run context manager (#13888, @WeichenXu123)
- [Scoring] Fix spark_udf conditional check on remote spark-connect client or Databricks Serverless (#13827, @WeichenXu123)
- [Models] Allow changing max_workers for built-in LLM-as-a-Judge metrics (#13858, @B-Step62)
- [Models] Support saving all langchain runnables using code-based logging (#13821, @serena-ruan)
- [Model Registry] return empty array when DatabricksSDKModelsArtifactRepository.list_artifacts is called on a file (#14027, @shichengzhou-db)
- [Tracking] Stringify param values in client.log_batch() (#14015, @B-Step62)
- [Tracking] Remove deprecated squared parameter (#14028, @B-Step62)
- [Tracking] Fix request/response field in the search_traces output (#13985, @B-Step62)

Documentation updates:

- [Docs] Add Ollama and Instructor examples in tracing doc (#13937, @B-Step62)

Small bug fixes and documentation updates:

#13972, #13968, #13917, #13912, #13906, #13846, @serena-ruan; #13969, #13959, #13957, #13958, #13925, #13882, #13879, #13881, #13869, #13870, #13868, #13854, #13849, #13847, #13836, #13823, #13811, #13820, #13775, #13768, #13764, @harupy; #13960, #13914, #13862, #13892, #13916, #13918, #13915, #13878, #13891, #13863, #13859, #13850, #13844, #13835, #13818, #13762, @B-Step62; #13913, #13848, #13774, @TomeHirata; #13936, #13954, #13883, @daniellok-db; #13947, @AHB102; #13929, #13922, @Ajay-Satish-01; #13857, @stevenchen-db; #13773, @BenWilson2; #13705, @williamjamir; #13745, #13743, @WeichenXu123; #13895, @chenmoneygithub; #14023, @theBeginner86

## 2.18.0 (2024-11-18)

We are excited to announce the release of MLflow 2.18.0! This release includes a number of significant features, enhancements, and bug fixes.

### Python Version Update

Python 3.8 is now at an end-of-life point. With official support being dropped for this legacy version, **MLflow now requires Python 3.9**
as a minimum supported version.

> Note: If you are currently using MLflow's `ChatModel` interface for authoring custom GenAI applications, please ensure that you
> have read the future breaking changes section below.

### Major New Features

- **🦺 Fluent API Thread/Process Safety** - MLflow's fluent APIs for tracking and the model registry have been overhauled to add support for both thread and multi-process safety. You are now no longer forced to use the Client APIs for managing experiments, runs, and logging from within multiprocessing and threaded applications. (#13456, #13419, @WeichenXu123)

- **🧩 DSPy flavor** - MLflow now supports logging, loading, and tracing of `DSPy` models, broadening the support for advanced GenAI authoring within MLflow. Check out the [MLflow DSPy Flavor](https://mlflow.org/docs/latest/llms/dspy/index.html) documentation to get started! (#13131, #13279, #13369, #13345, @chenmoneygithub, #13543, #13800, #13807, @B-Step62, #13289, @michael-berk)

- **🖥️ Enhanced Trace UI** - [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html)'s UI has undergone
  a significant overhaul to bring usability and quality of life updates to the experience of auditing and investigating the contents of GenAI traces, from enhanced span content rendering using markdown to a standardized span component structure, (#13685, #13357, #13242, @daniellok-db)

- **🚄 New Tracing Integrations** - [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) now supports **DSPy**, **LiteLLM**, and **Google Gemini**, enabling a one-line, fully automated tracing experience. These integrations unlock enhanced observability across a broader range of industry tools. Stay tuned for upcoming integrations and updates! (#13801, @TomeHirata, #13585, @B-Step62)

- **📊 Expanded LLM-as-a-Judge Support** - MLflow now enhances its evaluation capabilities with support for additional providers, including `Anthropic`, `Bedrock`, `Mistral`, and `TogetherAI`, alongside existing providers like `OpenAI`. Users can now also configure proxy endpoints or self-hosted LLMs that follow the provider API specs by using the new `proxy_url` and `extra_headers` options. Visit the [LLM-as-a-Judge](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#llm-as-a-judge-metrics) documentation for more details! (#13715, #13717, @B-Step62)

- **⏰ Environment Variable Detection** - As a helpful reminder for when you are deploying models, MLflow now detects and reminds users of environment variables set during model logging, ensuring they are configured for deployment. In addition to this, the `mlflow.models.predict` utility has also been updated to include these variables in serving simulations, improving pre-deployment validation. (#13584, @serena-ruan)

### Breaking Changes to ChatModel Interface

- **ChatModel Interface Updates** - As part of a broader unification effort within MLflow and services that rely on or deeply integrate
  with MLflow's GenAI features, we are working on a phased approach to making a consistent and standard interface for custom GenAI
  application development and usage. In the first phase (planned for release in the next few releases of MLflow), we are marking
  several interfaces as deprecated, as they will be changing. These changes will be:

  - **Renaming of Interfaces**:
    - `ChatRequest` → `ChatCompletionRequest` to provide disambiguation for future planned request interfaces.
    - `ChatResponse` → `ChatCompletionResponse` for the same reason as the input interface.
    - `metadata` fields within `ChatRequest` and `ChatResponse` → `custom_inputs` and `custom_outputs`, respectively.
  - **Streaming Updates**:
    - `predict_stream` will be updated to enable true streaming for custom GenAI applications. Currently, it returns a generator with synchronous outputs from predict. In a future release, it will return a generator of `ChatCompletionChunks`, enabling asynchronous streaming. While the API call structure will remain the same, the returned data payload will change significantly, aligning with LangChain's implementation.
  - **Legacy Dataclass Deprecation**:
    - Dataclasses in `mlflow.models.rag_signatures` will be deprecated, merging into unified `ChatCompletionRequest`, `ChatCompletionResponse`, and `ChatCompletionChunks`.

Other Features:

- [Evaluate] Add Huggingface BLEU metrics to MLflow Evaluate (#12799, @nebrass)
- [Models / Databricks] Add support for `spark_udf` when running on Databricks Serverless runtime, Databricks connect, and prebuilt python environments (#13276, #13496, @WeichenXu123)
- [Scoring] Add a `model_config` parameter for `pyfunc.spark_udf` for customization of batch inference payload submission (#13517, @WeichenXu123)
- [Tracing] Standardize retriever span outputs to a list of MLflow `Document`s (#13242, @daniellok-db)
- [UI] Add support for visualizing and comparing nested parameters within the MLflow UI (#13012, @jescalada)
- [UI] Add support for comparing logged artifacts within the Compare Run page in the MLflow UI (#13145, @jescalada)
- [Databricks] Add support for `resources` definitions for `Langchain` model logging (#13315, @sunishsheth2009)
- [Databricks] Add support for defining multiple retrievers within `dependencies` for Agent definitions (#13246, @sunishsheth2009)

Bug fixes:

- [Database] Cascade deletes to datasets when deleting experiments to fix a bug in MLflow's `gc` command when deleting experiments with logged datasets (#13741, @daniellok-db)
- [Models] Fix a bug with `Langchain`'s `pyfunc` predict input conversion (#13652, @serena-ruan)
- [Models] Fix signature inference for subclasses and `Optional` dataclasses that define a model's signature (#13440, @bbqiu)
- [Tracking] Fix an issue with async logging batch splitting validation rules (#13722, @WeichenXu123)
- [Tracking] Fix an issue with `LangChain`'s autologging thread-safety behavior (#13672, @B-Step62)
- [Tracking] Disable support for running spark autologging in a threadpool due to limitations in Spark (#13599, @WeichenXu123)
- [Tracking] Mark `role` and `index` as required for chat schema (#13279, @chenmoneygithub)
- [Tracing] Handle raw response in openai autolog (#13802, @harupy)
- [Tracing] Fix a bug with tracing source run behavior when running inference with multithreading on `Langchain` models (#13610, @WeichenXu123)

Documentation updates:

- [Docs] Add docstring warnings for upcoming changes to ChatModel (#13730, @stevenchen-db)
- [Docs] Add a contributor's guide for implementing tracing integrations (#13333, @B-Step62)
- [Docs] Add guidance in the use of `model_config` when logging models as code (#13631, @sunishsheth2009)
- [Docs] Add documentation for the use of custom library artifacts with the `code_paths` model logging feature (#13702, @TomeHirata)
- [Docs] Improve `SparkML` `log_model` documentation with guidance on how return probabilities from classification models (#13684, @WeichenXu123)

Small bug fixes and documentation updates:

#13775, #13768, #13764, #13744, #13699, #13742, #13703, #13669, #13682, #13569, #13563, #13562, #13539, #13537, #13533, #13408, #13295, @serena-ruan; #13768, #13764, #13761, #13738, #13737, #13735, #13734, #13723, #13726, #13662, #13692, #13689, #13688, #13680, #13674, #13666, #13661, #13625, #13460, #13626, #13546, #13621, #13623, #13603, #13617, #13614, #13606, #13600, #13583, #13601, #13602, #13604, #13598, #13596, #13597, #13531, #13594, #13589, #13581, #13112, #13587, #13582, #13579, #13578, #13545, #13572, #13571, #13564, #13559, #13565, #13558, #13541, #13560, #13556, #13534, #13386, #13532, #13385, #13384, #13383, #13507, #13523, #13518, #13492, #13493, #13487, #13490, #13488, #13449, #13471, #13417, #13445, #13430, #13448, #13443, #13429, #13418, #13412, #13382, #13402, #13381, #13364, #13356, #13309, #13313, #13334, #13331, #13273, #13322, #13319, #13308, #13302, #13268, #13298, #13296, @harupy; #13705, @williamjamir; #13632, @shichengzhou-db; #13755, #13712, #13260, @BenWilson2; #13745, #13743, #13697, #13548, #13549, #13577, #13349, #13351, #13350, #13342, #13341, @WeichenXu123; #13807, #13798, #13787, #13786, #13762, #13749, #13733, #13678, #13721, #13611, #13528, #13444, #13450, #13360, #13416, #13415, #13336, #13305, #13271, @B-Step62; #13808, #13708, @smurching; #13739, @fedorkobak; #13728, #13719, #13695, #13677, @TomeHirata; #13776, #13736, #13649, #13285, #13292, #13282, #13283, #13267, @daniellok-db; #13711, @bhavya2109sharma; #13693, #13658, @aravind-segu; #13553, @dsuhinin; #13663, @gitlijian; #13657, #13629, @parag-shendye; #13630, @JohannesJungbluth; #13613, @itepifanio; #13480, @agjendem; #13627, @ilyaresh; #13592, #13410, #13358, #13233, @nojaf; #13660, #13505, @sunishsheth2009; #13414, @lmoros-DB; #13399, @Abubakar17; #13390, @KekmaTime; #13291, @michael-berk; #12511, @jgiannuzzi; #13265, @Ahar28; #13785, @Rick-McCoy; #13676, @hyolim-e; #13718, @annzhang-db; #13705, @williamjamir

## 2.17.2 (2024-10-31)

MLflow 2.17.2 includes several major features and improvements

Features:

- [Model Registry] DatabricksSDKModelsArtifactRepository support (#13203, @shichengzhou-db)
- [Tracking] Support extracting new UCFunctionToolkit as model resources (#13567, @serena-ruan)

Bug fixes:

- [Models] Fix RunnableBinding saving (#13566, @B-Step62)
- [Models] Pin numpy when pandas < 2.1.2 in pip requirements (#13580, @serena-ruan)

Documentation updates:

- [Docs] ChatModel tool calling tutorial (#13542, @daniellok-db)

Small bug fixes and documentation updates:

#13569, @serena-ruan; #13595, @BenWilson2; #13593, @mnijhuis-dnb;

## 2.17.1 (2024-10-25)

MLflow 2.17.1 includes several major features and improvements

Features:

- [Tracking] Support custom chat endpoint without endpoint type set as llm judge (#13538, @B-Step62)
- [Tracking] Support tracing for OpenAI Swarm (#13497, @B-Step62)
- [Tracking] Support UC Connections as model dependency and resources (#13481, #13491 @sunishsheth2009)
- [Tracking] Support Genie Spaces as model resources (#13441, @aravind-segu)
- [Models] Support new Transformers task for llm/v1/embedding (#13468, @B-Step62)

Bug fixes:

- [Tracking] Fix tool span inputs/outputs format in LangChain autolog (#13527, @B-Step62)
- [Models] Fix code_path handling for LlamaIndex flavor (#13486, @B-Step62)
- [Models] Fix signature inference for subclass and optional dataclasses (#13440, @bbqiu)
- [Tracking] Fix error thrown in set_retriever_schema's behavior when it's called twice (#13422, @sunishsheth2009)
- [Tracking] Fix dependency extraction from RunnableCallables (#13423, @aravind-segu)

Documentation updates:

- [Docs] Fixed typo in docs (#13478, @JAMNESIA)
- [Docs] Improve CLI docs - attention about setting MLFLOW_TRACKING_URI (#13465, @BartoszLitwiniuk)
- [Docs] Add documentation for infer_signature usage with GenAI flavors (#13407, @serena-ruan)

Small bug fixes and documentation updates:

#13293, #13510, #13501, #13506, #13446, @harupy; #13341, #13342, @WeichenXu123; #13396, @dvorst; #13535, @chenmoneygithub; #13503, #13469, #13416, @B-Step62; #13519, #13516, @serena-ruan; #13504, @sunishsheth2009; #13508, @KamilStachera; #13397, @kriscon-db

## 2.17.0 (2024-09-26)

We are excited to announce the release of MLflow 2.17.0! This release includes several enhancements to extend the
functionality of MLflow's ChatModel interface to further extend its versatility for handling custom GenAI application use cases.
Additionally, we've improved the interface within the tracing UI to provide a structured output for retrieved documents,
enhancing the ability to read the contents of those documents within the UI.
We're also starting the work on improving both the utility and the versatility of MLflow's evaluate functionality for GenAI,
initially with support for callable GenAI evaluation metrics.

### Major Features and notifications:

- **ChatModel enhancements** - As the GenAI-focused 'cousin' of `PythonModel`, `ChatModel` is getting some sizable functionality
  extensions. From native support for tool calling (a requirement for creating a custom agent), simpler conversions to the
  internal dataclass constructs needed to interface with `ChatModel` via the introduction of `from_dict` methods to all data structures,
  the addition of a `metadata` field to allow for full input payload customization, handling of the new `refusal` response type, to the
  inclusion of the interface type to the response structure to allow for greater integration compatibility.
  (#13191, #13180, #13143, @daniellok-db, #13102, #13071, @BenWilson2)

- **Callable GenAI Evaluation Metrics** - As the initial step in a much broader expansion of the functionalities of `mlflow.evaluate` for
  GenAI use cases, we've converted the GenAI evaluation metrics to be callable. This allows you to use them directly in packages that support
  callable GenAI evaluation metrics, as well as making it simpler to debug individual responses when prototyping solutions. (#13144, @serena-ruan)

- **Audio file support in the MLflow UI** - You can now directly 'view' audio files that have been logged and listen to them from within the MLflow UI's
  artifact viewer pane.

- **MLflow AI Gateway is no longer deprecated** - We've decided to revert our deprecation for the AI Gateway feature. We had renamed it to the
  MLflow Deployments Server, but have reconsidered and reverted the naming and namespace back to the original configuration.

Features:

- [Tracing] Add Standardization to retriever span outputs within MLflow tracing (#13242, @daniellok-db)
- [Models] Add support for LlamaIndex `Workflows` objects to be serialized when calling `log_model()` (#13277, #13305, #13336, @B-Step62)
- [Models] Add tool calling support for ChatModel (#13191, @daniellok-db)
- [Models] Add `from_dict()` function to ChatModel dataclasses (#13180, @daniellok-db)
- [Models] Add metadata field for ChatModel (#13143, @daniellok-db)
- [Models] Update ChatCompletionResponse to populate object type (#13102, @BenWilson2)
- [Models] Add support for LLM response refusal (#13071, @BenWilson2)
- [Models] Add support for resources to be passed in via `langchain.log_model()` (#13315, @sunishsheth2009)
- [Tracking] Add support for setting multiple retrievers' schema via `set_retriever_schema` (#13246, @sunishsheth2009)
- [Eval] Make Evaluation metrics callable (#13144, @serena-ruan)
- [UI] Add audio support to artifact viewer UI (#13017, @sydneyw-spotify)
- [Databricks] Add support for route_optimized parameter in databricks deployment client (#13222, @prabhatkgupta)

Bug fixes:

- [Tracking] Fix tracing for LangGraph (#13215, @B-Step62)
- [Tracking] Fix an issue with `presigned_url_artifact` requests being in the wrong format (#13366, @WeichenXu123)
- [Models] Update Databricks dependency extraction functionality to work with the `langchain-databricks` partner package. (#13266, @B-Step62)
- [Model Registry] Fix retry and credential refresh issues with artifact downloads from the model registry (#12935, @rohitarun-db)
- [Tracking] Fix LangChain autologging so that langchain-community is not required for partner packages (#13172, @B-Step62)
- [Artifacts] Fix issues with file removal for the local artifact repository (#13005, @rzalawad)

Documentation updates:

- [Docs] Add guide for building custom GenAI apps with ChatModel (#13207, @BenWilson2)
- [Docs] Add updates to the MLflow AI Gateway documentation (#13217, @daniellok-db)
- [Docs] Remove MLflow AI Gateway deprecation status (#13153, @BenWilson2)
- [Docs] Add contribution guide for MLflow tracing integrations (#13333, @B-Step62)
- [Docs] Add documentation regarding the `run_id` parameter within the `search_trace` API (#13251, @B-Step62)

Small bug fixes and documentation updates:

#13372, #13271, #13243, #13226, #13190, #13230, #13208, #13130, #13045, #13094, @B-Step62; #13302, #13238, #13234, #13205, #13200, #13196, #13198, #13193, #13192, #13194, #13189, #13184, #13182, #13161, #13179, #13178, #13110, #13162, #13173, #13171, #13169, #13168, #13167, #13156, #13127, #13133, #13089, #13073, #13057, #13058, #13067, #13062, #13061, #13052, @harupy; #13295, #13219, #13038, @serena-ruan; #13176, #13164, @WeichenXu123; #13163, @gabrielfu; #13186, @varshinimuthukumar1; #13128, #13115, @nojaf; #13120, @levscaut; #13152, #13075, @BenWilson2; #13138, @tanguylefloch-veesion; #13087, @SeanAverS; #13285, #13051, #13043, @daniellok-db; #13224, @levscaut;

## 2.16.2 (2024-09-17)

MLflow 2.16.2 includes several major features and improvements

Bug fixes:

- [Models] Revert "Update Dependency Extraction for Agents (#13105)" (#13155, @aravind-segu)

## 2.16.1 (2024-09-13)

MLflow 2.16.1 is a patch release that includes some minor feature improvements and addresses several bug fixes.

Features:

- [Tracing] Add Support for an Open Telemetry compatible exporter to configure external sinks for MLflow traces (#13118, @B-Step62)
- [Model Registry, AWS] Add support for utilizing AWS KMS-based encryption for the MLflow Model Registry (#12495, @artjen)
- [Model Registry] Add support for using the OSS Unity Catalog server as a Model Registry (#13034, #13065, #13066, @rohitarun-db)
- [Models] Introduce path-based transformers logging to reduce memory requirements for saving large transformers models (#13070, @B-Step62)

Bug fixes:

- [Tracking] Fix a data payload size issue with `Model.get_tags_dict` by eliminating the return of the internally-used `config` field (#13086, @harshilprajapati96)
- [Models] Fix an issue with LangChain Agents where sub-dependencies were not being properly extracted (#13105, @aravind-segu)
- [Tracking] Fix an issue where the wrong checkpoint for the current best model in auto checkpointing was being selected (#12981, @hareeen)
- [Tracking] Fix an issue where local timezones for trace initialization were not being taken into account in AutoGen tracing (#13047, @B-Step62)

Documentation updates:

- [Docs] Added RunLLM chat widget to MLflow's documentation site (#13123, @likawind)

Small bug fixes and documentation updates:

#13140, #13141, #13098, #13091, #13101, #13100, #13095, #13044, #13048, @B-Step62; #13142, #13092, #13132, #13055, #13049, @harupy; #13135, #13036, #13029, @serena-ruan; #13134, #13081, #13078, @daniellok-db; #13107, #13103, @kriscon-db; #13104, @arpitjasa-db; #13022, @nojaf; #13069, @minihat; #12879, @faizankshaikh

## 2.16.0 (2024-08-30)

We are excited to announce the release of MLflow 2.16.0. This release includes many major features and improvements!

### Major features:

- **LlamaIndex Enhancements**🦙 - to provide additional flexibility to the [LlamaIndex integration](https://mlflow.org/docs/latest/llms/llama-index/index.html), we now have support for the [models-from-code](https://mlflow.org/docs/latest/models.html#models-from-code) functionality for logging, extended engine-based logging, and broadened support for external vector stores.

- **LangGraph Support** - We've expanded the LangChain integration to support the agent framework [LangGraph](https://langchain-ai.github.io/langgraph/). With tracing and support for logging using the models-from-code feature, creating and storing agent applications has never been easier!

- **AutoGen Tracing** - Full automatic support for tracing multi-turn agent applications built with [Microsoft's AutoGen](https://microsoft.github.io/autogen/) framework is now available in MLflow. Enabling autologging via `mlflow.autogen.autolog()` will instrument your agents built with AutoGen.

- **Plugin support for AI Gateway** - You can now define your own provider interfaces that will work with MLflow's AI Gateway (also known as the MLflow Deployments Server). Creating an installable provider definition will allow you to connect the Gateway server to any GenAI service of your choosing.

Features:

- [UI] Add updated deployment usage examples to the MLflow artifact viewer (#13024, @serena-ruan, @daniellok-db)
- [Models] Support logging LangGraph applications via the models-from-code feature (#12996, @B-Step62)
- [Models] Extend automatic authorization pass-through support for Langgraph agents (#13001, @aravind-segu)
- [Models] Expand the support for LangChain application logging to include UCFunctionToolkit dependencies (#12966, @aravind-segu)
- [Models] Support saving LlamaIndex engine directly via the models-from-code feature (#12978, @B-Step62)
- [Models] Support models-from-code within the LlamaIndex flavor (#12944, @B-Step62)
- [Models] Remove the data structure conversion of input examples to ensure enhanced compatibility with inference signatures (#12782, @serena-ruan)
- [Models] Add the ability to retrieve the underlying model object from within `pyfunc` model wrappers (#12814, @serena-ruan)
- [Models] Add spark vector UDT type support for model signatures (#12758, @WeichenXu123)
- [Tracing] Add tracing support for AutoGen (#12913, @B-Step62)
- [Tracing] Reduce the latency overhead for tracing (#12885, @B-Step62)
- [Tracing] Add Async support for the trace decorator (#12877, @MPKonst)
- [Deployments] Introduce a plugin provider system to the AI Gateway (Deployments Server) (#12611, @gabrielfu)
- [Projects] Add support for parameter submission to MLflow Projects run in Databricks (#12854, @WeichenXu123)
- [Model Registry] Introduce support for Open Source Unity Catalog as a model registry service (#12888, @artjen)

Bug fixes:

- [Tracking] Reduce the contents of the `model-history` tag to only essential fields (#12983, @harshilprajapati96)
- [Models] Fix the behavior of defining the device to utilize when loading transformers models (#12977, @serena-ruan)
- [Models] Fix evaluate behavior for LlamaIndex (#12976, @B-Step62)
- [Models] Replace `pkg_resources` with `importlib.metadata` due to package deprecation (#12853, @harupy)
- [Tracking] Fix error handling for OpenAI autolog tracing (#12841, @B-Step62)
- [Tracking] Fix a condition where a deadlock can occur when connecting to an SFTP artifact store (#12938, @WeichenXu123)
- [Tracking] Fix an issue where code_paths dependencies were not properly initialized within the system path for LangChain models (#12923, @harshilprajapati96)
- [Tracking] Fix a type error for metrics value logging (#12876, @beomsun0829)
- [Tracking] Properly catch NVML errors when collecting GPU metrics (#12903, @chenmoneygithub)
- [Deployments] Improve Gateway schema support for the OpenAI provider (#12781, @danilopeixoto)
- [Model Registry] Fix deletion of artifacts when downloading from a non-standard DBFS location during UC model registration (#12821, @smurching)

Documentation updates:

- [Docs] Add documentation guides for LangGraph support (#13025, @BenWilson2)
- [Docs] Add additional documentation for models from code feature (#12936, @BenWilson2)
- [Docs] Add documentation for model serving input payloads (#12848, @serena-ruan)

Small bug fixes and documentation updates:

#12987, #12991, #12974, #12975, #12932, #12893, #12851, #12793, @serena-ruan; #13019, #13013, @aravind-segu; #12943, @piyushdaftary; #12906, #12898, #12757, #12750, #12727, @daniellok-db; #12995, #12985, #12964, #12962, #12960, #12953, #12951, #12937, #12914, #12929, #12907, #12897, #12880, #12865, #12864, #12862, #12850, #12847, #12833, #12835, #12826, #12824, #12795, #12796, @harupy; #12592, @antbbn; #12993, #12984, #12899, #12745, @BenWilson2; #12965, @nojaf; #12968, @bbqiu; #12956, @mickvangelderen; #12939, #12950, #12915, #12931, #12919, #12889, #12849, #12794, #12779, #12836, #12823, #12737, @B-Step62; #12903, @chenmoneygithub; #12905, @Atry; #12884, #12858, #12807, #12800, #10874, @WeichenXu123; #12342, @kriscon-db; #12742, @edwardfeng-db

## 2.15.1 (2024-08-06)

MLflow 2.15.1 is a patch release that addresses several bug fixes.

Bug fixes:

- [Tracking] Fix silent disabling of LangChain autologging for LangChain >= 0.2.10. (#12779, @B-Step62)
- [Tracking] Fix `mlflow.evaluate` crash on binary classification with data subset only contains single class (#12825, @serena-ruan)
- [Tracking] Fix incompatibility of MLflow Tracing with LlamaIndex >= 0.10.61 (#12890, @B-Step62)
- [Tracking] Record exceptions in OpenAI autolog tracing (#12841, @B-Step62)
- [Tracking] Fix url with e2 proxy (#12873, @chenmoneygithub)
- [Tracking] Fix regression of connecting to MLflow tracking server on other Databricks workspace (#12861, @WeichenXu123)
- [UI] Fix refresh button for model metrics on Experiment and Run pages (#12869, @beomsun0829)

Documentation updates:

- [Docs] Update doc for Spark ML vector type (#12827, @WeichenXu123)

Small bug fixes and documentation updates:

#12823, #12860, #12844, #12843, @B-Step62; #12863, #12828, @harupy; #12845, @djliden; #12820, @annzhang-db; #12831, @chenmoneygithub

## 2.15.0 (2024-07-29)

We are excited to announce the release candidate for MLflow 2.15.0. This release includes many major features and improvements!

### Major features:

- **LlamaIndex Flavor**🦙 - MLflow now offers a native integration with [LlamaIndex](https://www.llamaindex.ai/), one of the most popular libraries for building GenAI apps centered around custom data. This integration allows you to log LlamaIndex indices within MLflow, allowing for the loading and deployment of your indexed data for inference tasks with different engine types. MLflow also provides comprehensive tracing support for LlamaIndex operations, offering unprecedented transparency into complex queries. Check out the [MLflow LlamaIndex documentation](https://mlflow.org/docs/latest/llms/llama-index/index.html) to get started! (#12633, @michael-berk, @B-Step62)

- **OpenAI Tracing**🔍 - We've enhanced our OpenAI integration with a new tracing feature that works seamlessly with MLflow OpenAI autologging. You can now enable tracing of their OpenAI API usage with a single `mlflow.openai.autolog()` call, thereby MLflow will automatically log valuable metadata such as token usage and a history of your interactions, providing deeper insights into your OpenAI-powered applications. To start exploring this new capability, please check out [the tracing documentation](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing)! (#12267, @gabrielfu)

- **Enhanced Model Deployment with New Validation Feature**✅ - To improve the reliability of model deployments, MLflow has added a new method to validate your model before deploying it to an inference endpoint. This feature helps to eliminate typical errors in input and output handling, streamlining the process of model deployment and increasing confidence in your deployed models. By catching potential issues early, you can ensure a smoother transition from development to production. (#12710, @serena-ruan)

- **Custom Metrics Definition Recording for Evaluations**📊 - We've strengthened the flexibility of defining custom metrics for model evaluation by automatically logging and versioning metrics definitions, including models used as judges and prompt templates. With this new capability, you can ensure reproducibility of evaluations across different runs and easily reuse evaluation setups for consistency, facilitating more meaningful comparisons between different models or versions. (#12487, #12509, @xq-yin)

- **Databricks SDK Integration**🔐 - MLflow's interaction with Databricks endpoints has been fully migrated to use the [Databricks SDK](https://docs.databricks.com/en/dev-tools/sdk-python.html). This change brings more robust and reliable connections between MLflow and Databricks, and access to the latest Databricks features and capabilities. We mark the legacy databricks-cli support as deprecated and will remove in the future release. (#12313, @WeichenXu123)

- **Spark VectorUDT Support**💥 - MLflow's [Model Signature](https://mlflow.org/docs/latest/model/signatures.html) framework now supports Spark Vector UDT (User Defined Type), enabling logging and deployment of models using Spark VectorUDT with robust type validation. (#12758, @WeichenXu123)

### Other Notable Changes

Features:

- [Tracking] Add `parent_id` as a parameter to the `start_run` fluent API for alternative control flows (#12721, @Flametaa)
- [Tracking] Add U2M authentication support for connecting to Databricks from MLflow (#12713, @WeichenXu123)
- [Tracking] Support deleting remote artifacts with `mlflow gc` (#12451, @M4nouel)
- [Tracing] Traces can now be deleted conveniently via UI from the Traces tab in the experiments page (#12641, @daniellok-db)
- [Models] Introduce additional parameters for the `ChatModel` interface for GenAI flavors (#12612, @WeichenXu123)
- [Models] [Transformers] Support input images encoded with b64.encodebytes (#12087, @MadhuM02)
- [Models Registry] Add support for AWS KMS encryption for the Unity Catalog model registry integration (#12495, @artjen)
- [Models] Fix MLflow Dataset hashing logic for Pandas dataframe to use `iloc` for accessing rows (#12410, @julcsii)
- [Models Registry] Support presigned urls without headers for artifact location (#12349, @artjen)
- [UI] The experiments page in the MLflow UI has an updated look, and comes with some performance optimizations for line charts (#12641, @hubertzub-db)
- [UI] Line charts can now be configured to ignore outliers in the data (#12641, @daniellok-db)
- [UI] Creating compatibility with Kubeflow Dashboard UI (#12663, @cgilviadee)
- [UI] Add a new section to the artifact page in the Tracking UI, which shows code snippet to validate model input format before deployment (#12729, @serena-ruan)

Bug fixes:

- [Tracking] Fix the model construction bug in MLflow SHAP evaluation for scikit-learn model (#12599, @serena-ruan)
- [Tracking] File store get_experiment_by_name returns all stage experiments (#12788, @serena-ruan)
- [Tracking] Fix Langchain callback injection logic for async/streaming request (#12773, @B-Step62)
- [Tracing] [OpenAI] Fix stream tracing for OpenAI to record the correct chunk structure (#12629, @BenWilson2)
- [Tracing] [LangChain] Fix LangChain tracing bug for `.batch` call due to thread unsafety (#12701, @B-Step62)
- [Tracing] [LangChain] Fix nested trace issue in LangChain tracing. (#12705, @B-Step62)
- [Tracing] Prevent intervention between MLflow Tracing and other OpenTelemetry-based libraries (#12457, @B-Step62)
- [Models] Fix `log_model` issue in MLflow >= 2.13 that causes databricks DLT py4j service crashing (#12514, @WeichenXu123)
- [Models] [Transformers] Fix batch inference issue for Transformers Whisper model (#12575, @B-Step62)
- [Models] [LangChain] Fix the empty generator issue in `predict_stream` for `AgentExecutor` and other non-Runnable chains (#12518, @B-Step62)
- [Scoring] Fix Spark UDF permission denied issue in Databricks runtime (#12774, @WeichenXu123)

Documentation updates:

- Add documentation on authentication for Databricks UC Model Registry (#12552, @WeichenXu123)
- Adding model-from-code documentation for LangChain and Pyfunc (#12325, #12336, @sunishsheth2009)
- Add FAQ entry for viewing trace exceptions (#12309, @BenWilson2)
- Add note about `fork` vs `spawn` method when using multiprocessing for parallel runs (#12337, @B-Step62)
- Add example usage of `extract_fields` for `mlflow.search_traces` (#12319, @xq-yin)
- Replace GPT-3.5-turbo with GPT-4o-mini (#12740, #12746, @Acksout)

Small bug fixes and documentation updates:

#12727, #12709, #12685, #12667, #12673, #12602, #12601, #12655, #12641, #12635, #12634, #12584, #12428, #12388, #12352, #12298, #12750, #12727, #12757, @daniellok-db; #12726, #12733, #12691, #12622, #12579, #12581, #12285, #12311, #12357, #12339, #12338, #12705, #12797, #12787, #12784, #12771, #12737, @B-Step62; #12715, @hubertzub-db; #12722, #12804, @annzhang-db; #12676, #12680, #12665, #12664, #12671, #12651, #12649, #12647, #12637, #12632, #12603, #12343, #12328, #12286, #12793, #12770, @serena-ruan; #12670, #12613, #12473, #12506, #12485, #12477, #12468, #12464, #12443, #12807, #12800, #10874, #12761, @WeichenXu123; #12690, #12678, #12686, #12545, #12621, #12598, #12583, #12582, #12510, #12580, #12570, #12571, #12559, #12538, #12537, #12519, #12515, #12507, #12508, #12502, #12499, #12497, #12447, #12467, #12426, #12448, #12430, #12420, #12385, #12371, #12359, #12284, #12345, #12316, #12287, #12303, #12291, #12795, #12786, #12796, #12792, #12791, #12778, #12777, #12755, #12751, #12753, #12749, @harupy; #12742, #12702, #12742 @edwardfeng-db; #12605, @alxhslm; #12662, @freemso; #12577, @rafyzg; #12512, @Jaishree2310; #12491, #1274, @BenWilson2; #12549, @besarthoxhaj; #12476, @jessechancy; #12541, @amanjam; #12479, #12472, #12433, #12289, @xq-yin; #12486, #12474, #11406, @jgiannuzzi; #12463, @jsuchome; #12460, @Venki1402; #12449, @yukimori; #12318, @RistoAle97; #12440, @victolee0; #12416, @Dev-98; #11771, @lababidi; #12417, @dannikay; #12663, @cgilviadee; #12410, @julcsii; #12600, @ZTZK; #12803, @hcmturner; #12747, @michael-berk; #12342, @kriscon-db; #12766, @artjen;

## 2.14.3 (2024-07-12)

MLflow 2.14.3 is a patch release that addresses bug fixes and additional documentation for released features

Features:

- [Model Registry] Add support for server-side encryption when uploading files to AWS S3 (#12495, @artjen)

Bug fixes:

- [Models] Fix stream trace logging with the OpenAI autologging implementation to record the correct chunk structure (#12629, @BenWilson2)
- [Models] Fix batch inference behavior for Whisper-based translation models to allow for multiple audio file inputs (#12575, @B-Step62)

Documentation updates:

- [Docs] Add documentation for OpenAI autologging (#12608, @BenWilson2)

Small bug fixes and documentation updates:

#12556, #12628, @B-Step62; #12582, #12560, @harupy; #12553, @nojaf

## 2.14.2 (2024-07-03)

MLflow 2.14.2 is a patch release that includes several important bug fixes and documentation enhancements.

Bug fixes:

- [Models] Fix an issue with requirements inference error handling when disabling the default warning-only behavior (#12547, @B-Step62)
- [Models] Fix dependency inference issues with Transformers models saved with the unified API `llm/v1/xxx` task definitions. (#12551, @B-Step62)
- [Models / Databricks] Fix an issue with MLlfow `log_model` introduced in MLflow 2.13.0 that causes Databricks DLT service to crash in some situations (#12514, @WeichenXu123)
- [Models] Fix an output data structure issue with the `predict_stream` implementation for LangChain AgentExecutor and other non-Runnable chains (#12518, @B-Step62)
- [Tracking] Fix an issue with the `predict_proba` inference method in the `sklearn` flavor when loading an sklearn pipeline object as `pyfunc` (#12554, @WeichenXu123)
- [Tracking] Fix an issue with the Tracing implementation where other services usage of OpenTelemetry would activate MLflow tracing and cause errors (#12457, @B-Step62)
- [Tracking / Databricks] Correct an issue when running dependency inference in Databricks that can cause duplicate dependency entries to be logged (#12493, @sunishsheth2009)

Documentation updates:

- [Docs] Add documentation and guides for the MLflow tracing schema (#12521, @BenWilson2)

Small bug fixes and documentation updates:

#12311, #12285, #12535, #12543, #12320, #12444, @B-Step62; #12310, #12340, @serena-ruan; #12409, #12432, #12471, #12497, #12499, @harupy; #12555, @nojaf; #12472, #12431, @xq-yin; #12530, #12529, #12528, #12527, #12526, #12524, #12531, #12523, #12525, #12522, @dbczumar; #12483, @jsuchome; #12465, #12441, @BenWilson2; #12450, @StarryZhang-whu

## 2.14.1 (2024-06-20)

MLflow 2.14.1 is a patch release that contains several bug fixes and documentation improvements

Bug fixes:

- [Models] Fix params and model_config handling for llm/v1/xxx Transformers model (#12401, @B-Step62)
- [UI] Fix dark mode user preference (#12386, @daniellok-db)
- [Docker] Fix docker image failing to build with `install_mlflow=False` (#12388, @daniellok-db)

Documentation updates:

- [Docs] Add link to langchain autologging page in doc (#12398, @xq-yin)
- [Docs] Add documentation for Models from Code (#12381, @BenWilson2)

Small bug fixes and documentation updates:

#12415, #12396, #12394, @harupy; #12403, #12382, @BenWilson2; #12397, @B-Step62

## 2.14.0 (2024-06-17)

MLflow 2.14.0 includes several major features and improvements that we're very excited to announce!

### Major features:

- **MLflow Tracing**: Tracing is powerful tool designed to enhance your ability to monitor, analyze, and debug GenAI applications by allowing you to inspect the intermediate outputs generated as your application handles a request. This update comes with an automatic LangChain integration to make it as easy as possible to get started, but we've also implemented high-level fluent APIs, and low-level client APIs for users who want more control over their trace instrumentation. For more information, check out the [guide in our docs](https://mlflow.org/docs/latest/llms/tracing/index.html)!
- **Unity Catalog Integration**: The MLflow Deployments server now has an integration with Unity Catalog, allowing you to leverage registered functions as tools for enhancing your chat application. For more information, check out [this guide](https://mlflow.org/docs/latest/llms/deployments/uc_integration.html)!
- **OpenAI Autologging**: Autologging support has now been added for the OpenAI model flavor. With this feature, MLflow will automatically log a model upon calling the OpenAI API. Each time a request is made, the inputs and outputs will be logged as artifacts. Check out [the guide](https://mlflow.org/docs/latest/llms/openai/guide/index.html#openai-autologging) for more information!

Other Notable Features:

- [Models] Support input images encoded with b64.encodebytes (#12087, @MadhuM02)
- [Tracking] Support async logging per X seconds (#12324, @chenmoneygithub)
- [Tracking] Provide a way to set urllib's connection number and max size (#12227, @chenmoneygithub)
- [Projects] Make MLflow project runner supporting submit spark job to databricks runtime >= 13 (#12139, @WeichenXu123)
- [UI] Add the "description" column to the runs table (#11996, @zhouyou9505)

Bug fixes:

- [Model Registry] Handle no headers presigned url (#12349, @artjen)
- [Models] Fix docstring order for ChatResponse class and make object field immutable (#12305, @xq-yin)
- [Databricks] Fix root user checking in get_databricks_nfs_temp_dir and get_databricks_local_temp_dir (#12186, @WeichenXu123)
- [Tracking] fix \_init_server process terminate hang (#12076, @zhouyou9505)
- [Scoring] Fix MLflow model container and slow test CI failure (#12042, @WeichenXu123)

Documentation updates:

- [Docs] Enhance documentation for autologging supported libraries (#12356, @xq-yin)
- [Tracking, Docs] Adding Langchain as a code example and doc string (#12325, @sunishsheth2009)
- [Tracking, Docs] Adding Pyfunc as a code example and doc string (#12336, @sunishsheth2009)
- [Docs] Add FAQ entry for viewing trace exceptions in Docs (#12309, @BenWilson2)
- [Docs] Add note about 'fork' vs 'spawn' method when using multiprocessing for parallel runs (#12337, @B-Step62)
- [Docs] Fix type error in tracing example for function wrapping (#12338, @B-Step62)
- [Docs] Add example usage of "extract_fields" for mlflow.search_traces in documentation (#12319, @xq-yin)
- [Docs] Update LangChain Autologging docs (#12306, @B-Step62)
- [Docs] Add Tracing documentation (#12191, @BenWilson2)

Small bug fixes and documentation updates:

#12359, #12308, #12350, #12284, #12345, #12316, #12287, #12303, #12291, #12288, #12265, #12170, #12248, #12263, #12249, #12251, #12239, #12241, #12240, #12235, #12242, #12172, #12215, #12228, #12216, #12164, #12225, #12203, #12181, #12198, #12195, #12192, #12146, #12171, #12163, #12166, #12124, #12106, #12113, #12112, #12074, #12077, #12058, @harupy; #12355, #12326, #12114, #12343, #12328, #12327, #12340, #12286, #12310, #12200, #12209, #12189, #12194, #12201, #12196, #12174, #12107, @serena-ruan; #12364, #12352, #12354, #12353, #12351, #12298, #12297, #12220, #12155, @daniellok-db; #12311, #12357, #12346, #12312, #12339, #12281, #12283, #12282, #12268, #12236, #12247, #12199, #12232, #12233, #12221, #12229, #12207, #12212, #12193, #12167, #12137, #12147, #12148, #12138, #12127, #12065, @B-Step62; #12289, #12253, #12330 @xq-yin; #11771, @lababidi; #12280, #12275, @BenWilson2; #12246, #12244, #12211, #12066, #12061, @WeichenXu123; #12278, @sunishsheth2009; #12136, @kriscon-db; #11911, @jessechancy; #12169, @hubertzub-db

## 2.13.2 (2024-06-06)

MLflow 2.13.2 is a patch release that includes several bug fixes and integration improvements to existing
features.

Features:

- [Tracking] Provide a way to set `urllib`'s connection number and max size (#12227, @chenmoneygithub)
- [Tracking] Support UC directory as MLflow MetaDataset (#12224, @chenmoneygithub)

Bug fixes:

- [Models] Fix inferring `mlflow[gateway]` as dependency when using `mlflow.deployment` module (#12264, @B-Step62)
- [Tracking] Flatten the model_config with `/` before logging as params (#12190, @sunishsheth2009)

Small bug fixes and documentation updates:

#12268, #12210, @B-Step62; #12214, @harupy; #12223, #12226, @annzhang-db; #12260, #12237, @prithvikannan; #12261, @BenWilson2; #12231, @serena-ruan; #12238, @sunishsheth2009

## 2.13.1 (2024-05-30)

MLflow 2.13.1 is a patch release that includes several bug fixes and integration improvements to existing features. New features that are introduced in this patch release are intended to provide a foundation to further major features that will be released in the next release.

Features:

- [MLflow] Add `mlflow[langchain]` extra that installs recommended versions of langchain with MLflow (#12182, @sunishsheth2009)
- [Tracking] Adding the ability to override the model_config in langchain flavor if loaded as pyfunc (#12085, @sunishsheth2009)
- [Model Registry] Automatically detect if Presigned URLs are required for Unity Catalog (#12177, @artjen)

Bug fixes:

- [Tracking] Use `getUserLocalTempDir` and `getUserNFSTempDir` to replace `getReplLocalTempDir` and `getReplNFSTempDir` in databricks runtime (#12105, @WeichenXu123)
- [Model] Updating chat model to take default input_example and predict to accept json during inference (#12115, @sunishsheth2009)
- [Tracking] Automatically call `load_context` when inferring signature in pyfunc (#12099, @sunishsheth2009)

Small bug fixes and documentation updates:

#12180, #12152, #12128, #12126, #12100, #12086, #12084, #12079, #12071, #12067, #12062, @serena-ruan; #12175, #12167, #12137, #12134, #12127, #12123, #12111, #12109, #12078, #12080, #12064, @B-Step62; #12142, @2maz; #12171, #12168, #12159, #12153, #12144, #12104, #12095, #12083, @harupy; #12160, @aravind-segu; #11990, @kriscon-db; #12178, #12176, #12090, #12036, @sunishsheth2009; #12162, #12110, #12088, #11937, #12075, @daniellok-db; #12133, #12131, @prithvikannan; #12132, #12035, @annzhang-db; #12121, #12120, @liangz1; #12122, #12094, @dbczumar; #12098, #12055, @mparkhe

## 2.13.0 (2024-05-20)

MLflow 2.13.0 includes several major features and improvements

With this release, we're happy to introduce several features that enhance the usability of MLflow broadly across a range of use cases.

### Major Features and Improvements:

- **Streamable Python Models**: The newly introduced `predict_stream` API for Python Models allows for custom model implementations that support the return of a generator object, permitting full customization for GenAI applications.

- **Enhanced Code Dependency Inference**: A new feature for automatically inferrring code dependencies based on detected dependencies within a model's implementation. As a supplement to the `code_paths` parameter, the introduced `infer_model_code_paths` option when logging a model will determine which additional code modules are needed in order to ensure that your models can be loaded in isolation, deployed, and reliably stored.

- **Standardization of MLflow Deployment Server**: Outputs from the Deployment Server's endpoints now conform to OpenAI's interfaces to provide a simpler integration with commonly used services.

Features:

- [Deployments] Update the MLflow Deployment Server interfaces to be OpenAI compatible (#12003, @harupy)
- [Deployments] Add `Togetherai` as a supported provider for the MLflow Deployments Server (#11557, @FotiosBistas)
- [Models] Add `predict_stream` API support for Python Models (#11791, @WeichenXu123)
- [Models] Enhance the capabilities of logging code dependencies for MLflow models (#11806, @WeichenXu123)
- [Models] Add support for RunnableBinding models in LangChain (#11980, @serena-ruan)
- [Model Registry / Databricks] Add support for renaming models registered to Unity Catalog (#11988, @artjen)
- [Model Registry / Databricks] Improve the handling of searching for invalid components from Unity Catalog registered models (#11961, @artjen)
- [Model Registry] Enhance retry logic and credential refresh to mitigate cloud provider token expiration failures when uploading or downloading artifacts (#11614, @artjen)
- [Artifacts / Databricks] Add enhanced lineage tracking for models loaded from Unity Catalog (#11305, @shichengzhou-db)
- [Tracking] Add resourcing metadata to Pyfunc models to aid in model serving environment configuration (#11832, @sunishsheth2009)
- [Tracking] Enhance LangChain signature inference for models as code (#11855, @sunishsheth2009)

Bug fixes:

- [Artifacts] Prohibit invalid configuration options for multi-part upload on AWS (#11975, @ian-ack-db)
- [Model Registry] Enforce registered model metadata equality (#12013, @artjen)
- [Models] Correct an issue with `hasattr` references in `AttrDict` usages (#11999, @BenWilson2)

Documentation updates:

- [Docs] Simplify the main documentation landing page (#12017, @BenWilson2)
- [Docs] Add documentation for the expanded code path inference feature (#11997, @BenWilson2)
- [Docs] Add documentation guidelines for the `predict_stream` API (#11976, @BenWilson2)
- [Docs] Add support for enhanced Documentation with the `JFrog` MLflow Plugin (#11426, @yonarbel)

Small bug fixes and documentation updates:

#12052, #12053, #12022, #12029, #12024, #11992, #12004, #11958, #11957, #11850, #11938, #11924, #11922, #11920, #11820, #11822, #11798, @serena-ruan; #12054, #12051, #12045, #12043, #11987, #11888, #11876, #11913, #11868, @sunishsheth2009; #12049, #12046, #12037, #11831, @dbczumar; #12047, #12038, #12020, #12021, #11970, #11968, #11967, #11965, #11963, #11941, #11956, #11953, #11934, #11921, #11454, #11836, #11826, #11793, #11790, #11776, #11765, #11763, #11746, #11748, #11740, #11735, @harupy; #12025, #12034, #12027, #11914, #11899, #11866, @BenWilson2; #12026, #11991, #11979, #11964, #11939, #11894, @daniellok-db; #11951, #11974, #11916, @annzhang-db; #12015, #11931, #11627, @jessechancy; #12014, #11917, @prithvikannan; #12012, @AveshCSingh; #12001, @yunpark93; #11984, #11983, #11977, #11977, #11949, @edwardfeng-db; #11973, @bbqiu; #11902, #11835, #11775, @B-Step62; #11845, @lababidi

## 2.12.2 (2024-05-08)

MLflow 2.12.2 is a patch release that includes several bug fixes and integration improvements to existing features. New features that are introduced in this patch release are intended to provide a foundation to further major features that will be released in the next 2 minor releases.

Features:

- [Models] Add an environment configuration flag to enable raising an exception instead of a warning for failures in model dependency inference (#11903, @BenWilson2)
- [Models] Add support for the `llm/v1/embeddings` task in the Transformers flavor to unify the input and output structures for embedding models (#11795, @B-Step62)
- [Models] Introduce model streaming return via `predict_stream()` for custom `pyfunc` models capable of returning a stream response (#11791, #11895, @WeichenXu123)
- [Evaluate] Add support for overriding the entire model evaluation judgment prompt within `mlflow.evaluate` for GenAI models (#11912, @apurva-koti)
- [Tracking] Add support for defining deployment resource metadata to configure deployment resources within `pyfunc` models (#11832, #11825, #11804, @sunishsheth2009)
- [Tracking] Add support for logging `LangChain` and custom `pyfunc` models as code (#11855, #11842, @sunishsheth2009)
- [Tracking] Modify MLflow client's behavior to read from a global asynchronous configuration state (#11778, #11780, @chenmoneygithub)
- [Tracking] Enhance system metrics data collection to include a GPU power consumption metric (#11747, @chenmoneygithub)

Bug fixes:

- [Models] Fix a validation issue when performing signature validation if `params` are specified (#11838, @WeichenXu123)
- [Databricks] Fix an issue where models cannot be loaded in the Databricks serverless runtime (#11758, @WeichenXu123)
- [Databricks] Fix an issue with the Databricks serverless runtime where scaled workers do not have authorization to read from the driver NFS mount (#11757, @WeichenXu123)
- [Databricks] Fix an issue in the Databricks serverless runtime where a model loaded via a `spark_udf` for inference fails due to a configuration issue (#11752, @WeichenXu123)
- [Server-infra] Upgrade the gunicorn dependency to version 22 to address a third-party security issue (#11742, @maitreyakv)

Documentation updates:

- [Docs] Add additional guidance on search syntax restrictions for search APIs (#11892, @BenWilson2)
- [Docs] Fix an issue with the quickstart guide where the Keras example model is defined incorrectly (#11848, @horw)
- [Docs] Provide fixes and updates to LangChain tutorials and guides (#11802, @BenWilson2)
- [Docs] Fix the model registry example within the docs for correct type formatting (#11789, @80rian)

Small bug fixes and documentation updates:

#11928, @apurva-koti; #11910, #11915, #11864, #11893, #11875, #11744, @BenWilson2; #11913, #11918, #11869, #11873, #11867, @sunishsheth2009; #11916, #11879, #11877, #11860, #11843, #11844, #11817, #11841, @annzhang-db; #11822, #11861, @serena-ruan; #11890, #11819, #11794, #11774, @B-Step62; #11880, @prithvikannan; #11833, #11818, #11954, @harupy; #11831, @dbczumar; #11812, #11816, #11800, @daniellok-db; #11788, @smurching; #11756, @IgorMilavec; #11627, @jessechancy

## 2.12.1 (2024-04-17)

MLflow 2.12.1 includes several major features and improvements

With this release, we're pleased to introduce several major new features that are focused on enhanced GenAI support, Deep Learning workflows involving images, expanded table logging functionality, and general usability enhancements within the UI and external integrations.

### Major Features and Improvements:

- **PromptFlow**: Introducing the new PromptFlow flavor, designed to enrich the GenAI landscape within MLflow. This feature simplifies the creation and management of dynamic prompts, enhancing user interaction with AI models and streamlining prompt engineering processes. (#11311, #11385 @brynn-code)

- **Enhanced Metadata Sharing for Unity Catalog**: MLflow now supports the ability to share metadata (and not model weights) within Databricks Unity Catalog. When logging a model, this functionality enables the automatic duplication of metadata into a dedicated subdirectory, distinct from the model's actual storage location, allowing for different sharing permissions and access control limits. (#11357, #11720 @WeichenXu123)

- **Code Paths Unification and Standardization**: We have unified and standardized the `code_paths` parameter across all MLflow flavors to ensure a cohesive and streamlined user experience. This change promotes consistency and reduces complexity in the model deployment lifecycle. (#11688, @BenWilson2)

- **ChatOpenAI and AzureChatOpenAI Support**: Support for the ChatOpenAI and AzureChatOpenAI interfaces has been integrated into the LangChain flavor, facilitating seamless deployment of conversational AI models. This development opens new doors for building sophisticated and responsive chat applications leveraging cutting-edge language models. (#11644, @B-Step62)

- **Custom Models in Sentence-Transformers**: The sentence-transformers flavor now supports custom models, allowing for a greater flexibility in deploying tailored NLP solutions. (#11635, @B-Step62)

- **Image Support for Log Table**: With the addition of image support in `log_table`, MLflow enhances its capabilities in handling rich media. This functionality allows for direct logging and visualization of images within the platform, improving the interpretability and analysis of visual data. (#11535, @jessechancy)

- **Streaming Support for LangChain**: The newly introduced `predict_stream` API for LangChain models supports streaming outputs, enabling real-time output for chain invocation via pyfunc. This feature is pivotal for applications requiring continuous data processing and instant feedback. (#11490, #11580 @WeichenXu123)

### Security Fixes:

- **Security Patch**: Addressed a critical Local File Read/Path Traversal vulnerability within the Model Registry, ensuring robust protection against unauthorized access and securing user data integrity. (#11376, @WeichenXu123)

Features:

- [Models] Add the PromptFlow flavor (#11311, #11385 @brynn-code)
- [Models] Add a new `predict_stream` API for streamable output for Langchain models and the `DatabricksDeploymentClient` (#11490, #11580 @WeichenXu123)
- [Models] Deprecate and add `code_paths` alias for `code_path` in `pyfunc` to be standardized to other flavor implementations (#11688, @BenWilson2)
- [Models] Add support for custom models within the `sentence-transformers` flavor (#11635, @B-Step62)
- [Models] Enable Spark `MapType` support within model signatures when used with Spark udf inference (#11265, @WeichenXu123)
- [Models] Add support for metadata-only sharing within Unity Catalog through the use of a subdirectory (#11357, #11720 @WeichenXu123)
- [Models] Add Support for the `ChatOpenAI` and `AzureChatOpenAI` LLM interfaces within the LangChain flavor (#11644, @B-Step62)
- [Artifacts] Add support for utilizing presigned URLs when uploading and downloading files when using Unity Catalog (#11534, @artjen)
- [Artifacts] Add a new `Image` object for handling the logging and optimized compression of images (#11404, @jessechancy)
- [Artifacts] Add time and step-based metadata to the logging of images (#11243, @jessechancy)
- [Artifacts] Add the ability to log a dataset to Unity Catalog by means of `UCVolumeDatasetSource` (#11301, @chenmoneygithub)
- [Tracking] Remove the restrictions for logging a table in Delta format to no longer require running within a Databricks environment (#11521, @chenmoneygithub)
- [Tracking] Add support for logging `mlflow.Image` files within tables (#11535, @jessechancy)
- [Server-infra] Introduce override configurations for controlling how http retries are handled (#11590, @BenWilson2)
- [Deployments] Implement `chat` & `chat streaming` for Anthropic within the MLflow deployments server (#11195, @gabrielfu)

Security fixes:

- [Model Registry] Fix Local File Read/Path Traversal (LFI) bypass vulnerability (#11376, @WeichenXu123)

Bug fixes:

- [Model Registry] Fix a registry configuration error that occurs within Databricks serverless clusters (#11719, @WeichenXu123)
- [Model Registry] Delete registered model permissions when deleting the underlying models (#11601, @B-Step62)
- [Model Registry] Disallow `%` in model names to prevent URL mangling within the UI (#11474, @daniellok-db)
- [Models] Fix an issue where critically important environment configurations were not being captured as langchain dependencies during model logging (#11679, @serena-ruan)
- [Models] Patch the `LangChain` loading functions to handle uncorrectable pickle-related exceptions that are thrown when loading a model in certain versions (#11582, @B-Step62)
- [Models] Fix a regression in the `sklearn` flavor to reintroduce support for custom prediction methods (#11577, @B-Step62)
- [Models] Fix an inconsistent and unreliable implementation for batch support within the `langchain` flavor (#11485, @WeichenXu123)
- [Models] Fix loading remote-code-dependent `transformers` models that contain custom code (#11412, @daniellok-db)
- [Models] Remove the legacy conversion logic within the `transformers` flavor that generates an inconsistent input example display within the MLflow UI (#11508, @B-Step62)
- [Models] Fix an issue with Keras autologging iteration input handling (#11394, @WeichenXu123)
- [Models] Fix an issue with `keras` autologging training dataset generator (#11383, @WeichenXu123)
- [Tracking] Fix an issue where a module would be imported multiple times when logging a langchain model (#11553, @sunishsheth2009)
- [Tracking] Fix the sampling logic within the `GetSampledHistoryBulkInterval` API to produce more consistent results when displayed within the UI (#11475, @daniellok-db)
- [Tracking] Fix import issues and properly resolve dependencies of `langchain` and `lanchain_community` within `langchain` models when logging (#11450, @sunishsheth2009)
- [Tracking] Improve the performance of asynchronous logging (#11346, @chenmoneygithub)
- [Deployments] Add middle-of-name truncation to excessively long deployment names for Sagemaker image deployment (#11523, @BenWilson2)

Documentation updates:

- [Docs] Add clarity and consistent documentation for `code_paths` docstrings in API documentation (#11675, @BenWilson2)
- [Docs] Add documentation guidance for `sentence-transformers` `OpenAI`-compatible API interfaces (#11373, @es94129)

Small bug fixes and documentation updates:

#11723, @freemin7; #11722, #11721, #11690, #11717, #11685, #11689, #11607, #11581, #11516, #11511, #11358, @serena-ruan; #11718, #11673, #11676, #11680, #11671, #11662, #11659, #11654, #11633, #11628, #11620, #11610, #11605, #11604, #11600, #11603, #11598, #11572, #11576, #11555, #11563, #11539, #11532, #11528, #11525, #11514, #11513, #11509, #11457, #11501, #11500, #11459, #11446, #11443, #11442, #11433, #11430, #11420, #11419, #11416, #11418, #11417, #11415, #11408, #11325, #11327, #11313, @harupy; #11707, #11527, #11663, #11529, #11517, #11510, #11489, #11455, #11427, #11389, #11378, #11326, @B-Step62; #11715, #11714, #11665, #11626, #11619, #11437, #11429, @BenWilson2; #11699, #11692, @annzhang-db; #11693, #11533, #11396, #11392, #11386, #11380, #11381, #11343, @WeichenXu123; #11696, #11687, #11683, @chilir; #11387, #11625, #11574, #11441, #11432, #11428, #11355, #11354, #11351, #11349, #11339, #11338, #11307, @daniellok-db; #11653, #11369, #11270, @chenmoneygithub; #11666, #11588, @jessechancy; #11661, @jmjeon94; #11640, @tunjan; #11639, @minkj1992; #11589, @tlm365; #11566, #11410, @brynn-code; #11570, @lababidi; #11542, #11375, #11345, @edwardfeng-db; #11463, @taranarmo; #11506, @ernestwong-db; #11502, @fzyzcjy; #11470, @clemenskol; #11452, @jkfran; #11413, @GuyAglionby; #11438, @victorsun123; #11350, @liangz1; #11370, @sunishsheth2009; #11379, #11304, @zhouyou9505; #11321, #11323, #11322, @michael-berk; #11333, @cdancette; #11228, @TomeHirata

## 2.12.0 (2024-04-16)

MLflow 2.12.0 has been yanked from PyPI due to an issue with packaging required JS components. MLflow 2.12.1 is its replacement.

## 2.11.3 (2024-03-21)

MLflow 2.11.3 is a patch release that addresses a security exploit with the Open Source MLflow tracking server and miscellaneous Databricks integration fixes

Bug fixes:

- [Security] Address an LFI exploit related to misuse of url parameters (#11473, @daniellok-db)
- [Databricks] Fix an issue with Databricks Runtime version acquisition when deploying a model using Databricks Docker Container Services (#11483, @WeichenXu123)
- [Databricks] Correct an issue with credential management within Databricks Model Serving (#11468, @victorsun123)
- [Models] Fix an issue with chat request validation for LangChain flavor (#11478, @BenWilson2)
- [Models] Fixes for LangChain models that are logged as code (#11494, #11436 @sunishsheth2009)

## 2.11.2 (2024-03-19)

MLflow 2.11.2 is a patch release that introduces corrections for the support of custom transformer models, resolves LangChain integration problems, and includes several fixes to enhance stability.

Bug fixes:

- [Security] Address LFI exploit (#11376, @WeichenXu123)
- [Models] Fix transformers models implementation to allow for custom model and component definitions to be loaded properly (#11412, #11428 @daniellok-db)
- [Models] Fix the LangChain flavor implementation to support defining an MLflow model as code (#11370, @sunishsheth2009)
- [Models] Fix LangChain VectorSearch parsing errors (#11438, @victorsun123)
- [Models] Fix LangChain import issue with the community package (#11450, @sunishsheth2009)
- [Models] Fix serialization errors with RunnableAssign in the LangChain flavor (#11358, @serena-ruan)
- [Models] Address import issues with LangChain community for Databricks models (#11350, @liangz1)
- [Registry] Fix model metadata sharing within Databricks Unity Catalog (#11357, #11392 @WeichenXu123)

Small bug fixes and documentation updates:

#11321, #11323, @michael-berk; #11326, #11455, @B-Step62; #11333, @cdancette; #11373, @es94129; #11429, @BenWilson2; #11413, @GuyAglionby; #11338, #11339, #11355, #11432, #11441, @daniellok-db; #11380, #11381, #11383, #11394, @WeichenXu123; #11446, @harupy;

## 2.11.1 (2024-03-06)

MLflow 2.11.1 is a patch release, containing fixes for some Databricks integrations and other various issues.

Bug fixes:

- [UI] Add git commit hash back to the run page UI (#11324, @daniellok-db)
- [Databricks Integration] Explicitly import vectorstores and embeddings in databricks_dependencies (#11334, @daniellok-db)
- [Databricks Integration] Modify DBR version parsing logic (#11328, @daniellok-db)

Small bug fixes and documentation updates:

#11336, #11335, @harupy; #11303, @B-Step62; #11319, @BenWilson2; #11306, @daniellok-db

## 2.11.0 (2024-03-01)

MLflow 2.11.0 includes several major features and improvements

With the MLflow 2.11.0 release, we're excited to bring a series of large and impactful features that span both GenAI and Deep Learning use cases.

- The MLflow Tracking UI got an overhaul to better support the review and comparison of training runs for Deep Learning workloads. From grouping to large-scale metric plotting throughout
  the iterations of a DL model's training cycle, there are a large number of quality of life improvements to enhance your Deep Learning MLOps workflow.

- Support for the popular [PEFT](https://www.mlflow.org/docs/latest/llms/transformers/guide/index.html#peft-models-in-mlflow-transformers-flavor) library from HuggingFace is now available
  in the `mlflow.transformers` flavor. In addition to PEFT support, we've removed the restrictions on Pipeline types
  that can be logged to MLflow, as well as the ability to, when developing and testing models, log a transformers pipeline without copying foundational model weights. These
  enhancements strive to make the transformers flavor more useful for cutting-edge GenAI models, new pipeline types, and to simplify the development process of prompt engineering, fine-tuning,
  and to make iterative development faster and cheaper. Give the updated flavor a try today! (#11240, @B-Step62)

- We've added support to both [PyTorch](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.autolog) and
  [TensorFlow](https://www.mlflow.org/docs/latest/python_api/mlflow.tensorflow.html#mlflow.tensorflow.autolog) for automatic model weights checkpointing (including resumption from a
  previous state) for the auto logging implementations within both flavors. This highly requested feature allows you to automatically configure long-running Deep Learning training
  runs to keep a safe storage of your best epoch, eliminating the risk of a failure late in training from losing the state of the model optimization. (#11197, #10935, @WeichenXu123)

- We've added a new interface to Pyfunc for GenAI workloads. The new `ChatModel` interface allows for interacting with a deployed GenAI chat model as you would with any other provider.
  The simplified interface (no longer requiring conformance to a Pandas DataFrame input type) strives to unify the API interface experience. (#10820, @daniellok-db)

- We now support Keras 3. This large overhaul of the Keras library introduced new fundamental changes to how Keras integrates with different DL frameworks, bringing with it
  a host of new functionality and interoperability. To learn more, see the [Keras 3.0 Tutorial](https://www.mlflow.org/docs/latest/deep-learning/keras/quickstart/quickstart_keras.html)
  to start using the updated model flavor today! (#10830, @chenmoneygithub)

- [Mistral AI](https://mistral.ai/) has been added as a native [provider](https://www.mlflow.org/docs/latest/llms/deployments/index.html#providers) for the MLflow Deployments Server. You can
  now create proxied connections to the Mistral AI services for completions and embeddings with their powerful GenAI models. (#11020, @thnguyendn)

- We've added compatibility support for the OpenAI 1.x SDK. Whether you're using an OpenAI LLM for model evaluation or calling OpenAI within a LangChain model, you'll now be able to
  utilize the 1.x family of the OpenAI SDK without having to point to deprecated legacy APIs. (#11123, @harupy)

Features:

- [UI] Revamp the MLflow Tracking UI for Deep Learning workflows, offering a more intuitive and efficient user experience (#11233, @daniellok-db)
- [Data] Introduce the ability to log datasets without loading them into memory, optimizing resource usage and processing time (#11172, @chenmoneygithub)
- [Models] Introduce logging frequency controls for TensorFlow, aligning it with Keras for consistent performance monitoring (#11094, @chenmoneygithub)
- [Models] Add PySpark DataFrame support in `mlflow.pyfunc.predict`, enhancing data compatibility and analysis options for batch inference (#10939, @ernestwong-db)
- [Models] Introduce new CLI commands for updating model requirements, facilitating easier maintenance, validation and updating of models without having to re-log (#11061, @daniellok-db)
- [Models] Update Embedding API for sentence transformers to ensure compatibility with OpenAI format, broadening model application scopes (#11019, @lu-wang-dl)
- [Models] Improve input and signature support for text-generation models, optimizing for Chat and Completions tasks (#11027, @es94129)
- [Models] Enable chat and completions task outputs in the text-generation pipeline, expanding interactive capabilities (#10872, @es94129)
- [Tracking] Add node id to system metrics for enhanced logging in multi-node setups, improving diagnostics and monitoring (#11021, @chenmoneygithub)
- [Tracking] Implement `mlflow.config.enable_async_logging` for asynchronous logging, improving log handling and system performance (#11138, @chenmoneygithub)
- [Evaluate] Enhance model evaluation with endpoint URL support, streamlining performance assessments and integrations (#11262, @B-Step62)
- [Deployments] Implement chat & chat streaming support for Cohere, enhancing interactive model deployment capabilities (#10976, @gabrielfu)
- [Deployments] Enable Cohere streaming support, allowing real-time interaction functionalities for the MLflow Deployments server with the Cohere provider (#10856, @gabrielfu)
- [Docker / Scoring] Optimize Docker images for model serving, ensuring more efficient deployment and scalability (#10954, @B-Step62)
- [Scoring] Support completions (`prompt`) and embeddings (`input`) format inputs in the scoring server, increasing model interaction flexibility (#10958, @es94129)

Bug Fixes:

- [Model Registry] Correct the oversight of not utilizing the default credential file in model registry setups (#11261, @B-Step62)
- [Model Registry] Address the visibility issue of aliases in the model versions table within the registered model detail page (#11223, @smurching)
- [Models] Ensure `load_context()` is called when enforcing `ChatModel` outputs so that all required external references are included in the model object instance (#11150, @daniellok-db)
- [Models] Rectify the keras output dtype in signature mismatches, ensuring data consistency and accuracy (#11230, @chenmoneygithub)
- [Models] Resolve spark model loading failures, enhancing model reliability and accessibility (#11227, @WeichenXu123)
- [Models] Eliminate false warnings for missing signatures in Databricks, improving the user experience and model validation processes (#11181, @B-Step62)
- [Models] Implement a timeout for signature/requirement inference during Transformer model logging, optimizing the logging process and avoiding delays (#11037, @B-Step62)
- [Models] Address the missing dtype issue for transformer pipelines, ensuring data integrity and model accuracy (#10979, @B-Step62)
- [Models] Correct non-idempotent predictions due to in-place updates to model-config, stabilizing model outputs (#11014, @B-Step62)
- [Models] Fix an issue where specifying `torch.dtype` as a string was not being applied correctly to the underlying transformers model (#11297, #11295, @harupy)
- [Tracking] Fix `mlflow.evaluate` `col_mapping` bug for non-LLM/custom metrics, ensuring accurate evaluation and metric calculation (#11156, @sunishsheth2009)
- [Tracking] Resolve the `TensorInfo` TypeError exception message issue, ensuring clarity and accuracy in error reporting for users (#10953, @leecs0503)
- [Tracking] Enhance `RestException` objects to be picklable, improving their usability in distributed computing scenarios where serialization is essential (#10936, @WeichenXu123)
- [Tracking] Address the handling of unrecognized response error codes, ensuring robust error processing and improved user feedback in edge cases (#10918, @chenmoneygithub)
- [Spark] Update hardcoded `io.delta:delta-spark_2.12:3.0.0` dependency to the correct scala version, aligning dependencies with project requirements (#11149, @WeichenXu123)
- [Server-infra] Adapt to newer versions of python by avoiding `importlib.metadata.entry_points().get`, enhancing compatibility and stability (#10752, @raphaelauv)
- [Server-infra / Tracking] Introduce an environment variable to disable mlflow configuring logging on import, improving configurability and user control (#11137, @jmahlik)
- [Auth] Enhance auth validation for `mlflow.login()`, streamlining the authentication process and improving security (#11039, @chenmoneygithub)

Documentation Updates:

- [Docs] Introduce a comprehensive notebook demonstrating the use of ChatModel with Transformers and Pyfunc, providing users with practical insights and guidelines for leveraging these models (#11239, @daniellok-db)
- [Tracking / Docs] Stabilize the dataset logging APIs, removing the experimental status (#11229, @dbczumar)
- [Docs] Revise and update the documentation on authentication database configuration, offering clearer instructions and better support for setting up secure authentication mechanisms (#11176, @gabrielfu)
- [Docs] Publish a new guide and tutorial for MLflow data logging and `log_input`, enriching the documentation with actionable advice and examples for effective data handling (#10956, @BenWilson2)
- [Docs] Upgrade the documentation visuals by replacing low-resolution and poorly dithered GIFs with high-quality HTML5 videos, significantly enhancing the learning experience (#11051, @BenWilson2)
- [Docs / Examples] Correct the compatibility matrix for OpenAI in MLflow Deployments Server documentation, providing users with accurate integration details and supporting smoother deployments (#11015, @BenWilson2)

Small bug fixes and documentation updates:

#11284, #11096, #11285, #11245, #11254, #11252, #11250, #11249, #11234, #11248, #11242, #11244, #11236, #11208, #11220, #11222, #11221, #11219, #11218, #11210, #11209, #11207, #11196, #11194, #11177, #11205, #11183, #11192, #11179, #11178, #11175, #11174, #11166, #11162, #11151, #11168, #11167, #11153, #11158, #11143, #11141, #11119, #11123, #11124, #11117, #11121, #11078, #11097, #11079, #11095, #11082, #11071, #11076, #11070, #11072, #11073, #11069, #11058, #11034, #11046, #10951, #11055, #11045, #11035, #11044, #11043, #11031, #11030, #11023, #10932, #10986, #10949, #10943, #10928, #10929, #10925, #10924, #10911, @harupy; #11289, @BenWilson2; #11290, #11145, #11125, #11098, #11053, #11006, #11001, #11011, #11007, #10985, #10944, #11231, @daniellok-db; #11276, #11280, #11275, #11263, #11247, #11257, #11258, #11256, #11224, #11211, #11182, #11059, #11056, #11048, #11008, #10923, @serena-ruan; #11129, #11086, @victorsun123; #11292, #11004, #11204, #11148, #11165, #11146, #11115, #11099, #11092, #11029, #10983, @B-Step62; #11189, #11191, #11022, #11160, #11110, #11088, #11042, #10879, #10832, #10831, #10888, #10908, @michael-berk; #10627, #11217, #11200, #10969, @liangz1; #11215, #11173, #11000, #10931, @edwardfeng-db; #11188, #10711, @TomeHirata; #11186, @xhochy; #10916, @annzhang-db; #11131, #11010, #11060, @WeichenXu123; #11063, #10981, #10889, ##11269, @chenmoneygithub; #11054, #10921, @smurching; #11018, @mingyangge-db; #10989, @minkj1992; #10796, @kriscon-db; #10984, @eltociear; #10982, @holzman; #10972, @bmuskalla; #10959, @prithvikannan; #10941, @mahesh-venkatachalam; #10915, @Cokral; #10904, @dannyfriar; #11134, @WP-LKL; #11287, @serkef;

## 2.10.2 (2024-02-09)

MLflow 2.10.2 includes several major features and improvements

Small bug fixes and documentation updates:

#11065, @WeichenXu123

## 2.10.1 (2024-02-06)

MLflow 2.10.1 is a patch release, containing fixes for various bugs in the `transformers` and `langchain` flavors, the MLflow UI, and the S3 artifact store. More details can be found in the patch notes below.

Bug fixes:

- [UI] Fixed a bug that prevented datasets from showing up in the MLflow UI (#10992, @daniellok-db)
- [Artifact Store] Fixed directory bucket region name retrieval (#10967, @kriscon-db)
- Bug fixes for Transformers flavor
  - [Models] Fix an issue with transformer pipelines not inheriting the torch dtype specified on the model, causing pipeline inference to consume more resources than expected. (#10979, @B-Step62)
  - [Models] Fix non-idempotent prediction due to in-place update to model-config (#11014, @B-Step62)
  - [Models] Fixed a bug affecting prompt templating with Text2TextGeneration pipelines. Previously, calling `predict()` on a pyfunc-loaded Text2TextGeneration pipeline would fail for `string` and `List[string]` inputs. (#10960, @B-Step62)
- Bug fixes for Langchain flavor
  - Fixed errors that occur when logging inputs and outputs with different lengths (#10952, @serena-ruan)

Documentation updates:

- [Docs] Add indications of DL UI capabilities to the DL landing page (#10991, @BenWilson2)
- [Docs] Fix incorrect logo on LLMs landing page (#11017, @BenWilson2)

Small bug fixes and documentation updates:

#10930, #11005, @serena-ruan; #10927, @harupy

## 2.10.0 (2024-01-26)

MLflow 2.10.0 includes several major features and improvements

In MLflow 2.10, we're introducing a number of significant new features that are preparing the way for current and future enhanced support for Deep Learning use cases, new features to support a broadened support for GenAI applications, and some quality of life improvements for the MLflow Deployments Server (formerly the AI Gateway).

Our biggest features this release are:

- We have a new [home](https://mlflow.org). The new site landing page is fresh, modern, and contains more content than ever. We're adding new content and blogs all of the time.

- Objects and Arrays are now available as configurable input and output schema elements. These new types are particularly useful for GenAI-focused flavors that can have complex input and output types. See the new [Signature and Input Example documentation](https://mlflow.org/docs/latest/model/signatures.html) to learn more about how to use these new signature types.

- LangChain has autologging support now! When you invoke a chain, with autologging enabled, we will automatically log most chain implementations, recording and storing your configured LLM application for you. See the new [Langchain documentation](https://mlflow.org/docs/latest/llms/langchain/index.html#mlflow-langchain-autologging) to learn more about how to use this feature.

- The MLflow `transformers` flavor now supports prompt templates. You can now specify an application-specific set of instructions to submit to your GenAI pipeline in order to simplify, streamline, and integrate sets of system prompts to be supplied with each input request. Check out the updated [guide to transformers](https://www.mlflow.org/docs/latest/llms/transformers/index.html) to learn more and see examples!

- The [MLflow Deployments Server](https://mlflow.org/docs/latest/llms/deployments/index.html) now supports two new requested features: (1) OpenAI endpoints that support streaming responses. You can now configure an endpoint to return realtime responses for Chat and Completions instead of waiting for the entire text contents to be completed. (2) Rate limits can now be set per endpoint in order to help control cost overrun when using SaaS models.

- Continued the push for enhanced documentation, guides, tutorials, and examples by expanding on core MLflow functionality ([Deployments](https://mlflow.org/docs/latest/deployment/index.html), [Signatures](https://mlflow.org/docs/latest/model/signatures.html), and [Model Dependency management](https://mlflow.org/docs/latest/model/dependencies.html)), as well as entirely new pages for GenAI flavors. Check them out today!

Features:

- [Models] Introduce `Objects` and `Arrays` support for model signatures (#9936, @serena-ruan)
- [Models] Support saving prompt templates for transformers (#10791, @daniellok-db)
- [Models] Enhance the MLflow Models `predict` API to serve as a pre-logging validator of environment compatibility. (#10759, @B-Step62)
- [Models] Add support for Image Classification pipelines within the transformers flavor (#10538, @KonakanchiSwathi)
- [Models] Add support for retrieving and storing license files for transformers models (#10871, @BenWilson2)
- [Models] Add support for model serialization in the Visual NLP format for JohnSnowLabs flavor (#10603, @C-K-Loan)
- [Models] Automatically convert OpenAI input messages to LangChain chat messages for `pyfunc` predict (#10758, @dbczumar)
- [Tracking] Add support for Langchain autologging (#10801, @serena-ruan)
- [Tracking] Enhance async logging functionality by ensuring flush is called on `Futures` objects (#10715, @chenmoneygithub)
- [Tracking] Add support for a non-interactive mode for the `login()` API (#10623, @henxing)
- [Scoring] Allow MLflow model serving to support direct `dict` inputs with the `messages` key (#10742, @daniellok-db, @B-Step62)
- [Deployments] Add streaming support to the MLflow Deployments Server for OpenAI streaming return compatible routes (#10765, @gabrielfu)
- [Deployments] Add the ability to set rate limits on configured endpoints within the MLflow deployments server API (#10779, @TomeHirata)
- [Deployments] Add support for directly interfacing with OpenAI via the MLflow Deployments server (#10473, @prithvikannan)
- [UI] Introduce a number of new features for the MLflow UI (#10864, @daniellok-db)
- [Server-infra] Add an environment variable that can disallow HTTP redirects (#10655, @daniellok-db)
- [Artifacts] Add support for Multipart Upload for Azure Blob Storage (#10531, @gabrielfu)

Bug fixes:

- [Models] Add deduplication logic for pip requirements and extras handling for MLflow models (#10778, @BenWilson2)
- [Models] Add support for paddle 2.6.0 release (#10757, @WeichenXu123)
- [Tracking] Fix an issue with an incorrect retry default timeout for urllib3 1.x (#10839, @BenWilson2)
- [Recipes] Fix an issue with MLflow Recipes card display format (#10893, @WeichenXu123)
- [Java] Fix an issue with metadata collection when using Streaming Sources on certain versions of Spark where Delta is the source (#10729, @daniellok-db)
- [Scoring] Fix an issue where SageMaker tags were not propagating correctly (#9310, @clarkh-ncino)
- [Windows / Databricks] Fix an issue with executing Databricks run commands from within a Window environment (#10811, @wolpl)
- [Models / Databricks] Disable `mlflowdbfs` mounts for JohnSnowLabs flavor due to flakiness (#9872, @C-K-Loan)

Documentation updates:

- [Docs] Fixed the `KeyError: 'loss'` bug for the Quickstart guideline (#10886, @yanmxa)
- [Docs] Relocate and supplement Model Signature and Input Example docs (#10838, @BenWilson2)
- [Docs] Add the HuggingFace Model Evaluation Notebook to the website (#10789, @BenWilson2)
- [Docs] Rewrite the search run documentation (#10863, @chenmoneygithub)
- [Docs] Create documentation for transformers prompt templates (#10836, @daniellok-db)
- [Docs] Refactoring of the Getting Started page (#10798, @BenWilson2)
- [Docs] Add a guide for model dependency management (#10807, @B-Step62)
- [Docs] Add tutorials and guides for LangChain (#10770, @BenWilson2)
- [Docs] Refactor portions of the Deep Learning documentation landing page (#10736, @chenmoneygithub)
- [Docs] Refactor and overhaul the Deployment documentation and add new tutorials (#10726, @B-Step62)
- [Docs] Add a PyTorch landing page, quick start, and guide (#10687, #10737 @chenmoneygithub)
- [Docs] Add additional tutorials to OpenAI flavor docs (#10700, @BenWilson2)
- [Docs] Enhance the guides on quickly getting started with MLflow by demonstrating how to use Databricks Community Edition (#10663, @BenWilson2)
- [Docs] Create the OpenAI Flavor landing page and intro notebooks (#10622, @BenWilson2)
- [Docs] Refactor the Tensorflow flavor API docs (#10662, @chenmoneygithub)

Small bug fixes and documentation updates:

#10538, #10901, #10903, #10876, #10833, #10859, #10867, #10843, #10857, #10834, #10814, #10805, #10764, #10771, #10733, #10724, #10703, #10710, #10696, #10691, #10692, @B-Step62; #10882, #10854, #10395, #10725, #10695, #10712, #10707, #10667, #10665, #10654, #10638, #10628, @harupy; #10881, #10875, #10835, #10845, #10844, #10651, #10806, #10786, #10785, #10781, #10741, #10772, #10727, @serena-ruan; #10873, #10755, #10750, #10749, #10619, @WeichenXu123; #10877, @amueller; #10852, @QuentinAmbard; #10822, #10858, @gabrielfu; #10862, @jerrylian-db; #10840, @ernestwong-db; #10841, #10795, #10792, #10774, #10776, #10672, @BenWilson2; #10827, #10826, #10825, #10732, #10481, @michael-berk; #10828, #10680, #10629, @daniellok-db; #10799, #10800, #10578, #10782, #10783, #10723, #10464, @annzhang-db; #10803, #10731, #10708, @kriscon-db; #10797, @dbczumar; #10756, #10751, @Ankit8848; #10784, @AveshCSingh; #10769, #10763, #10717, @chenmoneygithub; #10698, @rmalani-db; #10767, @liangz1; #10682, @cdreetz; #10659, @prithvikannan; #10639, #10609, @TomeHirata

## 2.9.2 (2023-12-14)

MLflow 2.9.2 is a patch release, containing several critical security fixes and configuration updates to support extremely large model artifacts.

Features:

- [Deployments] Add the `mlflow.deployments.openai` API to simplify direct access to OpenAI services through the deployments API (#10473, @prithvikannan)
- [Server-infra] Add a new environment variable that permits disabling http redirects within the Tracking Server for enhanced security in publicly accessible tracking server deployments (#10673, @daniellok-db)
- [Artifacts] Add environment variable configurations for both Multi-part upload and Multi-part download that permits modifying the per-chunk size to support extremely large model artifacts (#10648, @harupy)

Security fixes:

- [Server-infra] Disable the ability to inject malicious code via manipulated YAML files by forcing YAML rendering to be performed in a secure Sandboxed mode (#10676, @BenWilson2, #10640, @harupy)
- [Artifacts] Prevent path traversal attacks when querying artifact URI locations by disallowing `..` path traversal queries (#10653, @B-Step62)
- [Data] Prevent a mechanism for conducting a malicious file traversal attack on Windows when using tracking APIs that interface with `HTTPDatasetSource` (#10647, @BenWilson2)
- [Artifacts] Prevent a potential path traversal attack vector via encoded url traversal paths by decoding paths prior to evaluation (#10650, @B-Step62)
- [Artifacts] Prevent the ability to conduct path traversal attacks by enforcing the use of sanitized paths with the tracking server (#10666, @harupy)
- [Artifacts] Prevent path traversal attacks when using an FTP server as a backend store by enforcing base path declarations prior to accessing user-supplied paths (#10657, @harupy)

Documentation updates:

- [Docs] Add an end-to-end tutorial for RAG creation and evaluation (#10661, @AbeOmor)
- [Docs] Add Tensorflow landing page (#10646, @chenmoneygithub)
- [Deployments / Tracking] Add endpoints to LLM evaluation docs (#10660, @prithvikannan)
- [Examples] Add retriever evaluation tutorial for LangChain and improve the Question Generation tutorial notebook (#10419, @liangz1)

Small bug fixes and documentation updates:

#10677, #10636, @serena-ruan; #10652, #10649, #10641, @harupy; #10643, #10632, @BenWilson2

## 2.9.1 (2023-12-07)

MLflow 2.9.1 is a patch release, containing a critical bug fix related to loading `pyfunc` models that were saved in previous versions of MLflow.

Bug fixes:

- [Models] Revert Changes to PythonModel that introduced loading issues for models saved in earlier versions of MLflow (#10626, @BenWilson2)

Small bug fixes and documentation updates:

#10625, @BenWilson2

## 2.9.0 (2023-12-05)

MLflow 2.9.0 includes several major features and improvements.

MLflow AI Gateway deprecation (#10420, @harupy):

The feature previously known as MLflow AI Gateway has been moved to utilize [the MLflow deployments API](https://mlflow.org/docs/latest/llms/deployments/index.html).
For guidance on migrating from the AI Gateway to the new deployments API, please see the [MLflow AI Gateway Migration Guide](https://mlflow.org/docs/latest/llms/gateway/migration.html.

MLflow Tracking docs overhaul (#10471, @B-Step62):

[The MLflow tracking docs](https://mlflow.org/docs/latest/tracking.html) have been overhauled. We'd like your feedback on the new tracking docs!

Security fixes:

Three security patches have been filed with this release and CVE's have been issued with the details involved in the security patch and potential attack vectors. Please review and update your tracking server deployments if your tracking server is not securely deployed and has open access to the internet.

- Sanitize `path` in `HttpArtifactRepository.list_artifacts` (#10585, @harupy)
- Sanitize `filename` in `Content-Disposition` header for `HTTPDatasetSource` (#10584, @harupy).
- Validate `Content-Type` header to prevent POST XSS (#10526, @B-Step62)

Features:

- [Tracking] Use `backoff_jitter` when making HTTP requests (#10486, @ajinkyavbhandare)
- [Tracking] Add default `aggregate_results` if the score type is numeric in `make_metric` API (#10490, @sunishsheth2009)
- [Tracking] Add string type of score types for metric value for genai (#10307, @sunishsheth2009)
- [Artifacts] Support multipart upload for for proxy artifact access (#9521, @harupy)
- [Models] Support saving `torch_dtype` for transformers models (#10586, @serena-ruan)
- [Models] Add built-in metric `ndcg_at_k` to retriever evaluation (#10284, @liangz1)
- [Model Registry] Implement universal `copy_model_version` (#10308, @jerrylian-db)
- [Models] Support saving/loading `RunnableSequence`, `RunnableParallel`, and `RunnableBranch` (#10521, #10611, @serena-ruan)

Bug fixes:

- [Tracking] Resume system metrics logging when resuming an existing run (#10312, @chenmoneygithub)
- [UI] Fix incorrect sorting order in line chart (#10553, @B-Step62)
- [UI] Remove extra whitespace in git URLs (#10506, @mrplants)
- [Models] Make spark_udf use NFS to broadcast model to spark executor on databricks runtime and spark connect mode (#10463, @WeichenXu123)
- [Models] Fix promptlab pyfunc models not working for chat routes (#10346, @daniellok-db)

Documentation updates:

- [Docs] Add a quickstart guide for Tensorflow (#10398, @chenmoneygithub)
- [Docs] Improve the parameter tuning guide (#10344, @chenmoneygithub)
- [Docs] Add a guide for system metrics logging (#10429, @chenmoneygithub)
- [Docs] Add instructions on how to configure credentials for Azure OpenAI (#10560, @BenWilson2)
- [Docs] Add docs and tutorials for Sentence Transformers flavor (#10476, @BenWilson2)
- [Docs] Add tutorials, examples, and guides for Transformers Flavor (#10360, @BenWilson2)

Small bug fixes and documentation updates:

#10567, #10559, #10348, #10342, #10264, #10265, @B-Step62; #10595, #10401, #10418, #10394, @chenmoneygithub; #10557, @dan-licht; #10584, #10462, #10445, #10434, #10432, #10412, #10411, #10408, #10407, #10403, #10361, #10340, #10339, #10310, #10276, #10268, #10260, #10224, #10214, @harupy; #10415, @jessechancy; #10579, #10555, @annzhang-db; #10540, @wllgrnt; #10556, @smurching; #10546, @mbenoit29; #10534, @gabrielfu; #10532, #10485, #10444, #10433, #10375, #10343, #10192, @serena-ruan; #10480, #10416, #10173, @jerrylian-db; #10527, #10448, #10443, #10442, #10441, #10440, #10439, #10381, @prithvikannan; #10509, @keenranger; #10508, #10494, @WeichenXu123; #10489, #10266, #10210, #10103, @TomeHirata; #10495, #10435, #10185, @daniellok-db; #10319, @michael-berk; #10417, @bbqiu; #10379, #10372, #10282, @BenWilson2; #10297, @KonakanchiSwathi; #10226, #10223, #10221, @milinddethe15; #10222, @flooxo; #10590, @letian-w;

## 2.8.1 (2023-11-14)

MLflow 2.8.1 is a patch release, containing some critical bug fixes and an update to our continued work on reworking our docs.

Notable details:

- The API `mlflow.llm.log_predictions` is being marked as deprecated, as its functionality has been incorporated into `mlflow.log_table`. This API will be removed in the 2.9.0 release. (#10414, @dbczumar)

Bug fixes:

- [Artifacts] Fix a regression in 2.8.0 where downloading a single file from a registered model would fail (#10362, @BenWilson2)
- [Evaluate] Fix the `Azure OpenAI` integration for `mlflow.evaluate` when using LLM `judge` metrics (#10291, @prithvikannan)
- [Evaluate] Change `Examples` to optional for the `make_genai_metric` API (#10353, @prithvikannan)
- [Evaluate] Remove the `fastapi` dependency when using `mlflow.evaluate` for LLM results (#10354, @prithvikannan)
- [Evaluate] Fix syntax issues and improve the formatting for generated prompt templates (#10402, @annzhang-db)
- [Gateway] Fix the Gateway configuration validator pre-check for OpenAI to perform instance type validation (#10379, @BenWilson2)
- [Tracking] Fix an intermittent issue with hanging threads when using asynchronous logging (#10374, @chenmoneygithub)
- [Tracking] Add a timeout for the `mlflow.login()` API to catch invalid hostname configuration input errors (#10239, @chenmoneygithub)
- [Tracking] Add a `flush` operation at the conclusion of logging system metrics (#10320, @chenmoneygithub)
- [Models] Correct the prompt template generation logic within the Prompt Engineering UI so that the prompts can be used in the Python API (#10341, @daniellok-db)
- [Models] Fix an issue in the `SHAP` model explainability functionality within `mlflow.shap.log_explanation` so that duplicate or conflicting dependencies are not registered when logging (#10305, @BenWilson2)

Documentation updates:

- [Docs] Add MLflow Tracking Quickstart (#10285, @BenWilson2)
- [Docs] Add tracking server configuration guide (#10241, @chenmoneygithub)
- [Docs] Refactor and improve the model deployment quickstart guide (#10322, @prithvikannan)
- [Docs] Add documentation for system metrics logging (#10261, @chenmoneygithub)

Small bug fixes and documentation updates:

#10367, #10359, #10358, #10340, #10310, #10276, #10277, #10247, #10260, #10220, #10263, #10259, #10219, @harupy; #10313, #10303, #10213, #10272, #10282, #10283, #10231, #10256, #10242, #10237, #10238, #10233, #10229, #10211, #10231, #10256, #10242, #10238, #10237, #10229, #10233, #10211, @BenWilson2; #10375, @serena-ruan; #10330, @Haxatron; #10342, #10249, #10249, @B-Step62; #10355, #10301, #10286, #10257, #10236, #10270, #10236, @prithvikannan; #10321, #10258, @jerrylian-db; #10245, @jessechancy; #10278, @daniellok-db; #10244, @gabrielfu; #10226, @milinddethe15; #10390, @bbqiu; #10232, @sunishsheth2009

## 2.8.0 (2023-10-28)

MLflow 2.8.0 includes several notable new features and improvements

- The MLflow Evaluate API has had extensive feature development in this release to support LLM workflows and multiple new evaluation modalities. See the new documentation, guides, and tutorials for MLflow LLM Evaluate to learn more.
- The MLflow Docs modernization effort has started. You will see a very different look and feel to the docs when visiting them, along with a batch of new tutorials and guides. More changes will be coming soon to the docs!
- 4 new LLM providers have been added! Google PaLM 2, AWS Bedrock, AI21 Labs, and HuggingFace TGI can now be configured and used within the AI Gateway. Learn more in the new AI Gateway docs!

Features:

- [Gateway] Add support for AWS Bedrock as a provider in the AI Gateway (#9598, @andrew-christianson)
- [Gateway] Add support for Huggingface Text Generation Inference as a provider in the AI Gateway (#10072, @SDonkelaarGDD)
- [Gateway] Add support for Google PaLM 2 as a provider in the AI Gateway (#9797, @arpitjasa-db)
- [Gateway] Add support for AI21labs as a provider in the AI Gateway (#9828, #10168, @zhe-db)
- [Gateway] Introduce a simplified method for setting the configuration file location for the AI Gateway via environment variable (#9822, @danilopeixoto)
- [Evaluate] Introduce default provided LLM evaluation metrics for MLflow evaluate (#9913, @prithvikannan)
- [Evaluate] Add support for evaluating inference datasets in MLflow evaluate (#9830, @liangz1)
- [Evaluate] Add support for evaluating single argument functions in MLflow evaluate (#9718, @liangz1)
- [Evaluate] Add support for Retriever LLM model type evaluation within MLflow evaluate (#10079, @liangz1)
- [Models] Add configurable parameter for external model saving in the ONNX flavor to address a regression (#10152, @daniellok-db)
- [Models] Add support for saving inference parameters in a logged model's input example (#9655, @serena-ruan)
- [Models] Add support for `completions` in the OpenAI flavor (#9838, @santiagxf)
- [Models] Add support for inference parameters for the OpenAI flavor (#9909, @santiagxf)
- [Models] Introduce support for configuration arguments to be specified when loading a model (#9251, @santiagxf)
- [Models] Add support for integrated Azure AD authentication for the OpenAI flavor (#9704, @santiagxf)
- [Models / Scoring] Introduce support for model training lineage in model serving (#9402, @M4nouel)
- [Model Registry] Introduce the `copy_model_version` client API for copying model versions across registered models (#9946, #10078, #10140, @jerrylian-db)
- [Tracking] Expand the limits of parameter value length from 500 to 6000 (#9709, @serena-ruan)
- [Tracking] Introduce support for Spark 3.5's SparkConnect mode within MLflow to allow logging models created using this operation mode of Spark (#9534, @WeichenXu123)
- [Tracking] Add support for logging system metrics to the MLflow fluent API (#9557, #9712, #9714, @chenmoneygithub)
- [Tracking] Add callbacks within MLflow for Keras and Tensorflow (#9454, #9637, #9579, @chenmoneygithub)
- [Tracking] Introduce a fluent login API for Databricks within MLflow (#9665, #10180, @chenmoneygithub)
- [Tracking] Add support for customizing auth for http requests from the MLflow client via a plugin extension (#10049, @lu-ohai)
- [Tracking] Introduce experimental asynchronous logging support for metrics, params, and tags (#9705, @sagarsumant)
- [Auth] Modify the behavior of user creation in MLflow Authentication so that only admins can create new users (#9700, @gabrielfu)
- [Artifacts] Add support for using `xethub` as an artifact store via a plugin extension (#9957, @Kelton8Z)
- [UI] Add new opt-in Model Registry UI that supports model aliases and tags (#10163, @hubertzub-db, @jerrylian-db)

Bug fixes:

- [Evaluate] Fix a bug with Azure OpenAI configuration usage within MLflow evaluate (#9982, @sunishsheth2009)
- [Models] Fix a data consistency issue when saving models that have been loaded in heterogeneous memory configuration within the transformers flavor (#10087, @BenWilson2)
- [Models] Fix an issue in the transformers flavor for complex input types by adding dynamic dataframe typing (#9044, @wamartin-aml)
- [Models] Fix an issue in the langchain flavor to provide support for chains with multiple outputs (#9497, @bbqiu)
- [Docker] Fix an issue with Docker image generation by changing the default env-manager to virtualenv (#9938, @Beramos)
- [Auth] Fix an issue with complex passwords in MLflow Auth to support a richer character set range (#9760, @dotdothu)
- [R] Fix a bug with configuration access when running MLflow R in Databricks (#10117, @zacdav-db)

Documentation updates:

- [Docs] Introduce the first phase of a larger documentation overhaul (#10197, @BenWilson2)
- [Docs] Add guide for LLM eval (#10058, #10199, @chenmoneygithub)
- [Docs] Add instructions on how to force single file serialization within the onnx flavor's save and log functions (#10178, @BenWilson2)
- [Docs] Add documentation for the relevance metric for MLflow evaluate (#10170, @sunishsheth2009)
- [Docs] Add a style guide for the contributing guide for how to structure pydoc strings (#9907, @mberk06)
- [Docs] Fix issues with the pytorch lightning autolog code example (#9964, @chenmoneygithub)
- [Docs] Update the example for `mlflow.data.from_numpy()` (#9885, @chenmoneygithub)
- [Docs] Add clear instructions for installing MLflow within R (#9835, @darshan8850)
- [Docs] Update model registry documentation to add content regarding support for model aliases (#9721, @jerrylian-db)

Small bug fixes and documentation updates:

#10202, #10189, #10188, #10159, #10175, #10165, #10154, #10083, #10082, #10081, #10071, #10077, #10070, #10053, #10057, #10055, #10020, #9928, #9929, #9944, #9979, #9923, #9842, @annzhang-db; #10203, #10196, #10172, #10176, #10145, #10115, #10107, #10054, #10056, #10018, #9976, #9999, #9998, #9995, #9978, #9973, #9975, #9972, #9974, #9960, #9925, #9920, @prithvikannan; #10144, #10166, #10143, #10129, #10059, #10123, #9555, #9619, @bbqiu; #10187, #10191, #10181, #10179, #10151, #10148, #10126, #10119, #10099, #10100, #10097, #10089, #10096, #10091, #10085, #10068, #10065, #10064, #10060, #10023, #10030, #10028, #10022, #10007, #10006, #9988, #9961, #9963, #9954, #9953, #9937, #9932, #9931, #9910, #9901, #9852, #9851, #9848, #9847, #9841, #9844, #9825, #9820, #9806, #9802, #9800, #9799, #9790, #9787, #9791, #9788, #9785, #9786, #9784, #9754, #9768, #9770, #9753, #9697, #9749, #9747, #9748, #9751, #9750, #9729, #9745, #9735, #9728, #9725, #9716, #9694, #9681, #9666, #9643, #9641, #9621, #9607, @harupy; #10200, #10201, #10142, #10139, #10133, #10090, #10086, #9934, #9933, #9845, #9831, #9794, #9692, #9627, #9626, @chenmoneygithub; #10110, @wenfeiy-db; #10195, #9895, #9880, #9679, @BenWilson2; #10174, #10177, #10109, #9706, @jerrylian-db; #10113, #9765, @smurching; #10150, #10138, #10136, @dbczumar; #10153, #10032, #9986, #9874, #9727, #9707, @serena-ruan; #10155, @shaotong-db; #10160, #10131, #10048, #10024, #10017, #10016, #10002, #9966, #9924, @sunishsheth2009; #10121, #10116, #10114, #10102, #10098, @B-Step62; #10095, #10026, #9991, @daniellok-db; #10050, @Dennis40816; #10062, #9868, @Gekko0114; #10033, @Anushka-Bhowmick; #9983, #10004, #9958, #9926, #9690, @liangz1; #9997, #9940, #9922, #9919, #9890, #9888, #9889, #9810, @TomeHirata; #9994, #9970, #9950, @lightnessofbein; #9965, #9677, @ShorthillsAI; #9906, @jessechancy; #9942, #9771, @Sai-Suraj-27; #9902, @remyleone; #9892, #9865, #9866, #9853, @montanarograziano; #9875, @Raghavan-B; #9858, @Salz0; #9878, @maksboyarin; #9882, @lukasz-gawron; #9827, @Bncer; #9819, @gabrielfu; #9792, @harshk461; #9726, @Chiragasourabh; #9663, @Abhishek-TyRnT; #9670, @mberk06; #9755, @simonlsk; #9757, #9775, #9776, #9774, @AmirAflak; #9782, @garymm; #9756, @issamarabi; #9645, @shichengzhou-db; #9671, @zhe-db; #9660, @mingyu89; #9575, @akshaya-a; #9629, @pnacht; #9876, @C-K-Loan

## 2.7.1 (2023-09-17)

MLflow 2.7.1 is a patch release containing the following features, bug fixes and changes:

Features:

- [Gateway / Databricks] Add the `set_limits` and `get_limits` APIs for AI Gateway routes within Databricks (#9516, @zhe-db)
- [Artifacts / Databricks] Add support for parallelized download and upload of artifacts within Unity Catalog (#9498, @jerrylian-db)

Bug fixes:

- [Models / R] Fix a critical bug with the `R` client that prevents models from being loaded (#9624, @BenWilson2)
- [Artifacts / Databricks] Disable multi-part download functionality for UC Volumes local file destination when downloading models (#9631, @BenWilson2)

Small bug fixes and documentation updates:

#9640, @annzhang-db; #9622, @harupy

## 2.7.0 (2023-09-12)

MLflow 2.7.0 includes several major features and improvements

- [UI / Gateway] We are excited to announce the Prompt Engineering UI. This new addition offers a suite of tools tailored for efficient prompt development, testing, and evaluation for LLM use cases. Integrated directly into the MLflow AI Gateway, it provides a seamless experience for designing, tracking, and deploying prompt templates. To read about this new feature, see the documentation at https://mlflow.org/docs/latest/llms/prompt-engineering.html (#9503, @prithvikannan)

Features:

- [Gateway] Introduce `MosaicML` as a supported provider for the MLflow `AI Gateway` (#9459, @arpitjasa-db)
- [Models] Add support for using a snapshot download location when loading a `transformers` model as `pyfunc` (#9362, @serena-ruan)
- [Server-infra] Introduce plugin support for MLflow `Tracking Server` authentication (#9191, @barrywhart)
- [Artifacts / Model Registry] Add support for storing artifacts using the `R2` backend (#9490, @shichengzhou-db)
- [Artifacts] Improve upload and download performance for Azure-based artifact stores (#9444, @jerrylian-db)
- [Sagemaker] Add support for deploying models to Sagemaker Serverless inference endpoints (#9085, @dogeplusplus)

Bug fixes:

- [Gateway] Fix a credential expiration bug by re-resolving `AI Gateway` credentials before each request (#9518, @dbczumar)
- [Gateway] Fix a bug where `search_routes` would raise an exception when no routes have been defined on the `AI Gateway` server (#9387, @QuentinAmbard)
- [Gateway] Fix compatibility issues with `pydantic` 2.x for `AI gateway` (#9339, @harupy)
- [Gateway] Fix an initialization issue in the `AI Gateway` that could render MLflow nonfunctional at import if dependencies were conflicting. (#9337, @BenWilson2)
- [Artifacts] Fix a correctness issue when downloading large artifacts to `fuse mount` paths on `Databricks` (#9545, @BenWilson2)

Documentation updates:

- [Docs] Add documentation for the `Giskard` community plugin for `mlflow.evaluate` (#9183, @rabah-khalek)

Small bug fixes and documentation updates:

#9605, #9603, #9602, #9595, #9597, #9587, #9590, #9588, #9586, #9584, #9583, #9582, #9581, #9580, #9577, #9546, #9566, #9569, #9562, #9564, #9561, #9528, #9506, #9503, #9492, #9491, #9485, #9445, #9430, #9429, #9427, #9426, #9424, #9421, #9419, #9409, #9408, #9407, #9394, #9389, #9395, #9393, #9390, #9370, #9356, #9359, #9357, #9345, #9340, #9328, #9329, #9326, #9304, #9325, #9323, #9322, #9319, #9314, @harupy; #9568, #9520, @dbczumar; #9593, @jerrylian-db; #9574, #9573, #9480, #9332, #9335, @BenWilson2; #9556, @shichengzhou-db; #9570, #9540, #9533, #9517, #9354, #9453, #9338, @prithvikannan; #9565, #9560, #9536, #9504, #9476, #9481, #9450, #9466, #9418, #9397, @serena-ruan; #9489, @dnerini; #9512, #9479, #9355, #9351, #9289 @chenmoneygithub; #9488, @bbqiu; #9474, @apurva-koti; #9505, @arpitjasa-db; #9261, @donour; #9336, #9414, #9353, @mberk06; #9451, @Bncer; #9432, @barrywhart; #9347, @GraceBrigham; #9428, #9420, #9406, @WeichenXu123; #9410, @aloahPGF; #9396, #9384, #9372, @Godwin-T; #9373, @fabiansefranek; #9382, @Sai-Suraj-27; #9378, @saidattu2003; #9375, @Increshi; #9358, @smurching; #9366, #9330, @Dev-98; #9364, @Sandeep1005; #9349, #9348, @AmirAflak; #9308, @danilopeixoto; #9596, @ShorthillsAI; #9567, @Beramos; #9524, @rabah-khalek; #9312, @dependabot[bot]

## 2.6.0 (2023-08-15)

MLflow 2.6.0 includes several major features and improvements

Features:

- [Models / Scoring] Add support for passing extra params during inference for PyFunc models (#9068, @serena-ruan)
- [Gateway] Add support for MLflow serving to MLflow AI Gateway (#9199, @BenWilson2)
- [Tracking] Support `save_kwargs` for `mlflow.log_figure` to specify extra options when saving a figure (#9179, @stroblme)
- [Artifacts] Display progress bars when uploading/download artifacts (#9195, @serena-ruan)
- [Models] Add support for logging LangChain's retriever models (#8808, @liangz1)
- [Tracking] Add support to log customized tags to runs created by autologging (#9114, @thinkall)

Bug fixes:

- [Models] Fix `text_pair` functionality for transformers `TextClassification` pipelines (#9215, @BenWilson2)
- [Models] Fix LangChain compatibility with SQLDatabase (#9192, @dbczumar)
- [Tracking] Remove patching `sklearn.metrics.get_scorer_names` in `mlflow.sklearn.autolog` to avoid duplicate logging (#9095, @WeichenXu123)

Documentation updates:

- [Docs / Examples] Add examples and documentation for MLflow AI Gateway support for MLflow model serving (#9281, @BenWilson2)
- [Docs / Examples] Add `sentence-transformers` doc & example (#9047, @es94129)

Deprecation:

- [Models] The `mlflow.mleap` module has been marked as deprecated and will be removed in a future release (#9311, @BenWilson2)

Small bug fixes and documentation updates:

#9309, #9252, #9198, #9189, #9186, #9184, @BenWilson2; #9307, @AmirAflak; #9285, #9126, @dependabot[bot]; #9302, #9209, #9194, #9187, #9175, #9177, #9163, #9161, #9129, #9123, #9053, @serena-ruan; #9305, #9303, #9271, @KekmaTime; #9300, #9299, @itsajay1029; #9294, #9293, #9274, #9268, #9264, #9246, #9255, #9253, #9254, #9245, #9202, #9243, #9238, #9234, #9233, #9227, #9226, #9223, #9224, #9222, #9225, #9220, #9208, #9212, #9207, #9203, #9201, #9200, #9154, #9146, #9147, #9153, #9148, #9145, #9136, #9132, #9131, #9128, #9121, #9124, #9125, #9108, #9103, #9100, #9098, #9101, @harupy; #9292, @Aman123lug; #9290, #9164, #9157, #9086, @Bncer; #9291, @kunal642; #9284, @NavneetSinghArora; #9286, #9262, #9142, @smurching; #9267, @tungbq; #9258, #9250, @Kunj125; #9167, #9139, #9120, #9118, #9097, @viktoriussuwandi; #9244, #9240, #9239, @Sai-Suraj-27; #9221, #9168, #9130, @gabrielfu; #9218, @tjni; #9216, @Rukiyav; #9158, #9051, @EdAbati; #9211, @scarlettrobe; #9049, @annzhang-db; #9140, @kriscon-db; #9141, @xAIdrian; #9135, @liangz1; #9067, @jmmonteiro; #9112, @WeichenXu123; #9106, @shaikmoeed; #9105, @Ankit8848; #9104, @arnabrahman

## 2.5.0 (2023-07-17)

MLflow 2.5.0 includes several major features and improvements:

- [MLflow AI Gateway] We are excited to announce the release of MLflow AI Gateway, a powerful tool designed to streamline the usage and management of various large language model (LLM) providers, such as OpenAI and Anthropic, within an organization. It offers a standardized interface that simplifies the interaction with these services and delivers centralized, secure management of credentials. To get started with MLflow AI Gateway, check out the docs at https://mlflow.org/docs/latest/gateway/index.html. (#8694, @harupy, @BenWilson2, @dbczumar)
- [Auth]: We are excited to announce the release of authentication and authorization support for MLflow Tracking and the MLflow Model Registry, providing integrated access control capabilities to both services. To get started, check out the docs at https://mlflow.org/docs/latest/auth/index.html. (#9000, #8975, #8626, #8837, #8841, @gabrielfu, @harupy)

Features:

- [Models] Add Support to the LangChain flavor for chains that contain unserializable components (#8736, @liangz1)
- [Scoring] Infer spark udf return type from model output schema (#8934, @WeichenXu123)
- [Models] Add support for automated signature inference (#8860, #8782 #8795, #8725, @jerrylian-db)

Bug fixes:

- [Security] Improve robustness to LFI attacks on Windows by enhancing path validation (#8999, @serena-ruan)
  - If you are using `mlflow server` or `mlflow ui` on Windows, we recommend upgrading to MLflow 2.5.0 as soon as possible.
- [Scoring] Support nullable array type values as spark_udf return values (#9014, @WeichenXu123)
- [Models] Revert cache deletion of system modules when adding custom model code to the system path (#8722, @trungn1)
- [Models] add micro version to mlflow version pinning (#8687, @C-K-Loan)
- [Artifacts] Prevent manually deleted artifacts from causing artifact garbage collection to fail (#8498, @PenHsuanWang)

Documentation updates:

- [Docs] Update .push_model_to_sagemaker docs (#8851, @pdifranc)
- [Docs] Fix invalid link for Azure ML documentation (#8800, @dunnkers)
- [Artifacts / Docs / Models / Projects] Adds information on the OCI MLflow plugins for seamless integration with Oralce Cloud Infrastructure services. (#8707, @mrDzurb)

Deprecation:

- [Models] Deprecate the `gluon` model flavor. The `mlflow.gluon` module will be removed in a future release. (#8968, @harupy)

Small bug fixes and documentation updates:

#9069, #9056, #9055, #9054, #9048, #9043, #9035, #9034, #9037, #9038, #8993, #8966, #8985, @BenWilson2; #9039, #9036, #8902, #8924, #8866, #8861, #8810, #8761, #8544, @jerrylian-db; #8903, @smurching; #9080, #9079, #9078, #9076, #9075, #9074, #9071, #9063, #9062, #9032, #9031, #9027, #9023, #9022, #9020, #9005, #8994, #8979, #8983, #8984, #8982, #8970, #8962, #8969, #8968, #8959, #8960, #8958, #8956, #8955, #8954, #8949, #8950, #8952, #8948, #8946, #8947, #8943, #8944, #8916, #8917, #8933, #8929, #8932, #8927, #8930, #8925, #8921, #8873, #8915, #8909, #8908, #8911, #8910, #8907, #8906, #8898, #8893, #8889, #8892, #8891, #8887, #8875, #8876, #8882, #8874, #8868, #8872, #8869, #8828, #8852, #8857, #8853, #8854, #8848, #8850, #8840, #8835, #8832, #8831, #8830, #8829, #8839, #8833, #8838, #8819, #8814, #8825, #8818, #8787, #8775, #8749, #8766, #8756, #8753, #8751, #8748, #8744, #8731, #8717, #8730, #8691, #8720, #8723, #8719, #8688, #8721, #8715, #8716, #8718, #8696, #8698, #8692, #8693, #8690, @harupy; #9030, @AlimurtuzaCodes; #9029, #9025, #9021, #9013, @viktoriussuwandi; #9010, @Bncer; #9011, @Pecunia201; #9007, #9003, @EdAbati; #9002, @prithvikannan; #8991, #8867, @AveshCSingh; #8951, #8896, #8888, #8849, @gabrielfu; #8913, #8885, #8871, #8870, #8788, #8772, #8771, @serena-ruan; #8879, @maciejskorski; #7752, @arunkumarkota; #9083, #9081, #8765, #8742, #8685, #8682, #8683, @dbczumar; #8791, @mhattingpete; #8739, @yunpark93

## 2.4.2 (2023-07-10)

MLflow 2.4.2 is a patch release containing the following bug fixes and changes:

Bug fixes:

- [Models] Add compatibility for legacy transformers serialization (#8964, @BenWilson2)
- [Models] Fix downloading MLmodel files from alias-based models:/ URIs (#8764, @smurching)
- [Models] Fix reading model flavor config from URI for models in UC (#8728, @smurching)
- [Models] Support `feature_deps` in ModelVersion creation for UC (#8867, #8815, @AveshCSingh)
- [Models] Add support for listing artifacts in UC model registry artifact repo (#8803, @smurching)
- [Core] Include resources for recipes in mlflow-skinny (#8895, @harupy)
- [UI] Enable datasets tracking UI (#8886, @harupy)
- [Artifacts] Use `MLFLOW_ENABLE_MULTIPART_DOWNLOAD` in `DatabricksArtifactRepository` (#8884, @harupy)

Documentation updates:

- [Examples / Docs] Add question-answering and summarization examples and docs with LLMs (#8695, @dbczumar)
- [Examples / Docs] Add johnsnowlabs flavor example and doc (#8689, @C-K-Loan)

Small bug fixes and documentation updates:

#8966, @BenWilson2; #8881, @harupy; #8846, #8760, @smurching

## 2.4.1 (2023-06-09)

MLflow 2.4.1 is a patch release containing the following features, bug fixes and changes:

Features:

- [Tracking] Extend SearchRuns to support datasets (#8622, @prithvikannan)
- [Models] Add an `mlflow.johnsnowlabs` flavor for the `johnsnowlabs` package (#8556, @C-K-Loan)
- [Models] Add a warning for duplicate pip requirements specified in `save_model` and `log_model` for the `transformers` flavor (#8678, @BenWilson2)

Bug fixes:

- [Security] Improve robustness to LFI attacks (#8648, @serena-ruan)
  - If you are using `mlflow server` or `mlflow ui`, we recommend upgrading to MLflow 2.4.1 as soon as possible.
- [Models] Fix an issue with `transformers` serialization for ModelCards that contain invalid characters (#8652, @BenWilson2)
- [Models] Fix connection pooling deadlocks that occurred during large file downloads (#8682, @dbczumar; #8660, @harupy)

Small bug fixes and documentation updates:

#8677, #8674, #8646, #8647, @dbczumar; #8654, #8653, #8660, #8650, #8642, #8636, #8599, #8637, #8608, #8633, #8623, #8628, #8619, @harupy; #8655, #8609, @BenWilson2; #8648, @serena-ruan; #8521, @ka1mar; #8638, @smurching; #8634, @PenHsuanWang

## 2.4.0 (2023-06-06)

MLflow 2.4.0 includes several major features and improvements

Features:

- [Tracking] Introduce dataset tracking APIs: `mlflow.data` and `mlflow.log_input()` (#8186, @prithvikannan)
- [Tracking] Add `mlflow.log_table()` and `mlflow.load_table()` APIs for logging evaluation tables (#8523, #8467, @sunishsheth2009)
- [Tracking] Introduce `mlflow.get_parent_run()` fluent API (#8493, @annzhang-db)
- [Tracking / Model Registry] Re-introduce faster artifact downloads on Databricks (#8352, @dbczumar; #8561, @harupy)
- [UI] Add dataset tracking information to MLflow Tracking UI (#8602, @prithvikannan, @hubertzub-db)
- [UI] Introduce Artifact View for comparing inputs, outputs, and metadata across models (#8602, @hubertzub-db)
- [Models] Extend `mlflow.evaluate()` to support LLM tasks (#8484, @harupy)
- [Models] Support logging subclasses of `Chain` and `LLMChain` in `mlflow.langchain` flavor (#8453, @liangz1)
- [Models] Add support for LangChain Agents to the `mlflow.langchain` flavor (#8297, @sunishsheth2009)
- [Models] Add a `mlflow.sentence_transformers` flavor for SentenceTransformers (#8479, @BenWilson2; #8547, @Loquats)
- [Models] Add support for multi-GPU inference and efficient weight loading for `mlflow.transformers` flavor (#8448, @ankit-db)
- [Models] Support the `max_shard_size` parameter in the `mlflow.transformers` flavor (#8567, @wenfeiy-db)
- [Models] Add support for audio transcription pipelines in the `mlflow.transformers` flavor (#8464, @BenWilson2)
- [Models] Add support for audio classification to `mlflow.transformers` flavor (#8492, @BenWilson2)
- [Models] Add support for URI inputs in audio models logged with the `mlflow.transformers` flavor (#8495, @BenWilson2)
- [Models] Add support for returning classifier scores in `mlflow.transformers` pyfunc outputs (#8512, @BenWilson2)
- [Models] Support optional inputs in model signatures (#8438, @apurva-koti)
- [Models] Introduce an `mlflow.models.set_signature()` API to set the signature of a logged model (#8476, @jerrylian-db)
- [Models] Persist ONNX Runtime InferenceSession options when logging a model with `mlflow.onnx.log_model()` (#8433, @leqiao-1)

Bug fixes:

- [Tracking] Terminate Spark callback server when Spark Autologging is disabled or Spark Session is shut down (#8508, @WeichenXu123)
- [Tracking] Fix compatibility of `mlflow server` with `Flask<2.0` (#8463, @kevingreer)
- [Models] Convert `mlflow.transformers` pyfunc scalar string output to list of strings during batch inference (#8546, @BenWilson2)
- [Models] Fix a bug causing outdated pyenv versions to be installed by `mlflow models build-docker` (#8488, @Hellzed)
- [Model Registry] Remove aliases from storage when a Model Version is deleted (#8459, @arpitjasa-db)

Documentation updates:

- [Docs] Publish a new MLOps Quickstart for model selection and deployment (#8462, @lobrien)
- [Docs] Add MLflavors library to Community Model Flavors documentation (#8420, @benjaminbluhm)
- [Docs] Add documentation for Registered Model Aliases (#8445, @arpitjasa-db)
- [Docs] Fix errors in documented `mlflow models` CLI command examples (#8480, @vijethmoudgalya)

Small bug fixes and documentation updates:

#8611, #8587, @dbczumar; #8617, #8620, #8615, #8603, #8604, #8601, #8596, #8598, #8597, #8589, #8580, #8581, #8575, #8582, #8577, #8576, #8578, #8561, #8568, #8551, #8528, #8550, #8489, #8530, #8534, #8533, #8532, #8524, #8520, #8517, #8516, #8515, #8514, #8506, #8503, #8500, #8504, #8496, #8486, #8485, #8468, #8471, #8473, #8470, #8458, #8447, #8446, #8434, @harupy; #8607, #8538, #8513, #8452, #8466, #8465, @serena-ruan; #8586, #8595, @prithvikannan; #8593, #8541, @kriscon-db; #8592, #8566, @annzhang-db; #8588, #8565, #8559, #8537, @BenWilson2; #8545, @apurva-koti; #8564, @DavidSpek; #8436, #8490, @jerrylian-db; #8505, @eliaskoromilas; #8483, @WeichenXu123; #8472, @leqiao-1; #8429, @jinzhang21; #8581, #8548, #8499, @gabrielfu;

## 2.3.2 (2023-05-12)

MLflow 2.3.2 is a patch release containing the following features, bug fixes and changes:

Features:

- [Models] Add GPU support for `transformers` models `pyfunc` inference and serving (#8375, @ankit-db)
- [Models] Disable autologging functionality for non-relevant models when training a `transformers` model (#8405, @BenWilson2)
- [Models] Add support for preserving and overriding `torch_dtype` values in `transformers` pipelines (#8421, @BenWilson2)
- [Models] Add support for `Feature Extraction` pipelines in the `transformers` flavor (#8423, @BenWilson2)
- [Tracking] Add basic HTTP auth support for users, registered models, and experiments permissions (#8286, @gabrielfu)

Bug Fixes:

- [Models] Fix inferred schema issue with `Text2TextGeneration` pipelines in the `transformers` flavor (#8391, @BenWilson2)
- [Models] Change MLflow dependency pinning in logged models from a range value to an exact major and minor version (#8422, @harupy)

Documentation updates:

- [Examples] Add `signature` logging to all examples and documentation (#8410, #8401, #8400, #8387 @jerrylian-db)
- [Examples] Add `sentence-transformers` examples to the `transformers` examples suite (#8425, @BenWilson2)
- [Docs] Add a new MLflow Quickstart documentation page (#8171, @lobrien)
- [Docs] Add a new introduction to MLflow page (#8365, @lobrien)
- [Docs] Add a community model plugin example and documentation for `trubrics` (#8371, @jeffkayne)
- [Docs] Add `gluon` pyfunc example to Model flavor documentation (#8403, @ericvincent18)
- [Docs] Add `statsmodels` pyfunc example to `Models` flavor documentation (#8394, @ericvincent18)

Small bug fixes and documentation updates:

#8415, #8412, #8411, #8355, #8354, #8353, #8348, @harupy; #8374, #8367, #8350, @dbczumar; #8358 @mrkaye97; #8392, #8362, @smurching; #8427, #8408, #8399, #8381, @BenWilson2; #8395, #8390, @jerrylian-db; #8402, #8398, @WeichenXu123; #8377, #8363, @arpitjasa-db; #8385, @prithvikannan; #8418, @Jeukoh;

## 2.3.1 (2023-04-27)

MLflow 2.3.1 is a patch release containing the following bug fixes and changes:

Bug fixes:

- [Security] Fix critical LFI attack vulnerability by disabling the ability to provide relative paths in registered model sources (#8281, @BenWilson2)
  - **If you are using `mlflow server` or `mlflow ui`, we recommend upgrading to MLflow 2.3.1 as soon as possible.** For more details, see https://github.com/mlflow/mlflow/security/advisories/GHSA-xg73-94fp-g449.
- [Tracking] Fix an issue causing file and model uploads to hang on Databricks (#8348, @harupy)
- [Tracking / Model Registry] Fix an issue causing file and model downloads to hang on Databricks (#8350, @dbczumar)
- [Scoring] Fix regression in schema enforcement for model serving when using the `inputs` format for inference (#8326, @BenWilson2)
- [Model Registry] Fix regression in model naming parsing where special characters were not accepted in model names (#8322, @arpitjasa-db)
- [Recipes] Fix card rendering with the pandas profiler to handle columns containing all null values (#8263, @sunishsheth2009)

Documentation updates:

- [Docs] Add an H2O pyfunc usage example to the models documentation (#8292, @ericvincent18)
- [Examples] Add a TensorFlow Core 2.x API usage example (#8235, @dheerajnbhat)

Small bug fixes and documentation updates:

#8324, #8325, @smurching; #8313, @dipanjank; #8323, @liangz1; #8331, #8328, #8319, #8316, #8308, #8293, #8289, #8283, #8284, #8285, #8282, #8241, #8270, #8272, #8271, #8268, @harupy; #8312, #8294, #8295, #8279, #8267, @BenWilson2; #8290, @jinzhang21; #8257, @WeichenXu123; #8307, @arpitjasa-db

## 2.3.0 (2023-04-18)

MLflow 2.3.0 includes several major features and improvements

Features:

- [Models] Introduce a new `transformers` named flavor (#8236, #8181, #8086, @BenWilson2)
- [Models] Introduce a new `openai` named flavor (#8191, #8155, @harupy)
- [Models] Introduce a new `langchain` named flavor (#8251, #8197, @liangz1, @sunishsheth2009)
- [Models] Add support for `Pytorch` and `Lightning` 2.0 (#8072, @shrinath-suresh)
- [Tracking] Add support for logging LLM input, output, and prompt artifacts (#8234, #8204, @sunishsheth2009)
- [Tracking] Add support for HTTP Basic Auth in the MLflow tracking server (#8130, @gabrielfu)
- [Tracking] Add `search_model_versions` to the fluent API (#8223, @mariusschlegel)
- [Artifacts] Add support for parallelized artifact downloads (#8116, @apurva-koti)
- [Artifacts] Add support for parallelized artifact uploads for AWS (#8003, @harupy)
- [Artifacts] Add content type headers to artifact upload requests for the `HttpArtifactRepository` (#8048, @WillEngler)
- [Model Registry] Add alias support for logged models within Model Registry (#8164, #8094, #8055 @arpitjasa-db)
- [UI] Add support for custom domain git providers (#7933, @gusghrlrl101)
- [Scoring] Add plugin support for customization of MLflow serving endpoints (#7757, @jmahlik)
- [Scoring] Add support to MLflow serving that allows configuration of multiple inference workers (#8035, @M4nouel)
- [Sagemaker] Add support for asynchronous inference configuration on Sagemaker (#8009, @thomasbell1985)
- [Build] Remove `shap` as a core dependency of MLflow (#8199, @jmahlik)

Bug fixes:

- [Models] Fix a bug with `tensorflow` autologging for models with multiple inputs (#8097, @jaume-ferrarons)
- [Recipes] Fix a bug with `Pandas` 2.0 updates for profiler rendering of datetime types (#7925, @sunishsheth2009)
- [Tracking] Prevent exceptions from being raised if a parameter is logged with an existing key whose value is identical to the logged parameter (#8038, @AdamStelmaszczyk)
- [Tracking] Fix an issue with deleting experiments in the FileStore backend (#8178, @mariusschlegel)
- [Tracking] Fix a UI bug where the "Source Run" field in the Model Version page points to an incorrect set of artifacts (#8156, @WeichenXu123)
- [Tracking] Fix a bug wherein renaming a run reverts its current lifecycle status to `UNFINISHED` (#8154, @WeichenXu123)
- [Tracking] Fix a bug where a file URI could be used as a model version source (#8126, @harupy)
- [Projects] Fix an issue with MLflow projects that have submodules contained within a project (#8050, @kota-iizuka)
- [Examples] Fix `lightning` hyperparameter tuning examples (#8039, @BenWilson2)
- [Server-infra] Fix bug with Cache-Control headers for static server files (#8016, @jmahlik)

Documentation updates:

- [Examples] Add a new and thorough example for the creation of custom model flavors (#7867, @benjaminbluhm)

Small bug fixes and documentation updates:

#8262, #8252, #8250, #8228, #8221, #8203, #8134, #8040, #7994, #7934, @BenWilson2; #8258, #8255, #8253, #8248, #8247, #8245, #8243, #8246, #8244, #8242, #8240, #8229, #8198, #8192, #8112, #8165, #8158, #8152, #8148, #8144, #8143, #8120, #8107, #8105, #8102, #8088, #8089, #8096, #8075, #8073, #8076, #8063, #8064, #8033, #8024, #8023, #8021, #8015, #8005, #7982, #8002, #7987, #7981, #7968, #7931, #7930, #7929, #7917, #7918, #7916, #7914, #7913, @harupy; #7955, @arjundc-db; #8219, #8110, #8093, #8087, #8091, #8092, #8029, #8028, #8031, @jerrylian-db; #8187, @apurva-koti; #8210, #8001, #8000, @arpitjasa-db; #8161, #8127, #8095, #8090, #8068, #8043, #7940, #7924, #7923, @dbczumar; #8147, @morelen17; #8106, @WeichenXu123; #8117, @eltociear; #8100, @laerciop; #8080, @elado; #8070, @grofte; #8066, @yukimori; #8027, #7998, @liangz1; #7999, @martlaf; #7964, @viditjain99; #7928, @alekseyolg; #7909, #7901, #7844, @smurching; #7971, @n30111; #8012, @mingyu89; #8137, @lobrien; #7992, @robmarkcole; #8263, @sunishsheth2009

## 2.2.2 (2023-03-14)

MLflow 2.2.2 is a patch release containing the following bug fixes:

- [Model Registry] Allow `source` to be a local path within a run's artifact directory if a `run_id` is specified (#7993, @harupy)
- [Model Registry] Fix a bug where a windows UNC path is considered a local path (#7988, @WeichenXu123)
- [Model Registry] Disallow `name` to be a file path in `FileStore.get_registered_model` (#7965, @harupy)

## 2.2.1 (2023-03-02)

MLflow 2.2.1 is a patch release containing the following bug fixes:

- [Model Registry] Fix a bug that caused too many results to be requested by default when calling `MlflowClient.search_model_versions()` (#7935, @dbczumar)
- [Model Registry] Patch for GHSA-xg73-94fp-g449 (#7908, @harupy)
- [Model Registry] Patch for GHSA-wp72-7hj9-5265 (#7965, @harupy)

## 2.2.0 (2023-02-28)

MLflow 2.2.0 includes several major features and improvements

Features:

- [Recipes] Add support for score calibration to the classification recipe (#7744, @sunishsheth2009)
- [Recipes] Add automatic label encoding to the classification recipe (#7711, @sunishsheth2009)
- [Recipes] Support custom data splitting logic in the classification and regression recipes (#7815, #7588, @sunishsheth2009)
- [Recipes] Introduce customizable MLflow Run name prefixes to the classification and regression recipes (#7746, @kamalesh0406; #7763, @sunishsheth2009)
- [UI] Add a new Chart View to the MLflow Experiment Page for model performance insights (#7864, @hubertzub-db, @apurva-koti, @prithvikannan, @ridhimag11, @sunishseth2009, @dbczumar)
- [UI] Modernize and improve parallel coordinates chart for model tuning (#7864, @hubertzub-db, @apurva-koti, @prithvikannan, @ridhimag11, @sunishseth2009, @dbczumar)
- [UI] Add typeahead suggestions to the MLflow Experiment Page search bar (#7864, @hubertzub-db, @apurva-koti, @prithvikannan, @ridhimag11, @sunishseth2009, @dbczumar)
- [UI] Improve performance of Experiments Sidebar for large numbers of experiments (#7804, @jmahlik)
- [Tracking] Introduce autologging support for native PyTorch models (#7627, @temporaer)
- [Tracking] Allow specifying `model_format` when autologging XGBoost models (#7781, @guyrosin)
- [Tracking] Add `MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT` environment variable to configure artifact operation timeouts (#7783, @wamartin-aml)
- [Artifacts] Include `Content-Type` response headers for artifacts downloaded from `mlflow server` (#7827, @bali0019)
- [Model Registry] Introduce the `searchModelVersions()` API to the Java client (#7880, @gabrielfu)
- [Model Registry] Introduce `max_results`, `order_by` and `page_token` arguments to `MlflowClient.search_model_versions()` (#7623, @serena-ruan)
- [Models] Support logging large ONNX models by using external data (#7808, @dogeplusplus)
- [Models] Add support for logging Diviner models fit in Spark (#7800, @BenWilson2)
- [Models] Introduce `MLFLOW_DEFAULT_PREDICTION_DEVICE` environment variable to set the device for pyfunc model inference (#7922, @ankit-db)
- [Scoring] Publish official Docker images for the MLflow Model scoring server at github.com/mlflow/mlflow/pkgs (#7759, @dbczumar)

Bug fixes:

- [Recipes] Fix dataset format validation in the ingest step for custom dataset sources (#7638, @sunishsheth2009)
- [Recipes] Fix bug in identification of worst performing examples during training (#7658, @sunishsheth2009)
- [Recipes] Ensure consistent rendering of the recipe graph when `inspect()` is called (#7852, @sunishsheth2009)
- [Recipes] Correctly respect `positive_class` configuration in the transform step (#7626, @sunishsheth2009)
- [Recipes] Make logged metric names consistent with `mlflow.evaluate()` (#7613, @sunishsheth2009)
- [Recipes] Add `run_id` and `artifact_path` keys to logged MLmodel files (#7651, @sunishsheth2009)
- [UI] Fix bugs in UI validation of experiment names, model names, and tag keys (#7818, @subramaniam02)
- [Tracking] Resolve artifact locations to absolute paths when creating experiments (#7670, @bali0019)
- [Tracking] Exclude Delta checkpoints from Spark datasource autologging (#7902, @harupy)
- [Tracking] Consistently return an empty list from GetMetricHistory when a metric does not exist (#7589, @bali0019; #7659, @harupy)
- [Artifacts] Fix support for artifact operations on Windows paths in UNC format (#7750, @bali0019)
- [Artifacts] Fix bug in HDFS artifact listing (#7581, @pwnywiz)
- [Model Registry] Disallow creation of model versions with local filesystem sources in `mlflow server` (#7908, @harupy)
- [Model Registry] Fix handling of deleted model versions in FileStore (#7716, @harupy)
- [Model Registry] Correctly initialize Model Registry SQL tables independently of MLflow Tracking (#7704, @harupy)
- [Models] Correctly move PyTorch model outputs from GPUs to CPUs during inference with pyfunc (#7885, @ankit-db)
- [Build] Fix compatiblility issues with Python installations compiled using `PYTHONOPTIMIZE=2` (#7791, @dbczumar)
- [Build] Fix compatibility issues with the upcoming pandas 2.0 release (#7899, @harupy; #7910, @dbczumar)

Documentation updates:

- [Docs] Add an example of saving and loading Spark MLlib models with MLflow (#7706, @dipanjank)
- [Docs] Add usage examples for `mlflow.lightgbm` APIs (#7565, @canerturkseven)
- [Docs] Add an example of custom model flavor creation with `sktime` (#7624, @benjaminbluhm)
- [Docs] Clarify `precision_recall_auc` metric calculation in `mlflow.evaluate()` (#7701, @BenWilson2)
- [Docs] Remove outdated example links (#7587, @asloan7)

Small bug fixes and documentation updates:

#7866, #7751, #7724, #7699, #7697, #7666, @alekseyolg; #7896, #7861, #7858, #7862, #7872, #7859, #7863, #7767, #7766, #7765, #7741, @smurching; #7895, #7877, @viditjain99; #7898, @midhun1998; #7891, #7892, #7886, #7882, #7883, #7875, #7874, #7871, #7868, #7854, #7847, #7845, #7838, #7830, #7837, #7836, #7834, #7831, #7828, #7825, #7826, #7824, #7823, #7778, #7780, #7776, #7775, #7773, #7772, #7769, #7756, #7768, #7764, #7685, #7726, #7722, #7720, #7423, #7712, #7710, #7713, #7688, #7663, #7674, #7673, #7672, #7662, #7653, #7646, #7615, #7614, #7586, #7601, #7598, #7602, #7599, #7577, #7585, #7583, #7584, @harupy; #7865, #7803, #7753, #7719, @dipanjank; #7796, @serena-ruan; #7849, @turbotimon; #7822, #7600, @WeichenXu123; #7811, @guyrosin; #7812, #7788, #7787, #7748, #7730, #7616, #7593, @dbczumar; #7793, @Joel-hanson; #7792, #7694, #7643, @BenWilson2; #7771, #7657, #7644, @nsenno-dbr; #7738, @wkrt7; #7740, @Ark-kun; #7739, #7733, @bali0019; #7723, @andrehp; #7691, #7582, @agoyot; #7721, @Eseeldur; #7709, @srowen; #7693, @ry3s; #7649, @funkypenguin; #7665, @benjaminbluhm; #7668, @eltociear; #7550, @danielhstahl; #7920, @arjundc-db

## 2.1.0 (2022-12-21)

MLflow 2.1.0 includes several major features and improvements

Features:

- [Recipes] Introduce support for multi-class classification (#7458, @mshtelma)
- [Recipes] Extend the pyfunc representation of classification models to output scores in addition to labels (#7474, @sunishsheth2009)
- [UI] Add user ID and lifecycle stage quick search links to the Runs page (#7462, @jaeday)
- [Tracking] Paginate the GetMetricHistory API (#7523, #7415, @BenWilson2)
- [Tracking] Add Runs search aliases for Run name and start time that correspond to UI column names (#7492, @apurva-koti)
- [Tracking] Add a `/version` endpoint to `mlflow server` for querying the server's MLflow version (#7273, @joncarter1)
- [Model Registry] Add FileStore support for the Model Registry (#6605, @serena-ruan)
- [Model Registry] Introduce an `mlflow.search_registered_models()` fluent API (#7428, @TSienki)
- [Model Registry / Java] Add a `getRegisteredModel()` method to the Java client (#6602) (#7511, @drod331)
- [Model Registry / R] Add an `mlflow_set_model_version_tag()` method to the R client (#7401, @leeweijie)
- [Models] Introduce a `metadata` field to the MLmodel specification and `log_model()` methods (#7237, @jdonzallaz)
- [Models] Extend `Model.load()` to support loading MLmodel specifications from remote locations (#7517, @dbczumar)
- [Models] Pin the major version of MLflow in Models' `requirements.txt` and `conda.yaml` files (#7364, @BenWilson2)
- [Scoring] Extend `mlflow.pyfunc.spark_udf()` to support StructType results (#7527, @WeichenXu123)
- [Scoring] Extend TensorFlow and Keras Models to support multi-dimensional inputs with `mlflow.pyfunc.spark_udf()`(#7531, #7291, @WeichenXu123)
- [Scoring] Support specifying deployment environment variables and tags when deploying models to SageMaker (#7433, @jhallard)

Bug fixes:

- [Recipes] Fix a bug that prevented use of custom `early_stop` functions during model tuning (#7538, @sunishsheth2009)
- [Recipes] Fix a bug in the logic used to create a Spark session during data ingestion (#7307, @WeichenXu123)
- [Tracking] Make the metric names produced by `mlflow.autolog()` consistent with `mlflow.evaluate()` (#7418, @wenfeiy-db)
- [Tracking] Fix an autologging bug that caused nested, redundant information to be logged for XGBoost and LightGBM models (#7404, @WeichenXu123)
- [Tracking] Correctly classify SQLAlchemy OperationalErrors as retryable HTTP errors (#7240, @barrywhart)
- [Artifacts] Correctly handle special characters in credentials when using FTP artifact storage (#7479, @HCTsai)
- [Models] Address an issue that prevented MLeap models from being saved on Windows (#6966, @dbczumar)
- [Scoring] Fix a permissions issue encountered when using NFS during model scoring with `mlflow.pyfunc.spark_udf()` (#7427, @WeichenXu123)

Documentation updates:

- [Docs] Add more examples to the Runs search documentation page (#7487, @apurva-koti)
- [Docs] Add documentation for Model flavors developed by the community (#7425, @mmerce)
- [Docs] Add an example for logging and scoring ONNX Models (#7398, @Rusteam)
- [Docs] Fix a typo in the model scoring REST API example for inputs with the `dataframe_split` format (#7540, @zhouyangyu)
- [Docs] Fix a typo in the model scoring REST API example for inputs with the `dataframe_records` format (#7361, @dbczumar)

Small bug fixes and documentation updates:

#7571, #7543, #7529, #7435, #7399, @WeichenXu123; #7568, @xiaoye-hua; #7549, #7557, #7509, #7498, #7499, #7485, #7486, #7484, #7391, #7388, #7390, #7381, #7366, #7348, #7346, #7334, #7340, #7323, @BenWilson2; #7561, #7562, #7560, #7553, #7546, #7539, #7544, #7542, #7541, #7533, #7507, #7470, #7469, #7467, #7466, #7464, #7453, #7449, #7450, #7440, #7430, #7436, #7429, #7426, #7410, #7406, #7409, #7407, #7405, #7396, #7393, #7395, #7384, #7376, #7379, #7375, #7354, #7353, #7351, #7352, #7350, #7345, #6493, #7343, #7344, @harupy; #7494, @dependabot[bot]; #7526, @tobycheese; #7489, @liangz1; #7534, @Jingnan-Jia; #7496, @danielhstahl; #7504, #7503, #7459, #7454, #7447, @tsugumi-sys; #7461, @wkrt7; #7451, #7414, #7372, #7289, @sunishsheth2009; #7441, @ikrizanic; #7432, @Pochingto; #7386, @jhallard; #7370, #7373, #7371, #7336, #7341, #7342, @dbczumar; #7335, @prithvikannan

## 2.0.1 (2022-11-14)

The 2.0.1 version of MLflow is a major milestone release that focuses on simplifying the management of end-to-end MLOps workflows, providing new feature-rich functionality, and expanding upon the production-ready MLOps capabilities offered by MLflow.
This release contains several important breaking changes from the 1.x API, additional major features and improvements.

Features:

- [Recipes] MLflow Pipelines is now MLflow Recipes - a framework that enables data scientists to quickly develop high-quality models and deploy them to production
- [Recipes] Add support for classification models to MLflow Recipes (#7082, @bbarnes52)
- [UI] Introduce support for pinning runs within the experiments UI (#7177, @harupy)
- [UI] Simplify the layout and provide customized displays of metrics, parameters, and tags within the experiments UI (#7177, @harupy)
- [UI] Simplify run filtering and ordering of runs within the experiments UI (#7177, @harupy)
- [Tracking] Update `mlflow.pyfunc.get_model_dependencies()` to download all referenced requirements files for specified models (#6733, @harupy)
- [Tracking] Add support for selecting the Keras model `save_format` used by `mlflow.tensorflow.autolog()` (#7123, @balvisio)
- [Models] Set `mlflow.evaluate()` status to stable as it is now a production-ready API
- [Models] Simplify APIs for specifying custom metrics and custom artifacts during model evaluation with `mlflow.evaluate()` (#7142, @harupy)
- [Models] Correctly infer the positive label for binary classification within `mlflow.evaluate()` (#7149, @dbczumar)
- [Models] Enable automated signature logging for `tensorflow` and `keras` models when `mlflow.tensorflow.autolog()` is enabled (#6678, @BenWilson2)
- [Models] Add support for native Keras and Tensorflow Core models within `mlflow.tensorflow` (#6530, @WeichenXu123)
- [Models] Add support for defining the `model_format` used by `mlflow.xgboost.save/log_model()` (#7068, @AvikantSrivastava)
- [Scoring] Overhaul the model scoring REST API to introduce format indicators for inputs and support multiple output fields (#6575, @tomasatdatabricks; #7254, @adriangonz)
- [Scoring] Add support for ragged arrays in model signatures (#7135, @trangevi)
- [Java] Add `getModelVersion` API to the java client (#6955, @wgottschalk)

Breaking Changes:

The following list of breaking changes are arranged by their order of significance within each category.

- [Core] Support for Python 3.7 has been dropped. MLflow now requires Python >=3.8
- [Recipes] `mlflow.pipelines` APIs have been replaced with `mlflow.recipes`
- [Tracking / Registry] Remove `/preview` routes for Tracking and Model Registry REST APIs (#6667, @harupy)
- [Tracking] Remove deprecated `list` APIs for experiments, models, and runs from Python, Java, R, and REST APIs (#6785, #6786, #6787, #6788, #6800, #6868, @dbczumar)
- [Tracking] Remove deprecated `runs` response field from `Get Experiment` REST API response (#6541, #6524 @dbczumar)
- [Tracking] Remove deprecated `MlflowClient.download_artifacts` API (#6537, @WeichenXu123)
- [Tracking] Change the behavior of environment variable handling for `MLFLOW_EXPERIMENT_NAME` such that the value is always used when creating an experiment (#6674, @BenWilson2)
- [Tracking] Update `mlflow server` to run in `--serve-artifacts` mode by default (#6502, @harupy)
- [Tracking] Update Experiment ID generation for the Filestore backend to enable threadsafe concurrency (#7070, @BenWilson2)
- [Tracking] Remove `dataset_name` and `on_data_{name | hash}` suffixes from `mlflow.evaluate()` metric keys (#7042, @harupy)
- [Models / Scoring / Projects] Change default environment manager to `virtualenv` instead of `conda` for model inference and project execution (#6459, #6489 @harupy)
- [Models] Move Keras model logging APIs to the `mlflow.tensorflow` flavor and drop support for TensorFlow Estimators (#6530, @WeichenXu123)
- [Models] Remove deprecated `mlflow.sklearn.eval_and_log_metrics()` API in favor of `mlflow.evaluate()` API (#6520, @dbczumar)
- [Models] Require `mlflow.evaluate()` model inputs to be specified as URIs (#6670, @harupy)
- [Models] Drop support for returning custom metrics and artifacts from the same function when using `mlflow.evaluate()`, in favor of `custom_artifacts` (#7142, @harupy)
- [Models] Extend `PyFuncModel` spec to support `conda` and `virtualenv` subfields (#6684, @harupy)
- [Scoring] Remove support for defining input formats using the `Content-Type` header (#6575, @tomasatdatabricks; #7254, @adriangonz)
- [Scoring] Replace the `--no-conda` CLI option argument for native serving with `--env-manager='local'` (#6501, @harupy)
- [Scoring] Remove public APIs for `mlflow.sagemaker.deploy()` and `mlflow.sagemaker.delete()` in favor of MLflow deployments APIs, such as `mlflow deployments -t sagemaker` (#6650, @dbczumar)
- [Scoring] Rename input argument `df` to `inputs` in `mlflow.deployments.predict()` method (#6681, @BenWilson2)
- [Projects] Replace the `use_conda` argument with the `env_manager` argument within the `run` CLI command for MLflow Projects (#6654, @harupy)
- [Projects] Modify the MLflow Projects docker image build options by renaming `--skip-image-build` to `--build-image` with a default of `False` (#7011, @harupy)
- [Integrations/Azure] Remove deprecated `mlflow.azureml` modules from MLflow in favor of the `azure-mlflow` deployment plugin (#6691, @BenWilson2)
- [R] Remove conda integration with the R client (#6638, @harupy)

Bug fixes:

- [Recipes] Fix rendering issue with profile cards polyfill (#7154, @hubertzub-db)
- [Tracking] Set the MLflow Run name correctly when specified as part of the `tags` argument to `mlflow.start_run()` (#7228, @Cokral)
- [Tracking] Fix an issue with conflicting MLflow Run name assignment if the `mlflow.runName` tag is set (#7138, @harupy)
- [Scoring] Fix incorrect payload constructor error in SageMaker deployment client `predict()` API (#7193, @dbczumar)
- [Scoring] Fix an issue where `DataCaptureConfig` information was not preserved when updating a Sagemaker deployment (#7281, @harupy)

Small bug fixes and documentation updates:

#7309, #7314, #7288, #7276, #7244, #7207, #7175, #7107, @sunishsheth2009; #7261, #7313, #7311, #7249, #7278, #7260, #7284, #7283, #7263, #7266, #7264, #7267, #7265, #7250, #7259, #7247, #7242, #7143, #7214, #7226, #7230, #7227, #7229, #7225, #7224, #7223, #7210, #7192, #7197, #7196, #7204, #7198, #7191, #7189, #7184, #7182, #7170, #7183, #7131, #7165, #7151, #7164, #7168, #7150, #7128, #7028, #7118, #7117, #7102, #7072, #7103, #7101, #7100, #7099, #7098, #7041, #7040, #6978, #6768, #6719, #6669, #6658, #6656, #6655, #6538, #6507, #6504 @harupy; #7310, #7308, #7300, #7290, #7239, #7220, #7127, #7091, #6713 @BenWilson2; #7332, #7299, #7271, #7209, #7180, #7179, #7158, #7147, #7114, @prithvikannan; #7275, #7245, #7134, #7059, @jinzhang21; #7306, #7298, #7287, #7272, #7258, #7236, @ayushthe1; #7279, @tk1012; #7219, @rddefauw; #7333, #7218, #7208, #7188, #7190, #7176, #7137, #7136, #7130, #7124, #7079, #7052, #6541 @dbczumar; #6640, @WeichenXu123; #7200, @hubertzub-db; #7121, @Gonmeso; #6988, @alonisser; #7141, @pdifranc; #7086, @jerrylian-db; #7286, @shogohida

## 1.30.0 (2022-10-19)

MLflow 1.30.0 includes several major features and improvements

Features:

- [Pipelines] Introduce hyperparameter tuning support to MLflow Pipelines (#6859, @prithvikannan)
- [Pipelines] Introduce support for prediction outlier comparison to training data set (#6991, @jinzhang21)
- [Pipelines] Introduce support for recording all training parameters for reproducibility (#7026, #7094, @prithvikannan)
- [Pipelines] Add support for `Delta` tables as a datasource in the ingest step (#7010, @sunishsheth2009)
- [Pipelines] Add expanded support for data profiling up to 10,000 columns (#7035, @prithvikanna)
- [Pipelines] Add support for AutoML in MLflow Pipelines using FLAML (#6959, @mshtelma)
- [Pipelines] Add support for simplified transform step execution by allowing for unspecified configuration (#6909, @apurva-koti)
- [Pipelines] Introduce a data preview tab to the transform step card (#7033, @prithvikannan)
- [Tracking] Introduce `run_name` attribute for `create_run`, `get_run` and `update_run` APIs (#6782, #6798 @apurva-koti)
- [Tracking] Add support for searching by `creation_time` and `last_update_time` for the `search_experiments` API (#6979, @harupy)
- [Tracking] Add support for search terms `run_id IN` and `run ID NOT IN` for the `search_runs` API (#6945, @harupy)
- [Tracking] Add support for searching by `user_id` and `end_time` for the `search_runs` API (#6881, #6880 @subramaniam02)
- [Tracking] Add support for searching by `run_name` and `run_id` for the `search_runs` API (#6899, @harupy; #6952, @alexacole)
- [Tracking] Add support for synchronizing run `name` attribute and `mlflow.runName` tag (#6971, @BenWilson2)
- [Tracking] Add support for signed tracking server requests using AWSSigv4 and AWS IAM (#7044, @pdifranc)
- [Tracking] Introduce the `update_run()` API for modifying the `status` and `name` attributes of existing runs (#7013, @gabrielfu)
- [Tracking] Add support for experiment deletion in the `mlflow gc` cli API (#6977, @shaikmoeed)
- [Models] Add support for environment restoration in the `evaluate()` API (#6728, @jerrylian-db)
- [Models] Remove restrictions on binary classification labels in the `evaluate()` API (#7077, @dbczumar)
- [Scoring] Add support for `BooleanType` to `mlflow.pyfunc.spark_udf()` (#6913, @BenWilson2)
- [SQLAlchemy] Add support for configurable `Pool` class options for `SqlAlchemyStore` (#6883, @mingyu89)

Bug fixes:

- [Pipelines] Enable Pipeline subprocess commands to create a new `SparkSession` if one does not exist (#6846, @prithvikannan)
- [Pipelines] Fix a rendering issue with `bool` column types in Step Card data profiles (#6907, @sunishsheth2009)
- [Pipelines] Add validation and an exception if required step files are missing (#7067, @mingyu89)
- [Pipelines] Change step configuration validation to only be performed during runtime execution of a step (#6967, @prithvikannan)
- [Tracking] Fix infinite recursion bug when inferring the model schema in `mlflow.pyspark.ml.autolog()` (#6831, @harupy)
- [UI] Remove the browser error notification when failing to fetch artifacts (#7001, @kevingreer)
- [Models] Allow `mlflow-skinny` package to serve as base requirement in `MLmodel` requirements (#6974, @BenWilson2)
- [Models] Fix an issue with code path resolution for loading SparkML models (#6968, @dbczumar)
- [Models] Fix an issue with dependency inference in logging SparkML models (#6912, @BenWilson2)
- [Models] Fix an issue involving potential duplicate downloads for SparkML models (#6903, @serena-ruan)
- [Models] Add missing `pos_label` to `sklearn.metrics.precision_recall_curve` in `mlflow.evaluate()` (#6854, @dbczumar)
- [SQLAlchemy] Fix a bug in `SqlAlchemyStore` where `set_tag()` updates the incorrect tags (#7027, @gabrielfu)

Documentation updates:

- [Models] Update details regarding the default `Keras` serialization format (#7022, @balvisio)

Small bug fixes and documentation updates:

#7093, #7095, #7092, #7064, #7049, #6921, #6920, #6940, #6926, #6923, #6862, @jerrylian-db; #6946, #6954, #6938, @mingyu89; #7047, #7087, #7056, #6936, #6925, #6892, #6860, #6828, @sunishsheth2009; #7061, #7058, #7098, #7071, #7073, #7057, #7038, #7029, #6918, #6993, #6944, #6976, #6960, #6933, #6943, #6941, #6900, #6901, #6898, #6890, #6888, #6886, #6887, #6885, #6884, #6849, #6835, #6834, @harupy; #7094, #7065, #7053, #7026, #7034, #7021, #7020, #6999, #6998, #6996, #6990, #6989, #6934, #6924, #6896, #6895, #6876, #6875, #6861, @prithvikannan; #7081, #7030, #7031, #6965, #6750, @bbarnes52; #7080, #7069, #7051, #7039, #7012, #7004, @dbczumar; #7054, @jinzhang21; #7055, #7037, #7036, #6949, #6951, @apurva-koti; #6815, @michaguenther; #6897, @chaturvedakash; #7025, #6981, #6950, #6948, #6937, #6829, #6830, @BenWilson2; #6982, @vadim; #6985, #6927, @kriscon-db; #6917, #6919, #6872, #6855, @WeichenXu123; #6980, @utkarsh867; #6973, #6935, @wentinghu; #6930, @mingyangge-db; #6956, @RohanBha1; #6916, @av-maslov; #6824, @shrinath-suresh; #6732, @oojo12; #6807, @ikrizanic; #7066, @subramaniam20jan; #7043, @AvikantSrivastava; #6879, @jspablo

## 1.29.0 (2022-09-16)

MLflow 1.29.0 includes several major features and improvements

Features:

- [Pipelines] Improve performance and fidelity of dataset profiling in the scikit-learn regression Pipeline (#6792, @sunishsheth2009)
- [Pipelines] Add an `mlflow pipelines get-artifact` CLI for retrieving Pipeline artifacts (#6517, @prithvikannan)
- [Pipelines] Introduce an option for skipping dataset profiling to the scikit-learn regression Pipeline (#6456, @apurva-koti)
- [Pipelines / UI] Display an `mlflow pipelines` CLI command for reproducing a Pipeline run in the MLflow UI (#6376, @hubertzub-db)
- [Tracking] Automatically generate friendly names for Runs if not supplied by the user (#6736, @BenWilson2)
- [Tracking] Add `load_text()`, `load_image()` and `load_dict()` fluent APIs for convenient artifact loading (#6475, @subramaniam02)
- [Tracking] Add `creation_time` and `last_update_time` attributes to the Experiment class (#6756, @subramaniam02)
- [Tracking] Add official MLflow Tracking Server Dockerfiles to the MLflow repository (#6731, @oojo12)
- [Tracking] Add `searchExperiments` API to Java client and deprecate `listExperiments` (#6561, @dbczumar)
- [Tracking] Add `mlflow_search_experiments` API to R client and deprecate `mlflow_list_experiments` (#6576, @dbczumar)
- [UI] Make URLs clickable in the MLflow Tracking UI (#6526, @marijncv)
- [UI] Introduce support for csv data preview within the artifact viewer pane (#6567, @nnethery)
- [Model Registry / Models] Introduce `mlflow.models.add_libraries_to_model()` API for adding libraries to an MLflow Model (#6586, @arjundc-db)
- [Models] Add model validation support to `mlflow.evaluate()` (#6582, @jerrylian-db)
- [Models] Introduce `sample_weights` support to `mlflow.evaluate()` (#6806, @dbczumar)
- [Models] Add `pos_label` support to `mlflow.evaluate()` for identifying the positive class (#6696, @harupy)
- [Models] Make the metric name prefix and dataset info configurable in `mlflow.evaluate()` (#6593, @dbczumar)
- [Models] Add utility for validating the compatibility of a dataset with a model signature (#6494, @serena-ruan)
- [Models] Add `predict_proba()` support to the pyfunc representation of scikit-learn models (#6631, @skylarbpayne)
- [Models] Add support for Decimal type inference to MLflow Model schemas (#6600, @shitaoli-db)
- [Models] Add new CLI command for generating Dockerfiles for model serving (#6591, @anuarkaliyev23)
- [Scoring] Add `/health` endpoint to scoring server (#6574, @gabriel-milan)
- [Scoring] Support specifying a `variant_name` during Sagemaker deployment (#6486, @nfarley-soaren)
- [Scoring] Support specifying a `data_capture_config` during SageMaker deployment (#6423, @jonwiggins)

Bug fixes:

- [Tracking] Make Run and Experiment deletion and restoration idempotent (#6641, @dbczumar)
- [UI] Fix an alignment bug affecting the Experiments list in the MLflow UI (#6569, @sunishsheth2009)
- [Models] Fix a regression in the directory path structure of logged Spark Models that occurred in MLflow 1.28.0 (#6683, @gwy1995)
- [Models] No longer reload the `__main__` module when loading model code (#6647, @Jooakim)
- [Artifacts] Fix an `mlflow server` compatibility issue with HDFS when running in `--serve-artifacts` mode (#6482, @shidianshifen)
- [Scoring] Fix an inference failure with 1-dimensional tensor inputs in TensorFlow and Keras (#6796, @LiamConnell)

Documentation updates:

- [Tracking] Mark the SearchExperiments API as stable (#6551, @dbczumar)
- [Tracking / Model Registry] Deprecate the ListExperiments, ListRegisteredModels, and `list_run_infos()` APIs (#6550, @dbczumar)
- [Scoring] Deprecate `mlflow.sagemaker.deploy()` in favor of `SageMakerDeploymentClient.create()` (#6651, @dbczumar)

Small bug fixes and documentation updates:

#6803, #6804, #6801, #6791, #6772, #6745, #6762, #6760, #6761, #6741, #6725, #6720, #6666, #6708, #6717, #6704, #6711, #6710, #6706, #6699, #6700, #6702, #6701, #6685, #6664, #6644, #6653, #6629, #6639, #6624, #6565, #6558, #6557, #6552, #6549, #6534, #6533, #6516, #6514, #6506, #6509, #6505, #6492, #6490, #6478, #6481, #6464, #6463, #6460, #6461, @harupy; #6810, #6809, #6727, #6648, @BenWilson2; #6808, #6766, #6729, @jerrylian-db; #6781, #6694, @marijncv; #6580, #6661, @bbarnes52; #6778, #6687, #6623, @shraddhafalane; #6662, #6737, #6612, #6595, @sunishsheth2009; #6777, @aviralsharma07; #6665, #6743, #6573, @liangz1; #6784, @apurva-koti; #6753, #6751, @mingyu89; #6690, #6455, #6484, @kriscon-db; #6465, #6689, @hubertzub-db; #6721, @WeichenXu123; #6722, #6718, #6668, #6663, #6621, #6547, #6508, #6474, #6452, @dbczumar; #6555, #6584, #6543, #6542, #6521, @dsgibbons; #6634, #6596, #6563, #6495, @prithvikannan; #6571, @smurching; #6630, #6483, @serena-ruan; #6642, @thinkall; #6614, #6597, @jinzhang21; #6457, @cnphil; #6570, #6559, @kumaryogesh17; #6560, #6540, @iamthen0ise; #6544, @Monkero; #6438, @ahlag; #3292, @dolfinus; #6637, @ninabacc-db; #6632, @arpitjasa-db

## 1.28.0 (2022-08-09)

MLflow 1.28.0 includes several major features and improvements:

Features:

- [Pipelines] Log the full Pipeline runtime configuration to MLflow Tracking during Pipeline execution (#6359, @jinzhang21)
- [Pipelines] Add `pipeline.yaml` configurations to specify the Model Registry backend used for model registration (#6284, @sunishsheth2009)
- [Pipelines] Support optionally skipping the `transform` step of the scikit-learn regression pipeline (#6362, @sunishsheth2009)
- [Pipelines] Add UI links to Runs and Models in Pipeline Step Cards on Databricks (#6294, @dbczumar)
- [Tracking] Introduce `mlflow.search_experiments()` API for searching experiments by name and by tags (#6333, @WeichenXu123; #6227, #6172, #6154, @harupy)
- [Tracking] Increase the maximum parameter value length supported by File and SQL backends to 500 characters (#6358, @johnyNJ)
- [Tracking] Introduce an `--older-than` flag to `mlflow gc` for removing runs based on deletion time (#6354, @Jason-CKY)
- [Tracking] Add `MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE` environment variable for recycling SQLAlchemy connections (#6344, @postrational)
- [UI] Display deeply nested runs in the Runs Table on the Experiment Page (#6065, @tospe)
- [UI] Add box plot visualization for metrics to the Compare Runs page (#6308, @ahlag)
- [UI] Display tags on the Compare Runs page (#6164, @CaioCavalcanti)
- [UI] Use scientific notation for axes when viewing metric plots in log scale (#6176, @RajezMariner)
- [UI] Add button to Metrics page for downloading metrics as CSV (#6048, @rafaelvp-db)
- [UI] Include NaN and +/- infinity values in plots on the Metrics page (#6422, @hubertzub-db)
- [Tracking / Model Registry] Introduce environment variables to control retry behavior and timeouts for REST API requests (#5745, @peterdhansen)
- [Tracking / Model Registry] Make `MlflowClient` importable as `mlflow.MlflowClient` (#6085, @subramaniam02)
- [Model Registry] Add support for searching registered models and model versions by tags (#6413, #6411, #6320, @WeichenXu123)
- [Model Registry] Add `stage` parameter to `set_model_version_tag()` (#6185, @subramaniam02)
- [Model Registry] Add `--registry-store-uri` flag to `mlflow server` for specifying the Model Registry backend URI (#6142, @Secbone)
- [Models] Improve performance of Spark Model logging on Databricks (#6282, @bbarnes52)
- [Models] Include Pandas Series names in inferred model schemas (#6361, @RynoXLI)
- [Scoring] Make `model_uri` optional in `mlflow models build-docker` to support building generic model serving images (#6302, @harupy)
- [R] Support logging of NA and NaN parameter values (#6263, @nathaneastwood)

Bug fixes and documentation updates:

- [Pipelines] Improve scikit-learn regression pipeline latency by limiting dataset profiling to the first 100 columns (#6297, @sunishsheth2009)
- [Pipelines] Use `xdg-open` instead of `open` for viewing Pipeline results on Linux systems (#6326, @strangiato)
- [Pipelines] Fix a bug that skipped Step Card rendering in Jupyter Notebooks (#6378, @apurva-koti)
- [Tracking] Use the 401 HTTP response code in authorization failure REST API responses, instead of 500 (#6106, @balvisio)
- [Tracking] Correctly classify artifacts as files and directories when using Azure Blob Storage (#6237, @nerdinand)
- [Tracking] Fix a bug in the File backend that caused run metadata to be lost in the event of a failed write (#6388, @dbczumar)
- [Tracking] Adjust `mlflow.pyspark.ml.autolog()` to only log model signatures for supported input / output data types (#6365, @harupy)
- [Tracking] Adjust `mlflow.tensorflow.autolog()` to log TensorFlow early stopping callback info when `log_models=False` is specified (#6170, @WeichenXu123)
- [Tracking] Fix signature and input example logging errors in `mlflow.sklearn.autolog()` for models containing transformers (#6230, @dbczumar)
- [Tracking] Fix a failure in `mlflow gc` that occurred when removing a run whose artifacts had been previously deleted (#6165, @dbczumar)
- [Tracking] Add missing `sqlparse` library to MLflow Skinny client, which is required for search support (#6174, @dbczumar)
- [Tracking / Model Registry] Fix an `mlflow server` bug that rejected parameters and tags with empty string values (#6179, @dbczumar)
- [Model Registry] Fix a failure preventing model version schemas from being downloaded with `--serve-arifacts` enabled (#6355, @abbas123456)
- [Scoring] Patch the Java Model Server to support MLflow Models logged on recent versions of the Databricks Runtime (#6337, @dbczumar)
- [Scoring] Verify that either the deployment name or endpoint is specified when invoking the `mlflow deployments predict` CLI (#6323, @dbczumar)
- [Scoring] Properly encode datetime columns when performing batch inference with `mlflow.pyfunc.spark_udf()` (#6244, @harupy)
- [Projects] Fix an issue where local directory paths were misclassified as Git URIs when running Projects (#6218, @ElefHead)
- [R] Fix metric logging behavior for +/- infinity values (#6271, @nathaneastwood)
- [Docs] Move Python API docs for `MlflowClient` from `mlflow.tracking` to `mlflow.client` (#6405, @dbczumar)
- [Docs] Document that MLflow Pipelines requires Make (#6216, @dbczumar)
- [Docs] Improve documentation for developing and testing MLflow JS changes in `CONTRIBUTING.rst` (#6330, @ahlag)

Small bug fixes and doc updates (#6322, #6321, #6213, @KarthikKothareddy; #6409, #6408, #6396, #6402, #6399, #6398, #6397, #6390, #6381, #6386, #6385, #6373, #6375, #6380, #6374, #6372, #6363, #6353, #6352, #6350, #6351, #6349, #6347, #6287, #6341, #6342, #6340, #6338, #6319, #6314, #6316, #6317, #6318, #6315, #6313, #6311, #6300, #6292, #6291, #6289, #6290, #6278, #6279, #6276, #6272, #6252, #6243, #6250, #6242, #6241, #6240, #6224, #6220, #6208, #6219, #6207, #6171, #6206, #6199, #6196, #6191, #6190, #6175, #6167, #6161, #6160, #6153, @harupy; #6193, @jwgwalton; #6304, #6239, #6234, #6229, @sunishsheth2009; #6258, @xanderwebs; #6106, @balvisio; #6303, @bbarnes52; #6117, @wenfeiy-db; #6389, #6214, @apurva-koti; #6412, #6420, #6277, #6266, #6260, #6148, @WeichenXu123; #6120, @ameya-parab; #6281, @nathaneastwood; #6426, #6415, #6417, #6418, #6257, #6182, #6157, @dbczumar; #6189, @shrinath-suresh; #6309, @SamirPS; #5897, @temporaer; #6251, @herrmann; #6198, @sniafas; #6368, #6158, @jinzhang21; #6236, @subramaniam02; #6036, @serena-ruan; #6430, @ninabacc-db)

## 1.27.0 (2022-06-27)

MLflow 1.27.0 includes several major features and improvements:

- [**Pipelines**] With MLflow 1.27.0, we are excited to announce the release of
  [**MLflow Pipelines**](https://mlflow.org/docs/latest/pipelines.html), an opinionated framework for
  structuring MLOps workflows that simplifies and standardizes machine learning application development
  and productionization. MLflow Pipelines makes it easy for data scientists to follow best practices
  for creating production-ready ML deliverables, allowing them to focus on developing excellent models.
  MLflow Pipelines also enables ML engineers and DevOps teams to seamlessly deploy models to production
  and incorporate them into applications. To get started with MLflow Pipelines, check out the docs at
  https://mlflow.org/docs/latest/pipelines.html. (#6115)

- [UI] Introduce UI support for searching and comparing runs across multiple Experiments (#5971, @r3stl355)

More features:

- [Tracking] When using batch logging APIs, automatically split large sets of metrics, tags, and params into multiple requests (#6052, @nzw0301)
- [Tracking] When an Experiment is deleted, SQL-based backends also move the associate Runs to the "deleted" lifecycle stage (#6064, @AdityaIyengar27)
- [Tracking] Add support for logging single-element `ndarray` and tensor instances as metrics via the `mlflow.log_metric()` API (#5756, @ntakouris)
- [Models] Add support for `CatBoostRanker` models to the `mlflow.catboost` flavor (#6032, @danielgafni)
- [Models] Integrate SHAP's `KernelExplainer` with `mlflow.evaluate()`, enabling model explanations on categorical data (#6044, #5920, @WeichenXu123)
- [Models] Extend `mlflow.evaluate()` to automatically log the `score()` outputs of scikit-learn models as metrics (#5935, #5903, @WeichenXu123)

Bug fixes and documentation updates:

- [UI] Fix broken model links in the Runs table on the MLflow Experiment Page (#6014, @hctpbl)
- [Tracking/Installation] Require `sqlalchemy>=1.4.0` upon MLflow installation, which is necessary for usage of SQL-based MLflow Tracking backends (#6024, @sniafas)
- [Tracking] Fix a regression that caused `mlflow server` to reject `LogParam` API requests containing empty string values (#6031, @harupy)
- [Tracking] Fix a failure in scikit-learn autologging that occurred when `matplotlib` was not installed on the host system (#5995, @fa9r)
- [Tracking] Fix a failure in TensorFlow autologging that occurred when training models on `tf.data.Dataset` inputs (#6061, @dbczumar)
- [Artifacts] Address artifact download failures from SFTP locations that occurred due to mismanaged concurrency (#5840, @rsundqvist)
- [Models] Fix a bug where MLflow Models did not restore bundled code properly if multiple models use the same code module name (#5926, @BFAnas)
- [Models] Address an issue where `mlflow.sklearn.model()` did not properly restore bundled model code (#6037, @WeichenXu123)
- [Models] Fix a bug in `mlflow.evaluate()` that caused input data objects to be mutated when evaluating certain scikit-learn models (#6141, @dbczumar)
- [Models] Fix a failure in `mlflow.pyfunc.spark_udf` that occurred when the UDF was invoked on an empty RDD partition (#6063, @WeichenXu123)
- [Models] Fix a failure in `mlflow models build-docker` that occurred when `env-manager=local` was specified (#6046, @bneijt)
- [Projects] Improve robustness of the git repository check that occurs prior to MLflow Project execution (#6000, @dkapur17)
- [Projects] Address a failure that arose when running a Project that does not have a `master` branch (#5889, @harupy)
- [Docs] Correct several typos throughout the MLflow docs (#5959, @ryanrussell)

Small bug fixes and doc updates (#6041, @drsantos89; #6138, #6137, #6132, @sunishsheth2009; #6144, #6124, #6125, #6123, #6057, #6060, #6050, #6038, #6029, #6030, #6025, #6018, #6019, #5962, #5974, #5972, #5957, #5947, #5907, #5938, #5906, #5932, #5919, #5914, #5888, #5890, #5886, #5873, #5865, #5843, @harupy; #6113, @comojin1994; #5930, @yashaswikakumanu; #5837, @shrinath-suresh; #6067, @deepyaman; #5997, @idlefella; #6021, @BenWilson2; #5984, @Sumanth077; #5929, @krunal16-c; #5879, @kugland; #5875, @ognis1205; #6006, @ryanrussell; #6140, @jinzhang21; #5983, @elk15; #6022, @apurva-koti; #5982, @EB-Joel; #5981, #5980, @punitkashyup; #6103, @ikrizanic; #5988, #5969, @SaumyaBhushan; #6020, #5991, @WeichenXu123; #5910, #5912, @Dark-Knight11; #6005, @Asinsa; #6023, @subramaniam02; #5999, @Regis-Caelum; #6007, @CaioCavalcanti; #5943, @kvaithin; #6017, #6002, @NeoKish; #6111, @T1b4lt; #5986, @seyyidibrahimgulec; #6053, @Zohair-coder; #6146, #6145, #6143, #6139, #6134, #6136, #6135, #6133, #6071, #6070, @dbczumar; #6026, @rotate2050)

## 1.26.1 (2022-05-27)

MLflow 1.26.1 is a patch release containing the following bug fixes:

- [Installation] Fix compatibility issue with `protobuf >= 4.21.0` (#5945, @harupy)
- [Models] Fix `get_model_dependencies` behavior for `models:` URIs containing artifact paths (#5921, @harupy)
- [Models] Revert a problematic change to `artifacts` persistence in `mlflow.pyfunc.log_model()` that was introduced in MLflow 1.25.0 (#5891, @kyle-jarvis)
- [Models] Close associated image files when `EvaluationArtifact` outputs from `mlflow.evaluate()` are garbage collected (#5900, @WeichenXu123)

Small bug fixes and updates (#5874, #5942, #5941, #5940, #5938, @harupy; #5893, @PrajwalBorkar; #5909, @yashaswikakumanu; #5937, @BenWilson2)

## 1.26.0 (2022-05-16)

MLflow 1.26.0 includes several major features and improvements:

Features:

- [CLI] Add endpoint naming and options configuration to the deployment CLI (#5731, @trangevi)
- [Build,Doc] Add development environment setup script for Linux and MacOS x86 Operating Systems (#5717, @BenWilson2)
- [Tracking] Update `mlflow.set_tracking_uri` to add support for paths defined as `pathlib.Path` in addition to existing `str` path declarations (#5824, @cacharle)
- [Scoring] Add custom timeout override option to the scoring server CLI to support high latency models (#5663, @sniafas)
- [UI] Add sticky header to experiment run list table to support column name visibility when scrolling beyond page fold (#5818, @hubertzub-db)
- [Artifacts] Add GCS support for MLflow garbage collection (#5811, @aditya-iyengar-rtl-de)
- [Evaluate] Add `pos_label` argument for `eval_and_log_metrics` API to support accurate binary classifier evaluation metrics (#5807, @yxiong)
- [UI] Add fields for latest, minimum and maximum metric values on metric display page (#5574, @adamreeve)
- [Models] Add support for `input_example` and `signature` logging for pyspark ml flavor when using autologging (#5719, @bali0019)
- [Models] Add `virtualenv` environment manager support for `mlflow models docker-build` CLI (#5728, @harupy)
- [Models] Add support for wildcard module matching in log_model_allowlist for PySpark models (#5723, @serena-ruan)
- [Projects] Add `virtualenv` environment manager support for MLflow projects (#5631, @harupy)
- [Models] Add `virtualenv` environment manager support for MLflow Models (#5380, @harupy)
- [Models] Add `virtualenv` environment manager support for `mlflow.pyfunc.spark_udf` (#5676, @WeichenXu123)
- [Models] Add support for `input_example` and `signature` logging for `tensorflow` flavor when using autologging (#5510, @bali0019)
- [Server-infra] Add JSON Schema Type Validation to enable raising 400 errors on malformed requests to REST API endpoints (#5458, @mrkaye97)
- [Scoring] Introduce abstract `endpoint` interface for mlflow deployments (#5378, @trangevi)
- [UI] Add `End Time` and `Duration` fields to run comparison page (#3378, @RealArpanBhattacharya)
- [Serving] Add schema validation support when parsing input csv data for model serving (#5531, @vvijay-bolt)

Bug fixes and documentation updates:

- [Models] Fix REPL ID propagation from datasource listener to publisher for Spark data sources (#5826, @dbczumar)
- [UI] Update `ag-grid` and implement `getRowId` to improve performance in the runs table visualization (#5725, @adamreeve)
- [Serving] Fix `tf-serving` parsing to support columnar-based formatting (#5825, @arjundc-db)
- [Artifacts] Update `log_artifact` to support models larger than 2GB in HDFS (#5812, @hitchhicker)
- [Models] Fix autologging to support `lightgbm` metric names with "@" symbols within their names (#5785, @mengchendd)
- [Models] Pyfunc: Fix code directory resolution of subdirectories (#5806, @dbczumar)
- [Server-Infra] Fix mlflow-R server starting failure on windows (#5767, @serena-ruan)
- [Docs] Add documentation for `virtualenv` environment manager support for MLflow projects (#5727, @harupy)
- [UI] Fix artifacts display sizing to support full width rendering in preview pane (#5606, @szczeles)
- [Models] Fix local hostname issues when loading spark model by binding driver address to localhost (#5753, @WeichenXu123)
- [Models] Fix autologging validation and batch_size calculations for `tensorflow` flavor (#5683, @MarkYHZhang)
- [Artifacts] Fix `SqlAlchemyStore.log_batch` implementation to make it log data in batches (#5460, @erensahin)

Small bug fixes and doc updates (#5858, #5859, #5853, #5854, #5845, #5829, #5842, #5834, #5795, #5777, #5794, #5766, #5778, #5765, #5763, #5768, #5769, #5760, #5727, #5748, #5726, #5721, #5711, #5710, #5708, #5703, #5702, #5696, #5695, #5669, #5670, #5668, #5661, #5638, @harupy; #5749, @arpitjasa-db; #5675, @Davidswinkels; #5803, #5797, @ahlag; #5743, @kzhang01; #5650, #5805, #5724, #5720, #5662, @BenWilson2; #5627, @cterrelljones; #5646, @kutal10; #5758, @davideli-db; #5810, @rahulporuri; #5816, #5764, @shrinath-suresh; #5869, #5715, #5737, #5752, #5677, #5636, @WeichenXu123; #5735, @subramaniam02; #5746, @akaigraham; #5734, #5685, @lucalves; #5761, @marcelatoffernet; #5707, @aashish-khub; #5808, @ketangangal; #5730, #5700, @shaikmoeed; #5775, @dbczumar; #5747, @zhixuanevelynwu)

## 1.25.1 (2022-04-13)

MLflow 1.25.1 is a patch release containing the following bug fixes:

- [Models] Fix a `pyfunc` artifact overwrite bug for when multiple artifacts are saved in sub-directories (#5657, @kyle-jarvis)
- [Scoring] Fix permissions issue for Spark workers accessing model artifacts from a temp directory created by the driver (#5684, @WeichenXu123)

## 1.25.0 (2022-04-11)

MLflow 1.25.0 includes several major features and improvements:

Features:

- [Tracking] Introduce a new fluent API `mlflow.last_active_run()` that provides the most recent fluent active run (#5584, @MarkYHZhang)
- [Tracking] Add `experiment_names` argument to the `mlflow.search_runs()` API to support searching runs by experiment names (#5564, @r3stl355)
- [Tracking] Add a `description` parameter to `mlflow.start_run()` (#5534, @dogeplusplus)
- [Tracking] Add `log_every_n_step` parameter to `mlflow.pytorch.autolog()` to control metric logging frequency (#5516, @adamreeve)
- [Tracking] Log `pyspark.ml.param.Params` values as MLflow parameters during PySpark autologging (#5481, @serena-ruan)
- [Tracking] Add support for `pyspark.ml.Transformer`s to PySpark autologging (#5466, @serena-ruan)
- [Tracking] Add input example and signature autologging for Keras models (#5461, @bali0019)
- [Models] Introduce `mlflow.diviner` flavor for large-scale [time series forecasting](https://databricks-diviner.readthedocs.io/en/latest/?badge=latest) (#5553, @BenWilson2)
- [Models] Add `pyfunc.get_model_dependencies()` API to retrieve reproducible environment specifications for MLflow Models with the pyfunc flavor (#5503, @WeichenXu123)
- [Models] Add `code_paths` argument to all model flavors to support packaging custom module code with MLflow Models (#5448, @stevenchen-db)
- [Models] Support creating custom artifacts when evaluating models with `mlflow.evaluate()` (#5405, #5476 @MarkYHZhang)
- [Models] Add `mlflow_version` field to MLModel specification (#5515, #5576, @r3stl355)
- [Models] Add support for logging models to preexisting destination directories (#5572, @akshaya-a)
- [Scoring / Projects] Introduce `--env-manager` configuration for specifying environment restoration tools (e.g. `conda`) and deprecate `--no-conda` (#5567, @harupy)
- [Scoring] Support restoring model dependencies in `mlflow.pyfunc.spark_udf()` to ensure accurate predictions (#5487, #5561, @WeichenXu123)
- [Scoring] Add support for `numpy.ndarray` type inputs to the TensorFlow pyfunc `predict()` function (#5545, @WeichenXu123)
- [Scoring] Support deployment of MLflow Models to Sagemaker Serverless (#5610, @matthewmayo)
- [UI] Add MLflow version to header beneath logo (#5504, @adamreeve)
- [Artifacts] Introduce a `mlflow.artifacts.download_artifacts()` API mirroring the functionality of the `mlflow artifacts download` CLI (#5585, @dbczumar)
- [Artifacts] Introduce environment variables for controlling GCS artifact upload/download chunk size and timeouts (#5438, #5483, @mokrueger)

Bug fixes and documentation updates:

- [Tracking/SQLAlchemy] Create an index on `run_uuid` for PostgreSQL to improve query performance (#5446, @harupy)
- [Tracking] Remove client-side validation of metric, param, tag, and experiment fields (#5593, @BenWilson2)
- [Projects] Support setting the name of the MLflow Run when executing an MLflow Project (#5187, @bramrodenburg)
- [Scoring] Use pandas `split` orientation for DataFrame inputs to SageMaker deployment `predict()` API to preserve column ordering (#5522, @dbczumar)
- [Server-Infra] Fix runs search compatibility bugs with PostgreSQL, MySQL, and MSSQL (#5540, @harupy)
- [CLI] Fix a bug in the `mlflow-skinny` client that caused `mlflow --version` to fail (#5573, @BenWilson2)
- [Docs] Update guidance and examples for model deployment to AzureML to recommend using the `mlflow-azureml` package (#5491, @santiagxf)

Small bug fixes and doc updates (#5591, #5629, #5597, #5592, #5562, #5477, @BenWilson2; #5554, @juntai-zheng; #5570, @tahesse; #5605, @guelate; #5633, #5632, #5625, #5623, #5615, #5608, #5600, #5603, #5602, #5596, #5587, #5586, #5580, #5577, #5568, #5290, #5556, #5560, #5557, #5548, #5547, #5538, #5513, #5505, #5464, #5495, #5488, #5485, #5468, #5455, #5453, #5454, #5452, #5445, #5431, @harupy; #5640, @nchittela; #5520, #5422, @Ark-kun; #5639, #5604, @nishipy; #5543, #5532, #5447, #5435, @WeichenXu123; #5502, @singankit; #5500, @Sohamkayal4103; #5449, #5442, @apurva-koti; #5552, @vinijaiswal; #5511, @adamreeve; #5428, @jinzhang21; #5309, @sunishsheth2009; #5581, #5559, @Kr4is; #5626, #5618, #5529, @sisp; #5652, #5624, #5622, #5613, #5509, #5459, #5437, @dbczumar; #5616, @liangz1)

## 1.24.0 (2022-02-27)

MLflow 1.24.0 includes several major features and improvements:

Features:

- [Tracking] Support uploading, downloading, and listing artifacts through the MLflow server via `mlflow server --serve-artifacts` (#5320, @BenWilson2, @harupy)
- [Tracking] Add the `registered_model_name` argument to `mlflow.autolog()` for automatic model registration during autologging (#5395, @WeichenXu123)
- [UI] Improve and restructure the Compare Runs page. Additions include "show diff only" toggles and scrollable tables (#5306, @WeichenXu123)
- [Models] Introduce `mlflow.pmdarima` flavor for pmdarima models (#5373, @BenWilson2)
- [Models] When loading an MLflow Model, print a warning if a mismatch is detected between the current environment and the Model's dependencies (#5368, @WeichenXu123)
- [Models] Support computing custom scalar metrics during model evaluation with `mlflow.evaluate()` (#5389, @MarkYHZhang)
- [Scoring] Add support for deploying and evaluating SageMaker models via the [`MLflow Deployments API`](https://mlflow.org/docs/latest/models.html#deployment-to-custom-targets) (#4971, #5396, @jamestran201)

Bug fixes and documentation updates:

- [Tracking / UI] Fix artifact listing and download failures that occurred when operating the MLflow server in `--serve-artifacts` mode (#5409, @dbczumar)
- [Tracking] Support environment-variable-based authentication when making artifact requests to the MLflow server in `--serve-artifacts` mode (#5370, @TimNooren)
- [Tracking] Fix bugs in hostname and path resolution when making artifacts requests to the MLflow server in `--serve-artifacts` mode (#5384, #5385, @mert-kirpici)
- [Tracking] Fix an import error that occurred when `mlflow.log_figure()` was used without `matplotlib.figure` imported (#5406, @WeichenXu123)
- [Tracking] Correctly log XGBoost metrics containing the `@` symbol during autologging (#5403, @maxfriedrich)
- [Tracking] Fix a SQL Server database error that occurred during Runs search (#5382, @dianacarvalho1)
- [Tracking] When downloading artifacts from HDFS, store them in the user-specified destination directory (#5210, @DimaClaudiu)
- [Tracking / Model Registry] Improve performance of large artifact and model downloads (#5359, @mehtayogita)
- [Models] Fix fast.ai PyFunc inference behavior for models with 2D outputs (#5411, @santiagxf)
- [Models] Record Spark model information to the active run when `mlflow.spark.log_model()` is called (#5355, @szczeles)
- [Models] Restore onnxruntime execution providers when loading ONNX models with `mlflow.pyfunc.load_model()` (#5317, @ecm200)
- [Projects] Increase Docker image push timeout when using Projects with Docker (#5363, @zanitete)
- [Python] Fix a bug that prevented users from enabling DEBUG-level Python log outputs (#5362, @dbczumar)
- [Docs] Add a developer guide explaining how to build custom plugins for `mlflow.evaluate()` (#5333, @WeichenXu123)

Small bug fixes and doc updates (#5298, @wamartin-aml; #5399, #5321, #5313, #5307, #5305, #5268, #5284, @harupy; #5329, @Ark-kun; #5375, #5346, #5304, @dbczumar; #5401, #5366, #5345, @BenWilson2; #5326, #5315, @WeichenXu123; #5236, @singankit; #5302, @timvink; #5357, @maitre-matt; #5347, #5344, @mehtayogita; #5367, @apurva-koti; #5348, #5328, #5310, @liangz1; #5267, @sunishsheth2009)

## 1.23.1 (2022-01-27)

MLflow 1.23.1 is a patch release containing the following bug fixes:

- [Models] Fix a directory creation failure when loading PySpark ML models (#5299, @arjundc-db)
- [Model Registry] Revert to using case-insensitive validation logic for stage names in `models:/` URIs (#5312, @lichenran1234)
- [Projects] Fix a race condition during Project tar file creation (#5303, @dbczumar)

## 1.23.0 (2022-01-17)

MLflow 1.23.0 includes several major features and improvements:

Features:

- [Models] Introduce an `mlflow.evaluate()` API for evaluating MLflow Models, providing performance and explainability insights. For an overview, see https://mlflow.org/docs/latest/models.html#model-evaluation (#5069, #5092, #5256, @WeichenXu123)
- [Models] `log_model()` APIs now return information about the logged MLflow Model, including artifact location, flavors, and schema (#5230, @liangz1)
- [Models] Introduce an `mlflow.models.Model.load_input_example()` Python API for loading MLflow Model input examples (#5212, @maitre-matt)
- [Models] Add a UUID field to the MLflow Model specification. MLflow Models now have a unique identifier (#5149, #5167, @WeichenXu123)
- [Models] Support passing SciPy CSC and CSR matrices as MLflow Model input examples (#5016, @WeichenXu123)
- [Model Registry] Support specifying `latest` in model URI to get the latest version of a model regardless of the stage (#5027, @lichenran1234)
- [Tracking] Add support for LightGBM scikit-learn models to `mlflow.lightgbm.autolog()` (#5130, #5200, #5271 @jwyyy)
- [Tracking] Improve S3 artifact download speed by caching boto clients (#4695, @Samreay)
- [UI] Automatically update metric plots for in-progress runs (#5017, @cedkoffeto, @harupy)

Bug fixes and documentation updates:

- [Models] Fix a bug in MLflow Model schema enforcement where strings were incorrectly cast to Pandas objects (#5134, @stevenchen-db)
- [Models] Fix a bug where keyword arguments passed to `mlflow.pytorch.load_model()` were not applied for scripted models (#5163, @schmidt-jake)
- [Model Registry/R] Fix bug in R client `mlflow_create_model_version()` API that caused model `source` to be set incorrectly (#5185, @bramrodenburg)
- [Projects] Fix parsing behavior for Project URIs containing quotes (#5117, @dinaldoap)
- [Scoring] Use the correct 400-level error code for malformed MLflow Model Server requests (#5003, @abatomunkuev)
- [Tracking] Fix a bug where `mlflow.start_run()` modified user-supplied tags dictionary (#5191, @matheusMoreno)
- [UI] Fix a bug causing redundant scroll bars to be displayed on the Experiment Page (#5159, @sunishsheth2009)

Small bug fixes and doc updates (#5275, #5264, #5244, #5249, #5255, #5248, #5243, #5240, #5239, #5232, #5234, #5235, #5082, #5220, #5219, #5226, #5217, #5194, #5188, #5132, #5182, #5183, #5180, #5177, #5165, #5164, #5162, #5015, #5136, #5065, #5125, #5106, #5127, #5120, @harupy; #5045, @BenWilson2; #5156, @pbezglasny; #5202, @jwyyy; #3863, @JoshuaAnickat; #5205, @abhiramr; #4604, @OSobky; #4256, @einsmein; #5140, @AveshCSingh; #5273, #5186, #5176, @WeichenXu123; #5260, #5229, #5206, #5174, #5160, @liangz1)

## 1.22.0 (2021-11-29)

MLflow 1.22.0 includes several major features and improvements:

Features:

- [UI] Add a share button to the Experiment page (#4936, @marijncv)
- [UI] Improve readability of column sorting dropdown on Experiment page (#5022, @WeichenXu123; #5018, @NieuweNils, @coder-freestyle)
- [Tracking] Mark all autologging integrations as stable by removing `@experimental` decorators (#5028, @liangz1)
- [Tracking] Add optional `experiment_id` parameter to `mlflow.set_experiment()` (#5012, @dbczumar)
- [Tracking] Add support for XGBoost scikit-learn models to `mlflow.xgboost.autolog()` (#5078, @jwyyy)
- [Tracking] Improve statsmodels autologging performance by removing unnecessary metrics (#4942, @WeichenXu123)
- [Tracking] Update R client to tag nested runs with parent run ID (#4197, @yitao-li)
- [Models] Support saving and loading all XGBoost model types (#4954, @jwyyy)
- [Scoring] Support specifying AWS account and role when deploying models to SageMaker (#4923, @andresionek91)
- [Scoring] Support serving MLflow models with MLServer (#4963, @adriangonz)

Bug fixes and documentation updates:

- [UI] Fix bug causing Metric Plot page to crash when metric values are too large (#4947, @ianshan0915)
- [UI] Fix bug causing parallel coordinate curves to vanish (#5087, @harupy)
- [UI] Remove `Creator` field from Model Version page if user information is absent (#5089, @jinzhang21)
- [UI] Fix model loading instructions for non-pyfunc models in Artifact Viewer (#5006, @harupy)
- [Models] Fix a bug that added `mlflow` to `conda.yaml` even if a hashed version was already present (#5058, @maitre-matt)
- [Docs] Add Python documentation for metric, parameter, and tag key / value length limits (#4991, @westford14)
- [Examples] Update Python version used in Prophet example to fix installation errors (#5101, @BenWilson2)
- [Examples] Fix Kubernetes `resources` specification in MLflow Projects + Kubernetes example (#4948, @jianyuan)

Small bug fixes and doc updates (#5119, #5107, #5105, #5103, #5085, #5088, #5051, #5081, #5039, #5073, #5072, #5066, #5064, #5063, #5060, #4718, #5053, #5052, #5041, #5043, #5047, #5036, #5037, #5029, #5031, #5032, #5030, #5007, #5019, #5014, #5008, #4998, #4985, #4984, #4970, #4966, #4980, #4967, #4978, #4979, #4968, #4976, #4975, #4934, #4956, #4938, #4950, #4946, #4939, #4913, #4940, #4935, @harupy; #5095, #5070, #5002, #4958, #4945, @BenWilson2; #5099, @chaosddp; #5005, @you-n-g; #5042, #4952, @shrinath-suresh; #4962, #4995, @WeichenXu123; #5010, @lichenran1234; #5000, @wentinghu; #5111, @alexott; #5102, #5024, #5011, #4959, @dbczumar; #5075, #5044, #5026, #4997, #4964, #4989, @liangz1; #4999, @stevenchen-db)

## 1.21.0 (2021-10-23)

MLflow 1.21.0 includes several major features and improvements:

Features:

- [UI] Add a diff-only toggle to the runs table for filtering out columns with constant values (#4862, @marijncv)
- [UI] Add a duration column to the runs table (#4840, @marijncv)
- [UI] Display the default column sorting order in the runs table (#4847, @marijncv)
- [UI] Add `start_time` and `duration` information to exported runs CSV (#4851, @marijncv)
- [UI] Add lifecycle stage information to the run page (#4848, @marijncv)
- [UI] Collapse run page sections by default for space efficiency, limit artifact previews to 50MB (#4917, @dbczumar)
- [Tracking] Introduce autologging capabilities for PaddlePaddle model training (#4751, @jinminhao)
- [Tracking] Add an optional tags field to the CreateExperiment API (#4788, @dbczumar; #4795, @apurva-koti)
- [Tracking] Add support for deleting artifacts from SFTP stores via the `mlflow gc` CLI (#4670, @afaul)
- [Tracking] Support AzureDefaultCredential for authenticating with Azure artifact storage backends (#4002, @marijncv)
- [Models] Upgrade the fastai model flavor to support fastai V2 (`>=2.4.1`) (#4715, @jinzhang21)
- [Models] Introduce an `mlflow.prophet` model flavor for Prophet time series models (#4773, @BenWilson2)
- [Models] Introduce a CLI for publishing MLflow Models to the SageMaker Model Registry (#4669, @jinnig)
- [Models] Print a warning when inferred model dependencies are not available on PyPI (#4891, @dbczumar)
- [Models, Projects] Add `MLFLOW_CONDA_CREATE_ENV_CMD` for customizing Conda environment creation (#4746, @giacomov)

Bug fixes and documentation updates:

- [UI] Fix an issue where column selections made in the runs table were persisted across experiments (#4926, @sunishsheth2009)
- [UI] Fix an issue where the text `null` was displayed in the runs table column ordering dropdown (#4924, @harupy)
- [UI] Fix a bug causing the metric plot view to display NaN values upon click (#4858, @arpitjasa-db)
- [Tracking] Fix a model load failure for paths containing spaces or special characters on UNIX systems (#4890, @BenWilson2)
- [Tracking] Correct a migration issue that impacted usage of MLflow Tracking with SQL Server (#4880, @marijncv)
- [Tracking] Spark datasource autologging tags now respect the maximum allowable size for MLflow Tracking (#4809, @dbczumar)
- [Model Registry] Add previously-missing certificate sources for Model Registry REST API requests (#4731, @ericgosno91)
- [Model Registry] Throw an exception when users supply invalid Model Registry URIs for Databricks (#4877, @yunpark93)
- [Scoring] Fix a schema enforcement error that incorrectly cast date-like strings to datetime objects (#4902, @wentinghu)
- [Docs] Expand the documentation for the MLflow Skinny Client (#4113, @eedeleon)

Small bug fixes and doc updates (#4928, #4919, #4927, #4922, #4914, #4899, #4893, #4894, #4884, #4864, #4823, #4841, #4817, #4796, #4797, #4767, #4768, #4757, @harupy; #4863, #4838, @marijncv; #4834, @ksaur; #4772, @louisguitton; #4801, @twsl; #4929, #4887, #4856, #4843, #4789, #4780, @WeichenXu123; #4769, @Ark-kun; #4898, #4756, @apurva-koti; #4784, @lakshikaparihar; #4855, @ianshan0915; #4790, @eedeleon; #4931, #4857, #4846, 4777, #4748, @dbczumar)

## 1.20.2 (2021-09-03)

MLflow 1.20.2 is a patch release containing the following features and bug fixes:

Features:

- Enabled auto dependency inference in spark flavor in autologging (#4759, @harupy)

Bug fixes and documentation updates:

- Increased MLflow client HTTP request timeout from 10s to 120s (#4764, @jinzhang21)
- Fixed autologging compatibility bugs with TensorFlow and Keras version `2.6.0` (#4766, @dbczumar)

Small bug fixes and doc updates (#4770, @WeichenXu123)

## 1.20.1 (2021-08-26)

MLflow 1.20.1 is a patch release containing the following bug fixes:

- Avoid calling `importlib_metadata.packages_distributions` upon `mlflow.utils.requirements_utils` import (#4741, @dbczumar)
- Avoid depending on `importlib_metadata==4.7.0` (#4740, @dbczumar)

## 1.20.0 (2021-08-25)

MLflow 1.20.0 includes several major features and improvements:

Features:

- Autologging for scikit-learn now records post training metrics when scikit-learn evaluation APIs, such as `sklearn.metrics.mean_squared_error`, are called (#4491, #4628 #4638, @WeichenXu123)
- Autologging for PySpark ML now records post training metrics when model evaluation APIs, such as `Evaluator.evaluate()`, are called (#4686, @WeichenXu123)
- Add `pip_requirements` and `extra_pip_requirements` to `mlflow.*.log_model` and `mlflow.*.save_model` for directly specifying the pip requirements of the model to log / save (#4519, #4577, #4602, @harupy)
- Added `stdMetrics` entries to the training metrics recorded during PySpark CrossValidator autologging (#4672, @WeichenXu123)
- MLflow UI updates:
  1. Improved scalability of the parallel coordinates plot for run performance comparison,
  2. Added support for filtering runs based on their start time on the experiment page,
  3. Added a dropdown for runs table column sorting on the experiment page,
  4. Upgraded the AG Grid plugin, which is used for runs table loading on the experiment page, to version 25.0.0,
  5. Fixed a bug on the experiment page that caused the metrics section of the runs table to collapse when selecting columns from other table sections (#4712, @dbczumar)
- Added support for distributed execution to autologging for PyTorch Lightning (#4717, @dbczumar)
- Expanded R support for Model Registry functionality (#4527, @bramrodenburg)
- Added model scoring server support for defining custom prediction response wrappers (#4611, @Ark-kun)
- `mlflow.*.log_model` and `mlflow.*.save_model` now automatically infer the pip requirements of the model to log / save based on the current software environment (#4518, @harupy)
- Introduced support for running Sagemaker Batch Transform jobs with MLflow Models (#4410, #4589, @YQ-Wang)

Bug fixes and documentation updates:

- Deprecate `requirements_file` argument for `mlflow.*.save_model` and `mlflow.*.log_model` (#4620, @harupy)
- set nextPageToken to null (#4729, @harupy)
- Fix a bug in MLflow UI where the pagination token for run search is not refreshed when switching experiments (#4709, @harupy)
- Fix a bug in the model scoring server that rejected requests specifying a valid `Content-Type` header with the charset parameter (#4609, @Ark-kun)
- Fixed a bug that caused SQLAlchemy backends to exhaust DB connections. (#4663, @arpitjasa-db)
- Improve docker build procedures to raise exceptions if docker builds fail (#4610, @Ark-kun)
- Disable autologging for scikit-learn `cross_val_*` APIs, which are incompatible with autologging (#4590, @WeichenXu123)
- Deprecate MLflow Models support for fast.ai V1 (#4728, @dbczumar)
- Deprecate the old Azure ML deployment APIs `mlflow.azureml.cli.build_image` and `mlflow.azureml.build_image` (#4646, @trangevi)
- Deprecate MLflow Models support for TensorFlow < 2.0 and Keras < 2.3 (#4716, @harupy)

Small bug fixes and doc updates (#4730, #4722, #4725, #4723, #4703, #4710, #4679, #4694, #4707, #4708, #4706, #4705, #4625, #4701, #4700, #4662, #4699, #4682, #4691, #4684, #4683, #4675, #4666, #4648, #4653, #4651, #4641, #4649, #4627, #4637, #4632, #4634, #4621, #4619, #4622, #4460, #4608, #4605, #4599, #4600, #4581, #4583, #4565, #4575, #4564, #4580, #4572, #4570, #4574, #4576, #4568, #4559, #4537, #4542, @harupy; #4698, #4573, @Ark-kun; #4674, @kvmakes; #4555, @vagoston; #4644, @zhengjxu; #4690, #4588, @apurva-koti; #4545, #4631, #4734, @WeichenXu123; #4633, #4292, @shrinath-suresh; #4711, @jinzhang21; #4688, @murilommen; #4635, @ryan-duve; #4724, #4719, #4640, #4639, #4629, #4612, #4613, #4586, @dbczumar)

## 1.19.0 (2021-07-14)

MLflow 1.19.0 includes several major features and improvements:

Features:

- Add support for plotting per-class feature importance computed on linear boosters in XGBoost autologging (#4523, @dbczumar)
- Add `mlflow_create_registered_model` and `mlflow_delete_registered_model` for R to create/delete registered models.
- Add support for setting tags while resuming a run (#4497, @dbczumar)
- MLflow UI updates (#4490, @sunishsheth2009)

  - Add framework for internationalization support.
  - Move metric columns before parameter and tag columns in the runs table.
  - Change the display format of run start time to elapsed time (e.g. 3 minutes ago) from timestamp (e.g. 2021-07-14 14:02:10) in the runs table.

Bug fixes and documentation updates:

- Fix a bug causing MLflow UI to crash when sorting a column containing both `NaN` and empty values (#3409, @harupy)

Small bug fixes and doc updates (#4541, #4534, #4533, #4517, #4508, #4513, #4512, #4509, #4503, #4486, #4493, #4469, @harupy; #4458, @KasirajanA; #4501, @jimmyxu-db; #4521, #4515, @jerrylian-db; #4359, @shrinath-suresh; #4544, @WeichenXu123; #4549, @smurching; #4554, @derkomai; #4506, @tomasatdatabricks; #4551, #4516, #4494, @dbczumar; #4511, @keypointt)

## 1.18.0 (2021-06-18)

MLflow 1.18.0 includes several major features and improvements:

Features:

- Autologging performance improvements for XGBoost, LightGBM, and scikit-learn (#4416, #4473, @dbczumar)
- Add new PaddlePaddle flavor to MLflow Models (#4406, #4439, @jinminhao)
- Introduce paginated ListExperiments API (#3881, @wamartin-aml)
- Include Runtime version for MLflow Models logged on Databricks (#4421, @stevenchen-db)
- MLflow Models now log dependencies in pip requirements.txt format, in addition to existing conda format (#4409, #4422, @stevenchen-db)
- Add support for limiting the number child runs created by autologging for scikit-learn hyperparameter search models (#4382, @mohamad-arabi)
- Improve artifact upload / download performance on Databricks (#4260, @dbczumar)
- Migrate all model dependencies from conda to "pip" section (#4393, @WeichenXu123)

Bug fixes and documentation updates:

- Fix an MLflow UI bug that caused git source URIs to be rendered improperly (#4403, @takabayashi)
- Fix a bug that prevented reloading of MLflow Models based on the TensorFlow SavedModel format (#4223) (#4319, @saschaschramm)
- Fix a bug in the behavior of `KubernetesSubmittedRun.get_status()` for Kubernetes MLflow Project runs (#3962) (#4159, @jcasse)
- Fix a bug in TLS verification for MLflow artifact operations on S3 (#4047, @PeterSulcs)
- Fix a bug causing the MLflow server to crash after deletion of the default experiment (#4352, @asaf400)
- Fix a bug causing `mlflow models serve` to crash on Windows 10 (#4377, @simonvanbernem)
- Fix a crash in runs search when ordering by metric values against the MSSQL backend store (#2551) (#4238, @naor2013)
- Fix an autologging incompatibility issue with TensorFlow 2.5 (#4371, @dbczumar)
- Fix a bug in the `disable_for_unsupported_versions` autologging argument that caused library versions to be incorrectly compared (#4303, @WeichenXu123)

Small bug fixes and doc updates (#4405, @mohamad-arabi; #4455, #4461, #4459, #4464, #4453, #4444, #4449, #4301, #4424, #4418, #4417, #3759, #4398, #4389, #4386, #4385, #4384, #4380, #4373, #4378, #4372, #4369, #4348, #4364, #4363, #4349, #4350, #4174, #4285, #4341, @harupy; #4446, @kHarshit; #4471, @AveshCSingh; #4435, #4440, #4368, #4360, @WeichenXu123; #4431, @apurva-koti; #4428, @stevenchen-db; #4467, #4402, #4261, @dbczumar)

## 1.17.0 (2021-05-07)

MLflow 1.17.0 includes several major features and improvements:

Features:

- Add support for hyperparameter-tuning models to `mlflow.pyspark.ml.autolog()` (#4270, @WeichenXu123)

Bug fixes and documentation updates:

- Fix PyTorch Lightning callback definition for compatibility with PyTorch Lightning 1.3.0 (#4333, @dbczumar)
- Fix a bug in scikit-learn autologging that omitted artifacts for unsupervised models (#4325, @dbczumar)
- Support logging `datetime.date` objects as part of model input examples (#4313, @vperiyasamy)
- Implement HTTP request retries in the MLflow Java client for 500-level responses (#4311, @dbczumar)
- Include a community code of conduct (#4310, @dennyglee)

Small bug fixes and doc updates (#4276, #4263, @WeichenXu123; #4289, #4302, #3599, #4287, #4284, #4265, #4266, #4275, #4268, @harupy; #4335, #4297, @dbczumar; #4324, #4320, @tleyden)

## 1.16.0 (2021-04-22)

MLflow 1.16.0 includes several major features and improvements:

Features:

- Add `mlflow.pyspark.ml.autolog()` API for autologging of `pyspark.ml` estimators (#4228, @WeichenXu123)
- Add `mlflow.catboost.log_model`, `mlflow.catboost.save_model`, `mlflow.catboost.load_model` APIs for CatBoost model persistence (#2417, @harupy)
- Enable `mlflow.pyfunc.spark_udf` to use column names from model signature by default (#4236, @Loquats)
- Add `datetime` data type for model signatures (#4241, @vperiyasamy)
- Add `mlflow.sklearn.eval_and_log_metrics` API that computes and logs metrics for the given scikit-learn model and labeled dataset. (#4218, @alkispoly-db)

Bug fixes and documentation updates:

- Fix a database migration error for PostgreSQL (#4211, @dolfinus)
- Fix autologging silent mode bugs (#4231, @dbczumar)

Small bug fixes and doc updates (#4255, #4252, #4254, #4253, #4242, #4247, #4243, #4237, #4233, @harupy; #4225, @dmatrix; #4207, @shrinath-suresh; #4264, @WeichenXu123; #3884, #3866, #3885, @ankan94; #4274, #4216, @dbczumar)

## 1.15.0 (2021-03-26)

MLflow 1.15.0 includes several features, bug fixes and improvements. Notably, it includes a number of improvements to MLflow autologging:

Features:

- Add `silent=False` option to all autologging APIs, to allow suppressing MLflow warnings and logging statements during autologging setup and training (#4173, @dbczumar)
- Add `disable_for_unsupported_versions=False` option to all autologging APIs, to disable autologging for versions of ML frameworks that have not been explicitly tested against the current version of the MLflow client (#4119, @WeichenXu123)

Bug fixes:

- Autologged runs are now terminated when execution is interrupted via SIGINT (#4200, @dbczumar)
- The R `mlflow_get_experiment` API now returns the same tag structure as `mlflow_list_experiments` and `mlflow_get_run` (#4017, @lorenzwalthert)
- Fix bug where `mlflow.tensorflow.autolog` would previously mutate the user-specified callbacks list when fitting `tf.keras` models (#4195, @dbczumar)
- Fix bug where SQL-backed MLflow tracking server initialization failed when using the MLflow skinny client (#4161, @eedeleon)
- Model version creation (e.g. via `mlflow.register_model`) now fails if the model version status is not READY (#4114, @ankit-db)

Small bug fixes and doc updates (#4191, #4149, #4162, #4157, #4155, #4144, #4141, #4138, #4136, #4133, #3964, #4130, #4118, @harupy; #4139, @WeichenXu123; #4193, @smurching; #4029, @architkulkarni; #4134, @xhochy; #4116, @wenleix; #4160, @wentinghu; #4203, #4184, #4167, @dbczumar)

## 1.14.1 (2021-03-01)

MLflow 1.14.1 is a patch release containing the following bug fix:

- Fix issues in handling flexible numpy datatypes in TensorSpec (#4147, @arjundc-db)

## 1.14.0 (2021-02-18)

MLflow 1.14.0 includes several major features and improvements:

- MLflow's model inference APIs (`mlflow.pyfunc.predict`), built-in model serving tools (`mlflow models serve`), and model signatures now support tensor inputs. In particular, MLflow now provides built-in support for scoring PyTorch, TensorFlow, Keras, ONNX, and Gluon models with tensor inputs. For more information, see https://mlflow.org/docs/latest/models.html#deploy-mlflow-models (#3808, #3894, #4084, #4068 @wentinghu; #4041 @tomasatdatabricks, #4099, @arjundc-db)
- Add new `mlflow.shap.log_explainer`, `mlflow.shap.load_explainer` APIs for logging and loading `shap.Explainer` instances (#3989, @vivekchettiar)
- The MLflow Python client is now available with a reduced dependency set via the `mlflow-skinny` PyPI package (#4049, @eedeleon)
- Add new `RequestHeaderProvider` plugin interface for passing custom request headers with REST API requests made by the MLflow Python client (#4042, @jimmyxu-db)
- `mlflow.keras.log_model` now saves models in the TensorFlow SavedModel format by default instead of the older Keras H5 format (#4043, @harupy)
- `mlflow_log_model` now supports logging MLeap models in R (#3819, @yitao-li)
- Add `mlflow.pytorch.log_state_dict`, `mlflow.pytorch.load_state_dict` for logging and loading PyTorch state dicts (#3705, @shrinath-suresh)
- `mlflow gc` can now garbage-collect artifacts stored in S3 (#3958, @sklingel)

Bug fixes and documentation updates:

- Enable autologging for TensorFlow estimators that extend `tensorflow.compat.v1.estimator.Estimator` (#4097, @mohamad-arabi)
- Fix for universal autolog configs overriding integration-specific configs (#4093, @dbczumar)
- Allow `mlflow.models.infer_signature` to handle dataframes containing `pandas.api.extensions.ExtensionDtype` (#4069, @caleboverman)
- Fix bug where `mlflow_restore_run` doesn't propagate the `client` parameter to `mlflow_get_run` (#4003, @yitao-li)
- Fix bug where scoring on served model fails when request data contains a string that looks like URL and pandas version is later than 1.1.0 (#3921, @Secbone)
- Fix bug causing `mlflow_list_experiments` to fail listing experiments with tags (#3942, @lorenzwalthert)
- Fix bug where metrics plots are computed from incorrect target values in scikit-learn autologging (#3993, @mtrencseni)
- Remove redundant / verbose Python event logging message in autologging (#3978, @dbczumar)
- Fix bug where `mlflow_load_model` doesn't load metadata associated to MLflow model flavor in R (#3872, @yitao-li)
- Fix `mlflow.spark.log_model`, `mlflow.spark.load_model` APIs on passthrough-enabled environments against ACL'd artifact locations (#3443, @smurching)

Small bug fixes and doc updates (#4102, #4101, #4096, #4091, #4067, #4059, #4016, #4054, #4052, #4051, #4038, #3992, #3990, #3981, #3949, #3948, #3937, #3834, #3906, #3774, #3916, #3907, #3938, #3929, #3900, #3902, #3899, #3901, #3891, #3889, @harupy; #4014, #4001, @dmatrix; #4028, #3957, @dbczumar; #3816, @lorenzwalthert; #3939, @pauldj54; #3740, @jkthompson; #4070, #3946, @jimmyxu-db; #3836, @t-henri; #3982, @neo-anderson; #3972, #3687, #3922, @eedeleon; #4044, @WeichenXu123; #4063, @yitao-li; #3976, @whiteh; #4110, @tomasatdatabricks; #4050, @apurva-koti; #4100, #4084, @wentinghu; #3947, @vperiyasamy; #4021, @trangevi; #3773, @ankan94; #4090, @jinzhang21; #3918, @danielfrg)

## 1.13.1 (2020-12-30)

MLflow 1.13.1 is a patch release containing bug fixes and small changes:

- Fix bug causing Spark autologging to ignore configuration options specified by `mlflow.autolog()` (#3917, @dbczumar)
- Fix bugs causing metrics to be dropped during TensorFlow autologging (#3913, #3914, @dbczumar)
- Fix incorrect value of optimizer name parameter in autologging PyTorch Lightning (#3901, @harupy)
- Fix model registry database `allow_null_for_run_id` migration failure affecting MySQL databases (#3836, @t-henri)
- Fix failure in `transition_model_version_stage` when uncanonical stage name is passed (#3929, @harupy)
- Fix an undefined variable error causing AzureML model deployment to fail (#3922, @eedeleon)
- Reclassify scikit-learn as a pip dependency in MLflow Model conda environments (#3896, @harupy)
- Fix experiment view crash and artifact view inconsistency caused by artifact URIs with redundant slashes (#3928, @dbczumar)

## 1.13 (2020-12-22)

MLflow 1.13 includes several major features and improvements:

Features:

New fluent APIs for logging in-memory objects as artifacts:

- Add `mlflow.log_text` which logs text as an artifact (#3678, @harupy)
- Add `mlflow.log_dict` which logs a dictionary as an artifact (#3685, @harupy)
- Add `mlflow.log_figure` which logs a figure object as an artifact (#3707, @harupy)
- Add `mlflow.log_image` which logs an image object as an artifact (#3728, @harupy)

UI updates / fixes (#3867, @smurching):

- Add model version link in compact experiment table view
- Add logged/registered model links in experiment runs page view
- Enhance artifact viewer for MLflow models
- Model registry UI settings are now persisted across browser sessions
- Add model version `description` field to model version table

Autologging enhancements:

- Improve robustness of autologging integrations to exceptions (#3682, #3815, dbczumar; #3860, @mohamad-arabi; #3854, #3855, #3861, @harupy)
- Add `disable` configuration option for autologging (#3682, #3815, dbczumar; #3838, @mohamad-arabi; #3854, #3855, #3861, @harupy)
- Add `exclusive` configuration option for autologging (#3851, @apurva-koti; #3869, @dbczumar)
- Add `log_models` configuration option for autologging (#3663, @mohamad-arabi)
- Set tags on autologged runs for easy identification (and add tags to start_run) (#3847, @dbczumar)

More features and improvements:

- Allow Keras models to be saved with `SavedModel` format (#3552, @skylarbpayne)
- Add support for `statsmodels` flavor (#3304, @olbapjose)
- Add support for nested-run in mlflow R client (#3765, @yitao-li)
- Deploying a model using `mlflow.azureml.deploy` now integrates better with the AzureML tracking/registry. (#3419, @trangevi)
- Update schema enforcement to handle integers with missing values (#3798, @tomasatdatabricks)

Bug fixes and documentation updates:

- When running an MLflow Project on Databricks, the version of MLflow installed on the Databricks cluster will now match the version used to run the Project (#3880, @FlorisHoogenboom)
- Fix bug where metrics are not logged for single-epoch `tf.keras` training sessions (#3853, @dbczumar)
- Reject boolean types when logging MLflow metrics (#3822, @HCoban)
- Fix alignment of Keras / `tf.Keras` metric history entries when `initial_epoch` is different from zero. (#3575, @garciparedes)
- Fix bugs in autologging integrations for newer versions of TensorFlow and Keras (#3735, @dbczumar)
- Drop global `filterwwarnings` module at import time (#3621, @jogo)
- Fix bug that caused preexisting Python loggers to be disabled when using MLflow with the SQLAlchemyStore (#3653, @arthury1n)
- Fix `h5py` library incompatibility for exported Keras models (#3667, @tomasatdatabricks)

Small changes, bug fixes and doc updates (#3887, #3882, #3845, #3833, #3830, #3828, #3826, #3825, #3800, #3809, #3807, #3786, #3794, #3731, #3776, #3760, #3771, #3754, #3750, #3749, #3747, #3736, #3701, #3699, #3698, #3658, #3675, @harupy; #3723, @mohamad-arabi; #3650, #3655, @shrinath-suresh; #3850, #3753, #3725, @dmatrix; ##3867, #3670, #3664, @smurching; #3681, @sueann; #3619, @andrewnitu; #3837, @javierluraschi; #3721, @szczeles; #3653, @arthury1n; #3883, #3874, #3870, #3877, #3878, #3815, #3859, #3844, #3703, @dbczumar; #3768, @wentinghu; #3784, @HCoban; #3643, #3649, @arjundc-db; #3864, @AveshCSingh, #3756, @yitao-li)

## 1.12.1 (2020-11-19)

MLflow 1.12.1 is a patch release containing bug fixes and small changes:

- Fix `run_link` for cross-workspace model versions (#3681, @sueann)
- Remove hard dependency on matplotlib for sklearn autologging (#3703, @dbczumar)
- Do not disable existing loggers when initializing alembic (#3653, @arthury1n)

## 1.12.0 (2020-11-10)

MLflow 1.12.0 includes several major features and improvements, in particular a number of improvements to autologging and MLflow's Pytorch integrations:

Features:

Autologging:

- Add universal `mlflow.autolog` which enables autologging for all supported integrations (#3561, #3590, @andrewnitu)
- Add `mlflow.pytorch.autolog` API for automatic logging of metrics, params, and models from Pytorch Lightning training (#3601, @shrinath-suresh, #3636, @karthik-77). This API is also enabled by `mlflow.autolog`.
- Scikit-learn, XGBoost, and LightGBM autologging now support logging model signatures and input examples (#3386, #3403, #3449, @andrewnitu)
- `mlflow.sklearn.autolog` now supports logging metrics (e.g. accuracy) and plots (e.g. confusion matrix heat map) (#3423, #3327, @willzhan-db, @harupy)

PyTorch:

- `mlflow.pytorch.log_model`, `mlflow.pytorch.load_model` now support logging/loading TorchScript models (#3557, @shrinath-suresh)
- `mlflow.pytorch.log_model` supports passing `requirements_file` & `extra_files` arguments to log additional artifacts along with a model (#3436, @shrinath-suresh)

More features and improvements:

- Add `mlflow.shap.log_explanation` for logging model explanations generated by SHAP (#3513, @harupy)
- `log_model` and `create_model_version` now supports an `await_creation_for` argument (#3376, @andychow-db)
- Put preview paths before non-preview paths for backwards compatibility (#3648, @sueann)
- Clean up model registry endpoint and client method definitions (#3610, @sueann)
- MLflow deployments plugin now supports 'predict' CLI command (#3597, @shrinath-suresh)
- Support H2O for R (#3416, @yitao-li)
- Add `MLFLOW_S3_IGNORE_TLS` environment variable to enable skipping TLS verification of S3 endpoint (#3345, @dolfinus)

Bug fixes and documentation updates:

- Ensure that results are synced across distributed processes if ddp enabled (no-op else) (#3651, @SeanNaren)
- Remove optimizer step override to ensure that all accelerator cases are covered by base module (#3635, @SeanNaren)
- Fix `AttributeError` in keras autologgging (#3611, @sephib)
- Scikit-learn autologging: Exclude feature extraction / selection estimator (#3600, @dbczumar)
- Scikit-learn autologging: Fix behavior when a child and its parent are both patched (#3582, @dbczumar)
- Fix a bug where `lightgbm.Dataset(None)` fails after running `mlflow.lightgbm.autolog` (#3594, @harupy)
- Fix a bug where `xgboost.DMatrix(None)` fails after running `mlflow.xgboost.autolog` (#3584, @harupy)
- Pass `docker_args` in non-synchronous mlflow project runs (#3563, @alfozan)
- Fix a bug of `FTPArtifactRepository.log_artifacts` with `artifact_path` keyword argument (issue #3388) (#3391, @kzm4269)
- Exclude preprocessing & imputation steps from scikit-learn autologging (#3491, @dbczumar)
- Fix duplicate stderr logging during artifact logging and project execution in the R client (#3145, @yitao-li)
- Don't call `atexit.register(_flush_queue)` in `__main__` scope of `mlflow/tensorflow.py` (#3410, @harupy)
- Fix for restarting terminated run not setting status correctly (#3329, @apurva-koti)
- Fix model version run_link URL for some Databricks regions (#3417, @sueann)
- Skip JSON validation when endpoint is not MLflow REST API (#3405, @harupy)
- Document `mlflow-torchserve` plugin (#3634, @karthik-77)
- Add `mlflow-elasticsearchstore` to the doc (#3462, @AxelVivien25)
- Add code snippets for fluent and MlflowClient APIs (#3385, #3437, #3489 #3573, @dmatrix)
- Document `mlflow-yarn` backend (#3373, @fhoering)
- Fix a breakage in loading Tensorflow and Keras models (#3667, @tomasatdatabricks)

Small bug fixes and doc updates (#3607, #3616, #3534, #3598, #3542, #3568, #3349, #3554, #3544, #3541, #3533, #3535, #3516, #3512, #3497, #3522, #3521, #3492, #3502, #3434, #3422, #3394, #3387, #3294, #3324, #3654, @harupy; #3451, @jgc128; #3638, #3632, #3608, #3452, #3399, @shrinath-suresh; #3495, #3459, #3662, #3668, #3670 @smurching; #3488, @edgan8; #3639, @karthik-77; #3589, #3444, #3276, @lorenzwalthert; #3538, #3506, #3509, #3507, #3510, #3508, @rahulporuri; #3504, @sbrugman; #3486, #3466, @apurva-koti; #3477, @juntai-zheng; #3617, #3609, #3605, #3603, #3560, @dbczumar; #3411, @danielvdende; #3377, @willzhan-db; #3420, #3404, @andrewnitu; #3591, @mateiz; #3465, @abawchen; #3543, @emptalk; #3302, @bramrodenburg; #3468, @ghisvail; #3496, @extrospective; #3549, #3501, #3435, @yitao-li; #3243, @OlivierBondu; #3439, @andrewnitu; #3651, #3635 @SeanNaren, #3470, @ankit-db)

## 1.11.0 (2020-08-31)

MLflow 1.11.0 includes several major features and improvements:

Features:

- New `mlflow.sklearn.autolog()` API for automatic logging of metrics, params, and models from scikit-learn model training (#3287, @harupy; #3323, #3358 @dbczumar)
- Registered model & model version creation APIs now support specifying an initial `description` (#3271, @sueann)
- The R `mlflow_log_model` and `mlflow_load_model` APIs now support XGBoost models (#3085, @lorenzwalthert)
- New `mlflow.list_run_infos` fluent API for listing run metadata (#3183, @trangevi)
- Added section for visualizing and comparing model schemas to model version and model-version-comparison UIs (#3209, @zhidongqu-db)
- Enhanced support for using the model registry across Databricks workspaces: support for registering models to a Databricks workspace from outside the workspace (#3119, @sueann), tracking run-lineage of these models (#3128, #3164, @ankitmathur-db; #3187, @harupy), and calling `mlflow.<flavor>.load_model` against remote Databricks model registries (#3330, @sueann)
- UI support for setting/deleting registered model and model version tags (#3187, @harupy)
- UI support for archiving existing staging/production versions of a model when transitioning a new model version to staging/production (#3134, @harupy)

Bug fixes and documentation updates:

- Fixed parsing of MLflow project parameter values containing'=' (#3347, @dbczumar)
- Fixed a bug preventing listing of WASBS artifacts on the latest version of Azure Blob Storage (12.4.0) (#3348, @dbczumar)
- Fixed a bug where artifact locations become malformed when using an SFTP file store in Windows (#3168, @harupy)
- Fixed bug where `list_artifacts` returned incorrect results on GCS, preventing e.g. loading SparkML models from GCS (#3242, @santosh1994)
- Writing and reading artifacts via `MlflowClient` to a DBFS location in a Databricks tracking server specified through the `tracking_uri` parameter during the initialization of `MlflowClient` now works properly (#3220, @sueann)
- Fixed bug where `FTPArtifactRepository` returned artifact locations as absolute paths, rather than paths relative to the artifact repository root (#3210, @shaneing), and bug where calling `log_artifacts` against an FTP artifact location copied the logged directory itself into the FTP location, rather than the contents of the directory.
- Fixed bug where Databricks project execution failed due to passing of GET request params as part of the request body rather than as query parameters (#2947, @cdemonchy-pro)
- Fix bug where artifact viewer did not correctly render PDFs in MLflow 1.10 (#3172, @ankitmathur-db)
- Fixed parsing of `order_by` arguments to MLflow search APIs when ordering by fields whose names contain spaces (#3118, @jdlesage)
- Fixed bug where MLflow model schema enforcement raised exceptions when validating string columns using pandas >= 1.0 (#3130, @harupy)
- Fixed bug where `mlflow.spark.log_model` did not save model signature and input examples (#3151, @harupy)
- Fixed bug in runs UI where tags table did not reflect deletion of tags. (#3135, @ParseDark)
- Added example illustrating the use of RAPIDS with MLflow (#3028, @drobison00)

Small bug fixes and doc updates (#3326, #3344, #3314, #3289, #3225, #3288, #3279, #3265, #3263, #3260, #3255, #3267, #3266, #3264, #3256, #3253, #3231, #3245, #3191, #3238, #3192, #3188, #3189, #3180, #3178, #3166, #3181, #3142, #3165, #2960, #3129, #3244, #3359 @harupy; #3236, #3141, @AveshCSingh; #3295, #3163, @arjundc-db; #3241, #3200, @zhidongqu-db; #3338, #3275, @sueann; #3020, @magnus-m; #3322, #3219, @dmatrix; #3341, #3179, #3355, #3360, #3363 @smurching; #3124, @jdlesage; #3232, #3146, @ankitmathur-db; #3140, @andreakress; #3062, @cafeal; #3193, @tomasatdatabricks; 3115, @fhoering; #3328, @apurva-koti; #3046, @OlivierBondu; #3194, #3158, @dmatrix; #3250, @shivp950; #3259, @simonhessner; #3357 @dbczumar)

## 1.10.0 (2020-07-20)

MLflow 1.10.0 includes several major features and improvements, in particular the release of several new model registry Python client APIs.

Features:

- `MlflowClient.transition_model_version_stage` now supports an
  `archive_existing_versions` argument for archiving existing staging or production model
  versions when transitioning a new model version to staging or production (#3095, @harupy)
- Added `set_registry_uri`, `get_registry_uri` APIs. Setting the model registry URI causes
  fluent APIs like `mlflow.register_model` to communicate with the model registry at the specified
  URI (#3072, @sueann)
- Added paginated `MlflowClient.search_registered_models` API (#2939, #3023, #3027 @ankitmathur-db; #2966, @mparkhe)
- Added syntax highlighting when viewing text files (YAML etc) in the MLflow runs UI (#3041, @harupy)
- Added REST API and Python client support for setting and deleting tags on model versions and registered models,
  via the `MlflowClient.create_registered_model`, `MlflowClient.create_model_version`,
  `MlflowClient.set_registered_model_tag`, `MlflowClient.set_model_version_tag`,
  `MlflowClient.delete_registered_model_tag`, and `MlflowClient.delete_model_version_tag` APIs (#3094, @zhidongqu-db)

Bug fixes and documentation updates:

- Removed usage of deprecated `aws ecr get-login` command in `mlflow.sagemaker` (#3036, @mrugeles)
- Fixed bug where artifacts could not be viewed and downloaded from the artifact UI when using
  Azure Blob Storage (#3014, @Trollgeir)
- Databricks credentials are now propagated to the project subprocess when running MLflow projects
  within a notebook (#3035, @smurching)
- Added docs explaining how to fetching an MLflow model from the model registry (#3000, @andychow-db)

Small bug fixes and doc updates (#3112, #3102, #3089, #3103, #3096, #3090, #3049, #3080, #3070, #3078, #3083, #3051, #3050, #2875, #2982, #2949, #3121 @harupy; #3082, @ankitmathur-db; #3084, #3019, @smurching)

## 1.9.1 (2020-06-25)

MLflow 1.9.1 is a patch release containing a number of bug-fixes and improvements:

Bug fixes and improvements:

- Fixes `AttributeError` when pickling an instance of the Python `MlflowClient` class (#2955, @Polyphenolx)
- Fixes bug that prevented updating model-version descriptions in the model registry UI (#2969, @AnastasiaKol)
- Fixes bug where credentials were not properly propagated to artifact CLI commands when logging artifacts from Java to the DatabricksArtifactRepository (#3001, @dbczumar)
- Removes use of new Pandas API in new MLflow model-schema functionality, so that it can be used with older Pandas versions (#2988, @aarondav)

Small bug fixes and doc updates (#2998, @dbczumar; #2999, @arjundc-db)

## 1.9.0 (2020-06-19)

MLflow 1.9.0 includes numerous major features and improvements, and a breaking change to
experimental APIs:

Breaking Changes:

- The `new_name` argument to `MlflowClient.update_registered_model`
  has been removed. Call `MlflowClient.rename_registered_model` instead. (#2946, @mparkhe)
- The `stage` argument to `MlflowClient.update_model_version`
  has been removed. Call `MlflowClient.transition_model_version_stage` instead. (#2946, @mparkhe)

Features (MLflow Models and Flavors)

- `log_model` and `save_model` APIs now support saving model signatures (the model's input and output schema)
  and example input along with the model itself (#2698, #2775, @tomasatdatabricks). Model signatures are used
  to reorder and validate input fields when scoring/serving models using the pyfunc flavor, `mlflow models`
  CLI commands, or `mlflow.pyfunc.spark_udf` (#2920, @tomasatdatabricks and @aarondav)
- Introduce fastai model persistence and autologging APIs under `mlflow.fastai` (#2619, #2689 @antoniomdk)
- Add pluggable `mlflow.deployments` API and CLI for deploying models to custom serving tools, e.g. RedisAI
  (#2327, @hhsecond)
- Enables loading and scoring models whose conda environments include dependencies in conda-forge (#2797, @dbczumar)
- Add support for scoring ONNX-persisted models that return Python lists (#2742, @andychow-db)

Features (MLflow Projects)

- Add plugin interface for executing MLflow projects against custom backends (#2566, @jdlesage)
- Add ability to specify additional cluster-wide Python and Java libraries when executing
  MLflow projects remotely on Databricks (#2845, @pogil)
- Allow running MLflow projects against remote artifacts stored in any location with a corresponding
  ArtifactRepository implementation (Azure Blob Storage, GCS, etc) (#2774, @trangevi)
- Allow MLflow projects running on Kubernetes to specify a different tracking server to log to via the
  `KUBE_MLFLOW_TRACKING_URI` for passing a different tracking server to the kubernetes job (#2874, @catapulta)

Features (UI)

- Significant performance and scalability improvements to metric comparison and scatter plots in
  the UI (#2447, @mjlbach)
- The main MLflow experiment list UI now includes a link to the model registry UI (#2805, @zhidongqu-db),
- Enable viewing PDFs logged as artifacts from the runs UI (#2859, @ankmathur96)
- UI accessibility improvements: better color contrast (#2872, @Zangr), add child roles to DOM elements (#2871, @Zangr)

Features (Tracking Client and Server)

- Adds ability to pass client certs as part of REST API requests when using the tracking or model
  registry APIs. (#2843, @PhilipMay)
- New community plugin: support for storing artifacts in Aliyun (Alibaba Cloud) (#2917, @SeaOfOcean)
- Infer and set content type and encoding of objects when logging models and artifacts to S3 (#2881, @hajapy)
- Adds support for logging artifacts to HDFS Federation ViewFs (#2782, @fhoering)
- Add healthcheck endpoint to the MLflow server at `/health` (#2725, @crflynn)
- Improves performance of default file-based tracking storage backend by using LibYAML (if installed)
  to read experiment and run metadata (#2707, @Higgcz)

Bug fixes and documentation updates:

- Several UI fixes: remove margins around icon buttons (#2827, @harupy),
  fix alignment issues in metric view (#2811, @zhidongqu-db), add handling of `NaN`
  values in metrics plot (#2773, @dbczumar), truncate run ID in the run name when
  comparing multiple runs (#2508, @harupy)
- Database engine URLs are no longer logged when running `mlflow db upgrade` (#2849, @hajapy)
- Updates `log_artifact`, `log_model` APIs to consistently use posix paths, rather than OS-dependent
  paths, when computing artifact subpaths. (#2784, @mikeoconnor0308)
- Fix `ValueError` when scoring `tf.keras` 1.X models using `mlflow.pyfunc.predict` (#2762, @juntai-zheng)
- Fixes conda environment activation bug when running MLflow projects on Windows (#2731, @MynherVanKoek)
- `mlflow.end_run` will now clear the active run even if the run cannot be marked as
  terminated (e.g. because it's been deleted), (#2693, @ahmed-shariff)
- Add missing documentation for `mlflow.spacy` APIs (#2771, @harupy)

Small bug fixes and doc updates (#2919, @willzhan-db; #2940, #2942, #2941, #2943, #2927, #2929, #2926, #2914, #2928, #2913, #2852, #2876, #2808, #2810, #2442, #2780, #2758, #2732, #2734, #2431, #2733, #2716, @harupy; #2915, #2897, @jwgwalton; #2856, @jkthompson; #2962, @hhsecond; #2873, #2829, #2582, @dmatrix; #2908, #2865, #2880, #2866, #2833, #2785, #2723, @smurching; #2906, @dependabot[bot]; #2724, @aarondav; #2896, @ezeeetm; #2864, @tallen94; #2726, @crflynn; #2710, #2951 @mparkhe; #2935, #2921, @ankitmathur-db; #2963, #2739, @dbczumar; #2853, @stat4jason; #2709, #2792, @juntai-zheng @juntai-zheng; #2749, @HiromuHota; #2957, #2911, #2718, @arjundc-db; #2885, @willzhan-db; #2803, #2761, @pogil; #2392, @jnmclarty; #2794, @Zethson; #2766, #2916 @shubham769)

## 1.8.0 (2020-04-16)

MLflow 1.8.0 includes several major features and improvements:

Features:

- Added `mlflow.azureml.deploy` API for deploying MLflow models to AzureML (#2375 @csteegz, #2711, @akshaya-a)
- Added support for case-sensitive LIKE and case-insensitive ILIKE queries (e.g. `'params.framework LIKE '%sklearn%'`) with the SearchRuns API & UI when running against a SQLite backend (#2217, @t-henri; #2708, @mparkhe)
- Improved line smoothing in MLflow metrics UI using exponential moving averages (#2620, @Valentyn1997)
- Added `mlflow.spacy` module with support for logging and loading spaCy models (#2242, @arocketman)
- Parameter values that differ across runs are highlighted in run comparison UI (#2565, @gabrielbretschner)
- Added ability to compare source runs associated with model versions from the registered model UI (#2537, @juntai-zheng)
- Added support for alphanumerical experiment IDs in the UI. (#2568, @jonas)
- Added support for passing arguments to `docker run` when running docker-based MLflow projects (#2608, @ksanjeevan)
- Added Windows support for `mlflow sagemaker build-and-push-container` CLI & API (#2500, @AndreyBulezyuk)
- Improved performance of reading experiment data from local filesystem when LibYAML is installed (#2707, @Higgcz)
- Added a healthcheck endpoint to the REST API server at `/health` that always returns a 200 response status code, to be used to verify health of the server (#2725, @crflynn)
- MLflow metrics UI plots now scale to rendering thousands of points using scattergl (#2447, @mjlbach)

Bug fixes:

- Fixed CLI summary message in `mlflow azureml build_image` CLI (#2712, @dbczumar)
- Updated `examples/flower_classifier/score_images_rest.py` with multiple bug fixes (#2647, @tfurmston)
- Fixed pip not found error while packaging models via `mlflow models build-docker` (#2699, @HiromuHota)
- Fixed bug in `mlflow.tensorflow.autolog` causing erroneous deletion of TensorBoard logging directory (#2670, @dbczumar)
- Fixed a bug that truncated the description of the `mlflow gc` subcommand in `mlflow --help` (#2679, @dbczumar)
- Fixed bug where `mlflow models build-docker` was failing due to incorrect Miniconda download URL (#2685, @michaeltinsley)
- Fixed a bug in S3 artifact logging functionality where `MLFLOW_S3_ENDPOINT_URL` was ignored (#2629, @poppash)
- Fixed a bug where Sqlite in-memory was not working as a tracking backend store by modifying DB upgrade logic (#2667, @dbczumar)
- Fixed a bug to allow numerical parameters with values >= 1000 in R `mlflow::mlflow_run()` API (#2665, @lorenzwalthert)
- Fixed a bug where AWS creds was not found in the Windows platform due path differences (#2634, @AndreyBulezyuk)
- Fixed a bug to add pip when necessary in `_mlflow_conda_env` (#2646, @tfurmston)
- Fixed error code to be more meaningful if input to model version is incorrect (#2625, @andychow-db)
- Fixed multiple bugs in model registry (#2638, @aarondav)
- Fixed support for conda env dicts with `mlflow.pyfunc.log_model` (#2618, @dbczumar)
- Fixed a bug where hiding the start time column in the UI would also hide run selection checkboxes (#2559, @harupy)

Documentation updates:

- Added links to source code to mlflow.org (#2627, @harupy)
- Documented fix for pandas-records payload (#2660, @SaiKiranBurle)
- Fixed documentation bug in TensorFlow `load_model` utility (#2666, @pogil)
- Added the missing Model Registry description and link on the first page (#2536, @dmatrix)
- Added documentation for expected datatype for step argument in `log_metric` to match REST API (#2654, @mparkhe)
- Added usage of the model registry to the `log_model` function in `sklearn_elasticnet_wine/train.py` example (#2609, @netanel246)

Small bug fixes and doc updates (#2594, @Trollgeir; #2703,#2709, @juntai-zheng; #2538, #2632, @keigohtr; #2656, #2553, @lorenzwalthert; #2622, @pingsutw; #1391, @sueann; #2613, #2598, #2534, #2723, @smurching; #2652, #2710, @mparkhe; #2706, #2653, #2639, @tomasatdatabricks; #2611, @9dogs; #2700, #2705, @aarondav; #2675, #2540, @mengxr; #2686, @RensDimmendaal; #2694, #2695, #2532, @dbczumar; #2733, #2716, @harupy; #2726, @crflynn; #2582, #2687, @dmatrix)

## 1.7.2 (2020-03-20)

MLflow 1.7.2 is a patch release containing a minor change:

- Pin alembic version to 1.4.1 or below to prevent pep517-related installation errors
  (#2612, @smurching)

## 1.7.1 (2020-03-17)

MLflow 1.7.1 is a patch release containing bug fixes and small changes:

- Remove usage of Nonnull annotations and findbugs dependency in Java package (#2583, @mparkhe)
- Add version upper bound (<=1.3.13) to sqlalchemy dependency in Python package (#2587, @smurching)

Other bugfixes and doc updates (#2595, @mparkhe; #2567, @jdlesage)

## 1.7.0 (2020-03-02)

MLflow 1.7.0 includes several major features and improvements, and some notable breaking changes:

MLflow support for Python 2 is now deprecated and will be dropped in a future release. At that
point, existing Python 2 workflows that use MLflow will continue to work without modification, but
Python 2 users will no longer get access to the latest MLflow features and bugfixes. We recommend
that you upgrade to Python 3 - see https://docs.python.org/3/howto/pyporting.html for a migration
guide.

Breaking changes to Model Registry REST APIs:

Model Registry REST APIs have been updated to be more consistent with the other MLflow APIs. With
this release Model Registry APIs are intended to be stable until the next major version.

- Python and Java client APIs for Model Registry have been updated to use the new REST APIs. When using an MLflow client with a server using updated REST endpoints, you won't need to change any code but will need to upgrade to a new client version. The client APIs contain deprecated arguments, which for this release are backward compatible, but will be dropped in future releases. (#2457, @tomasatdatabricks; #2502, @mparkhe).
- The Model Registry UI has been updated to use the new REST APIs (#2476 @aarondav; #2507, @mparkhe)

Other Features:

- Ability to click through to individual runs from metrics plot (#2295, @harupy)
- Added `mlflow gc` CLI for permanent deletion of runs (#2265, @t-henri)
- Metric plot state is now captured in page URLs for easier link sharing (#2393, #2408, #2498 @smurching; #2459, @harupy)
- Added experiment management to MLflow UI (create/rename/delete experiments) (#2348, @ggliem)
- Ability to search for experiments by name in the UI (#2324, @ggliem)
- MLflow UI page titles now reflect the content displayed on the page (#2420, @AveshCSingh)
- Added a new `LogModel` REST API endpoint for capturing model metadata, and call it from the Python and R clients (#2369, #2430, #2468 @tomasatdatabricks)
- Java Client API to download model artifacts from Model Registry (#2308, @andychow-db)

Bug fixes and documentation updates:

- Updated Model Registry documentation page with code snippets and examples (#2493, @dmatrix; #2517, @harupy)
- Better error message for Model Registry, when using incompatible backend server (#2456, @aarondav)
- matplotlib is no longer required to use XGBoost and LightGBM autologging (#2423, @harupy)
- Fixed bug where matplotlib figures were not closed in XGBoost and LightGBM autologging (#2386, @harupy)
- Fixed parameter reading logic to support param values with newlines in FileStore (#2376, @dbczumar)
- Improve readability of run table column selector nodes (#2388, @dbczumar)
- Validate experiment name supplied to `UpdateExperiment` REST API endpoint (#2357, @ggliem)
- Fixed broken MLflow DB README link in CLI docs (#2377, @dbczumar)
- Change copyright year across docs to 2020 (#2349, @ParseThis)

Small bug fixes and doc updates (#2378, #2449, #2402, #2397, #2391, #2387, #2523, #2527 @harupy; #2314, @juntai-zheng; #2404, @andychow-db; #2343, @pogil; #2366, #2370, #2364, #2356, @AveshCSingh; #2373, #2365, #2363, @smurching; #2358, @jcuquemelle; #2490, @RensDimmendaal; #2506, @dbczumar; #2234 @Zangr; #2359 @lbernickm; #2525, @mparkhe)

## 1.6.0 (2020-01-29)

MLflow 1.6.0 includes several new features, including a better runs table interface, a utility for easier parameter tuning, and automatic logging from XGBoost, LightGBM, and Spark. It also implements a long-awaited fix allowing @ symbols in database URLs. A complete list is below:

Features:

- Adds a new runs table column view based on `ag-grid` which adds functionality for nested runs, serverside sorting, column reordering, highlighting, and more. (#2251, @Zangr)
- Adds contour plot to the run comparison page to better support parameter tuning (#2225, @harupy)
- If you use EarlyStopping with Keras autologging, MLflow now automatically captures the best model trained and the associated metrics (#2301, #2219, @juntai-zheng)
- Adds autologging functionality for LightGBM and XGBoost flavors to log feature importance, metrics per iteration, the trained model, and more. (#2275, #2238, @harupy)
- Adds an experimental mlflow.spark.autolog() API for automatic logging of Spark datasource information to the current active run. (#2220, @smurching)
- Optimizes the file store to load less data from disk for each operation (#2339, @jonas)
- Upgrades from ubuntu:16.04 to ubuntu:18.04 when building a Docker image with `mlflow models build-docker` (#2256, @andychow-db)

Bug fixes and documentation updates:

- Fixes bug when running server against database URLs with @ symbols (#2289, @hershaw)
- Fixes model Docker image build on Windows (#2257, @jahas)
- Documents the SQL Server plugin (#2320, @avflor)
- Adds a help file for the R package (#2259, @lorenzwalthert)
- Adds an example of using the Search API to find the best performing model (#2313, @AveshCSingh)
- Documents how to write and use MLflow plugins (#2270, @smurching)

Small bug fixes and doc updates (#2293, #2328, #2244, @harupy; #2269, #2332, #2306, #2307, #2292, #2267, #2191, #2231, @juntai-zheng; #2325, @shubham769; #2291, @sueann; #2315, #2249, #2288, #2278, #2253, #2181, @smurching; #2342, @tomasatdatabricks; #2245, @dependabot[bot]; #2338, @jcuquemelle; #2285, @avflor; #2340, @pogil; #2237, #2226, #2243, #2272, #2286, @dbczumar; #2281, @renaudhager; #2246, @avaucher; #2258, @lorenzwalthert; #2261, @smith-kyle; 2352, @dbczumar)

## 1.5.0 (2019-12-19)

MLflow 1.5.0 includes several major features and improvements:

New Model Flavors and Flavor Updates:

- New support for a LightGBM flavor (#2136, @harupy)
- New support for a XGBoost flavor (#2124, @harupy)
- New support for a Gluon flavor and autologging (#1973, @cosmincatalin)
- Runs automatically created by `mlflow.tensorflow.autolog()` and `mlflow.keras.autolog()` (#2088) are now automatically ended after training and/or exporting your model. See the [`docs`](https://mlflow.org/docs/latest/tracking.html#automatic-logging-from-tensorflow-and-keras-experimental) for more details (#2094, @juntai-zheng)

More features and improvements:

- When using the `mlflow server` CLI command, you can now expose metrics on `/metrics` for Prometheus via the optional --activate-parameter argument (#2097, @t-henri)
- The `mlflow ui` CLI command now has a `--host`/`-h` option to specify user-input IPs to bind to (#2176, @gandroz)
- MLflow now supports pulling Git submodules while using MLflow Projects (#2103, @badc0re)
- New `mlflow models prepare-env` command to do any preparation necessary to initialize an environment. This allows distinguishing configuration and user errors during predict/serve time (#2040, @aarondav)
- TensorFlow.Keras and Keras parameters are now logged by `autolog()` (#2119, @juntai-zheng)
- MLflow `log_params()` will recognize Spark ML params as keys and will now extract only the name attribute (#2064, @tomasatdatabricks)
- Exposes `mlflow.tracking.is_tracking_uri_set()` (#2026, @fhoering)
- The artifact image viewer now displays "Loading..." when it is loading an image (#1958, @harupy)
- The artifact image view now supports animated GIFs (#2070, @harupy)
- Adds ability to mount volumes and specify environment variables when using mlflow with docker (#1994, @nlml)
- Adds run context for detecting job information when using MLflow tracking APIs within Databricks Jobs. The following job types are supported: notebook jobs, Python Task jobs (#2205, @dbczumar)
- Performance improvement when searching for runs (#2030, #2059, @jcuquemelle; #2195, @rom1504)

Bug fixes and documentation updates:

- Fixed handling of empty directories in FS based artifact repositories (#1891, @tomasatdatabricks)
- Fixed `mlflow.keras.save_model()` usage with DBFS (#2216, @andychow-db)
- Fixed several build issues for the Docker image (#2107, @jimthompson5802)
- Fixed `mlflow_list_artifacts()` (R package) (#2200, @lorenzwalthert)
- Entrypoint commands of Kubernetes jobs are now shell-escaped (#2160, @zanitete)
- Fixed project run Conda path issue (#2147, @Zangr)
- Fixed spark model load from model repository (#2175, @tomasatdatabricks)
- Stripped "dev" suffix from PySpark versions (#2137, @dbczumar)
- Fixed note editor on the experiment page (#2054, @harupy)
- Fixed `models serve`, `models predict` CLI commands against models:/ URIs (#2067, @smurching)
- Don't unconditionally format values as metrics in generic HtmlTableView component (#2068, @smurching)
- Fixed remote execution from Windows using posixpath (#1996, @aestene)
- Add XGBoost and LightGBM examples (#2186, @harupy)
- Add note about active run instantiation side effect in fluent APIs (#2197, @andychow-db)
- The tutorial page has been refactored to be be a 'Tutorials and Examples' page (#2182, @juntai-zheng)
- Doc enhancements for XGBoost and LightGBM flavors (#2170, @harupy)
- Add doc for XGBoost flavor (#2167, @harupy)
- Updated `active_run()` docs to clarify it cannot be used accessing current run data (#2138, @juntai-zheng)
- Document models:/ scheme for URI for load_model methods (#2128, @stbof)
- Added an example using Prophet via pyfunc (#2043, @dr3s)
- Added and updated some screenshots and explicit steps for the model registry (#2086, @stbof)

Small bug fixes and doc updates (#2142, #2121, #2105, #2069, #2083, #2061, #2022, #2036, #1972, #2034, #1998, #1959, @harupy; #2202, @t-henri; #2085, @stbof; #2098, @AdamBarnhard; #2180, #2109, #1977, #2039, #2062, @smurching; #2013, @aestene; #2146, @joelcthomas; #2161, #2120, #2100, #2095, #2088, #2076, #2057, @juntai-zheng; #2077, #2058, #2027, @sueann; #2149, @zanitete; #2204, #2188, @andychow-db; #2110, #2053, @jdlesage; #2003, #1953, #2004, @Djailla; #2074, @nlml; #2116, @Silas-Asamoah; #1104, @jimthompson5802; #2072, @cclauss; #2221, #2207, #2157, #2132, #2114, #2063, #2065, #2055, @dbczumar; #2033, @cthoyt; #2048, @philip-khor; #2002, @jspoorta; #2000, @christang; #2078, @dennyglee; #1986, @vguerra; #2020, @dependabot[bot])

## 1.4.0 (2019-10-30)

MLflow 1.4.0 includes several major features:

- Model Registry (Beta). Adds an experimental model registry feature, where you can manage, version, and keep lineage of your production models. (#1943, @mparkhe, @Zangr, @sueann, @dbczumar, @smurching, @gioa, @clemens-db, @pogil, @mateiz; #1988, #1989, #1995, #2021, @mparkhe; #1983, #1982, #1967, @dbczumar)
- TensorFlow updates

  - MLflow Keras model saving, loading, and logging has been updated to be compatible with TensorFlow 2.0. (#1927, @juntai-zheng)
  - Autologging for `tf.estimator` and `tf.keras` models has been updated to be compatible with TensorFlow 2.0. The same functionalities of autologging in TensorFlow 1.x are available in TensorFlow 2.0, namely when fitting `tf.keras` models and when exporting saved `tf.estimator` models. (#1910, @juntai-zheng)
  - Examples and READMEs for both TensorFlow 1.X and TensorFlow 2.0 have been added to `mlflow/examples/tensorflow`. (#1946, @juntai-zheng)

More features and improvements:

- [API] Add functions `get_run`, `get_experiment`, `get_experiment_by_name` to the fluent API (#1923, @fhoering)
- [UI] Use Plotly as artifact image viewer, which allows zooming and panning (#1934, @harupy)
- [UI] Support deleting tags from the run details page (#1933, @harupy)
- [UI] Enable scrolling to zoom in metric and run comparison plots (#1929, @harupy)
- [Artifacts] Add support of viewfs URIs for HDFS federation for artifacts (#1947, @t-henri)
- [Models] Spark UDFs can now be called with struct input if the underlying spark implementation supports it. The data is passed as a pandas DataFrame with column names matching those in the struct. (#1882, @tomasatdatabricks)
- [Models] Spark models will now load faster from DFS by skipping unnecessary copies (#2008, @tomasatdatabricks)

Bug fixes and documentation updates:

- [Projects] Make detection of `MLproject` files case-insensitive (#1981, @smurching)
- [UI] Fix a bug where viewing metrics containing forward-slashes in the name would break the MLflow UI (#1968, @smurching)
- [CLI] `models serve` command now works in Windows (#1949, @rboyes)
- [Scoring] Fix a dependency installation bug in Java MLflow model scoring server (#1913, @smurching)

Small bug fixes and doc updates (#1932, #1935, @harupy; #1907, @marnixkoops; #1911, @HackyRoot; #1931, @jmcarp; #2007, @deniskovalenko; #1966, #1955, #1952, @Djailla; #1915, @sueann; #1978, #1894, @smurching; #1940, #1900, #1904, @mparkhe; #1914, @jerrygb; #1857, @mengxr; #2009, @dbczumar)

## 1.3 (2019-09-30)

MLflow 1.3.0 includes several major features and improvements:

Features:

- The Python client now supports logging & loading models using TensorFlow 2.0 (#1872, @juntai-zheng)
- Significant performance improvements when fetching runs and experiments in MLflow servers that use SQL database-backed storage (#1767, #1878, #1805 @dbczumar)
- New `GetExperimentByName` REST API endpoint, used in the Python client to speed up `set_experiment` and `get_experiment_by_name` (#1775, @smurching)
- New `mlflow.delete_run`, `mlflow.delete_experiment` fluent APIs in the Python client(#1396, @MerelTheisenQB)
- New CLI command (`mlflow experiments csv`) to export runs of an experiment into a CSV (#1705, @jdlesage)
- Directories can now be logged as artifacts via `mlflow.log_artifact` in the Python fluent API (#1697, @apurva-koti)
- HTML and geojson artifacts are now rendered in the run UI (#1838, @sim-san; #1803, @spadarian)
- Keras autologging support for `fit_generator` Keras API (#1757, @charnger)
- MLflow models packaged as docker containers can be executed via Google Cloud Run (#1778, @ngallot)
- Artifact storage configurations are propagated to containers when executing docker-based MLflow projects locally (#1621, @nlaille)
- The Python, Java, R clients and UI now retry HTTP requests on 429 (Too Many Requests) errors (#1846, #1851, #1858, #1859 @tomasatdatabricks; #1847, @smurching)

Bug fixes and documentation updates:

- The R `mlflow_list_artifact` API no longer throws when listing artifacts for an empty run (#1862, @smurching)
- Fixed a bug preventing running the MLflow server against an MS SQL database (#1758, @sifanLV)
- MLmodel files (artifacts) now correctly display in the run UI (#1819, @ankitmathur-db)
- The Python `mlflow.start_run` API now throws when resuming a run whose experiment ID differs from the
  active experiment ID set via `mlflow.set_experiment` (#1820, @mcminnra).
- `MlflowClient.log_metric` now logs metric timestamps with millisecond (as opposed to second) resolution (#1804, @ustcscgyer)
- Fixed bugs when listing (#1800, @ahutterTA) and downloading (#1890, @jdlesage) artifacts stored in HDFS.
- Fixed a bug preventing Kubernetes Projects from pushing to private Docker repositories (#1788, @dbczumar)
- Fixed a bug preventing deploying Spark models to AzureML (#1769, @Ben-Epstein)
- Fixed experiment id resolution in projects (#1715, @drewmcdonald)
- Updated parallel coordinates plot to show all fields available in compared runs (#1753, @mateiz)
- Streamlined docs for getting started with hosted MLflow (#1834, #1785, #1860 @smurching)

Small bug fixes and doc updates (#1848, @pingsutw; #1868, @iver56; #1787, @apurvakoti; #1741, #1737, @apurva-koti; #1876, #1861, #1852, #1801, #1754, #1726, #1780, #1807 @smurching; #1859, #1858, #1851, @tomasatdatabricks; #1841, @ankitmathur-db; #1744, #1746, #1751, @mateiz; #1821, #1730, @dbczumar; #1727, cfmcgrady; #1716, @axsaucedo; #1714, @fhoering; #1405, @ancasarb; #1502, @jimthompson5802; #1720, jke-zq; #1871, @mehdi254; #1782, @stbof)

## 1.2 (2019-08-09)

MLflow 1.2 includes the following major features and improvements:

- Experiments now have editable tags and descriptions (#1630, #1632, #1678, @ankitmathur-db)
- Search latency has been significantly reduced in the SQLAlchemyStore (#1660, @t-henri)

**More features and improvements**

- Backend stores now support run tag values up to 5000 characters in length. Some store implementations may support longer tag values (#1687, @ankitmathur-db)
- Gunicorn options can now be configured for the `mlflow models serve` CLI with the `GUNICORN_CMD_ARGS` environment variable (#1557, @LarsDu)
- Jsonnet artifacts can now be previewed in the UI (#1683, @ankitmathur-db)
- Adds an optional `python_version` argument to `mlflow_install` for specifying the Python version (e.g. "3.5") to use within the conda environment created for installing the MLflow CLI. If `python_version` is unspecified, `mlflow_install` defaults to using Python 3.6. (#1722, @smurching)

**Bug fixes and documentation updates**

- [Tracking] The Autologging feature is now more resilient to tracking errors (#1690, @apurva-koti)
- [Tracking] The `runs` field in in the `GetExperiment.Response` proto has been deprecated & will be removed in MLflow 2.0. Please use the `Search Runs` API for fetching runs instead (#1647, @dbczumar)
- [Projects] Fixed a bug that prevented docker-based MLflow Projects from logging artifacts to the `LocalArtifactRepository` (#1450, @nlaille)
- [Projects] Running MLflow projects with the `--no-conda` flag in R no longer requires Anaconda to be installed (#1650, @spadarian)
- [Models/Scoring] Fixed a bug that prevented Spark UDFs from being loaded on Databricks (#1658, @smurching)
- [UI] AJAX requests made by the MLflow Server Frontend now specify correct MIME-Types (#1679, @ynotzort)
- [UI] Previews now render correctly for artifacts with uppercase file extensions (e.g., `.JSON`, `.YAML`) (#1664, @ankitmathur-db)
- [UI] Fixed a bug that caused search API errors to surface a Niagara Falls page (#1681, @dbczumar)
- [Installation] MLflow dependencies are now selected properly based on the target installation platform (#1643, @akshaya-a)
- [UI] Fixed a bug where the "load more" button in the experiment view did not appear on browsers in Windows (#1718, @Zangr)

Small bug fixes and doc updates (#1663, #1719, @dbczumar; #1693, @max-allen-db; #1695, #1659, @smurching; #1675, @jdlesage; #1699, @ankitmathur-db; #1696, @aarondav; #1710, #1700, #1656, @apurva-koti)

## 1.1 (2019-07-22)

MLflow 1.1 includes several major features and improvements:

In MLflow Tracking:

- Experimental support for autologging from Tensorflow and Keras. Using `mlflow.tensorflow.autolog()` will enable automatic logging of metrics and optimizer parameters from TensorFlow to MLflow. The feature will work with TensorFlow versions `1.12 <= v < 2.0`. (#1520, #1601, @apurva-koti)
- Parallel coordinates plot in the MLflow compare run UI. Adds out of the box support for a parallel coordinates plot. The plot allows users to observe relationships between a n-dimensional set of parameters to metrics. It visualizes all runs as lines that are color-coded based on the value of a metric (e.g. accuracy), and shows what parameter values each run took on. (#1497, @Zangr)
- Pandas based search API. Adds the ability to return the results of a search as a pandas dataframe using the new `mlflow.search_runs` API. (#1483, #1548, @max-allen-db)
- Java fluent API. Adds a new set of APIs to create and log to MLflow runs. This API contrasts with the existing low level `MlflowClient` API which simply wraps the REST APIs. The new fluent API allows you to create and log runs similar to how you would using the Python fluent API. (#1508, @andrewmchen)
- Run tags improvements. Adds the ability to add and edit tags from the run view UI, delete tags from the API, and view tags in the experiment search view. (#1400, #1426, @Zangr; #1548, #1558, @ankitmathur-db)
- Search API improvements. Adds order by and pagination to the search API. Pagination allows you to read a large set of runs in small page sized chunks. This allows clients and backend implementations to handle an unbounded set of runs in a scalable manner. (#1444, @sueann; #1437, #1455, #1482, #1485, #1542, @aarondav; #1567, @max-allen-db; #1217, @mparkhe)
- Windows support for running the MLflow tracking server and UI. (#1080, @akshaya-a)

In MLflow Projects:

- Experimental support to run Docker based MLprojects in Kubernetes. Adds the first fully open source remote execution backend for MLflow projects. With this, you can leverage elastic compute resources managed by kubernetes for their ML training purposes. For example, you can run grid search over a set of hyperparameters by running several instances of an MLproject in parallel. (#1181, @marcusrehm, @tomasatdatabricks, @andrewmchen; #1566, @stbof, @dbczumar; #1574 @dbczumar)

**More features and improvements**

In MLflow Tracking:

- Paginated "load more" and backend sorting for experiment search view UI. This change allows the UI to scalably display the sorted runs from large experiments. (#1564, @Zangr)
- Search results are encoded in the URL. This allows you to share searches through their URL and to deep link to them. (#1416, @apurva-koti)
- Ability to serve MLflow UI behind `jupyter-server-proxy` or outside of the root path `/`. Previous to MLflow 1.1, the UI could only be hosted on `/` since the Javascript makes requests directly to `/ajax-api/...`. With this patch, MLflow will make requests to `ajax-api/...` or a path relative to where the HTML is being served. (#1413, @xhochy)

In MLflow Models:

- Update `mlflow.spark.log_model()` to accept descendants of pyspark.Model (#1519, @ankitmathur-db)
- Support for saving custom Keras models with `custom_objects`. This field is semantically equivalent to custom_objects parameter of `keras.models.load_model()` function (#1525, @ankitmathur-db)
- New more performant split orient based input format for pyfunc scoring server (#1479, @lennon310)
- Ability to specify gunicorn server options for pyfunc scoring server built with `mlflow models build-docker`. #1428, @lennon310)

**Bug fixes and documentation updates**

- [Tracking] Fix database migration for MySQL. `mlflow db upgrade` should now work for MySQL backends. (#1404, @sueann)
- [Tracking] Make CLI `mlflow server` and `mlflow ui` commands to work with SQLAlchemy URIs that specify a database driver. (#1411, @sueann)
- [Tracking] Fix usability bugs related to FTP artifact repository. (#1398, @kafendt; #1421, @nlaille)
- [Tracking] Return appropriate HTTP status codes for MLflowException (#1434, @max-allen-db)
- [Tracking] Fix sorting by user ID in the experiment search view. (#1401, @andrewmchen)
- [Tracking] Allow calling log_metric with NaNs and infs. (#1573, @tomasatdatabricks)
- [Tracking] Fixes an infinite loop in downloading artifacts logged via dbfs and retrieved via S3. (#1605, @sueann)
- [Projects] Docker projects should preserve directory structure (#1436, @ahutterTA)
- [Projects] Fix conda activation for newer versions of conda. (#1576, @avinashraghuthu, @smurching)
- [Models] Allow you to log Tensorflow keras models from the `tf.keras` module. (#1546, @tomasatdatabricks)

Small bug fixes and doc updates (#1463, @mateiz; #1641, #1622, #1418, @sueann; #1607, #1568, #1536, #1478, #1406, #1408, @smurching; #1504, @LizaShak; #1490, @acroz; #1633, #1631, #1603, #1589, #1569, #1526, #1446, #1438, @apurva-koti; #1456, @Taur1ne; #1547, #1495, @aarondav; #1610, #1600, #1492, #1493, #1447, @tomasatdatabricks; #1430, @javierluraschi; #1424, @nathansuh; #1488, @henningsway; #1590, #1427, @Zangr; #1629, #1614, #1574, #1521, #1522, @dbczumar; #1577, #1514, @ankitmathur-db; #1588, #1566, @stbof; #1575, #1599, @max-allen-db; #1592, @abaveja313; #1606, @andrewmchen)

## 1.0 (2019-06-03)

MLflow 1.0 includes many significant features and improvements. From this version, MLflow is no longer beta, and all APIs except those marked as experimental are intended to be stable until the next major version. As such, this release includes a number of breaking changes.

Major features, improvements, and breaking changes

- Support for recording, querying, and visualizing metrics along a new "step" axis (x coordinate), providing increased flexibility for examining model performance relative to training progress. For example, you can now record performance metrics as a function of the number of training iterations or epochs. MLflow 1.0's enhanced metrics UI enables you to visualize the change in a metric's value as a function of its step, augmenting MLflow's existing UI for plotting a metric's value as a function of wall-clock time. (#1202, #1237, @dbczumar; #1132, #1142, #1143, @smurching; #1211, #1225, @Zangr; #1372, @stbof)
- Search improvements. MLflow 1.0 includes additional support in both the API and UI for searching runs within a single experiment or a group of experiments. The search filter API supports a simplified version of the `SQL WHERE` clause. In addition to searching using run's metrics and params, the API has been enhanced to support a subset of run attributes as well as user and [system tags](https://mlflow.org/docs/latest/tracking.html#system-tags). For details see [Search syntax](https://mlflow.org/docs/latest/search-syntax.html#syntax) and [examples for programmatically searching runs](https://mlflow.org/docs/latest/search-syntax.html#programmatically-searching-runs). (#1245, #1272, #1323, #1326, @mparkhe; #1052, @Zangr; #1363, @aarondav)
- Logging metrics in batches. MLflow 1.0 now has a `runs/log-batch` REST API endpoint for logging multiple metrics, params, and tags in a single API request. The endpoint useful for performant logging of multiple metrics at the end of a model training epoch (see [example](https://github.com/mlflow/mlflow/blob/bb8c7602dcb6a3a8786301fe6b98f01e8d3f288d/examples/hyperparam/search_hyperopt.py#L161)), or logging of many input model parameters at the start of training. You can call this batched-logging endpoint from Python (`mlflow.log_metrics`, `mlflow.log_params`, `mlflow.set_tags`), R (`mlflow_log_batch`), and Java (`MlflowClient.logBatch`). (#1214, @dbczumar; see 0.9.1 and 0.9.0 for other changes)
- Windows support for MLflow Tracking. The Tracking portion of the MLflow client is now supported on Windows. (#1171, @eedeleon, @tomasatdatabricks)
- HDFS support for artifacts. Hadoop artifact repository with Kerberos authorization support was added, so you can use HDFS to log and retrieve models and other artifacts. (#1011, @jaroslawk)
- CLI command to build Docker images for serving. Added an `mlflow models build-docker` CLI command for building a Docker image capable of serving an MLflow model. The model is served at port 8080 within the container by default. Note that this API is experimental and does not guarantee that the arguments nor format of the Docker container will remain the same. (#1329, @smurching, @tomasatdatabricks)
- New `onnx` model flavor for saving, loading, and evaluating ONNX models with MLflow. ONNX flavor APIs are available in the `mlflow.onnx` module. (#1127, @avflor, @dbczumar; #1388, #1389, @dbczumar)
- Major breaking changes:

  - Some of the breaking changes involve database schema changes in the SQLAlchemy tracking store. If your database instance's schema is not up-to-date, MLflow will issue an error at the start-up of `mlflow server` or `mlflow ui`. To migrate an existing database to the newest schema, you can use the `mlflow db upgrade` CLI command. (#1155, #1371, @smurching; #1360, @aarondav)
  - [Installation] The MLflow Python package no longer depends on `scikit-learn`, `mleap`, or `boto3`. If you want to use the `scikit-learn` support, the `MLeap` support, or `s3` artifact repository / `sagemaker` support, you will have to install these respective dependencies explicitly. (#1223, @aarondav)
  - [Artifacts] In the Models API, an artifact's location is now represented as a URI. See the [documentation](https://mlflow.org/docs/latest/tracking.html#artifact-locations) for the list of accepted URIs. (#1190, #1254, @dbczumar; #1174, @dbczumar, @sueann; #1206, @tomasatdatabricks; #1253, @stbof)

    - The affected methods are:

      - Python: `<model-type>.load_model`, `azureml.build_image`, `sagemaker.deploy`, `sagemaker.run_local`, `pyfunc._load_model_env`, `pyfunc.load_pyfunc`, and `pyfunc.spark_udf`
      - R: `mlflow_load_model`, `mlflow_rfunc_predict`, `mlflow_rfunc_serve`
      - CLI: `mlflow models serve`, `mlflow models predict`, `mlflow sagemaker`, `mlflow azureml` (with the new `--model-uri` option)

    - To allow referring to artifacts in the context of a run, MLflow introduces a new URI scheme of the form `runs:/<run_id>/relative/path/to/artifact`. (#1169, #1175, @sueann)

  - [CLI] `mlflow pyfunc` and `mlflow rfunc` commands have been unified as `mlflow models` (#1257, @tomasatdatabricks; #1321, @dbczumar)
  - [CLI] `mlflow artifacts download`, `mlflow artifacts download-from-uri` and `mlflow download` commands have been consolidated into `mlflow artifacts download` (#1233, @sueann)
  - [Runs] Expose `RunData` fields (`metrics`, `params`, `tags`) as dictionaries. Note that the `mlflow.entities.RunData` constructor still accepts lists of `metric`/`param`/`tag` entities. (#1078, @smurching)
  - [Runs] Rename `run_uuid` to `run_id` in Python, Java, and REST API. Where necessary, MLflow will continue to accept `run_uuid` until MLflow 1.1. (#1187, @aarondav)

Other breaking changes

CLI:

- The `--file-store` option is deprecated in `mlflow server` and `mlflow ui` commands. (#1196, @smurching)
- The `--host` and `--gunicorn-opts` options are removed in the `mlflow ui` command. (#1267, @aarondav)
- Arguments to `mlflow experiments` subcommands, notably `--experiment-name` and `--experiment-id` are now options (#1235, @sueann)
- `mlflow sagemaker list-flavors` has been removed (#1233, @sueann)

Tracking:

- The `user` property of `Run`s has been moved to tags (similarly, the `run_name`, `source_type`, `source_name` properties were moved to tags in 0.9.0). (#1230, @acroz; #1275, #1276, @aarondav)
- In R, the return values of experiment CRUD APIs have been updated to more closely match the REST API. In particular, `mlflow_create_experiment` now returns a string experiment ID instead of an experiment, and the other APIs return NULL. (#1246, @smurching)
- `RunInfo.status`'s type is now string. (#1264, @mparkhe)
- Remove deprecated `RunInfo` properties from `start_run`. (#1220, @aarondav)
- As deprecated in 0.9.1 and before, the `RunInfo` fields `run_name`, `source_name`, `source_version`, `source_type`, and `entry_point_name` and the `SearchRuns` field `anded_expressions` have been removed from the REST API and Python, Java, and R tracking client APIs. They are still available as tags, documented in the REST API documentation. (#1188, @aarondav)

Models and deployment:

- In Python, require arguments as keywords in `log_model`, `save_model` and `add_to_model` methods in the `tensorflow` and `mleap` modules to avoid breaking changes in the future (#1226, @sueann)
- Remove the unsupported `jars` argument from ``spark.log_model` in Python (#1222, @sueann)
- Introduce `pyfunc.load_model` to be consistent with other Models modules. `pyfunc.load_pyfunc` will be deprecated in the near future. (#1222, @sueann)
- Rename `dst_path` parameter in `pyfunc.save_model` to `path` (#1221, @aarondav)
- R flavors refactor (#1299, @kevinykuo)

  - `mlflow_predict()` has been added in favor of `mlflow_predict_model()` and `mlflow_predict_flavor()` which have been removed.
  - `mlflow_save_model()` is now a generic and `mlflow_save_flavor()` is no longer needed and has been removed.
  - `mlflow_predict()` takes `...` to pass to underlying predict methods.
  - `mlflow_load_flavor()` now has the signature `function(flavor, model_path)` and flavor authors should implement `mlflow_load_flavor.mlflow_flavor_{FLAVORNAME}`. The flavor argument is inferred from the inputs of user-facing `mlflow_load_model()` and does not need to be explicitly provided by the user.

Projects:

- Remove and rename some `projects.run` parameters for generality and consistency. (#1222, @sueann)
- In R, the `mlflow_run` API for running MLflow projects has been modified to more closely reflect the Python `mlflow.run` API. In particular, the order of the `uri` and `entry_point` arguments has been reversed and the `param_list` argument has been renamed to `parameters`. (#1265, @smurching)

R:

- Remove `mlflow_snapshot` and `mlflow_restore_snapshot` APIs. Also, the `r_dependencies` argument used to specify the path to a packrat r-dependencies.txt file has been removed from all APIs. (#1263, @smurching)
- The `mlflow_cli` and `crate` APIs are now private. (#1246, @smurching)

Environment variables:

- Prefix environment variables with "MLFLOW\_" (#1268, @aarondav). Affected variables are:

  - [Tracking] `_MLFLOW_SERVER_FILE_STORE`, `_MLFLOW_SERVER_ARTIFACT_ROOT`, `_MLFLOW_STATIC_PREFIX`
  - [SageMaker] `MLFLOW_SAGEMAKER_DEPLOY_IMG_URL`, `MLFLOW_DEPLOYMENT_FLAVOR_NAME`
  - [Scoring] `MLFLOW_SCORING_SERVER_MIN_THREADS`, `MLFLOW_SCORING_SERVER_MAX_THREADS`

More features and improvements

- [Tracking] Non-default driver support for SQLAlchemy backends: `db+driver` is now a valid tracking backend URI scheme (#1297, @drewmcdonald; #1374, @mparkhe)
- [Tracking] Validate backend store URI before starting tracking server (#1218, @luke-zhu, @sueann)
- [Tracking] Add `GetMetricHistory` client API in Python and Java corresponding to the REST API. (#1178, @smurching)
- [Tracking] Add `view_type` argument to `MlflowClient.list_experiments()` in Python. (#1212, @smurching)
- [Tracking] Dictionary values provided to `mlflow.log_params` and `mlflow.set_tags` in Python can now be non-string types (e.g., numbers), and they are automatically converted to strings. (#1364, @aarondav)
- [Tracking] R API additions to be at parity with REST API and Python (#1122, @kevinykuo)
- [Tracking] Limit number of results returned from `SearchRuns` API and UI for faster load (#1125, @mparkhe; #1154, @andrewmchen)
- [Artifacts] To avoid having many copies of large model files in serving, `ArtifactRepository.download_artifacts` no longer copies local artifacts (#1307, @andrewmchen; #1383, @dbczumar)
- [Artifacts/Projects] Support GCS in download utilities. `gs://bucket/path` files are now supported by the `mlflow artifacts download` CLI command and as parameters of type `path` in MLProject files. (#1168, @drewmcdonald)
- [Models] All Python models exported by MLflow now declare `mlflow` as a dependency by default. In addition, we introduce a flag `--install-mlflow` users can pass to `mlflow models serve` and `mlflow models predict` methods to force installation of the latest version of MLflow into the model's environment. (#1308, @tomasatdatabricks)
- [Models] Update model flavors to lazily import dependencies in Python. Modules that define Model flavors now import extra dependencies such as `tensorflow`, `scikit-learn`, and `pytorch` inside individual _methods_, ensuring that these modules can be imported and explored even if the dependencies have not been installed on your system. Also, the `DEFAULT_CONDA_ENVIRONMENT` module variable has been replaced with a `get_default_conda_env()` function for each flavor. (#1238, @dbczumar)
- [Models] It is now possible to pass extra arguments to `mlflow.keras.load_model` that will be passed through to `keras.load_model`. (#1330, @yorickvP)
- [Serving] For better performance, switch to `gunicorn` for serving Python models. This does not change the user interface. (#1322, @tomasatdatabricks)
- [Deployment] For SageMaker, use the uniquely-generated model name as the S3 bucket prefix instead of requiring one. (#1183, @dbczumar)
- [REST API] Add support for API paths without the `preview` component. The `preview` paths will be deprecated in a future version of MLflow. (#1236, @mparkhe)

Bug fixes and documentation updates

- [Tracking] Log metric timestamps in milliseconds by default (#1177, @smurching; #1333, @dbczumar)
- [Tracking] Fix bug when deserializing integer experiment ID for runs in `SQLAlchemyStore` (#1167, @smurching)
- [Tracking] Ensure unique constraint names in MLflow tracking database (#1292, @smurching)
- [Tracking] Fix base64 encoding for basic auth in R tracking client (#1126, @freefrag)
- [Tracking] Correctly handle `file:` URIs for the `-—backend-store-uri` option in `mlflow server` and `mlflow ui` CLI commands (#1171, @eedeleon, @tomasatdatabricks)
- [Artifacts] Update artifact repository download methods to return absolute paths (#1179, @dbczumar)
- [Artifacts] Make FileStore respect the default artifact location (#1332, @dbczumar)
- [Artifacts] Fix `log_artifact` failures due to existing directory on FTP server (#1327, @kafendt)
- [Artifacts] Fix GCS artifact logging of subdirectories (#1285, @jason-huling)
- [Projects] Fix bug not sharing `SQLite` database file with Docker container (#1347, @tomasatdatabricks; #1375, @aarondav)
- [Java] Mark `sendPost` and `sendGet` as experimental (#1186, @aarondav)
- [Python/CLI] Mark `azureml.build_image` as experimental (#1222, #1233 @sueann)
- [Docs] Document public MLflow environment variables (#1343, @aarondav)
- [Docs] Document MLflow system tags for runs (#1342, @aarondav)
- [Docs] Autogenerate CLI documentation to include subcommands and descriptions (#1231, @sueann)
- [Docs] Update run selection description in `mlflow_get_run` in R documentation (#1258, @dbczumar)
- [Examples] Update examples to reflect API changes (#1361, @tomasatdatabricks; #1367, @mparkhe)

Small bug fixes and doc updates (#1359, #1350, #1331, #1301, #1270, #1271, #1180, #1144, #1135, #1131, #1358, #1369, #1368, #1387, @aarondav; #1373, @akarloff; #1287, #1344, #1309, @stbof; #1312, @hchiuzhuo; #1348, #1349, #1294, #1227, #1384, @tomasatdatabricks; #1345, @withsmilo; #1316, @ancasarb; #1313, #1310, #1305, #1289, #1256, #1124, #1097, #1162, #1163, #1137, #1351, @smurching; #1319, #1244, #1224, #1195, #1194, #1328, @dbczumar; #1213, #1200, @Kublai-Jing; #1304, #1320, @andrewmchen; #1311, @Zangr; #1306, #1293, #1147, @mateiz; #1303, @gliptak; #1261, #1192, @eedeleon; #1273, #1259, @kevinykuo; #1277, #1247, #1243, #1182, #1376, @mparkhe; #1210, @vgod-dbx; #1199, @ashtuchkin; #1176, #1138, #1365, @sueann; #1157, @cclauss; #1156, @clemens-db; #1152, @pogil; #1146, @srowen; #875, #1251, @jimthompson5802)

## 0.9.1 (2019-04-21)

MLflow 0.9.1 is a patch release on top of 0.9.0 containing mostly bug fixes and internal improvements. We have also included a one breaking API change in preparation for additions in MLflow 1.0 and later. This release also includes significant improvements to the Search API.

Breaking changes:

- [Tracking] Generalized experiment_id to string (from a long) to be more permissive of different ID types in different backend stores. While breaking for the REST API, this change is backwards compatible for python and R clients. (#1067, #1034 @eedeleon)

More features and improvements:

- [Search/API] Moving search filters into a query string based syntax, with Java client, Python client, and UI support. This also improves quote, period, and special character handling in query strings and adds the ability to search on tags in filter string. (#1042, #1055, #1063, #1068, #1099, #1106 @mparkhe; #1025 @andrewmchen; #1060 @smurching)
- [Tracking] Limits and validations to batch-logging APIs in OSS server (#958 @smurching)
- [Tracking/Java] Java client API for batch-logging (#1081 @mparkhe)
- [Tracking] Improved consistency of handling multiple metric values per timestamp across tracking stores (#972, #999 @dbczumar)

Bug fixes and documentation updates:

- [Tracking/Python] Reintroduces the parent_run_id argument to MlflowClient.create_run. This API is planned for removal in MLflow 1.0 (#1137 @smurching)
- [Tracking/Python] Provide default implementations of AbstractStore log methods (#1051 @acroz)
- [R] (Released on CRAN as MLflow 0.9.0.1) Small bug fixes with R (#1123 @smurching; #1045, #1017, #1019, #1039, #1048, #1098, #1101, #1107, #1108, #1119 @tomasatdatabricks)

Small bug fixes and doc updates (#1024, #1029 @bayethiernodiop; #1075 @avflor; #968, #1010, #1070, #1091, #1092 @smurching; #1004, #1085 @dbczumar; #1033, #1046 @sueann; #1053 @tomasatdatabricks; #987 @hanyucui; #935, #941 @jimthompson5802; #963 @amilbourne; #1016 @andrewmchen; #991 @jaroslawk; #1007 @mparkhe)

## 0.9.0.1 (2019-04-09)

Bugfix release (PyPI only) with the following changes:

- Rebuilt MLflow JS assets to fix an issue where form input was broken in MLflow 0.9.0 (identified
  in #1056, #1113 by @shu-yusa, @timothyjlaurent)

  0.9.0 (2019-03-13)

Major features:

- Support for running MLflow Projects in Docker containers. This allows you to include non-Python dependencies in their project environments and provides stronger isolation when running projects. See the [Projects documentation](https://mlflow.org/docs/latest/projects.html) for more information. (#555, @marcusrehm; #819, @mparkhe; #970, @dbczumar)
- Database stores for the MLflow Tracking Server. Support for a scalable and performant backend store was one of the top community requests. This feature enables you to connect to local or remote SQLAlchemy-compatible databases (currently supported flavors include MySQL, PostgreSQL, SQLite, and MS SQL) and is compatible with file backed store. See the [Tracking Store documentation](https://mlflow.org/docs/latest/tracking.html#storage) for more information. (#756, @AndersonReyes; #800, #844, #847, #848, #860, #868, #975, @mparkhe; #980, @dbczumar)
- Simplified custom Python model packaging. You can easily include custom preprocessing and postprocessing logic, as well as data dependencies in models with the `python_function` flavor using updated `mlflow.pyfunc` Python APIs. For more information, see the [Custom Python Models documentation](https://mlflow.org/docs/latest/models.html#custom-python-models). (#791, #792, #793, #830, #910, @dbczumar)
- Plugin systems allowing third party libraries to extend MLflow functionality. The [proposal document](https://gist.github.com/zblz/9e337a55a7ba73314890be68370fa69a) gives the full detail of the three main changes:

  - You can register additional providers of tracking stores using the `mlflow.tracking_store` entrypoint. (#881, @zblz)
  - You can register additional providers of artifact repositories using the `mlflow.artifact_repository` entrypoint. (#882, @mociarain)
  - The logic generating run metadata from the run context (e.g. `source_name`, `source_version`) has been refactored into an extendable system of run context providers. Plugins can register additional providers using the `mlflow.run_context_provider` entrypoint, which add to or overwrite tags set by the base library. (#913, #926, #930, #978, @acroz)

- Support for HTTP authentication to the Tracking Server in the R client. Now you can connect to secure Tracking Servers using credentials set in environment variables, or provide custom plugins for setting the credentials. As an example, this release contains a Databricks plugin that can detect existing Databricks credentials to allow you to connect to the Databricks Tracking Server. (#938, #959, #992, @tomasatdatabricks)

Breaking changes:

- [Scoring] The `pyfunc` scoring server now expects requests with the `application/json` content type to contain json-serialized pandas dataframes in the split format, rather than the records format. See the [documentation on deployment](https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-as-a-local-rest-api-endpoint) for more detail. (#960, @dbczumar) Also, when reading the pandas dataframes from JSON, the scoring server no longer automatically infers data types as it can result in unintentional conversion of data types (#916, @mparkhe).
- [API] Remove `GetMetric` & `GetParam` from the REST API as they are subsumed by `GetRun`. (#879, @aarondav)

More features and improvements:

- [UI] Add a button for downloading artifacts (#967, @mateiz)
- [CLI] Add CLI commands for runs: now you can `list`, `delete`, `restore`, and `describe` runs through the CLI (#720, @DorIndivo)
- [CLI] The `run` command now can take `--experiment-name` as an argument, as an alternative to the `--experiment-id` argument. You can also choose to set the `_EXPERIMENT_NAME_ENV_VAR` environment variable instead of passing in the value explicitly. (#889, #894, @mparkhe)
- [Examples] Add Image classification example with Keras. (#743, @tomasatdatabricks )
- [Artifacts] Add `get_artifact_uri()` and `_download_artifact_from_uri` convenience functions (#779)
- [Artifacts] Allow writing Spark models directly to the target artifact store when possible (#808, @smurching)
- [Models] PyTorch model persistence improvements to allow persisting definitions and dependencies outside the immediate scope:
  - Add a `code_paths` parameter to `mlflow.pytorch.save_model` and `mlflow.pytorch.log_model` to allow external module dependencies to be specified as paths to python files. (#842, @dbczumar)
  - Improve `mlflow.pytorch.save_model` to capture class definitions from notebooks and the `__main__` scope (#851, #861, @dbczumar)
- [Runs/R] Allow client to infer context info when creating new run in fluent API (#958, @tomasatdatabricks)
- [Runs/UI] Support Git Commit hyperlink for Gitlab and Bitbucket. Previously the clickable hyperlink was generated only for Github pages. (#901)
- [Search]/API] Allow param value to have any content, not just alphanumeric characters, `.`, and `-` (#788, @mparkhe)
- [Search/API] Support "filter" string in the `SearchRuns` API. Corresponding UI improvements are planned for the future (#905, @mparkhe)
- [Logging] Basic support for LogBatch. NOTE: The feature is currently experimental and the behavior is expected to change in the near future. (#950, #951, #955, #1001, @smurching)

Bug fixes and documentation updates:

- [Artifacts] Fix empty-file upload to DBFS in `log_artifact` and `log_artifacts` (#895, #818, @smurching)
- [Artifacts] S3 artifact store: fix path resolution error when artifact root is bucket root (#928, @dbczumar)
- [UI] Fix a bug with Databricks notebook URL links (#891, @smurching)
- [Export] Fix for missing run name in csv export (#864, @jimthompson5802)
- [Example] Correct missing tensorboardX module error in PyTorch example when running in MLflow Docker container (#809, @jimthompson5802)
- [Scoring/R] Fix local serving of rfunc models (#874, @kevinykuo)
- [Docs] Improve flavor-specific documentation in Models documentation (#909, @dbczumar)

Small bug fixes and doc updates (#822, #899, #787, #785, #780, #942, @hanyucui; #862, #904, #954, #806, #857, #845, @stbof; #907, #872, @smurching; #896, #858, #836, #859, #923, #939, #933, #931, #952, @dbczumar; #880, @zblz; #876, @acroz; #827, #812, #816, #829, @jimthompson5802; #837, #790, #897, #974, #900, @mparkhe; #831, #798, @aarondav; #814, @sueann; #824, #912, @mateiz; #922, #947, @tomasatdatabricks; #795, @KevYuen; #676, @mlaradji; #906, @4n4nd; #777, @tmielika; #804, @alkersan)

## 0.8.2 (2019-01-28)

MLflow 0.8.2 is a patch release on top of 0.8.1 containing only bug fixes and no breaking changes or features.

Bug fixes:

- [Python API] CloudPickle has been added to the set of MLflow library dependencies, fixing missing import errors when attempting to save models (#777, @tmielika)
- [Python API] Fixed a malformed logging call that prevented `mlflow.sagemaker.push_image_to_ecr()` invocations from succeeding (#784, @jackblandin)
- [Models] PyTorch models can now be saved with code dependencies, allowing model classes to be loaded successfully in new environments (#842, #836, @dbczumar)
- [Artifacts] Fixed a timeout when logging zero-length files to DBFS artifact stores (#818, @smurching)

Small docs updates (#845, @stbof; #840, @grahamhealy20; #839, @wilderrodrigues)

## 0.8.1 (2018-12-21)

MLflow 0.8.1 introduces several significant improvements:

- Improved UI responsiveness and load time, especially when displaying experiments containing hundreds to thousands of runs.
- Improved visualizations, including interactive scatter plots for MLflow run comparisons
- Expanded support for scoring Python models as Spark UDFs. For more information, see the [updated documentation for this feature](https://mlflow.org/docs/latest/models.html#export-a-python-function-model-as-an-apache-spark-udf).
- By default, saved models will now include a Conda environment specifying all of the dependencies necessary for loading them in a new environment.

Features:

- [API/CLI] Support for running MLflow projects from ZIP files (#759, @jmorefieldexpe)
- [Python API] Support for passing model conda environments as dictionaries to `save_model` and `log_model` functions (#748, @dbczumar)
- [Models] Default Anaconda environments have been added to many Python model flavors. By default, models produced by `save_model` and `log_model` functions will include an environment that specifies all of the versioned dependencies necessary to load and serve the models. Previously, users had to specify these environments manually. (#705, #707, #708, #749, @dbczumar)
- [Scoring] Support for synchronous deployment of models to SageMaker (#717, @dbczumar)
- [Tracking] Include the Git repository URL as a tag when tracking an MLflow run within a Git repository (#741, @whiletruelearn, @mateiz)
- [UI] Improved runs UI performance by using a react-virtualized table to optimize row rendering (#765, #762, #745, @smurching)
- [UI] Significant performance improvements for rendering run metrics, tags, and parameter information (#764, #747, @smurching)
- [UI] Scatter plots, including run comparison plots, are now interactive (#737, @mateiz)
- [UI] Extended CSRF support by allowing the MLflow UI server to specify a set of expected headers that clients should set when making AJAX requests (#733, @aarondav)

Bug fixes and documentation updates:

- [Python/Scoring] MLflow Python models that produce Pandas DataFrames can now be evaluated as Spark UDFs correctly. Spark UDF outputs containing multiple columns of primitive types are now supported (#719, @tomasatdatabricks)
- [Scoring] Fixed a serialization error that prevented models served with Azure ML from returning Pandas DataFrames (#754, @dbczumar)
- [Docs] New example demonstrating how the MLflow REST API can be used to create experiments and log run information (#750, kjahan)
- [Docs] R documentation has been updated for clarity and style consistency (#683, @stbof)
- [Docs] Added clarification about user setup requirements for executing remote MLflow runs on Databricks (#736, @andyk)

Small bug fixes and doc updates (#768, #715, @smurching; #728, dodysw; #730, mshr-h; #725, @kryptec; #769, #721, @dbczumar; #714, @stbof)

## 0.8.0 (2018-11-08)

MLflow 0.8.0 introduces several major features:

- Dramatically improved UI for comparing experiment run results:

  - Metrics and parameters are by default grouped into a single column, to avoid an explosion of mostly-empty columns. Individual metrics and parameters can be moved into their own column to help compare across rows.
  - Runs that are "nested" inside other runs (e.g., as part of a hyperparameter search or multistep workflow) now show up grouped by their parent run, and can be expanded or collapsed altogether. Runs can be nested by calling `mlflow.start_run` or `mlflow.run` while already within a run.
  - Run names (as opposed to automatically generated run UUIDs) now show up instead of the run ID, making comparing runs in graphs easier.
  - The state of the run results table, including filters, sorting, and expanded rows, is persisted in browser local storage, making it easier to go back and forth between an individual run view and the table.

- Support for deploying models as Docker containers directly to Azure Machine Learning Service Workspace (as opposed to the previously-recommended solution of Azure ML Workbench).

Breaking changes:

- [CLI] `mlflow sklearn serve` has been removed in favor of `mlflow pyfunc serve`, which takes the same arguments but works against any pyfunc model (#690, @dbczumar)

Features:

- [Scoring] pyfunc server and SageMaker now support the pandas "split" JSON format in addition to the "records" format. The split format allows the client to specify the order of columns, which is necessary for some model formats. We recommend switching client code over to use this new format (by sending the Content-Type header `application/json; format=pandas-split`), as it will become the default JSON format in MLflow 0.9.0. (#690, @dbczumar)
- [UI] Add compact experiment view (#546, #620, #662, #665, @smurching)
- [UI] Add support for viewing & tracking nested runs in experiment view (#588, @andrewmchen; #618, #619, @aarondav)
- [UI] Persist experiments view filters and sorting in browser local storage (#687, @smurching)
- [UI] Show run name instead of run ID when present (#476, @smurching)
- [Scoring] Support for deploying Models directly to Azure Machine Learning Service Workspace (#631, @dbczumar)
- [Server/Python/Java] Add `rename_experiment` to Tracking API (#570, @aarondav)
- [Server] Add `get_experiment_by_name` to RestStore (#592, @dmarkhas)
- [Server] Allow passing gunicorn options when starting mlflow server (#626, @mparkhe)
- [Python] Cloudpickle support for sklearn serialization (#653, @dbczumar)
- [Artifacts] FTP artifactory store added (#287, @Shenggan)

Bug fixes and documentation updates:

- [Python] Update TensorFlow integration to match API provided by other flavors (#612, @dbczumar; #670, @mlaradji)
- [Python] Support for TensorFlow 1.12 (#692, @smurching)
- [R] Explicitly loading Keras module at predict time no longer required (#586, @kevinykuo)
- [R] pyfunc serve can correctly load models saved with the R Keras support (#634, @tomasatdatabricks)
- [R] Increase network timeout of calls to the RestStore from 1 second to 60 seconds (#704, @aarondav)
- [Server] Improve errors returned by RestStore (#582, @andrewmchen; #560, @smurching)
- [Server] Deleting the default experiment no longer causes it to be immediately recreated (#604, @andrewmchen; #641, @schipiga)
- [Server] Azure Blob Storage artifact repo supports Windows paths (#642, @marcusrehm)
- [Server] Improve behavior when environment and run files are corrupted (#632, #654, #661, @mparkhe)
- [UI] Improve error page when viewing nonexistent runs or views (#600, @andrewmchen; #560, @andrewmchen)
- [UI] UI no longer throws an error if all experiments are deleted (#605, @andrewmchen)
- [Docs] Include diagram of workflow for multistep example (#581, @dennyglee)
- [Docs] Add reference tags and R and Java APIs to tracking documentation (#514, @stbof)
- [Docs/R] Use CRAN installation (#686, @javierluraschi)

Small bug fixes and doc updates (#576, #594, @javierluraschi; #585, @kevinykuo; #593, #601, #611, #650, #669, #671, #679, @dbczumar; #607, @suzil; #583, #615, @andrewmchen; #622, #681, @aarondav; #625, @pogil; #589, @tomasatdatabricks; #529, #635, #684, @stbof; #657, @mvsusp; #682, @mateiz; #678, vfdev-5; #596, @yutannihilation; #663, @smurching)

## 0.7.0 (2018-10-01)

MLflow 0.7.0 introduces several major features:

- An R client API (to be released on CRAN soon)
- Support for deleting runs (API + UI)
- UI support for adding notes to a run

The release also includes bugfixes and improvements across the Python and Java clients, tracking UI,
and documentation.

Breaking changes:

- [Python] The per-flavor implementation of load_pyfunc has been made private (#539, @tomasatdatabricks)
- [REST API, Java] logMetric now accepts a double metric value instead of a float (#566, @aarondav)

Features:

- [R] Support for R (#370, #471, @javierluraschi; #548 @kevinykuo)
- [UI] Add support for adding notes to Runs (#396, @aadamson)
- [Python] Python API, REST API, and UI support for deleting Runs (#418, #473, #526, #579 @andrewmchen)
- [Python] Set a tag containing the branch name when executing a branch of a Git project (#469, @adrian555)
- [Python] Add a set_experiment API to activate an experiment before starting runs (#462, @mparkhe)
- [Python] Add arguments for specifying a parent run to tracking & projects APIs (#547, @andrewmchen)
- [Java] Add Java set tag API (#495, @smurching)
- [Python] Support logging a conda environment with sklearn models (#489, @dbczumar)
- [Scoring] Support downloading MLflow scoring JAR from Maven during scoring container build (#507, @dbczumar)

Bug fixes:

- [Python] Print errors when the Databricks run fails to start (#412, @andrewmchen)
- [Python] Fix Spark ML PyFunc loader to work on Spark driver (#480, @tomasatdatabricks)
- [Python] Fix Spark ML load_pyfunc on distributed clusters (#490, @tomasatdatabricks)
- [Python] Fix error when downloading artifacts from a run's artifact root (#472, @dbczumar)
- [Python] Fix DBFS upload file-existence-checking logic during Databricks project execution (#510, @smurching)
- [Python] Support multi-line and unicode tags (#502, @mparkhe)
- [Python] Add missing DeleteExperiment, RestoreExperiment implementations in the Python REST API client (#551, @mparkhe)
- [Scoring] Convert Spark DataFrame schema to an MLeap schema prior to serialization (#540, @dbczumar)
- [UI] Fix bar chart always showing in metric view (#488, @smurching)

Small bug fixes and doc updates (#467 @drorata; #470, #497, #508, #518 @dbczumar; #455, #466, #492, #504, #527 @aarondav; #481, #475, #484, #496, #515, #517, #498, #521, #522, #573 @smurching; #477 @parkerzf; #494 @jainr; #501, #531, #532, #552 @mparkhe; #503, #520 @dmatrix; #509, #532 @tomasatdatabricks; #484, #486 @stbof; #533, #534 @javierluraschi; #542 @GCBallesteros; #511 @AdamBarnhard)

## 0.6.0 (2018-09-10)

MLflow 0.6.0 introduces several major features:

- A Java client API, available on Maven
- Support for saving and serving SparkML models as MLeap for low-latency serving
- Support for tagging runs with metadata, during and after the run completion
- Support for deleting (and restoring deleted) experiments

In addition to these features, there are a host of improvements and bugfixes to the REST API, Python API, tracking UI, and documentation. The [examples](https://github.com/mlflow/mlflow/tree/master/examples) subdirectory has also been revamped to make it easier to jump in, and examples demonstrating multistep workflows and hyperparameter tuning have been added.

Breaking changes:

We fixed a few inconsistencies in the the `mlflow.tracking` API, as introduced in 0.5.0:

- `MLflowService` has been renamed `MlflowClient` (#461, @mparkhe)
- You get an `MlflowClient` by calling `mlflow.tracking.MlflowClient()` (previously, this was `mlflow.tracking.get_service()`) (#461, @mparkhe)
- `MlflowService.list_runs` was changed to `MlflowService.list_run_infos` to reflect the information actually returned by the call. It now returns a `RunInfo` instead of a `Run` (#334, @aarondav)
- `MlflowService.log_artifact` and `MlflowService.log_artifacts` now take a `run_id` instead of `artifact_uri`. This now matches `list_artifacts` and `download_artifacts` (#444, @aarondav)

Features:

- Java client API added with support for the MLflow Tracking API (analogous to `mlflow.tracking`), allowing users to create and manage experiments, runs, and artifacts. The release includes a [usage example](https://github.com/mlflow/mlflow/blob/master/mlflow/java/client/src/main/java/org/mlflow/tracking/samples/QuickStartDriver.java>)and [Javadocs](https://mlflow.org/docs/latest/java_api/index.html). The client is published to Maven under `mlflow:mlflow` (#380, #394, #398, #409, #410, #430, #452, @aarondav)
- SparkML models are now also saved in MLeap format (https://github.com/combust/mleap), when applicable. Model serving platforms can choose to serve using this format instead of the SparkML format to dramatically decrease prediction latency. SageMaker now does this by default (#324, #327, #331, #395, #428, #435, #438, @dbczumar)
- [API] Experiments can now be deleted and restored via REST API, Python Tracking API, and MLflow CLI (#340, #344, #367, @mparkhe)
- [API] Tags can now be set via a SetTag API, and they have been moved to `RunData` from `RunInfo` (#342, @aarondav)
- [API] Added `list_artifacts` and `download_artifacts` to `MlflowService` to interact with a run's artifactory (#350, @andrewmchen)
- [API] Added `get_experiment_by_name` to Python Tracking API, and equivalent to Java API (#373, @vfdev-5)
- [API/Python] Version is now exposed via `mlflow.__version__`.
- [API/CLI] Added `mlflow artifacts` CLI to list, download, and upload to run artifact repositories (#391, @aarondav)
- [UI] Added icons to source names in MLflow Experiments UI (#381, @andrewmchen)
- [UI] Added support to view `.log` and `.tsv` files from MLflow artifacts UI (#393, @Shenggan; #433, @whiletruelearn)
- [UI] Run names can now be edited from within the MLflow UI (#382, @smurching)
- [Serving] Added `--host` option to `mlflow serve` to allow listening on non-local addresses (#401, @hamroune)
- [Serving/SageMaker] SageMaker serving takes an AWS region argument (#366, @dbczumar)
- [Python] Added environment variables to support providing HTTP auth (username, password, token) when talking to a remote MLflow tracking server (#402, @aarondav)
- [Python] Added support to override S3 endpoint for S3 artifactory (#451, @hamroune)
- MLflow nightly Python wheel and JAR snapshots are now available and linked from https://github.com/mlflow/mlflow (#352, @aarondav)

Bug fixes and documentation updates:

- [Python] `mlflow run` now logs default parameters, in addition to explicitly provided ones (#392, @mparkhe)
- [Python] `log_artifact` in FileStore now requires a relative path as the artifact path (#439, @mparkhe)
- [Python] Fixed string representation of Python entities, so they now display both their type and serialized fields (#371, @smurching)
- [UI] Entry point name is now shown in MLflow UI (#345, @aarondav)
- [Models] Keras model export now includes TensorFlow graph explicitly to ensure the model can always be loaded at deployment time (#440, @tomasatdatabricks)
- [Python] Fixed issue where FileStore ignored provided Run Name (#358, @adrian555)
- [Python] Fixed an issue where any `mlflow run` failing printed an extraneous exception (#365, @smurching)
- [Python] uuid dependency removed (#351, @antonpaquin)
- [Python] Fixed issues with remote execution on Databricks (#357, #361, @smurching; #383, #387, @aarondav)
- [Docs] Added [comprehensive example](https://github.com/mlflow/mlflow/tree/master/examples/multistep_workflow) of doing a multistep workflow, chaining MLflow runs together and reusing results (#338, @aarondav)
- [Docs] Added [comprehensive example](https://github.com/mlflow/mlflow/tree/master/examples/hyperparam) of doing hyperparameter tuning (#368, @tomasatdatabricks)
- [Docs] Added code examples to `mlflow.keras` API (#341, @dmatrix)
- [Docs] Significant improvements to Python API documentation (#454, @stbof)
- [Docs] Examples folder refactored to improve readability. The examples now reside in `examples/` instead of `example/`, too (#399, @mparkhe)
- Small bug fixes and doc updates (#328, #363, @ToonKBC; #336, #411, @aarondav; #284, @smurching; #377, @mparkhe; #389, gioa; #408, @aadamson; #397, @vfdev-5; #420, @adrian555; #459, #463, @stbof)

## 0.5.2 (2018-08-24)

MLflow 0.5.2 is a patch release on top of 0.5.1 containing only bug fixes and no breaking changes or features.

Bug fixes:

- Fix a bug with ECR client creation that caused `mlflow.sagemaker.deploy()` to fail when searching for a deployment Docker image (#366, @dbczumar)

## 0.5.1 (2018-08-23)

MLflow 0.5.1 is a patch release on top of 0.5.0 containing only bug fixes and no breaking changes or features.

Bug fixes:

- Fix `with mlflow.start_run() as run` to actually set `run` to the created Run (previously, it was None) (#322, @tomasatdatabricks)
- Fixes to DBFS artifactory to throw an exception if logging an artifact fails (#309) and to mimic FileStore's behavior of logging subdirectories (#347, @andrewmchen)
- Fix for Python 3.7 support with tarfiles (#329, @tomasatdatabricks)
- Fix spark.load_model not to delete the DFS tempdir (#335, @aarondav)
- MLflow UI now appropriately shows entrypoint if it's not main (#345, @aarondav)
- Make Python API forward-compatible with newer server versions of protos (#348, @aarondav)
- Improved API docs (#305, #284, @smurching)

## 0.5.0 (2018-08-17)

MLflow 0.5.0 offers some major improvements, including Keras and PyTorch first-class support as models, SFTP support as an artifactory, a new scatterplot visualization to compare runs, and a more complete Python SDK for experiment and run management.

Breaking changes:

- The Tracking API has been split into two pieces, a "basic logging" API and a "tracking service" API. The "basic logging" API deals with logging metrics, parameters, and artifacts to the currently-active active run, and is accessible in `mlflow` (e.g., `mlflow.log_param`). The tracking service API allow managing experiments and runs (especially historical runs) and is available in `mlflow.tracking`. The tracking service API will look analogous to the upcoming R and Java Tracking Service SDKs. Please be aware of the following breaking changes:

  - `mlflow.tracking` no longer exposes the basic logging API, only `mlflow`. So, code that was written like `from mlflow.tracking import log_param` will have to be `from mlflow import log_param` (note that almost all examples were already doing this).
  - Access to the service API goes through the `mlflow.tracking.get_service()` function, which relies on the same tracking server set by either the environment variable `MLFLOW_TRACKING_URI` or by code with `mlflow.tracking.set_tracking_uri()`. So code that used to look like `mlflow.tracking.get_run()` will now have to do `mlflow.tracking.get_service().get_run()`. This does not apply to the basic logging API.
  - `mlflow.ActiveRun` has been converted into a lightweight wrapper around `mlflow.entities.Run` to enable the Python `with` syntax. This means that there are no longer any special methods on the object returned when calling `mlflow.start_run()`. These can be converted to the service API.

  - The Python entities returned by the tracking service API are now accessible in `mlflow.entities` directly. Where previously you may have used `mlflow.entities.experiment.Experiment`, you would now just use `mlflow.entities.Experiment`. The previous version still exists, but is deprecated and may be hidden in a future version.

- REST API endpoint `/ajax-api/2.0/preview/mlflow/artifacts/get` has been moved to `$static_prefix/get-artifact`. This change is coversioned in the JavaScript, so should not be noticeable unless you were calling the REST API directly (#293, @andremchen)

Features:

- [Models] Keras integration: we now support logging Keras models directly in the log_model API, model format, and serving APIs (#280, @ToonKBC)
- [Models] PyTorch integration: we now support logging PyTorch models directly in the log_model API, model format, and serving APIs (#264, @vfdev-5)
- [UI] Scatterplot added to "Compare Runs" view to help compare runs using any two metrics as the axes (#268, @ToonKBC)
- [Artifacts] SFTP artifactory store added (#260, @ToonKBC)
- [Sagemaker] Users can specify a custom VPC when deploying SageMaker models (#304, @dbczumar)
- Pyfunc serialization now includes the Python version, and warns if the major version differs (can be suppressed by using `load_pyfunc(suppress_warnings=True)`) (#230, @dbczumar)
- Pyfunc serve/predict will activate conda environment stored in MLModel. This can be disabled by adding `--no-conda` to `mlflow pyfunc serve` or `mlflow pyfunc predict` (#225, @0wu)
- Python SDK formalized in `mlflow.tracking`. This includes adding SDK methods for `get_run`, `list_experiments`, `get_experiment`, and `set_terminated`. (#299, @aarondav)
- `mlflow run` can now be run against projects with no `conda.yaml` specified. By default, an empty conda environment will be created -- previously, it would just fail. You can still pass `--no-conda` to avoid entering a conda environment altogether (#218, @smurching)

Bug fixes:

- Fix numpy array serialization for int64 and other related types, allowing pyfunc to return such results (#240, @arinto)
- Fix DBFS artifactory calling `log_artifacts` with binary data (#295, @aarondav)
- Fix Run Command shown in UI to reproduce a run when the original run is targeted at a subdirectory of a Git repo (#294, @adrian555)
- Filter out ubiquitous dtype/ufunc warning messages (#317, @aarondav)
- Minor bug fixes and documentation updates (#261, @stbof; #279, @dmatrix; #313, @rbang1, #320, @yassineAlouini; #321, @tomasatdatabricks; #266, #282, #289, @smurching; #267, #265, @aarondav; #256, #290, @ToonKBC; #273, #263, @mateiz; #272, #319, @adrian555; #277, @aadamson; #283, #296, @andrewmchen)

## 0.4.2 (2018-08-07)

Breaking changes: None

Features:

- MLflow experiments REST API and `mlflow experiments create` now support providing `--artifact-location` (#232, @aarondav)
- [UI] Runs can now be sorted by columns, and added a Select All button (#227, @ToonKBC)
- Databricks File System (DBFS) artifactory support added (#226, @andrewmchen)
- databricks-cli version upgraded to >= 0.8.0 to support new DatabricksConfigProvider interface (#257, @aarondav)

Bug fixes:

- MLflow client sends REST API calls using snake_case instead of camelCase field names (#232, @aarondav)
- Minor bug fixes (#243, #242, @aarondav; #251, @javierluraschi; #245, @smurching; #252, @mateiz)

## 0.4.1 (2018-08-03)

Breaking changes: None

Features:

- [Projects] MLflow will use the conda installation directory given by the `$MLFLOW_CONDA_HOME`
  if specified (e.g. running conda commands by invoking `$MLFLOW_CONDA_HOME/bin/conda`), defaulting
  to running "conda" otherwise. (#231, @smurching)
- [UI] Show GitHub links in the UI for projects run from http(s):// GitHub URLs (#235, @smurching)

Bug fixes:

- Fix GCSArtifactRepository issue when calling list_artifacts on a path containing nested directories (#233, @jakeret)
- Fix Spark model support when saving/loading models to/from distributed filesystems (#180, @tomasatdatabricks)
- Add missing mlflow.version import to sagemaker module (#229, @dbczumar)
- Validate metric, parameter and run IDs in file store and Python client (#224, @mateiz)
- Validate that the tracking URI is a remote URI for Databricks project runs (#234, @smurching)
- Fix bug where we'd fetch git projects at SSH URIs into a local directory with the same name as
  the URI, instead of into a temporary directory (#236, @smurching)

## 0.4.0 (2018-08-01)

Breaking changes:

- [Projects] Removed the `use_temp_cwd` argument to `mlflow.projects.run()`
  (`--new-dir` flag in the `mlflow run` CLI). Runs of local projects now use the local project
  directory as their working directory. Git projects are still fetched into temporary directories
  (#215, @smurching)
- [Tracking] GCS artifact storage is now a pluggable dependency (no longer installed by default).
  To enable GCS support, install `google-cloud-storage` on both the client and tracking server via pip.
  (#202, @smurching)
- [Tracking] Clients running MLflow 0.4.0 and above require a server running MLflow 0.4.0
  or above, due to a fix that ensures clients no longer double-serialize JSON into strings when
  sending data to the server (#200, @aarondav). However, the MLflow 0.4.0 server remains
  backwards-compatible with older clients (#216, @aarondav)

Features:

- [Examples] Add a more advanced tracking example: using MLflow with PyTorch and TensorBoard (#203)
- [Models] H2O model support (#170, @ToonKBC)
- [Projects] Support for running projects in subdirectories of Git repos (#153, @juntai-zheng)
- [SageMaker] Support for specifying a compute specification when deploying to SageMaker (#185, @dbczumar)
- [Server] Added --static-prefix option to serve UI from a specified prefix to MLflow UI and server (#116, @andrewmchen)
- [Tracking] Azure blob storage support for artifacts (#206, @mateiz)
- [Tracking] Add support for Databricks-backed RestStore (#200, @aarondav)
- [UI] Enable productionizing frontend by adding CSRF support (#199, @aarondav)
- [UI] Update metric and parameter filters to let users control column order (#186, @mateiz)

Bug fixes:

- Fixed incompatible file structure returned by GCSArtifactRepository (#173, @jakeret)
- Fixed metric values going out of order on x axis (#204, @mateiz)
- Fixed occasional hanging behavior when using the projects.run API (#193, @smurching)

- Miscellaneous bug and documentation fixes from @aarondav, @andrewmchen, @arinto, @jakeret, @mateiz, @smurching, @stbof

## 0.3.0 (2018-07-18)

Breaking changes:

- [MLflow Server] Renamed `--artifact-root` parameter to `--default-artifact-root` in `mlflow server` to better reflect its purpose (#165, @aarondav)

Features:

- Spark MLlib integration: we now support logging SparkML Models directly in the log_model API, model format, and serving APIs (#72, @tomasatdatabricks)
- Google Cloud Storage is now supported as an artifact storage root (#152, @bnekolny)
- Support asychronous/parallel execution of MLflow runs (#82, @smurching)
- [SageMaker] Support for deleting, updating applications deployed via SageMaker (#145, @dbczumar)
- [SageMaker] Pushing the MLflow SageMaker container now includes the MLflow version that it was published with (#124, @sueann)
- [SageMaker] Simplify parameters to SageMaker deploy by providing sane defaults (#126, @sueann)
- [UI] One-element metrics are now displayed as a bar char (#118, @cryptexis)

Bug fixes:

- Require gitpython>=2.1.0 (#98, @aarondav)
- Fixed TensorFlow model loading so that columns match the output names of the exported model (#94, @smurching)
- Fix SparkUDF when number of columns >= 10 (#97, @aarondav)
- Miscellaneous bug and documentation fixes from @emres, @dmatrix, @stbof, @gsganden, @dennyglee, @anabranch, @mikehuston, @andrewmchen, @juntai-zheng

## 0.2.1 (2018-06-28)

This is a patch release fixing some smaller issues after the 0.2.0 release.

- Switch protobuf implementation to C, fixing a bug related to tensorflow/mlflow import ordering (issues #33 and #77, PR #74, @andrewmchen)
- Enable running mlflow server without git binary installed (#90, @aarondav)
- Fix Spark UDF support when running on multi-node clusters (#92, @aarondav)

## 0.2.0 (2018-06-27)

- Added `mlflow server` to provide a remote tracking server. This is akin to `mlflow ui` with new options:

  - `--host` to allow binding to any ports (#27, @mdagost)
  - `--artifact-root` to allow storing artifacts at a remote location, S3 only right now (#78, @mateiz)
  - Server now runs behind gunicorn to allow concurrent requests to be made (#61, @mateiz)

- TensorFlow integration: we now support logging TensorFlow Models directly in the log_model API, model format, and serving APIs (#28, @juntai-zheng)
- Added `experiments.list_experiments` as part of experiments API (#37, @mparkhe)
- Improved support for unicode strings (#79, @smurching)
- Diabetes progression example dataset and training code (#56, @dennyglee)
- Miscellaneous bug and documentation fixes from @Jeffwan, @yupbank, @ndjido, @xueyumusic, @manugarri, @tomasatdatabricks, @stbof, @andyk, @andrewmchen, @jakeret, @0wu, @aarondav

## 0.1.0 (2018-06-05)

- Initial version of mlflow.


--- CONTRIBUTING.md ---
# Contributing to MLflow

We welcome community contributions to MLflow. This page provides useful information about contributing to MLflow.

**Table of Contents**

- [Governance](#governance)
- [Core Members](#core-members)
- [Contribution process](#contribution-process)
- [Contribution guidelines](#contribution-guidelines)
  - [Write designs for significant changes](#write-designs-for-significant-changes)
  - [Make changes backwards compatible](#make-changes-backwards-compatible)
  - [Consider introducing new features as MLflow Plugins](#consider-introducing-new-features-as-mlflow-plugins)
  - [Python Style Guide](#python-style-guide)
- [Setting up the repository](#setting-up-the-repository)
- [Developing and testing MLflow](#developing-and-testing-mlflow)
  - [Environment Setup and Python configuration](#environment-setup-and-python-configuration)
    - [Automated Python development environment configuration](#automated-python-development-environment-configuration)
    - [Manual Python development environment configuration](#manual-python-development-environment-configuration)
  - [JavaScript and UI](#javascript-and-ui)
    - [Install Node Module Dependencies](#install-node-module-dependencies)
    - [Install Node Modules](#install-node-modules)
    - [Launching the Development UI](#launching-the-development-ui)
    - [Running the Javascript Dev Server](#running-the-javascript-dev-server)
    - [Testing a React Component](#testing-a-react-component)
    - [Linting Javascript Code](#linting-javascript-code)
  - [R](#r)
  - [Java](#java)
  - [Python](#python)
    - [Writing Python Tests](#writing-python-tests)
    - [Running Python Tests](#running-python-tests)
    - [Python Client](#python-client)
      - [Python Model Flavors](python-model-flavors)
    - [Python Server](#python-server)
      - [Building Protobuf Files](#building-protobuf-files)
      - [Database Schema Changes](#database-schema-changes)
  - [Writing MLflow Examples](#writing-mlflow-examples)
  - [Building a Distributable Artifact](#building-a-distributable-artifact)
  - [Writing Docs](#writing-docs)
  - [Sign your work](#sign-your-work)
- [Code of Conduct](#code-of-conduct)

## Governance

Governance of MLflow is conducted by the Technical Steering Committee
(TSC), which currently includes the following members:

- Patrick Wendell (<pwendell@gmail.com>)
- Reynold Xin (<reynoldx@gmail.com>)
- Matei Zaharia (<matei@cs.stanford.edu>)

The founding technical charter can be found
[here](https://github.com/mlflow/mlflow/blob/master/mlflow-charter.pdf).

## Core Members

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- [Harutaka Kawamura](https://github.com/harupy)
- [Weichen Xu](https://github.com/WeichenXu123)
- [Corey Zumar](https://github.com/dbczumar)
- [Ben Wilson](https://github.com/BenWilson2)
- [Serena Ruan](https://github.com/serena-ruan)
- [Yuki Watanabe](https://github.com/B-Step62)
- [Daniel Lok](https://github.com/daniellok-db)
- [Tomu Hirata](https://github.com/TomeHirata)
- [Gabriel Fu](https://github.com/gabrielfu)

## Contribution process

The MLflow contribution process starts with filing a GitHub issue.
MLflow defines four categories of issues: feature requests, bug reports,
documentation fixes, and installation issues. Details about each issue
type and the issue lifecycle are discussed in the [MLflow Issue
Policy](https://github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md).

MLflow committers actively [triage](ISSUE_TRIAGE.rst) and respond to
GitHub issues. In general, we recommend waiting for feedback from an
MLflow committer or community member before proceeding to implement a
feature or patch. This is particularly important for [significant
changes](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#write-designs-for-significant-changes),
and will typically be labeled during triage with `needs design`.

After you have agreed upon an implementation strategy for your feature
or patch with an MLflow committer, the next step is to introduce your
changes (see [developing
changes](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#developing-and-testing-mlflow))
as a pull request against the MLflow Repository (we recommend pull
requests be filed from a non-master branch on a repository fork) or as a
standalone MLflow Plugin. MLflow committers actively review pull
requests and are also happy to provide implementation guidance for
Plugins.

Once your pull request against the MLflow Repository has been merged,
your corresponding changes will be automatically included in the next
MLflow release. Every change is listed in the MLflow release notes and
[Changelog](https://github.com/mlflow/mlflow/blob/master/CHANGELOG.md).

Congratulations, you have just contributed to MLflow. We appreciate your
contribution\!

## Contribution guidelines

In this section, we provide guidelines to consider as you develop new
features and patches for MLflow.

### Write designs for significant changes

For significant changes to MLflow, we recommend outlining a design for
the feature or patch and discussing it with an MLflow committer before
investing heavily in implementation. During issue triage, we try to
proactively identify issues require design by labeling them with `needs design`. This is particularly important if your proposed implementation:

- Introduces changes or additions to the [MLflow REST
  API](https://mlflow.org/docs/latest/rest-api.html)
  - The MLflow REST API is implemented by a variety of open source
    and proprietary platforms. Changes to the REST API impact all of
    these platforms. Accordingly, we encourage developers to
    thoroughly explore alternatives before attempting to introduce
    REST API changes.
- Introduces new user-facing MLflow APIs
  - MLflow's API surface is carefully designed to generalize across
    a variety of common ML operations. It is important to ensure
    that new APIs are broadly useful to ML developers, easy to work
    with, and simple yet powerful.
- Adds new library dependencies to MLflow
- Makes changes to critical internal abstractions. Examples include:
  the Tracking Artifact Repository, the Tracking Abstract Store, and
  the Model Registry Abstract Store.

### Make changes backwards compatible

MLflow's users rely on specific platform and API behaviors in their
daily workflows. As new versions of MLflow are developed and released,
it is important to ensure that users' workflows continue to operate as
expected. Accordingly, please take care to consider backwards
compatibility when introducing changes to the MLflow code base. If you
are unsure of the backwards compatibility implications of a particular
change, feel free to ask an MLflow committer or community member for
input.

In addition to public APIs, any Python APIs within MLflow that are designated with the
annotation `@developer_stable` must remain backwards compatible. Any contribution
that adds features, modifies behavior, or otherwise changes the functionality within the
scope of these classes or methods will be closely reviewed by maintainers, and additional
backwards compatibility testing may be requested.

### Consider introducing new features as MLflow Plugins

[MLflow Plugins](https://mlflow.org/docs/latest/plugins.html) enable
integration of third-party modules with many of MLflow's components,
allowing you to maintain and iterate on certain features independently
of the MLflow Repository. Before implementing changes to the MLflow code
base, consider whether your feature might be better structured as an
MLflow Plugin. MLflow Plugins are a great choice for the following types
of changes:

1.  Supporting a new storage platform for MLflow artifacts
2.  Introducing a new implementation of the MLflow Tracking backend
    ([Abstract
    Store](https://github.com/mlflow/mlflow/blob/cdc6a651d5af0f29bd448d2c87a198cf5d32792b/mlflow/store/tracking/abstract_store.py))
    for a particular platform
3.  Introducing a new implementation of the Model Registry backend
    ([Abstract
    Store](https://github.com/mlflow/mlflow/blob/cdc6a651d5af0f29bd448d2c87a198cf5d32792b/mlflow/store/model_registry/abstract_store.py))
    for a particular platform
4.  Automatically capturing and recording information about MLflow Runs
    created in specific environments

MLflow committers and community members are happy to provide assistance
with the development and review of new MLflow Plugins.

Finally, MLflow maintains a list of Plugins developed by community
members, which is located at
<https://mlflow.org/docs/latest/plugins.html#community-plugins>. This is
an excellent way to inform MLflow users about your exciting new Plugins.
To list your plugin, simply introduce a new pull request against the
[corresponding docs section of the MLflow code
base](https://github.com/mlflow/mlflow/blob/cdc6a651d5af0f29bd448d2c87a198cf5d32792b/docs/source/plugins.rst#community-plugins).

For more information about Plugins, see
<https://mlflow.org/docs/latest/plugins.html>.

### Python Style Guide

##### Docstrings

We follow [Google's Python Style Guide](https://google.github.io/styleguide/pyguide.html)
for writing docstrings. Make sure your docstrings adhere to this style
guide.

###### Code Style

We use [prettier](https://prettier.io/),
[blacken-docs](https://pypi.org/project/blacken-docs/), [ruff](https://github.com/astral-sh/ruff), and
a number of custom lint checking scripts in our CI via
pre-commit Git hooks. If your code passes the CI checks, it's
formatted correctly.

To validate that your local versions of the above libraries
match those in the mlflow CI, refer to [lint-requirements.txt](https://github.com/mlflow/mlflow/blob/master/requirements/lint-requirements.txt).
You can compare these versions with your local using pip:

```bash
pip show ruff
```

## Setting up the repository

To set up the MLflow repository, run the following commands:

```bash
# Clone the repository
git clone --recurse-submodules git@github.com:<username>/mlflow.git
# The alternative way of cloning through https may cause permission error during branch push
# git clone --recurse-submodules https://github.com/<username>/mlflow.git

# Add the upstream repository
cd mlflow
git remote add upstream git@github.com:mlflow/mlflow.git
```

If you cloned the repository before without `--recurse-submodules`, run
this command to fetch submodules:

```bash
git submodule update --init --recursive
```

## Developing and testing MLflow

The majority of the MLflow codebase is developed in Python. This
includes the CLI, Tracking Server, Artifact Repositories (e.g., S3 or
Azure Blob Storage backends), and of course the Python fluent, tracking,
and model APIs.

### Environment Setup and Python configuration

Having a standardized development environment is advisable when working
on MLflow. Creating an environment that contains the required Python
packages (and versions), linting tools, and environment configurations
will help to prevent unnecessary CI failures when filing a PR. A
correctly configured local environment will also allow you to run tests
locally in an environment that mimics that of the CI execution
environment.

There are three means of setting up a base Python development environment
for MLflow: GitHub Codespaces, automated (through the
[dev-env-setup.sh](https://github.com/mlflow/mlflow/tree/master/dev/dev-env-setup.sh)
script) or manual. Even in a manual-based approach (i.e., testing
functionality of a specific version of a model flavor's package
version), the automated script can save a great deal of time and reduce
errors in creating the environment.

#### GitHub Codespaces

<img src="./assets/create-codespace.png" width="60%"/>

1. Navigate to https://github.com/mlflow/mlflow.git.
2. Above the file list, click `Code`, then select `Create codespace` and wait for your codespace to be created.

See [Quickstart for GitHub Codespaces](https://docs.github.com/en/codespaces/getting-started/quickstart) for more information.

#### Automated Python development environment configuration

The automated development environment setup script
([dev-env-setup.sh](https://github.com/mlflow/mlflow/tree/master/dev/dev-env-setup.sh))
can be used to setup a development environment that is configured with
all of the dependencies required and the environment configuration
needed to develop and locally test the Python code portions of MLflow.
This CLI tool's readme can be accessed via the root of the mlflow
repository as follows:

```bash
dev/dev-env-setup.sh -h
```

An example usage of this script that will build a development
environment using `virtualenv` and the minimum supported Python version
(to ensure compatibility) is:

```bash
dev/dev-env-setup.sh -d .venvs/mlflow-dev -q
```

The `-q` parameter is to "quiet" the pip install processes preventing
stdout printing during installation.

It is advised to follow all of the prompts to ensure that the
configuration of the environment, as well as git, are completed so that
your PR process is as effortless as possible.

**Note**

Frequently, a specific version of a library is required in order to
validate a feature's compatibility with older versions. Modifying your
primary development environment to test one-off compatibility can be
very error-prone and result in an environment that is significantly
different from that of the CI test environment. To support this use
case, the automated script can be used to create an environment that can
be easily modified to support testing a particular version of a model
flavor in an isolated environment. Simply run the `dev-env-setup.sh`
script, activate the new environment, and install the required version
for testing.

</div>

Example of installing an older version of `scikit-learn` to perform
isolated testing:

```bash
dev/dev-env-setup.sh -d ~/.venvs/sklearn-test -q
source ~/.venvs/sklearn-test/bin/activate
pip freeze | grep "scikit-learn"
>> scikit-learn==1.0.2
pip install scikit-learn==1.0.1
pip freeze | grep "scikit-learn"
>> scikit-learn==1.0.1
```

#### Manual Python development environment configuration

The manual process is recommended if you are going to use Conda or if
you are fond of terminal setup processes. To start with the manual
process, ensure that you have either conda or virtualenv installed.

First, ensure that your name and email are [configured in
git](https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup)
so that you can [sign your work](#sign-your-work) when committing code
changes and opening pull requests:

```bash
git config --global user.name "Your Name"
git config --global user.email yourname@example.com
```

For convenience, we provide a pre-commit git hook that validates that
commits are signed-off and runs `ruff check --fix` and `ruff format` to ensure the
code will pass the lint check for python. You can enable it by running:

```bash
pre-commit install --install-hooks
```

Then, install the Python MLflow package from source - this is required
for developing & testing changes across all languages and APIs. We
recommend installing MLflow in its own conda environment by running the
following from your checkout of MLflow:

```bash
conda create --name mlflow-dev-env python=3.8
conda activate mlflow-dev-env
pip install -e '.[extras]' # installs mlflow from current checkout with some useful extra utilities
```

If you plan on doing development and testing, you will also need to
install the following into the conda environment:

```bash
pip install -r requirements/dev-requirements.txt
pip install -e '.[extras]'  # installs mlflow from current checkout
pip install -e tests/resources/mlflow-test-plugin # installs `mlflow-test-plugin` that is required for running certain MLflow tests
```

You may need to run `conda install cmake` for the test requirements to
properly install, as `onnx` needs `cmake`.

Ensure [Docker](https://www.docker.com/) is installed.

Finally, we use `pytest` to test all Python contributed code. Install
`pytest`:

```bash
pip install pytest
```

### JavaScript and UI

The MLflow UI is written in JavaScript. `yarn` is required to run the
Javascript dev server and the tracking UI. You can verify that `yarn` is
on the PATH by running `yarn -v`, and [install
yarn](https://classic.yarnpkg.com/lang/en/docs/install) if needed.

#### Install Node Module Dependencies

On OSX, install the following packages required by the node modules:

```bash
brew install pixman cairo pango jpeg
```

Linux/Windows users will need to source these dependencies using the
appropriate package manager on their platforms.

#### Install Node Modules

Before running the Javascript dev server or building a distributable
wheel, install Javascript dependencies via:

```bash
cd mlflow/server/js
yarn install
cd - # return to root repository directory
```

If modifying dependencies in `mlflow/server/js/package.json`, run `yarn upgrade` within `mlflow/server/js` to install the updated dependencies.

#### Launching the Development UI

We recommend [Running the Javascript Dev
Server](#running-the-javascript-dev-server) - otherwise, the tracking
frontend will request files in the `mlflow/server/js/build` directory,
which is not checked into Git. Alternatively, you can generate the
necessary files in `mlflow/server/js/build` as described in [Building a
Distributable Artifact](#building-a-distributable-artifact).

#### Running the Javascript Dev Server

[Install Node Modules](#install-node-modules), then run the following in two separate shells:

In one shell:

```bash
mlflow ui
```

And in another shell:

```bash
cd mlflow/server/js
yarn start
```

The Javascript Dev Server will run at <http://localhost:3000> and the
MLflow server will run at <http://localhost:5000> and show runs logged
in `./mlruns`.

**Note:** On some versions of MacOS, the "Airplay Receiver" process runs on port 5000 by default,
which can cause [network request failures](https://stackoverflow.com/questions/72369320/why-always-something-is-running-at-port-5000-on-my-mac).
If you are encountering such issues, disable the
process via system settings, or specify another port (e.g. `mlflow server --port 8000`).

If specifying a different port, please set the following environment variables before running `yarn start`:

- `MLFLOW_PROXY=<tracking_server_uri>`
- `MLFLOW_DEV_PROXY_MODE=false`

For example:

```
$ mlflow server --port 8000
...

(in a separate shell)
$ export MLFLOW_PROXY=http://127.0.0.1:8000
$ export MLFLOW_DEV_PROXY_MODE=false
$ yarn install
$ yarn start
...

(UI should now be visible at localhost:3000)
```

#### Launching MLflow UI with MLflow AI Gateway for PromptLab

```sh
python dev/server.py
```

#### Testing a React Component

Add a test file in the same directory as the newly created React
component. For example, `CompareRunBox.test.js` should be added in the
same directory as `CompareRunBox.js`. Next, in `mlflow/server/js`, run
the following command to start the test.

```bash
# Run tests in CompareRunBox.test.js
yarn test CompareRunBox.test.js
# Run tests with a name that matches 'plot' in CompareRunBox.test.js
yarn test CompareRunBox.test.js -t 'plot'
# Run all tests
yarn test
```

#### Linting Javascript Code

In `mlflow/server/js`, run the following command to lint your code.

```bash
# Note this command only fixes auto-fixable issues (e.g. remove trailing whitespace)
yarn lint:fix
```

### R

If contributing to MLflow's R APIs, install
[R](https://cloud.r-project.org/) and make sure that you have satisfied
all the [Environment Setup and Python configuration](#environment-setup-and-python-configuration).

The `mlflow/R/mlflow` directory contains R wrappers for the Projects,
Tracking and Models components. These wrappers depend on the Python
package, so first install the Python package in a conda environment:

```bash
# Note that we don't pass the -e flag to pip, as the R tests attempt to run the MLflow UI
# via the CLI, which will not work if we run against the development tracking server
pip install .
```

[Install R](https://cloud.r-project.org/), then run the following to
install dependencies for building MLflow locally:

```bash
cd mlflow/R/mlflow
NOT_CRAN=true Rscript -e 'install.packages("devtools", repos = "https://cloud.r-project.org")'
NOT_CRAN=true Rscript -e 'devtools::install_deps(dependencies = TRUE)'
```

Build the R client via:

```bash
R CMD build .
```

Run tests:

```bash
R CMD check --no-build-vignettes --no-manual --no-tests mlflow*tar.gz
cd tests
NOT_CRAN=true LINTR_COMMENT_BOT=false Rscript ../.run-tests.R
cd -
```

Run linter:

```bash
Rscript -e 'lintr::lint_package()'
```

If opening a PR that makes API changes, please regenerate API
documentation as described in [Writing Docs](#writing-docs) and commit
the updated docs to your PR branch.

When developing, you can make Python changes available in R by running
(from mlflow/R/mlflow):

```bash
Rscript -e 'reticulate::conda_install("r-mlflow", "../../../.", pip = TRUE)'
```

Please also follow the recommendations from the [Advanced R - Style
Guide](http://adv-r.had.co.nz/Style.html) regarding naming and styling.

### Java

If contributing to MLflow's Java APIs or modifying Java documentation,
install [Java](https://www.java.com/) and [Apache
Maven](https://maven.apache.org/download.cgi).

A certain MLflow module is implemented in Java, under the `mlflow/java/`
directory. This is the Java Tracking API client (`mlflow/java/client`).

Other Java functionality (like artifact storage) depends on the Python
package, so first install the Python package in a conda environment as
described in [Environment Setup and Python configuration](#environment-setup-and-python-configuration).
[Install](https://www.oracle.com/technetwork/java/javase/downloads/index.html)
the Java 8 JDK (or above), and
[download](https://maven.apache.org/download.cgi) and
[install](https://maven.apache.org/install.html) Maven. You can then
build and run tests via:

```bash
cd mlflow/java
mvn compile test
```

If opening a PR that makes API changes, please regenerate API
documentation as described in [Writing Docs](#writing-docs) and commit
the updated docs to your PR branch.

### Python

If you are contributing in Python, make sure that you have satisfied all
the [Environment Setup and Python configuration](#environment-setup-and-python-configuration), including installing
`pytest`, as you will need it for the sections described below.

#### Writing Python Tests

If your PR includes code that isn't currently covered by our tests (e.g.
adding a new flavor, adding autolog support to a flavor, etc.), you
should write tests that cover your new code. Your tests should be added
to the relevant file under `tests`, or if there is no appropriate file,
in a new file prefixed with `test_` so that `pytest` includes that file
for testing.

If your tests require usage of a tracking URI, the [pytest
fixture](https://docs.pytest.org/en/3.2.1/fixture.html)
[tracking_uri_mock](https://github.com/mlflow/mlflow/blob/master/tests/conftest.py#L74)
is automatically set up for every tests. It sets up a mock tracking URI
that will set itself up before your test runs and tear itself down
after.

By default, runs are logged under a local temporary directory that's
unique to each test and torn down immediately after test execution. To
disable this behavior, decorate your test function with
`@pytest.mark.notrackingurimock`

#### Running Python Tests

Verify that the unit tests & linter pass before submitting a pull
request by running:

We use [ruff](https://docs.astral.sh/ruff/) to ensure a
consistent code format. You can auto-format your code by running:

```bash
ruff format .
ruff check .
```

Then, verify that the unit tests & linter pass before submitting a pull
request by running:

```bash
pre-commit run --all-files
pytest tests --quiet --requires-ssh --ignore-flavors --serve-wheel \
  --ignore=tests/examples --ignore=tests/evaluate
```

We use [pytest](https://docs.pytest.org/en/latest/contents.html) to run
Python tests. You can run tests for one or more test directories or
files via `pytest [file_or_dir] ... [file_or_dir]`. For example, to run
all pytest tests, you can run:

```bash
pytest tests/pyfunc
```

Note: Certain model tests are not well-isolated (can result in OOMs when
run in the same Python process), so simply invoking `pytest` or `pytest tests` may not work. If you'd like to run multiple model tests, we
recommend doing so via separate `pytest` invocations, e.g. `pytest tests/sklearn && pytest tests/tensorflow`

If opening a PR that changes or adds new APIs, please update or add
Python documentation as described in [Writing Docs](#writing-docs) and
commit the docs to your PR branch.

#### Python Client

For the client, if you are adding new model flavors, follow the
instructions below.

##### Python Model Flavors

If you are adding new framework flavor support, you'll need to modify
`pytest` and Github action configurations so tests for your code can run
properly. Generally, the files you'll have to edit are:

1.  `.github/workflows/master.yml`: lines where pytest runs with `--ignore-flavors` flag

    1. Add your tests to the ignore list, where the other frameworks are
       ignored
    2. Add a pytest command for your tests along with the other framework
       tests (as a separate command to avoid OOM issues)

2.  `requirements/test-requirements.txt`: add your framework and version
    to the list of requirements

You can see an example of a [flavor
PR](https://github.com/mlflow/mlflow/pull/2136/files).

#### Python Server

For the Python server, you can contribute in these two areas described
below.

##### Building Protobuf Files

To build protobuf files, simply run `./dev/generate-protos.sh`. The required
`protoc` version is `3.19.4`. You can find the URL of a
system-appropriate installation of `protoc` at
<https://github.com/protocolbuffers/protobuf/releases/tag/v3.19.4>, e.g.
<https://github.com/protocolbuffers/protobuf/releases/download/v3.19.4/protoc-3.19.4-osx-x86_64.zip>
if you're on 64-bit Mac OSX.

Alternatively, you can comment `/autoformat` on your PR to automatically compile the protobuf files and update the autogenerated code.

Once the autogenerated code is updated, verify that `.proto` files and autogenerated code are in sync by running `./dev/test-generate-protos.sh`.

##### Database Schema Changes

MLflow's Tracking component supports storing experiment and run data in
a SQL backend. To make changes to the tracking database schema, run the
following from your checkout of MLflow:

```bash
# starting at the root of the project
$ pwd
~/mlflow
$ cd mlflow
# MLflow relies on Alembic (https://alembic.sqlalchemy.org) for schema migrations.
$ alembic -c mlflow/store/db_migrations/alembic.ini revision -m "add new field to db"
  Generating ~/mlflow/mlflow/store/db_migrations/versions/b446d3984cfa_add_new_field_to_db.py
# Update schema files
$ ./tests/db/update_schemas.sh
```

These commands generate a new migration script (e.g., at
`~/mlflow/mlflow/alembic/versions/12341123_add_new_field_to_db.py`) that
you should then edit to add migration logic.

### Writing MLflow Examples

The `mlflow/examples` directory has a collection of quickstart tutorials
and various simple examples that depict MLflow tracking, project, model
flavors, model registry, and serving use cases. These examples provide
developers sample code, as a quick way to learn MLflow Python APIs.

To facilitate review, strive for brief examples that reflect real user
workflows, document how to run your example, and follow the recommended
steps below.

If you are contributing a new model flavor, follow these steps:

1.  Follow instructions in [Python Model Flavors](#python-model-flavors)
2.  Create a corresponding directory in
    `mlflow/examples/new-model-flavor`
3.  Implement your Python training `new-model-flavor` code in this
    directory
4.  Convert this directory's content into an [MLflow
    Project](https://mlflow.org/docs/latest/projects.html) executable
5.  Add `README.md`, `MLproject`, and `conda.yaml` files and your code
6.  Read instructions in the `mlflow/test/examples/README.md` and add a
    `pytest` entry in the `test/examples/test_examples.py`
7.  Add a short description in the `mlflow/examples/README.md` file

If you are contributing to the quickstart directory, we welcome changes
to the `quickstart/mlflow_tracking.py` that make it clearer or simpler.

If you'd like to provide an example of functionality that doesn't fit
into the above categories, follow these steps:

1.  Create a directory with meaningful name in
    `mlflow/examples/new-program-name` and implement your Python code
2.  Create `mlflow/examples/new-program-name/README.md` with
    instructions how to use it
3.  Read instructions in the `mlflow/test/examples/README.md`, and add a
    `pytest` entry in the `test/examples/test_examples.py`
4.  Add a short description in the `mlflow/examples/README.md` file

Finally, before filing a pull request, verify all Python tests pass.

### Building a Distributable Artifact

If you would like to build a fully functional version of MLflow from your local branch for testing or a local patch fix, first
[install the Node Modules](#install-node-modules), then run the following:

Generate JS files in `mlflow/server/js/build`:

```bash
cd mlflow/server/js
yarn build
```

Build a pip-installable wheel and a compressed code archive in `dist/`:

```bash
cd -
python -m build
```

### TOML formatting

We use [taplo](https://taplo.tamasfe.dev/) to enforce consistent TOML formatting. You can install it by following the instructions [here](https://taplo.tamasfe.dev/cli/introduction.html).

### Excluding Symlinks from IDE Searches

The `mlflow/skinny` symlink points to `../mlflow` and may cause duplicate entries in search results. To exclude it from searches, follow these steps:

**VSCode:**

1. Open `Settings`.
2. Search for `search.followSymlinks` and set it to `false`.

**PyCharm:**

1. Right-click `skinny/mlflow`.
2. Select `Mark Directory as` -> `Excluded`.

### Writing Docs

There are two separate build systems for the MLflow documentation:

#### API Docs

The [API reference](https://mlflow.org/docs/latest/api_reference/) is managed by [Sphinx](https://www.sphinx-doc.org/en/master/). The content is primarily populated by our Python docstrings, which are written in reStructuredText (RST).

For instructions on how to build the API docs, please check the [README.md](https://github.com/mlflow/mlflow/blob/master/docs/api_reference/README.md) in the `docs/api_reference/` subfolder.

#### Main Docs

The main MLflow docs (e.g. feature docs, tutorials, etc) are written using [Docusaurus](https://docusaurus.io/). The only prerequisite for building these docs is NodeJS >= 18.0. Please check out the [official NodeJS docs](https://nodejs.org/en/download) for platform-specific installation instructions.

To get started, simply run `yarn && yarn start` from the [`docs/`](https://github.com/mlflow/mlflow/blob/master/docs/) folder. This will spin up a development server that can be viewed at `http://localhost:3000/` (by default). The source files (primarily `.MDX`) are located in the [`docs/docs/`](https://github.com/mlflow/mlflow/blob/master/docs/docs/) subfolder. Changes to these files should be automatically reflected in the development server!

There are also some `.ipynb` files which serve as the source for some of our tutorials. These are converted to MDX via a custom script (`yarn convert-notebooks`). If you want to make changes to these, you will need to install the `nbconvert` Python package in order to preview your changes.

For more detailed information, please check the [README.md](https://github.com/mlflow/mlflow/blob/master/docs/README.md) in the `docs/` folder. We're looking forward to your contributions!

### Sign your work

In order to commit your work, you need to sign that you wrote the patch
or otherwise have the right to pass it on as an open-source patch. If
you can certify the below (from developercertificate.org):

    Developer Certificate of Origin
    Version 1.1

    Copyright (C) 2004, 2006 The Linux Foundation and its contributors.
    1 Letterman Drive
    Suite D4700
    San Francisco, CA, 94129

    Everyone is permitted to copy and distribute verbatim copies of this
    license document, but changing it is not allowed.


    Developer's Certificate of Origin 1.1

    By making a contribution to this project, I certify that:

    (a) The contribution was created in whole or in part by me and I
        have the right to submit it under the open source license
        indicated in the file; or

    (b) The contribution is based upon previous work that, to the best
        of my knowledge, is covered under an appropriate open source
        license and I have the right under that license to submit that
        work with modifications, whether created in whole or in part
        by me, under the same open source license (unless I am
        permitted to submit under a different license), as indicated
        in the file; or

    (c) The contribution was provided directly to me by some other
        person who certified (a), (b) or (c) and I have not modified
        it.

    (d) I understand and agree that this project and the contribution
        are public and that a record of the contribution (including all
        personal information I submit with it, including my sign-off) is
        maintained indefinitely and may be redistributed consistent with
        this project or the open source license(s) involved.

Then add a line to every git commit message:

    Signed-off-by: Jane Smith <jane.smith@email.com>

Use your real name (sorry, no pseudonyms or anonymous contributions).
You can sign your commit automatically with `git commit -s` after you
set your `user.name` and `user.email` git configs.

> NOTE: Failing to sign your commits will result in an inability to merge your PR!

## Code of Conduct

Refer to the [MLflow Contributor Covenant Code of
Conduct](./CODE_OF_CONDUCT.rst) for more information.


--- LICENSE.txt ---
Copyright 2018 Databricks, Inc.  All rights reserved.

				Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS
   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


--- SECURITY.md ---
# Security Policy

MLflow and its community take security bugs seriously. We appreciate efforts to improve the security of MLflow
and follow the [GitHub coordinated disclosure of security vulnerabilities](https://docs.github.com/en/code-security/security-advisories/about-coordinated-disclosure-of-security-vulnerabilities#about-reporting-and-disclosing-vulnerabilities-in-projects-on-github)
for responsible disclosure and prompt mitigation. We are committed to working with security researchers to
resolve the vulnerabilities they discover.

## Supported Versions

The latest version of MLflow has continued support. If a critical vulnerability is found in the current version
of MLflow, we may opt to backport patches to previous versions.

## Reporting a Vulnerability

When finding a security vulnerability in MLflow, please perform the following actions:

- [Open an issue](https://github.com/mlflow/mlflow/issues/new?assignees=&labels=bug&template=bug_report_template.md&title=%5BBUG%5D%20Security%20Vulnerability) on the MLflow repository. Ensure that you use `[BUG] Security Vulnerability` as the title and _do not_ mention any vulnerability details in the issue post.
- Send a notification [email](mailto:mlflow-oss-maintainers@googlegroups.com) to `mlflow-oss-maintainers@googlegroups.com` that contains, at a minimum:
  - The link to the filed issue stub.
  - Your GitHub handle.
  - Detailed information about the security vulnerability, evidence that supports the relevance of the finding and any reproducibility instructions for independent confirmation.

This first stage of reporting is to ensure that a rapid validation can occur without wasting the time and effort of a reporter. Future communication and vulnerability resolution will be conducted after validating
the veracity of the reported issue.

An MLflow maintainer will, after validating the report:

- Acknowledge the [bug](ISSUE_POLICY.md#bug-reports) during [triage](ISSUE_TRIAGE.rst)
- Mark the issue as `priority/critical-urgent`
- Open a draft [GitHub Security Advisory](https://docs.github.com/en/code-security/security-advisories/creating-a-security-advisory)
  to discuss the vulnerability details in private.

The private Security Advisory will be used to confirm the issue, prepare a fix, and publicly disclose it after the fix has been released.


--- dev/create_release_branch.py ---
import argparse
import os
import subprocess

from packaging.version import Version


def main(new_version: str, remote: str, dry_run=False):
    version = Version(new_version)
    release_branch = f"branch-{version.major}.{version.minor}"
    exists_on_remote = (
        subprocess.check_output(
            ["git", "ls-remote", "--heads", remote, release_branch], text=True
        ).strip()
        != ""
    )
    if exists_on_remote:
        print(f"{release_branch} already exists on {remote}, skipping branch creation")
        return

    prev_branch = subprocess.check_output(["git", "branch", "--show-current"], text=True).strip()
    try:
        exists_on_local = (
            subprocess.check_output(["git", "branch", "--list", release_branch], text=True).strip()
            != ""
        )
        if exists_on_local:
            print(f"Deleting existing {release_branch}")
            subprocess.check_call(["git", "branch", "-D", release_branch])

        print(f"Creating {release_branch}")
        subprocess.check_call(["git", "checkout", "-b", release_branch, "master"])
        print(f"Pushing {release_branch} to {remote}")
        subprocess.check_call(
            ["git", "push", remote, release_branch, *(["--dry-run"] if dry_run else [])]
        )
    finally:
        subprocess.check_call(["git", "checkout", prev_branch])


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create a release branch")
    parser.add_argument("--new-version", required=True, help="New version to release")
    parser.add_argument("--remote", default="origin", help="Git remote to use (default: origin)")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=os.environ.get("DRY_RUN", "true").lower() == "true",
        help="Dry run mode (default: True, can be set via DRY_RUN env var)",
    )
    parser.add_argument(
        "--no-dry-run",
        action="store_false",
        dest="dry_run",
        help="Disable dry run mode",
    )
    args = parser.parse_args()
    main(args.new_version, args.remote, args.dry_run)


--- dev/create_release_tag.py ---
"""
How to test this script
-----------------------
# Ensure origin points to your fork
git remote -v | grep origin

# Pretend we're releasing MLflow 9.0.0
git checkout -b branch-9.0

# First, test the dry run mode
python dev/create_release_tag.py --new-version 9.0.0 --dry-run
git tag -d v9.0.0

# Open https://github.com/<username>/mlflow/tree/v9.0.0 and verify that the tag does not exist.

# Then, test the non-dry run mode
python dev/create_release_tag.py --new-version 9.0.0 --no-dry-run
git tag -d v9.0.0

# Open https://github.com/<username>/mlflow/tree/v9.0.0 and verify that the tag exists now.

# Clean up the remote tag
git push --delete origin v9.0.0

# Clean up the local release branch
git checkout master
git branch -D branch-9.0
"""

import argparse
import os
import subprocess


def main(new_version: str, remote: str, dry_run: bool = False):
    release_tag = f"v{new_version}"
    subprocess.run(["git", "tag", release_tag], check=True)
    subprocess.run(
        ["git", "push", remote, release_tag, *(["--dry-run"] if dry_run else [])], check=True
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create a release tag")
    parser.add_argument("--new-version", required=True, help="New version to release")
    parser.add_argument("--remote", default="origin", help="Git remote to use (default: origin)")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=os.environ.get("DRY_RUN", "true").lower() == "true",
        help="Dry run mode (default: True, can be set via DRY_RUN env var)",
    )
    parser.add_argument(
        "--no-dry-run",
        action="store_false",
        dest="dry_run",
        help="Disable dry run mode",
    )
    args = parser.parse_args()
    main(args.new_version, args.remote, args.dry_run)


--- dev/show_package_release_dates.py ---
import asyncio
import json
import subprocess
import sys
import traceback

import aiohttp


def get_distributions() -> list[tuple[str, str]]:
    res = subprocess.check_output(
        [sys.executable, "-m", "pip", "list", "--format", "json"], text=True
    )
    return [(pkg["name"], pkg["version"]) for pkg in json.loads(res)]


async def get_release_date(session: aiohttp.ClientSession, package: str, version: str) -> str:
    try:
        async with session.get(f"https://pypi.python.org/pypi/{package}/json", timeout=10) as resp:
            if resp.status != 200:
                return ""

            resp_json = await resp.json()
            matched = [
                dist_files for ver, dist_files in resp_json["releases"].items() if ver == version
            ]
            if not matched or not matched[0]:
                return ""

            upload_time = matched[0][0]["upload_time"]
            return upload_time.replace("T", " ")  # return year-month-day hour:minute:second
    except Exception:
        traceback.print_exc()
        return ""


def get_longest_string_length(array: list[str]) -> int:
    return len(max(array, key=len))


async def main() -> None:
    distributions = get_distributions()
    async with aiohttp.ClientSession() as session:
        tasks = [get_release_date(session, pkg, ver) for pkg, ver in distributions]
        release_dates = await asyncio.gather(*tasks)

    packages, versions = list(zip(*distributions))
    package_length = get_longest_string_length(packages)
    version_length = get_longest_string_length(versions)
    release_timestamp_length = get_longest_string_length(release_dates)
    print(
        "Package".ljust(package_length),
        "Version".ljust(version_length),
        "Release Timestamp".ljust(release_timestamp_length),
    )
    print("-" * (package_length + version_length + release_timestamp_length + 2))
    for package, version, release_date in sorted(
        zip(packages, versions, release_dates),
        # Sort by release date in descending order
        key=lambda x: x[2],
        reverse=True,
    ):
        print(
            package.ljust(package_length),
            version.ljust(version_length),
            release_date.ljust(release_timestamp_length),
        )


if __name__ == "__main__":
    asyncio.run(main())


--- dev/update_changelog.py ---
import argparse
import os
import re
import subprocess
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, NamedTuple

import requests
from packaging.version import Version


def get_header_for_version(version):
    return "## {} ({})".format(version, datetime.now().strftime("%Y-%m-%d"))


def extract_pr_num_from_git_log_entry(git_log_entry):
    m = re.search(r"\(#(\d+)\)$", git_log_entry)
    return int(m.group(1)) if m else None


def format_label(label: str) -> str:
    key = label.split("/", 1)[-1]
    return {
        "model-registry": "Model Registry",
        "uiux": "UI",
    }.get(key, key.capitalize())


class PullRequest(NamedTuple):
    title: str
    number: int
    author: str
    labels: list[str]

    @property
    def url(self):
        return f"https://github.com/mlflow/mlflow/pull/{self.number}"

    @property
    def release_note_labels(self):
        return [l for l in self.labels if l.startswith("rn/")]

    def __str__(self):
        areas = " / ".join(
            sorted(
                map(
                    format_label,
                    filter(lambda l: l.split("/")[0] in ("area", "language"), self.labels),
                )
            )
        )
        return f"[{areas}] {self.title} (#{self.number}, @{self.author})"

    def __repr__(self):
        return str(self)


class Section(NamedTuple):
    title: str
    items: list[Any]

    def __str__(self):
        if not self.items:
            return ""
        return "\n\n".join(
            [
                self.title,
                "\n".join(f"- {item}" for item in self.items),
            ]
        )


def is_shallow():
    return (
        subprocess.check_output(
            [
                "git",
                "rev-parse",
                "--is-shallow-repository",
            ],
            text=True,
        ).strip()
        == "true"
    )


def batch_fetch_prs_graphql(pr_numbers: list[int]) -> list[PullRequest]:
    """
    Batch fetch PR data using GitHub GraphQL API.
    """
    if not pr_numbers:
        return []

    # GitHub GraphQL has query size limits, so batch in chunks
    MAX_PRS_PER_QUERY = 50  # Conservative limit to avoid query size issues
    all_prs: list[PullRequest] = []

    for i in range(0, len(pr_numbers), MAX_PRS_PER_QUERY):
        chunk = pr_numbers[i : i + MAX_PRS_PER_QUERY]
        chunk_prs = _fetch_pr_chunk_graphql(chunk)
        all_prs.extend(chunk_prs)

    return all_prs


def _fetch_pr_chunk_graphql(pr_numbers: list[int]) -> list[PullRequest]:
    """
    Fetch a chunk of PRs using GraphQL.
    """
    # Build GraphQL query with aliases for each PR
    query_parts = [
        "query($owner: String!, $repo: String!) {",
        "  repository(owner: $owner, name: $repo) {",
    ]

    for i, pr_num in enumerate(pr_numbers):
        query_parts.append(f"""
    pr{i}: pullRequest(number: {pr_num}) {{
      number
      title
      author {{
        login
      }}
      labels(first: 100) {{
        nodes {{
          name
        }}
      }}
    }}""")

    query_parts.extend(["  }", "}"])
    query = "\n".join(query_parts)

    # Headers with authentication
    headers = {"Content-Type": "application/json"}
    if token := os.getenv("GITHUB_TOKEN"):
        headers["Authorization"] = f"Bearer {token}"
    print(f"Batch fetching {len(pr_numbers)} PRs with GraphQL...")
    resp = requests.post(
        "https://api.github.com/graphql",
        json={
            "query": query,
            "variables": {"owner": "mlflow", "repo": "mlflow"},
        },
        headers=headers,
    )
    resp.raise_for_status()
    data = resp.json()
    if "errors" in data:
        raise Exception(f"GraphQL errors: {data['errors']}")

    # Extract PR data from response and create PullRequest objects
    repository_data = data["data"]["repository"]
    prs = []
    for i, pr_num in enumerate(pr_numbers):
        pr_info = repository_data.get(f"pr{i}")
        if pr_info and pr_info.get("author"):
            prs.append(
                PullRequest(
                    title=pr_info["title"],
                    number=pr_info["number"],
                    author=pr_info["author"]["login"],
                    labels=[label["name"] for label in pr_info["labels"]["nodes"]],
                )
            )
        else:
            print(f"Warning: Could not fetch data for PR #{pr_num}")

    return prs


def main(prev_version, release_version, remote):
    if is_shallow():
        print("Unshallowing repository to ensure `git log` works correctly")
        subprocess.check_call(["git", "fetch", "--unshallow"])
        print("Modifying .git/config to fetch remote branches")
        subprocess.check_call(
            ["git", "config", "remote.origin.fetch", "+refs/heads/*:refs/remotes/origin/*"]
        )
    release_tag = f"v{prev_version}"
    ver = Version(release_version)
    branch = f"branch-{ver.major}.{ver.minor}"
    subprocess.check_call(["git", "fetch", remote, "tag", release_tag])
    subprocess.check_call(["git", "fetch", remote, branch])
    git_log_output = subprocess.check_output(
        [
            "git",
            "log",
            "--left-right",
            "--graph",
            "--cherry-pick",
            "--pretty=format:%s",
            f"tags/{release_tag}...{remote}/{branch}",
        ],
        text=True,
    )
    logs = [l[2:] for l in git_log_output.splitlines() if l.startswith("> ")]

    # Extract all PR numbers first
    pr_numbers = []
    for log in logs:
        if pr_num := extract_pr_num_from_git_log_entry(log):
            pr_numbers.append(pr_num)

    prs = batch_fetch_prs_graphql(pr_numbers)
    label_to_prs = defaultdict(list)
    author_to_prs = defaultdict(list)
    unlabelled_prs = []
    for pr in prs:
        if pr.author == "mlflow-app":
            continue

        if len(pr.release_note_labels) == 0:
            unlabelled_prs.append(pr)

        for label in pr.release_note_labels:
            if label == "rn/none":
                author_to_prs[pr.author].append(pr)
            else:
                label_to_prs[label].append(pr)

    assert len(unlabelled_prs) == 0, "The following PRs need to be categorized:\n" + "\n".join(
        f"- {pr.url}" for pr in unlabelled_prs
    )

    unknown_labels = set(label_to_prs.keys()) - {
        "rn/highlight",
        "rn/feature",
        "rn/breaking-change",
        "rn/bug-fix",
        "rn/documentation",
        "rn/none",
    }
    assert len(unknown_labels) == 0, f"Unknown labels: {unknown_labels}"

    breaking_changes = Section("Breaking changes:", label_to_prs.get("rn/breaking-change", []))
    highlights = Section("Major new features:", label_to_prs.get("rn/highlight", []))
    features = Section("Features:", label_to_prs.get("rn/feature", []))
    bug_fixes = Section("Bug fixes:", label_to_prs.get("rn/bug-fix", []))
    doc_updates = Section("Documentation updates:", label_to_prs.get("rn/documentation", []))
    small_updates = [
        ", ".join([f"#{pr.number}" for pr in prs] + [f"@{author}"])
        for author, prs in author_to_prs.items()
    ]
    small_updates = "Small bug fixes and documentation updates:\n\n" + "; ".join(small_updates)
    sections = filter(
        str.strip,
        map(
            str,
            [
                get_header_for_version(release_version),
                f"MLflow {release_version} includes several major features and improvements",
                breaking_changes,
                highlights,
                features,
                bug_fixes,
                doc_updates,
                small_updates,
            ],
        ),
    )
    new_changelog = "\n\n".join(sections)
    changelog_header = "# CHANGELOG"
    changelog = Path("CHANGELOG.md")
    old_changelog = changelog.read_text().replace(f"{changelog_header}\n\n", "", 1)
    new_changelog = "\n\n".join(
        [
            changelog_header,
            new_changelog,
            old_changelog,
        ]
    )
    changelog.write_text(new_changelog)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Update CHANGELOG.md")
    parser.add_argument("--prev-version", required=True, help="Previous version")
    parser.add_argument("--release-version", required=True, help="MLflow version to release")
    parser.add_argument("--remote", default="origin", help="Git remote to use (default: origin)")
    args = parser.parse_args()
    main(args.prev_version, args.release_version, args.remote)


--- dev/validate_release_version.py ---
import argparse

from packaging.version import Version


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--version", help="Release version to validate, e.g., '1.2.3'", required=True
    )
    return parser.parse_args()


def main():
    args = parse_args()
    version = Version(args.version)
    msg = (
        f"Invalid release version: '{args.version}', "
        "must be in the format of <major>.<minor>.<micro>"
    )
    assert len(version.release) == 3, msg


if __name__ == "__main__":
    main()


--- libs/skinny/LICENSE.txt ---
../../LICENSE.txt

--- CLAUDE.md ---
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**For contribution guidelines, code standards, and additional development information not covered here, please refer to [CONTRIBUTING.md](./CONTRIBUTING.md).**

## Repository Overview

MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for:

- Experiment tracking
- Model versioning and deployment
- LLM observability and tracing
- Model evaluation
- Prompt management

## Quick Start: Development Server

### Start the Full Development Environment (Recommended)

```bash
# Kill any existing servers
pkill -f "mlflow server" || true; pkill -f "yarn start" || true

# Start both MLflow backend and React frontend dev servers
nohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &

# Monitor the logs
tail -f /tmp/mlflow-dev-server.log

# Servers will be available at:
# - MLflow backend: http://localhost:5000
# - React frontend: http://localhost:3000
```

This uses `uv` (fast Python package manager) to automatically manage dependencies and run the development environment.

### Start Development Server with Databricks Backend

To run the MLflow dev server that proxies requests to a Databricks workspace:

```bash
# IMPORTANT: All four environment variables below are REQUIRED for proper Databricks backend operation
# Set them in this exact order:
export DATABRICKS_HOST="https://your-workspace.databricks.com"  # Your Databricks workspace URL
export DATABRICKS_TOKEN="your-databricks-token"                # Your Databricks personal access token
export MLFLOW_TRACKING_URI="databricks"                        # Must be set to "databricks"
export MLFLOW_REGISTRY_URI="databricks-uc"                     # Use "databricks-uc" for Unity Catalog, or "databricks" for workspace model registry

# Start the dev server with these environment variables
nohup uv run bash dev/run-dev-server.sh > /tmp/mlflow-dev-server.log 2>&1 &

# Monitor the logs
tail -f /tmp/mlflow-dev-server.log

# The MLflow server will now proxy tracking and model registry requests to Databricks
# Access the UI at http://localhost:3000 to see your Databricks experiments and models
```

**Note**: The MLflow server acts as a proxy, forwarding API requests to your Databricks workspace while serving the local React frontend. This allows you to develop and test UI changes against real Databricks data.

## Development Commands

### Testing

```bash
# First-time setup: Install test dependencies
uv sync
uv pip install -r requirements/test-requirements.txt

# Run Python tests
uv run pytest tests/

# Run specific test file
uv run pytest tests/test_version.py

# Run tests with specific package versions
uv run --with abc==1.2.3 --with xyz==4.5.6 pytest tests/test_version.py

# Run tests with optional dependencies/extras
uv run --with transformers pytest tests/transformers
uv run --extra gateway pytest tests/gateway

# Run JavaScript tests
yarn --cwd mlflow/server/js test
```

**IMPORTANT**: `uv` may fail initially because the environment has not been set up yet. Follow the instructions to set up the environment and then rerun `uv` as needed.

### Code Quality

```bash
# Python linting and formatting with Ruff
uv run --only-group lint ruff check . --fix         # Lint with auto-fix
uv run --only-group lint ruff format .              # Format code

# Custom MLflow linting with Clint
uv run --only-group lint clint .                    # Run MLflow custom linter

# Check for MLflow spelling typos
uv run --only-group lint bash dev/mlflow-typo.sh .

# JavaScript linting and formatting
yarn --cwd mlflow/server/js lint
yarn --cwd mlflow/server/js prettier:check
yarn --cwd mlflow/server/js prettier:fix

# Type checking
yarn --cwd mlflow/server/js type-check

# Run all checks
yarn --cwd mlflow/server/js check-all
```

### Special Testing

```bash
# Run tests with minimal dependencies (skinny client)
uv run bash dev/run-python-skinny-tests.sh
```

### Documentation

```bash
# Build documentation site (needs gateway extras for API doc generation)
uv run --all-extras bash dev/build-docs.sh --build-api-docs

# Build with R docs included
uv run --all-extras bash dev/build-docs.sh --build-api-docs --with-r-docs

# Serve documentation locally (after building)
cd docs && yarn serve --port 8080
```

## Important Files

- `pyproject.toml`: Package configuration and tool settings
- `.python-version`: Minimum Python version (3.10)
- `requirements/`: Dependency specifications
- `mlflow/ml-package-versions.yml`: Supported ML framework versions

## Common Development Tasks

### Modifying the UI

See `mlflow/server/js/` for frontend development.

## Language-Specific Style Guides

- [Python](/dev/guides/python.md)

## Git Workflow

### Committing Changes

**IMPORTANT**: After making your commits, run pre-commit hooks on your PR changes to ensure code quality:

```bash
# Make your commit first (with DCO sign-off)
git commit -s -m "Your commit message"

# Then check all files changed in your PR
uv run --only-group lint pre-commit run --from-ref origin/master --to-ref HEAD

# Fix any issues and amend your commit if needed
git add <fixed files>
git commit --amend -s

# Re-run pre-commit to verify fixes
uv run --only-group lint pre-commit run --from-ref origin/master --to-ref HEAD

# Only push once all checks pass
git push origin <your-branch>
```

This workflow ensures you only check files you've actually modified in your PR, avoiding false positives from unrelated files.

**IMPORTANT**: You MUST sign all commits with DCO (Developer Certificate of Origin). Always use the `-s` flag:

```bash
# REQUIRED: Always use -s flag when committing
git commit -s -m "Your commit message"

# This will NOT work - missing -s flag
# git commit -m "Your commit message"  ❌
```

Commits without DCO sign-off will be rejected by CI.

**Frontend Changes**: If your PR touches any code in `mlflow/server/js/`, you MUST run `yarn check-all` before committing:

```bash
yarn --cwd mlflow/server/js check-all
```

### Creating Pull Requests

Follow [the PR template](./.github/pull_request_template.md) when creating pull requests. Remove any unused checkboxes from the template to keep your PR clean and focused.

### Checking CI Status

Use GitHub CLI to check for failing CI:

```bash
# Check workflow runs for current branch
gh run list --branch $(git branch --show-current)

# View details of a specific run
gh run view <run-id>

# Watch a run in progress
gh run watch
```

## Pre-commit Hooks

The repository uses pre-commit for code quality. Install hooks with:

```bash
uv run --only-group lint pre-commit install --install-hooks
```

Run pre-commit manually:

```bash
# Run on all files
uv run --only-group lint pre-commit run --all-files

# Run on all files, skipping hooks that require external tools
SKIP=taplo,typos,conftest uv run --only-group lint pre-commit run --all-files

# Run on specific files
uv run --only-group lint pre-commit run --files path/to/file.py

# Run a specific hook
uv run --only-group lint pre-commit run ruff --all-files
```

This runs Ruff, typos checker, and other tools automatically before commits.

**Note about external tools**: Some pre-commit hooks require external tools that aren't Python packages:

- `taplo` - TOML formatter
- `typos` - Spell checker
- `conftest` - Policy testing tool

To install these tools:

```bash
# Install all tools at once (recommended)
uv run --only-group lint bin/install.py
```

This automatically downloads and installs the correct versions of all external tools to the `bin/` directory. The tools work on both Linux and ARM Macs.

These tools are optional. Use `SKIP=taplo,typos,conftest` if they're not installed.

**Note**: If the typos hook fails, you only need to fix typos in code that was changed by your PR, not pre-existing typos in the codebase.


--- CODE_OF_CONDUCT.rst ---
MLflow Contributor Covenant Code of Conduct
===========================================

.. contents:: **Table of Contents**
  :local:
  :depth: 4

Our Pledge
##########

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

Our Standards
#############

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a professional setting

Our Responsibilities
####################

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

Scope
#####

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

Enforcement
###########

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the Technical Steering Committee defined `here <https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#governance>`_.
All complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

Attribution
###########

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq


--- COMMITTER.md ---
### Evaluation Criteria

When evaluating potential new MLflow committers, the following criteria will be considered:

- **Code Contributions**: Should have multiple non-trivial code contributions accepted and committed to the MLflow codebase. This demonstrates the ability to produce quality code aligned with the project's standards.
- **Technical Expertise**: Should demonstrate a deep understanding of MLflow's architecture and design principles, evidenced by making appropriate design choices and technical recommendations. History of caring about code quality, testing, maintainability, and ability to critically evaluate technical artifacts (PRs, designs, etc.) and provide constructive suggestions for improvement.
- **Subject Matter Breadth**: Contributions and learnings span multiple areas of the codebase, APIs, and integration points rather than a narrow niche.
- **Community Participation**: Active participation for at least 3 months prior to nomination by authoring code contributions and engaging in the code review process. Involvement in mailing lists, Slack channels, Stack Overflow, and GitHub issues is valued but not strictly required.
- **Communication**: Should maintain a constructive tone in communications, be receptive to feedback, and collaborate well with existing committers and other community members.
- **Project Commitment**: Demonstrate commitment to MLflow's long-term success, uphold project principles and values, and willingness to pitch in for "unglamorous" work.

### Committership Nomination

- Any current MLflow committer can nominate a contributor for committership by emailing MLflow's TSC members with a nomination packet.
- The nomination packet should provide details on the nominee's salient contributions, as well as justification on how they meet the evaluation criteria. Links to GitHub activity, mailing list threads, and other artifacts should be included.
- In addition to the nominator, every nomination must have a seconder -- a separate committer who advocates for the nominee. The seconder should be a more senior committer (active committer for >1 year) familiar with the nominee's work.
- It is the nominator's responsibility to identify a willing seconder and include their recommendation in the nomination packet.
- If no eligible seconder is available or interested, it may indicate insufficient support to proceed with the nomination at that time. This ensures there are two supporting committers invested in each nomination - the nominator and the seconder. The seconder's seniority and familiarity with the situation also help build more consensus among the TSC members during evaluation.

### Evaluation Process

- When a committer nomination is made, the TSC members closely review the proposal and evaluate the nominee's qualifications.
- Throughout the review, the nominator is responsible for addressing any questions from the TSC, and providing clarification or additional evidence as requested by TSC members.
- After adequate discussion (~1 week), the nominator calls for a formal consensus check among the TSC.
- A positive consensus requires at least 2 TSC +1 binding votes and no vetoes.
- Any vetoes must be accompanied by a clear rationale that can be debated.
- If consensus is not achieved, the nomination is rejected at that time.
- If consensus fails, the nominator summarizes substantive feedback and remaining gaps to the nominee for their growth and potential re-nomination later. Nomination can be tried again in 3 months after addressing any gaps identified.

### Onboarding a new committer

- Upon a positive consensus being reached, one of the TSC members will extend the formal invitation to the nominee to become a committer. They also field the private initial response from the nominee on willingness to accept.
- If the proposal is accepted, the nominator grants them the commit access and the new committer will be:
  - Added to the committer list in the README.md
  - Announced on the MLflow mailing lists, Slack channels, and the MLflow website
  - Spotlighted through a post on the MLflow LinkedIn and X handles
- The nominator will work with the new committer to identify well-scoped initial areas for the new committer to focus on, such as improvements to a specific component.
- The nominator will also set up periodic 1:1 mentorship check-ins with the new committer over their first month to provide guidance where needed.


--- EXTRA_DEPENDENCIES.rst ---
=========================
Extra MLflow Dependencies
=========================

When you `install the MLflow Python package <https://mlflow.org/docs/latest/quickstart.html#installing-mlflow>`_,
a set of core dependencies needed to use most MLflow functionality (tracking, projects, models APIs)
is also installed.

However, in order to use certain framework-specific MLflow APIs or configuration options,
you need to install additional, "extra" dependencies. For example, the model persistence APIs under
the ``mlflow.sklearn`` module require scikit-learn to be installed. Some of the most common MLflow
extra dependencies can be installed via ``pip install mlflow[extras]``.

The full set of extra dependencies are documented, along with the modules that depend on them,
in the following files:

* extra-ml-requirements.txt: ML libraries needed to use model persistence and inference APIs
* test-requirements.txt: Libraries required to use non-default artifact-logging and tracking server configurations


--- ISSUE_POLICY.md ---
# Issue Policy

The MLflow Issue Policy outlines the categories of MLflow GitHub issues and discusses the guidelines & processes
associated with each type of issue.

Before filing an issue, make sure to [search for related issues](https://github.com/mlflow/mlflow/issues) and check if
they address yours.

For support (ex. "How do I do X?"), please ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/mlflow).

## Issue Categories

Our policy is that GitHub issues fall into one of the following categories:

1. Feature Requests
2. Bug reports
3. Documentation fixes
4. Installation issues

Each category has its own GitHub issue template. Please do not delete the issue template unless you are certain your
issue is outside its scope.

### Feature Requests

#### Guidelines

Feature requests that are likely to be accepted:

- Are minimal in scope (note that it's always easier to add additional functionality later than remove functionality)
- Are extensible (e.g. if adding an integration with an ML framework, is it possible to add similar integrations with other frameworks?)
- Have user impact & value that justifies the maintenance burden of supporting the feature moving forwards. The
  [JQuery contributor guide](https://contribute.jquery.org/open-source/#contributing-something-new) has an excellent discussion on this.

#### Lifecycle

Feature requests typically go through the following lifecycle:

1. A feature request GitHub Issue is submitted, which contains a high-level description of the proposal and its motivation.
   We encourage requesters to provide an overview of the feature's implementation as well, if possible.
2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route feature requests to appropriate committers.
3. The feature request is discussed with a committer. The committer will provide input on the implementation overview or
   ask for a more detailed design, if applicable.
4. After discussion & agreement on the feature request and its implementation, an implementation owner is identified.
5. The implementation owner begins developing the feature and ultimately files associated pull requests against the
   MLflow Repository or packages the feature as an MLflow Plugin.

### Bug reports

#### Guidelines

In order to ensure that maintainers are able to assist in any reported bug:

- Ensure that the bug report template is filled out in its entirety with appropriate levels of detail, particularly in the `Code to reproduce issue` section.
- Verify that the bug you are reporting meets one of the following criteria:
  - A recent release of MLflow does not support the operation you are doing that an earlier release did (a regression).
  - A [documented feature](https://mlflow.org/docs/latest/index.html) or functionality does not work properly by executing a provided example from the docs.
  - Any exception raised is directly from MLflow and is not the result of an underlying package's exception (e.g., don't file an issue that MLflow can't log a model that can't be trained due to a tensorflow Exception)
- Make a best effort to diagnose and troubleshoot the issue prior to filing.
- Verify that the environment that you're experiencing the bug in is supported as defined in the docs.
- Validate that MLflow supports the functionality that you're having an issue with. _A lack of a feature does not constitute a bug_.
- Read the docs on the feature for the issue that you're reporting. If you're certain that you're following documented guidelines, please file a bug report.

Bug reports typically go through the following lifecycle:

1. A bug report GitHub Issue is submitted, which contains a high-level description of the bug and information required to reproduce it.
2. The [bug report is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route to request appropriate committers.
3. An MLflow committer reproduces the bug and provides feedback about how to implement a fix.
4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt
   ownership of severe bugs to ensure a timely fix.
5. The fix owner begins implementing the fix and ultimately files associated pull requests.

### Documentation fixes

Documentation issues typically go through the following lifecycle:

1. A documentation GitHub Issue is submitted, which contains a description of the issue and its location(s) in the MLflow documentation.
2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the request to appropriate committers.
3. An MLflow committer confirms the documentation issue and provides feedback about how to implement a fix.
4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt
   ownership of severe documentation issues to ensure a timely fix.
5. The fix owner begins implementing the fix and ultimately files associated pull requests.

### Installation issues

Installation issues typically go through the following lifecycle:

1. An installation GitHub Issue is submitted, which contains a description of the issue and the platforms its affects.
2. The [issue is triaged](ISSUE_TRIAGE.rst) to identify whether more information is needed from the author, give an indication of priority, and route the issue to appropriate committers.
3. An MLflow committer confirms the installation issue and provides feedback about how to implement a fix.
4. After an approach has been agreed upon, an owner for the fix is identified. MLflow committers may choose to adopt
   ownership of severe installation issues to ensure a timely fix.
5. The fix owner begins implementing the fix and ultimately files associated pull requests.


--- ISSUE_TRIAGE.rst ---

This document is a hands-on manual for doing issue and pull request triage for `MLflow issues
on GitHub <https://github.com/mlflow/mlflow/issues>`_ .
The purpose of triage is to speed up issue management and get community members faster responses.

Issue and pull request triage has three steps:

- assign one or more process labels (e.g. ``needs design`` or ``help wanted``),
- mark a priority, and
- label one or more relevant areas, languages, or integrations to help route issues to appropriate contributors or reviewers.

The remainder of the document describes the labels used in each of these steps and how to apply them.

Assign appropriate process labels
#######
Assign at least one process label to every issue you triage.

- ``needs author feedback``: We need input from the author of the issue or PR to proceed.
- | ``needs design``: This feature is large or tricky enough that we think it warrants a design doc
  | and review before someone begins implementation.
- | ``needs committer feedback``: The issue has a design that is ready for committer review, or there is
  | an issue or pull request that needs feedback from a committer about the approach or appropriateness
  | of the contribution.
- | ``needs review``: Use this label for issues that need a more detailed design review or pull
  | requests ready for review (all questions answered, PR updated if requests have been addressed,
  | tests passing).
- ``help wanted``: We would like community help for this issue.
- ``good first issue``: This would make a good first issue.


Assign priority
#######

You should assign a priority to each issue you triage. We use `kubernetes-style <https://github.com/
kubernetes/community/blob/master/contributors/guide/issue-triage.md#define-priority>`_ priority
labels.

- | ``priority/critical-urgent``: This is the highest priority and should be worked on by
  | somebody right now. This should typically be reserved for things like security bugs,
  | regressions, release blockers.
- | ``priority/important-soon``: The issue is worked on by the community currently or will
  | be very soon, ideally in time for the next release.
- | ``priority/important-longterm``: Important over the long term, but may not be staffed or
  | may need multiple releases to complete. Also used for things we know are on a
  | contributor's roadmap in the next few months. We can use this in conjunction with
  | ``help wanted`` to mark issues we would like to get help with. If someone begins actively
  | working on an issue with this label and we think it may be merged by the next release, change
  | the priority to ``priority/important-soon``.
- | ``priority/backlog``: We believe it is useful but don't see it being prioritized in the
  | next few months. Use this for issues that are lower priority than ``priority/important-longterm``.
  | We welcome community members to pick up a ``priority/backlog`` issue, but there may be some
  | delay in getting support through design review or pull request feedback.
- | ``priority/awaiting-more-evidence``: Lowest priority. Possibly useful, but not yet enough
  | support to actually get it done. This is a good place to put issues that could be useful but
  | require more evidence to demonstrate broad value. Don't use it as a way to say no.
  | If we think it doesn't fit in MLflow, we should just say that and why.

Label relevant areas
#######

Assign one more labels for relevant component or interface surface areas, languages, or
integrations. As a principle, we aim to have the minimal set of labels needed to help route issues
and PRs to appropriate contributors. For example, a ``language/python`` label would not be
particularly helpful for routing issues to committers, since most PRs involve Python code.
``language/java`` and ``language/r`` make sense to have, as the clients in these languages differ from the Python client and aren't maintained by many people. As with process labels, we
take inspiration from Kubernetes on naming conventions.

Components
""""""""
- ``area/artifacts``: Artifact stores and artifact logging
- ``area/build``: Build and test infrastructure for MLflow
- ``area/docs``: MLflow documentation pages
- ``area/evaluation``: MLflow model evaluation features, evaluation metrics, and evaluation workflows
- ``area/examples``: Example code
- ``area/gateway``: AI Gateway service, Gateway client APIs, third-party Gateway integrations
- ``area/model-registry``: Model Registry service, APIs, and the fluent client calls for Model Registry
- ``area/models``: MLmodel format, model serialization/deserialization, flavors
- ``area/projects``: MLproject format, project execution backends
- ``area/prompt``: MLflow prompt engineering features, prompt templates, and prompt management
- ``area/scoring``: MLflow Model server, model deployment tools, Spark UDFs
- ``area/server-infra``: MLflow Tracking server backend
- ``area/tracing``: MLflow Tracing features, tracing APIs, and LLM tracing functionality
- ``area/tracking``: Tracking Service, tracking client APIs, autologging

Interface Surface
""""""""
- ``area/uiux``: Front-end, user experience, plotting, JavaScript, JavaScript dev server
- ``area/docker``: Docker use across MLflow's components, such as MLflow Projects and MLflow Models
- ``area/sqlalchemy``: Use of SQLAlchemy in the Tracking Service or Model Registry
- ``area/windows``: Windows support

Language Surface
""""""""
- ``language/r``: R APIs and clients
- ``language/java``: Java APIs and clients
- ``language/new``: Proposals for new client languages

Integrations
""""""""
- ``integrations/azure``: Azure and Azure ML integrations
- ``integrations/sagemaker``: SageMaker integrations
- ``integrations/databricks``: Databricks integrations


--- README.md ---
<h1 align="center" style="border-bottom: none">
    <a href="https://mlflow.org/">
        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />
    </a>
</h1>
<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>

MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.

<div align="center">

[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)
[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)
[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)
<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">
<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"
      alt="follow on X(Twitter)"></a>
<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">
<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"
      alt="follow on LinkedIn"></a>
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)

</div>

<div align="center">
   <div>
      <a href="https://mlflow.org/"><strong>Website</strong></a> ·
      <a href="https://mlflow.org/docs/latest"><strong>Docs</strong></a> ·
      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> ·
      <a href="https://mlflow.org/blog"><strong>News</strong></a> ·
      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> ·
      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>
   </div>
</div>

<br>

## 🚀 Installation

To install the MLflow Python package, run the following command:

```
pip install mlflow
```

## 📦 Core Components

MLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.

### 💡 For LLM / GenAI Developers

<table>
  <tr>
    <td>
    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>🔍 Tracing / Observability</strong></a>
        <br><br>
        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>📊 LLM Evaluation</strong></a>
        <br><br>
        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>🤖 Prompt Management</strong></a>
        <br><br>
        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>📦 App Version Tracking</strong></a>
        <br><br>
        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
</table>

### 🎓 For Data Scientists

<table>
  <tr>
    <td colspan="2" align="center" >
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>📝 Experiment Tracking</strong></a>
        <br><br>
        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>💾 Model Registry</strong></a>
        <br><br>
        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>🚀 Deployment</strong></a>
        <br><br>
        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
</table>

## 🌐 Hosting MLflow Anywhere

<div align="center" >
  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>
</div>

You can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.

Trusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:

- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)
- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)
- [Databricks](https://www.databricks.com/product/managed-mlflow)
- [Nebius](https://nebius.com/services/managed-mlflow)

For hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).

## 🗣️ Supported Programming Languages

- [Python](https://pypi.org/project/mlflow/)
- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)
- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)
- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)

## 🔗 Integrations

MLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.

![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)

## Usage Examples

### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))

The following examples trains a simple regression model with scikit-learn, while enabling MLflow's [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.

```python
import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# Enable MLflow's automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog()

# Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting
rf.fit(X_train, y_train)
```

Once the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.

```
mlflow ui
```

### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))

The following example runs automatic evaluation for question-answering tasks with several built-in metrics.

```python
import mlflow
import pandas as pd

# Evaluation set contains (1) input question (2) model outputs (3) ground truth
df = pd.DataFrame(
    {
        "inputs": ["What is MLflow?", "What is Spark?"],
        "outputs": [
            "MLflow is an innovative fully self-driving airship powered by AI.",
            "Sparks is an American pop and rock duo formed in Los Angeles.",
        ],
        "ground_truth": [
            "MLflow is an open-source platform for productionizing AI.",
            "Apache Spark is an open-source, distributed computing system.",
        ],
    }
)
eval_dataset = mlflow.data.from_pandas(
    df, predictions="outputs", targets="ground_truth"
)

# Start an MLflow Run to record the evaluation results to
with mlflow.start_run(run_name="evaluate_qa"):
    # Run automatic evaluation with a set of built-in metrics for question-answering models
    results = mlflow.evaluate(
        data=eval_dataset,
        model_type="question-answering",
    )

print(results.tables["eval_results_table"])
```

### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))

MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.

```python
import mlflow
from openai import OpenAI

# Enable tracing for OpenAI
mlflow.openai.autolog()

# Query OpenAI LLM normally
response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hi!"}],
    temperature=0.1,
)
```

Then navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.

## 💭 Support

- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest).
- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.
- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.
- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).
- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)
  or join us on [Slack](https://mlflow.org/slack).

## 🤝 Contributing

We happily welcome contributions to MLflow!

- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)
- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)
- Writing about MLflow and sharing your experience

Please see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.

## ⭐️ Star History

<a href="https://star-history.com/#mlflow/mlflow&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />
 </picture>
</a>

## ✏️ Citation

If you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.

## 👥 Core Members

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- [Ben Wilson](https://github.com/BenWilson2)
- [Corey Zumar](https://github.com/dbczumar)
- [Daniel Lok](https://github.com/daniellok-db)
- [Gabriel Fu](https://github.com/gabrielfu)
- [Harutaka Kawamura](https://github.com/harupy)
- [Serena Ruan](https://github.com/serena-ruan)
- [Tomu Hirata](https://github.com/TomeHirata)
- [Weichen Xu](https://github.com/WeichenXu123)
- [Yuki Watanabe](https://github.com/B-Step62)


--- bin/README.md ---
# bin

Binary tools for MLflow development.

## Installation

```bash
python bin/install.py
```


--- docker-compose/README.md ---
# MLflow with Docker Compose (PostgreSQL + MinIO)

This directory provides a **Docker Compose** setup for running **MLflow** locally with a **PostgreSQL** backend store and **MinIO** (S3-compatible) artifact storage. It's intended for quick evaluation and local development.

---

## Overview

- **MLflow Tracking Server** — exposed on your host (default `http://localhost:5000`).
- **PostgreSQL** — persists MLflow's metadata (experiments, runs, params, metrics).
- **MinIO** — stores run artifacts via an S3-compatible API.

Compose automatically reads configuration from a local `.env` file in this directory.

---

## Prerequisites

- **Git**
- **Docker** and **Docker Compose**
  - Windows/macOS: [Docker Desktop](https://www.docker.com/products/docker-desktop/)
  - Linux: Docker Engine + the `docker compose` plugin

Verify your setup:

```bash
docker --version
docker compose version
```

---

## 1. Clone the Repository

```bash
git clone https://github.com/mlflow/mlflow.git
cd docker-compose
```

---

## 2. Configure Environment

Copy the example environment file and modify as needed:

```bash
cp .env.dev.example .env
```

The `.env` file defines container image tags, ports, credentials, and storage configuration. Open it and review values before starting the stack.

**Common variables** :

- **MLflow**
  - `MLFLOW_PORT=5000` — host port for the MLflow UI/API
  - `MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow/` — artifact store URI
  - `MLFLOW_S3_ENDPOINT_URL=http://minio:9000` — S3 endpoint (inside the Compose network)
- **PostgreSQL**
  - `POSTGRES_USER=mlflow`
  - `POSTGRES_PASSWORD=mlflow`
  - `POSTGRES_DB=mlflow`
- **MinIO (S3-compatible)**
  - `MINIO_ROOT_USER=minio`
  - `MINIO_ROOT_PASSWORD=minio123`
  - `MINIO_HOST=minio`
  - `MINIO_PORT=9000`
  - `MINIO_BUCKET=mlflow`

---

## 3. Launch the Stack

```bash
docker compose up -d
```

This:

- Builds/pulls images as needed
- Creates a user-defined network
- Starts **postgres**, **minio**, and **mlflow** containers

Check status:

```bash
docker compose ps
```

View logs (useful on first run):

```bash
docker compose logs -f
```

---

## 4. Access MLflow

Open the MLflow UI:

- **URL**: `http://localhost:5000` (or the port set in `.env`)

You can now create experiments, run training scripts, and log metrics, parameters, and artifacts to this local MLflow instance.

---

## 5. Shutdown

To stop and remove the containers and network:

```bash
docker compose down
```

> Data is preserved in Docker **volumes**. To remove volumes as well (irreversible), run:
>
> ```bash
> docker compose down -v
> ```

---

## Tips & Troubleshooting

- **Verify connectivity**  
  If MLflow can't write artifacts, confirm your S3 settings:

  - `MLFLOW_DEFAULT_ARTIFACT_ROOT` points to your MinIO bucket (e.g., `s3://mlflow/`)
  - `MLFLOW_S3_ENDPOINT_URL` is reachable from the MLflow container (often `http://minio:9000`)

- **Resetting the environment**  
  If you want a clean slate, stop the stack and remove volumes:

  ```bash
  docker compose down -v
  docker compose up -d
  ```

- **Logs**

  - MLflow server: `docker compose logs -f mlflow`
  - PostgreSQL: `docker compose logs -f postgres`
  - MinIO: `docker compose logs -f minio`

- **Port conflicts**  
  If `5000` (or any other port) is in use, change it in `.env` and restart:
  ```bash
  docker compose down
  docker compose up -d
  ```

---

## How It Works (at a Glance)

- MLflow uses **PostgreSQL** as the _backend store_ for experiment/run metadata.
- MLflow uses **MinIO** as the _artifact store_ via S3 APIs.
- Docker Compose wires services on a shared network; MLflow talks to PostgreSQL and MinIO by container name (e.g., `postgres`, `minio`).

---

## Next Steps

- Point your training scripts to this server:
  ```bash
  export MLFLOW_TRACKING_URI=http://localhost:5000
  ```
- Start logging runs with `mlflow.start_run()` (Python) or the MLflow CLI.
- Customize the `.env` and `docker-compose.yml` to fit your local workflow (e.g., change image tags, add volumes, etc.).

---

**You now have a fully local MLflow stack with persistent metadata and artifact storage—ideal for development and experimentation.**


--- prettier.config.js ---
module.exports = {
  printWidth: 100,
};


--- .github/workflows/README.md ---
# GitHub Actions workflows

## Testing

| File                      | Role                                                                 |
| :------------------------ | :------------------------------------------------------------------- |
| `cross-version-tests.yml` | Run cross version tests. See `cross-version-testing.md` for details. |
| `examples.yml`            | Run tests for example scripts & projects                             |
| `master.yml `             | Run unit and integration tests                                       |

## Automation

| File                        | Role                                                           |
| :-------------------------- | :------------------------------------------------------------- |
| `autoformat.yml`            | Apply autoformatting when a PR is commented with `autoformat`  |
| `autoformat.js`             | Define utility functions used in the `autoformat.yml` workflow |
| `labeling.yml`              | Automatically apply labels on issues and PRs                   |
| `notify-dco-failure.yml`    | Notify a DCO check failure                                     |
| `notify-dco-failure.js`     | The main script of the `notify-dco-failure.yml` workflow       |
| `release-note-category.yml` | Validate a release-note category label is applied on a PR      |
| `release-note-category.js`  | The main script of the `release-note-category.yml` workflow    |


--- .github/pull_request_template.md ---
### Related Issues/PRs

<!-- Uncomment 'Resolve' if this PR can close the linked items. -->
<!-- Resolve --> #xxx

### What changes are proposed in this pull request?

<!-- Please fill in changes proposed in this PR. -->

### How is this PR tested?

- [ ] Existing unit/integration tests
- [ ] New unit/integration tests
- [ ] Manual tests

<!-- Attach code, screenshot, video used for manual testing here. -->

### Does this PR require documentation update?

- [ ] No. You can skip the rest of this section.
- [ ] Yes. I've updated:
  - [ ] Examples
  - [ ] API references
  - [ ] Instructions

### Release Notes

#### Is this a user-facing change?

- [ ] No. You can skip the rest of this section.
- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.

<!-- Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change. -->

#### What component(s), interfaces, languages, and integrations does this PR affect?

Components

- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging
- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors
- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry
- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs
- [ ] `area/evaluation`: MLflow model evaluation features, evaluation metrics, and evaluation workflows
- [ ] `area/gateway`: MLflow AI Gateway client APIs, server, and third-party integrations
- [ ] `area/prompts`: MLflow prompt engineering features, prompt templates, and prompt management
- [ ] `area/tracing`: MLflow Tracing features, tracing APIs, and LLM tracing functionality
- [ ] `area/projects`: MLproject format, project running backends
- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server
- [ ] `area/build`: Build and test infrastructure for MLflow
- [ ] `area/docs`: MLflow documentation pages

<!--
Insert an empty named anchor here to allow jumping to this section with a fragment URL
(e.g. https://github.com/mlflow/mlflow/pull/123#user-content-release-note-category).
Note that GitHub prefixes anchor names in markdown with "user-content-".
-->

<a name="release-note-category"></a>

#### How should the PR be classified in the release notes? Choose one:

- [ ] `rn/none` - No description will be included. The PR will be mentioned only by the PR number in the "Small Bugfixes and Documentation Updates" section
- [ ] `rn/breaking-change` - The PR will be mentioned in the "Breaking Changes" section
- [ ] `rn/feature` - A new user-facing feature worth mentioning in the release notes
- [ ] `rn/bug-fix` - A user-facing bug fix worth mentioning in the release notes
- [ ] `rn/documentation` - A user-facing documentation change worth mentioning in the release notes

#### Should this PR be included in the next patch release?

`Yes` should be selected for bug fixes, documentation updates, and other small changes. `No` should be selected for new features and larger changes. If you're unsure about the release classification of this PR, leave this unchecked to let the maintainers decide.

<details>
<summary>What is a minor/patch release?</summary>

- Minor release: a release that increments the second part of the version number (e.g., 1.2.0 -> 1.3.0).
  Bug fixes, doc updates and new features usually go into minor releases.
- Patch release: a release that increments the third part of the version number (e.g., 1.2.0 -> 1.2.1).
  Bug fixes and doc updates usually go into patch releases.

</details>

<!-- Do not modify or remove any text inside the parentheses -->

- [ ] Yes (this PR will be cherry-picked and included in the next patch release)
- [ ] No (this PR will be included in the next minor release)


--- .github/workflows/advice.js ---
function sleep(ms) {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

async function getDcoCheck(github, owner, repo, sha) {
  const backoffs = [0, 2, 4, 6, 8];
  const numAttempts = backoffs.length;
  for (const [index, backoff] of backoffs.entries()) {
    await sleep(backoff * 1000);
    const resp = await github.rest.checks.listForRef({
      owner,
      repo,
      ref: sha,
      app_id: 1861, // ID of the DCO check app
    });

    const { check_runs } = resp.data;
    if (check_runs.length > 0 && check_runs[0].status === "completed") {
      return check_runs[0];
    }
    console.log(`[Attempt ${index + 1}/${numAttempts}]`, "The DCO check hasn't completed yet.");
  }
}

module.exports = async ({ context, github }) => {
  const { owner, repo } = context.repo;
  const { number: issue_number } = context.issue;
  const { sha, label } = context.payload.pull_request.head;
  const { user, body } = context.payload.pull_request;
  const messages = [];

  const title = "&#x1F6E0 DevTools &#x1F6E0";
  if (body && !body.includes(title)) {
    const codespacesBadge = `[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/${user.login}/mlflow/pull/${issue_number}?quickstart=1)`;
    const newSection = `
<details><summary>${title}</summary>
<p>

${codespacesBadge}

#### Install mlflow from this PR

\`\`\`
# mlflow
pip install git+https://github.com/mlflow/mlflow.git@refs/pull/${issue_number}/merge
# mlflow-skinny
pip install git+https://github.com/mlflow/mlflow.git@refs/pull/${issue_number}/merge#subdirectory=libs/skinny
\`\`\`

For Databricks, use the following command:

\`\`\`
%sh curl -LsSf https://raw.githubusercontent.com/mlflow/mlflow/HEAD/dev/install-skinny.sh | sh -s pull/${issue_number}/merge
\`\`\`

</p>
</details>
`.trim();
    await github.rest.pulls.update({
      owner,
      repo,
      pull_number: issue_number,
      body: `${newSection}\n\n${body}`,
    });
  }

  const dcoCheck = await getDcoCheck(github, owner, repo, sha);
  if (dcoCheck && dcoCheck.conclusion !== "success") {
    messages.push(
      "#### &#x26a0; DCO check\n\n" +
        "The DCO check failed. " +
        `Please sign off your commit(s) by following the instructions [here](${dcoCheck.html_url}). ` +
        "See https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md#sign-your-work for more " +
        "details."
    );
  }

  if (label.endsWith(":master")) {
    messages.push(
      "#### &#x26a0; PR branch check\n\n" +
        "This PR was filed from the master branch in your fork, which is not recommended " +
        "and may cause our CI checks to fail. Please close this PR and file a new PR from " +
        "a non-master branch."
    );
  }

  if (!(body || "").includes("How should the PR be classified in the release notes?")) {
    messages.push(
      "#### &#x26a0; Invalid PR template\n\n" +
        "This PR does not appear to have been filed using the MLflow PR template. " +
        "Please copy the PR template from [here](https://raw.githubusercontent.com/mlflow/mlflow/master/.github/pull_request_template.md) " +
        "and fill it out."
    );
  }

  if (messages.length > 0) {
    const body =
      `@${user.login} Thank you for the contribution! Could you fix the following issue(s)?\n\n` +
      messages.join("\n\n");
    await github.rest.issues.createComment({
      owner,
      repo,
      issue_number,
      body,
    });
  }
};


--- .github/workflows/autoformat.js ---
const createCommitStatus = async (context, github, sha, state) => {
  const { workflow, runId } = context;
  const { owner, repo } = context.repo;
  const target_url = `https://github.com/${owner}/${repo}/actions/runs/${runId}`;
  await github.rest.repos.createCommitStatus({
    owner,
    repo,
    sha,
    state,
    target_url,
    description: sha,
    context: workflow,
  });
};

const shouldAutoformat = (comment) => {
  return comment.body.trim() === "/autoformat";
};

const getPullInfo = async (context, github) => {
  const { owner, repo } = context.repo;
  const pull_number = context.issue.number;
  const pr = await github.rest.pulls.get({ owner, repo, pull_number });
  const {
    sha: head_sha,
    ref: head_ref,
    repo: { full_name },
  } = pr.data.head;
  const { sha: base_sha, ref: base_ref, repo: base_repo } = pr.data.base;
  return {
    repository: full_name,
    pull_number,
    head_sha,
    head_ref,
    base_sha,
    base_ref,
    base_repo: base_repo.full_name,
    author_association: pr.data.author_association,
  };
};

const createReaction = async (context, github) => {
  const { owner, repo } = context.repo;
  const { id: comment_id } = context.payload.comment;
  await github.rest.reactions.createForIssueComment({
    owner,
    repo,
    comment_id,
    content: "rocket",
  });
};

const createStatus = async (context, github, core) => {
  const { head_sha, head_ref, repository } = await getPullInfo(context, github);
  if (repository === "mlflow/mlflow" && head_ref === "master") {
    core.setFailed("Running autoformat bot against master branch of mlflow/mlflow is not allowed.");
  }
  await createCommitStatus(context, github, head_sha, "pending");
};

const updateStatus = async (context, github, sha, needs) => {
  const failed = Object.values(needs).some(({ result }) => result === "failure");
  const state = failed ? "failure" : "success";
  await createCommitStatus(context, github, sha, state);
};

const fetchWorkflowRuns = async ({ context, github, head_sha }) => {
  const { owner, repo } = context.repo;
  const SLEEP_DURATION_MS = 5000;
  const MAX_RETRIES = 5;
  let prevRuns = [];
  for (let i = 0; i < MAX_RETRIES; i++) {
    console.log(`Attempt ${i + 1} to fetch workflow runs`);
    const runs = await github.paginate(github.rest.actions.listWorkflowRunsForRepo, {
      owner,
      repo,
      head_sha,
      status: "action_required",
      actor: "mlflow-app[bot]",
    });

    // If the number of runs has not changed since the last attempt,
    // we can assume that all the workflow runs have been created.
    if (runs.length > 0 && runs.length === prevRuns.length) {
      return runs;
    }

    prevRuns = runs;
    await new Promise((resolve) => setTimeout(resolve, SLEEP_DURATION_MS));
  }
  return prevRuns;
};

const approveWorkflowRuns = async (context, github, head_sha) => {
  const { owner, repo } = context.repo;
  const workflowRuns = await fetchWorkflowRuns({ context, github, head_sha });
  const approvePromises = workflowRuns.map((run) =>
    github.rest.actions.approveWorkflowRun({
      owner,
      repo,
      run_id: run.id,
    })
  );
  const results = await Promise.allSettled(approvePromises);
  for (const result of results) {
    if (result.status === "rejected") {
      console.error(`Failed to approve run: ${result.reason}`);
    }
  }
};

const checkMaintainerAccess = async (context, github) => {
  const { owner, repo } = context.repo;
  const pull_number = context.issue.number;
  const { runId } = context;
  const pr = await github.rest.pulls.get({ owner, repo, pull_number });

  // Skip maintainer access check for copilot bot PRs
  // Copilot bot creates PRs that are owned by the repository and don't need the same permission model
  if (
    pr.data.user?.type?.toLowerCase() === "bot" &&
    pr.data.user?.login?.toLowerCase() === "copilot"
  ) {
    console.log(`Skipping maintainer access check for copilot bot PR #${pull_number}`);
    return;
  }

  const isForkPR = pr.data.head.repo.full_name !== pr.data.base.repo.full_name;
  if (isForkPR && !pr.data.maintainer_can_modify) {
    const workflowRunUrl = `https://github.com/${owner}/${repo}/actions/runs/${runId}`;

    await github.rest.issues.createComment({
      owner,
      repo,
      issue_number: pull_number,
      body: `❌ **Autoformat failed**: The "Allow edits and access to secrets by maintainers" checkbox must be checked for autoformat to work properly.

Please:
1. Check the "Allow edits and access to secrets by maintainers" checkbox on this pull request
2. Comment \`/autoformat\` again

This permission is required for the autoformat bot to push changes to your branch.

**Details:** [View workflow run](${workflowRunUrl})`,
    });

    throw new Error(
      'The "Allow edits and access to secrets by maintainers" checkbox must be checked for autoformat to work properly.'
    );
  }
};

module.exports = {
  shouldAutoformat,
  getPullInfo,
  createReaction,
  createStatus,
  updateStatus,
  approveWorkflowRuns,
  checkMaintainerAccess,
};


--- .github/workflows/autoformat.md ---
# Autoformat

## Testing

1. Checkout a new branch and make changes.
1. Push the branch to your fork (https://github.com/{your_username}/mlflow).
1. Switch the default branch of your fork to the branch you just pushed.
1. Create a GitHub token.
1. Create a new Actions secret with the name `MLFLOW_AUTOMATION_TOKEN` and put the token value.
1. Checkout another new branch and run the following commands to make dummy changes.

   ```shell
   # python
   echo "" >> setup.py
   # js
   echo "" >> mlflow/server/js/src/experiment-tracking/components/App.js
   # protos
   echo "message Foo {}" >> mlflow/protos/service.proto
   ```

1. Create a PR from the branch containing the dummy changes in your fork.
1. Comment `/autoformat` on the PR and ensure the workflow runs successfully.
   The workflow status can be checked at https://github.com/{your_username}/mlflow/actions/workflows/autoformat.yml.
1. Delete the GitHub token and reset the default branch.


--- .github/workflows/cancel.js ---
module.exports = async ({ context, github }) => {
  const owner = context.repo.owner;
  const repo = context.repo.repo;
  const headSha = context.payload.pull_request.head.sha;
  const prRuns = await github.paginate(github.rest.actions.listWorkflowRunsForRepo, {
    owner,
    repo,
    head_sha: headSha,
    event: "pull_request",
    per_page: 100,
  });
  const unfinishedRuns = prRuns.filter(
    ({ status, name }) =>
      // `post-merge` job in `release-note` workflow should not be cancelled
      status !== "completed" && name !== "release-note"
  );
  for (const run of unfinishedRuns) {
    try {
      // Some runs may have already completed, so we need to handle errors.
      await github.rest.actions.cancelWorkflowRun({
        owner,
        repo,
        run_id: run.id,
      });
      console.log(`Cancelled run ${run.id}`);
    } catch (error) {
      console.error(`Failed to cancel run ${run.id}`, error);
    }
  }
};


--- .github/workflows/closing-pr.js ---
// Regular expressions to capture a closing syntax in the PR body
// https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue
const CLOSING_SYNTAX_PATTERNS = [
  /(?:(?:close|fixe|resolve)[sd]?|fix)\s+(?:mlflow\/mlflow)?#(\d+)/gi,
  /(?:(?:close|fixe|resolve)[sd]?|fix)\s+(?:https?:\/\/github.com\/mlflow\/mlflow\/issues\/)(\d+)/gi,
];
const HAS_CLOSING_PR_LABEL = "has-closing-pr";

const getIssuesToClose = (body) => {
  const commentsExcluded = body.replace(/<!--(.+?)-->/gs, ""); // remove comments
  const matches = CLOSING_SYNTAX_PATTERNS.flatMap((pattern) =>
    Array.from(commentsExcluded.matchAll(pattern))
  );
  const issueNumbers = matches.map((match) => match[1]);
  return [...new Set(issueNumbers)].sort();
};

const arraysEqual = (a1, a2) => {
  return JSON.stringify(a1) == JSON.stringify(a2);
};

const assertArrayEqual = (a1, a2) => {
  if (!arraysEqual(a1, a2)) {
    throw `[${a1}] !== [${a2}]`;
  }
};

const capitalizeFirstLetter = (string) => {
  return string.charAt(0).toUpperCase() + string.slice(1);
};

const test = () => {
  ["close", "closes", "closed", "fix", "fixes", "fixed", "resolve", "resolves", "resolved"].forEach(
    (keyword) => {
      assertArrayEqual(getIssuesToClose(`${keyword} #123`), ["123"]);
      assertArrayEqual(getIssuesToClose(`${capitalizeFirstLetter(keyword)} #123`), ["123"]);
    }
  );

  const body2 = `
Fix mlflow/mlflow#123
Resolve https://github.com/mlflow/mlflow/issues/456
`;
  assertArrayEqual(getIssuesToClose(body2), ["123", "456"]);

  const body3 = `
Fix #123
Close #123
`;
  assertArrayEqual(getIssuesToClose(body3), ["123"]);

  const body4 = "Relates to #123";
  assertArrayEqual(getIssuesToClose(body4), []);

  const body5 = "<!-- close #123 -->";
  assertArrayEqual(getIssuesToClose(body5), []);

  const body6 = "Fixs #123 Fixd #456";
  assertArrayEqual(getIssuesToClose(body6), []);
};

// `node .github/workflows/closing-pr.js` runs this block
if (require.main === module) {
  test();
}

module.exports = async ({ context, github }) => {
  const { body } = context.payload.pull_request;
  const { owner, repo } = context.repo;
  for (const issue_number of getIssuesToClose(body || "")) {
    // Ignore PRs
    const { data: issue } = await github.rest.issues.get({
      owner,
      repo,
      issue_number,
    });
    if (issue.pull_request) {
      continue;
    }
    await github.rest.issues.addLabels({
      owner,
      repo,
      issue_number,
      labels: [HAS_CLOSING_PR_LABEL],
    });
  }
};


--- .github/workflows/cross-version-test-runner.js ---
async function main({ context, github }) {
  const { comment } = context.payload;
  const { owner, repo } = context.repo;
  const pull_number = context.issue.number;

  const { data: pr } = await github.rest.pulls.get({ owner, repo, pull_number });
  const flavorsMatch = comment.body.match(/\/(?:cross-version-test|cvt)\s+([^\n]+)\n?/);
  if (!flavorsMatch) {
    return;
  }

  // Run the workflow
  const flavors = flavorsMatch[1];
  const uuid = Array.from({ length: 16 }, () => Math.floor(Math.random() * 16).toString(16)).join(
    ""
  );
  const workflow_id = "cross-version-tests.yml";
  await github.rest.actions.createWorkflowDispatch({
    owner,
    repo,
    workflow_id,
    ref: pr.base.ref,
    inputs: {
      repository: `${owner}/${repo}`,
      ref: pr.merge_commit_sha,
      flavors,
      // The response of create-workflow-dispatch request doesn't contain the ID of the triggered
      // workflow run. We need to pass a unique identifier to the workflow run and find the run by
      // the identifier. See https://github.com/orgs/community/discussions/9752 for more details.
      uuid,
    },
  });

  // Find the triggered workflow run
  let run;
  const maxAttempts = 5;
  for (let i = 0; i < maxAttempts; i++) {
    await new Promise((resolve) => setTimeout(resolve, 5000));

    const { data: runs } = await github.rest.actions.listWorkflowRunsForRepo({
      owner,
      repo,
      workflow_id,
      event: "workflow_dispatch",
    });
    run = runs.workflow_runs.find((run) => run.name.includes(uuid));
    if (run) {
      break;
    }
  }

  if (!run) {
    await github.rest.issues.createComment({
      owner,
      repo,
      issue_number: pull_number,
      body: "Failed to find the triggered workflow run.",
    });
    return;
  }

  await github.rest.issues.createComment({
    owner,
    repo,
    issue_number: pull_number,
    body: `Cross-version test run started: ${run.html_url}`,
  });
}

module.exports = {
  main,
};


--- .github/workflows/cross-version-testing.md ---
# Cross version testing

## What is cross version testing?

Cross version testing is a testing strategy to ensure ML integrations in MLflow such as
`mlflow.sklearn` work properly with their associated packages across various versions.

## Key files

| File (relative path from the root)              | Role                                                           |
| :---------------------------------------------- | :------------------------------------------------------------- |
| [`mlflow/ml-package-versions.yml`][]            | Define which versions to test for each ML package.             |
| [`dev/set_matrix.py`][]                         | Generate a test matrix from `ml-package-versions.yml`.         |
| [`dev/update_ml_package_versions.py`][]         | Update `ml-package-versions.yml` when releasing a new version. |
| [`.github/workflows/cross-version-tests.yml`][] | Define a Github Actions workflow for cross version testing.    |

[`mlflow/ml-package-versions.yml`]: ../../mlflow/ml-package-versions.yml
[`dev/set_matrix.py`]: ../../dev/set_matrix.py
[`dev/update_ml_package_versions.py`]: ../../dev/update_ml_package_versions.py
[`.github/workflows/cross-version-tests.yml`]: ./cross-version-tests.yml

## Configuration keys in `ml-package-versions.yml`

```yml
# Note this is just an example and not the actual sklearn configuration.

# The top-level key specifies the integration name.
sklearn:
  package_info:
    # [Required] `pip_release` specifies the package this integration depends on.
    pip_release: "scikit-learn"

    # [Optional] `install_dev` specifies a set of commands to install the dev version of the package.
    # For example, the command below builds a wheel from the latest main branch of
    # the scikit-learn repository and installs it.
    #
    # The aim of testing the dev version is to spot issues as early as possible before they get
    # piled up, and fix them incrementally rather than fixing them at once when the package
    # releases a new version.
    install_dev: |
      pip install git+https://github.com/scikit-learn/scikit-learn.git

  # [At least one of `models` and `autologging` must be specified]
  # `models` specifies the configuration for model serialization and serving tests.
  # `autologging` specifies the configuration for autologging tests.
  models or autologging:
    # [Optional] `requirements` specifies additional pip requirements required for running tests.
    # For example, '">= 0.24.0": ["xgboost"]' is interpreted as 'if the version of scikit-learn
    # to install is newer than or equal to 0.24.0, install xgboost'.
    requirements:
      ">= 0.24.0": ["xgboost"]

    # [Required] `minimum` specifies the minimum supported version for the latest release of MLflow.
    minimum: "0.20.3"

    # [Required] `maximum` specifies the maximum supported version for the latest release of MLflow.
    maximum: "1.0"

    # [Optional] `unsupported` specifies a list of versions that should NOT be supported due to
    # unacceptable issues or bugs.
    unsupported: ["0.21.3"]

    # [Required] `run` specifies a set of commands to run tests.
    run: |
      pytest tests/sklearn/test_sklearn_model_export.py
```

## How do we determine which versions to test?

We determine which versions to test based on the following rules:

1. Only test [final][] (e.g. `1.0.0`) and [post][] (`1.0.0.post0`) releases.
2. Only test the latest micro version in each minor version.
   For example, if `1.0.0`, `1.0.1`, and `1.0.2` are available, we only test `1.0.2`.
3. The `maximum` version defines the maximum **major** version to test.
   For example, if the value of `maximum` is `1.0.0`, we test `1.1.0` (if available) but not `2.0.0`.
4. Always test the `minimum` version.

[final]: https://www.python.org/dev/peps/pep-0440/#final-releases
[post]: https://www.python.org/dev/peps/pep-0440/#post-releases

The table below describes which `scikit-learn` versions to test for the example configuration in
the previous section:

| Version       | Tested | Comment                                            |
| :------------ | :----- | -------------------------------------------------- |
| 0.20.3        | ✅     | The value of `minimum`                             |
| 0.20.4        | ✅     | The latest micro version of `0.20`                 |
| 0.21rc2       |        |                                                    |
| 0.21.0        |        |                                                    |
| 0.21.1        |        |                                                    |
| 0.21.2        | ✅     | The latest micro version of `0.21` without`0.21.3` |
| 0.21.3        |        | Excluded by `unsupported`                          |
| 0.22rc2.post1 |        |                                                    |
| 0.22rc3       |        |                                                    |
| 0.22          |        |                                                    |
| 0.22.1        |        |                                                    |
| 0.22.2        |        |                                                    |
| 0.22.2.post1  | ✅     | The latest micro version of `0.22`                 |
| 0.23.0rc1     |        |                                                    |
| 0.23.0        |        |                                                    |
| 0.23.1        |        |                                                    |
| 0.23.2        | ✅     | The latest micro version of `0.23`                 |
| 0.24.dev0     |        |                                                    |
| 0.24.0rc1     |        |                                                    |
| 0.24.0        |        |                                                    |
| 0.24.1        |        |                                                    |
| 0.24.2        | ✅     | The latest micro version of `0.24`                 |
| 1.0rc1        |        |                                                    |
| 1.0rc2        |        |                                                    |
| 1.0           |        | The value of `maximum`                             |
| 1.0.1         | ✅     | The latest micro version of `1.0`                  |
| 1.1.dev       | ✅     | The version installed by `install_dev`             |

## Why do we run tests against development versions?

In cross-version testing, we run daily tests against both publicly available and pre-release
development versions for all dependent libraries that are used by MLflow.
This section explains why.

### Without dev version test

First, let's take a look at what would happen **without** dev version test.

```
  |
  ├─ XGBoost merges a change on the master branch that breaks MLflow's XGBoost integration.
  |
  ├─ MLflow 1.20.0 release date
  |
  ├─ XGBoost 1.5.0 release date
  ├─ ❌ We notice the change here and might need to make a patch release if it's critical.
  |
  v
time
```

- We didn't notice the change until after XGBoost 1.5.0 was released.
- MLflow 1.20.0 doesn't work with XGBoost 1.5.0.

### With dev version test

Then, let's take a look at what would happen **with** dev version test.

```
  |
  ├─ XGBoost merges a change on the master branch that breaks MLflow's XGBoost integration.
  ├─ ✅ Tests for the XGBoost integration fail -> We can notice the change and apply a fix for it.
  |
  ├─ MLflow 1.20.0 release date
  |
  ├─ XGBoost 1.5.0 release date
  |
  v
time
```

- We can notice the change **before XGBoost 1.5.0 is released** and apply a fix for it **before releasing MLflow 1.20.0**.
- MLflow 1.20.0 works with XGBoost 1.5.0.

## When do we run cross version tests?

1. Daily at 7:00 UTC using a cron scheduler.
   [README on the repository root](../../README.md) has a badge ([![badge-img][]][badge-target]) that indicates the status of the most recent cron run.
2. When a PR that affects the ML integrations is created. Note we only run tests relevant to
   the affected ML integrations. For example, a PR that affects files in `mlflow/sklearn` triggers
   cross version tests for `sklearn`.

[badge-img]: https://github.com/mlflow/mlflow/workflows/Cross%20version%20tests/badge.svg?event=schedule
[badge-target]: https://github.com/mlflow/mlflow/actions?query=workflow%3ACross%2Bversion%2Btests+event%3Aschedule

## How to run cross version test for dev versions on a pull request

By default, cross version tests for dev versions are disabled on a pull request.
To enable them, the following steps are required.

1. Click `Labels` in the right sidebar.
2. Click the `enable-dev-tests` label and make sure it's applied on the pull request.
3. Push a new commit or re-run the `cross-version-tests` workflow.

See also:

- [GitHub Docs - Applying a label](https://docs.github.com/en/issues/using-labels-and-milestones-to-track-work/managing-labels#applying-a-label)
- [GitHub Docs - Re-running workflows and jobs](https://docs.github.com/en/actions/managing-workflow-runs/re-running-workflows-and-jobs)

## How to run cross version tests manually

The `cross-version-tests.yml` workflow can be run manually without creating a pull request.

1. Open https://github.com/mlflow/mlflow/actions/workflows/cross-version-tests.yml.
2. Click `Run workflow`.
3. Fill in the input parameters.
4. Click `Run workflow` at the bottom of the parameter input form.

See also:

- [GitHub Docs - Manually running a workflow](https://docs.github.com/en/actions/managing-workflow-runs/manually-running-a-workflow)


--- .github/workflows/delete-artifact.js ---
/**
 * Main function to handle documentation preview comments
 * @param {object} params - Parameters object containing context and github
 * @param {object} params.github - GitHub API client
 * @param {object} params.context - GitHub context
 * @param {object} params.env - Environment variables
 */
module.exports = async ({ github, context, env }) => {
  const artifactName = env.ARTIFACT_NAME;
  const runId = env.RUN_ID;

  if (!artifactName || !runId) {
    throw new Error("Missing required parameters: ARTIFACT_NAME, RUN_ID");
  }

  const { owner, repo } = context.repo;

  try {
    // INFO: https://octokit.github.io/rest.js/v22/#actions-list-workflow-run-artifacts
    const {
      data: { artifacts },
    } = await github.rest.actions.listWorkflowRunArtifacts({
      owner,
      repo,
      run_id: runId,
      name: artifactName,
    });

    const [artifact] = artifacts;

    // INFO: https://octokit.github.io/rest.js/v22/#actions-delete-artifact
    await github.rest.actions.deleteArtifact({
      owner,
      repo,
      artifact_id: artifact.id,
    });
  } catch (error) {
    console.error(`Could not find or delete the artifact for ${runId} and ${artifactName}`);
    throw error;
  }
};


--- dev/README.md ---
## MLflow Dev Scripts

This directory contains automation scripts for MLflow developers and the build infrastructure.

## Job Statuses

[![Examples Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/examples.yml.svg?branch=master&event=schedule&label=Examples&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/examples.yml?query=workflow%3AExamples+event%3Aschedule)
[![Cross Version Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/cross-version-tests.yml.svg?branch=master&event=schedule&label=Cross%20version%20tests&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/cross-version-tests.yml?query=workflow%3A%22Cross+version+tests%22+event%3Aschedule)
[![Cross Version Test Visualization](https://img.shields.io/github/actions/workflow/status/mlflow/dev/xtest-viz.yml.svg?branch=master&event=schedule&label=Test%20Results%20Viz&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/xtest-viz.yml)
[![R-devel Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/r.yml.svg?branch=master&event=schedule&label=r-devel&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/r.yml?query=workflow%3AR+event%3Aschedule)
[![Test Requirements Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/requirements.yml.svg?branch=master&event=schedule&label=test%20requirements&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/requirements.yml?query=workflow%3A%22Test+requirements%22+event%3Aschedule)
[![Push Images Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/push-images.yml.svg?event=release&label=push-images&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow/actions/workflows/push-images.yml?query=event%3Arelease)
[![Slow Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/slow-tests.yml.svg?branch=master&event=schedule&label=slow-tests&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/slow-tests.yml?query=event%3Aschedule)
[![Website E2E Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow-website/e2e.yml.svg?branch=main&event=schedule&label=website-e2e&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow-website/actions/workflows/e2e.yml?query=event%3Aschedule)


--- dev/clint/README.md ---
# Clint

A custom linter for mlflow to enforce rules that ruff doesn't cover.

## Installation

```
pip install -e dev/clint
```

## Usage

```bash
clint file.py ...
```

## Integrating with Visual Studio Code

1. Install [the Pylint extension](https://marketplace.visualstudio.com/items?itemName=ms-python.pylint)
2. Add the following setting in your `settings.json` file:

```json
{
  "pylint.path": ["${interpreter}", "-m", "clint"]
}
```

## Ignoring Rules for Specific Files or Lines

**To ignore a rule on a specific line (recommended):**

```python
foo()  # clint: disable=<rule_name>
```

Replace `<rule_name>` with the actual rule you want to disable.

**To ignore a rule for an entire file:**

Add the file path to the `exclude` list in your `pyproject.toml`:

```toml
[tool.clint]
exclude = [
  # ...existing entries...
  "path/to/file.py",
]
```

## Testing

```bash
pytest dev/clint
```


--- dev/proto_to_graphql/README.md ---
# MLflow Proto To GraphQL Autogeneration

## What is this

The system in `dev/proto_to_graphql` parses proto rpc definitions and generates graphql schema based on the proto rpc definition. The goal of this system is to quickly generate base GraphQL schema and resolver code so that we can easily take advantage of the data joining functionalities of GraphQL.

The autogenerated schema and resolver are in the following file: `mlflow/server/graphql/autogenerated_graphql_schema.py`

The autogenerated schema and resolvers are referenced and can be extended in this file `mlflow/server/graphql/graphql_schema_extensions.py`

You can run `python ./dev/proto_to_graphql/code_generator.py` or `./dev/generate-protos.sh` to trigger the codegen process.

## FAQs

### How to onboard a new rpc to GraphQL

- In your proto rpc definition, add `option (graphql) = {};` and re-run `./dev/generate-protos.sh`. You should see the changes in the generated schema. [Example](https://github.com/mlflow/mlflow/pull/11215/files#diff-8ab2ad3109b67a713e147edf557d4da88853563398ce354cc895bb5930950dc5R175).
- In `mlflow/server/handlers.py`, identify the handler function for your rpc, for example `_get_run`, make sure there exists a corresponding `get_run_impl` function that takes in a `request_message` and returns a response messages that is of the generated service_pb proto type. If no such function exists, you can easily extract it out like in this [example](https://github.com/mlflow/mlflow/pull/11215/files#diff-5c10a4e2ca47745f06fa9e7201087acfc102849756cb8d85e774a5ac468cb037R1779-R1795).
- Test manually with a localhost server, as well as adding a unit test in `tests/tracking/test_rest_tracking.py`. [Example](https://github.com/mlflow/mlflow/pull/11215/files#diff-2ec8756f67a20ecbaeec2d2c5e7bf33310a50c015fc3aa487e27100fc4c2f9a7R1771-R1802).

### How to customize a generated query/mutation to join multiple rpc endpoints

The proto to graphql autogeneration only supports 1 to 1 mapping from proto rpc to graphql operation. However, the power of GraphQL is to join multiple rpc endpoints together as one query. So we often would like to customize or extend the autogenerated operations to join these multiple endpoints.

For example, we would like to query data about `Experiment`, `ModelVersions` and `Run` in one query by extending the `MlflowRun` object.

```
query testQuery {
    mlflowGetRun(input: {runId: "my-id"}) {
        run {
            experiment {
                name
            }
            modelVersions {
                name
            }
        }
    }
}
```

To achieve joins, follow the steps below:

- Make sure the rpcs you would like to join are already onboarded to GraphQL by following the `How to onboard a new rpc to GraphQL` section
- Identify the class you would like to extend in `autogenerated_graphql_schema.py` and create a new class that inherits the target class, put it in `graphql_schema_extensions.py`. Add the new fields and the resolver function as you intended. [Example](https://github.com/mlflow/mlflow/pull/11173/files#diff-9e4f7bdf4d7f9d362338bed9ce6607a51b8f520ee605e2fd4c9bda5e43cb617cR21-R31)
- Run `python ./dev/proto_to_graphql/code_generator.py` or `./dev/generate-protos.sh`, you should see the autogenerated schema being updated to reference the extension class you just created.
- Add a test case in `tests/tracking/test_rest_tracking.py` [Example](https://github.com/mlflow/mlflow/pull/11173/files#diff-2ec8756f67a20ecbaeec2d2c5e7bf33310a50c015fc3aa487e27100fc4c2f9a7R1771-R1795)

### How to generate typescript types for a GraphQL operation

To generate typescript types, first make sure the generated schema is up-to-date by running `python ./dev/proto_to_graphql/code_generator.py`

Then write your new query or mutation in the mlflow/server/js/src folder, after that run the following commands:

- cd mlflow/server/js
- yarn graphql-codegen

You should be able to see the generated types in `mlflow/server/js/src/graphql/__generated__/`


--- dev/build.py ---
import argparse
import contextlib
import shutil
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path


@dataclass(frozen=True)
class Package:
    # name of the package on PyPI.
    pypi_name: str
    # type of the package, one of "dev", "skinny", "tracing", "release"
    type: str
    # path to the package relative to the root of the repository
    build_path: str


DEV = Package("mlflow", "dev", ".")
RELEASE = Package("mlflow", "release", ".")
SKINNY = Package("mlflow-skinny", "skinny", "libs/skinny")
TRACING = Package("mlflow-tracing", "tracing", "libs/tracing")

PACKAGES = [
    DEV,
    SKINNY,
    RELEASE,
    TRACING,
]


def parse_args():
    parser = argparse.ArgumentParser(description="Build MLflow package.")
    parser.add_argument(
        "--package-type",
        help="Package type to build. Default is 'dev'.",
        choices=[p.type for p in PACKAGES],
        default="dev",
    )
    parser.add_argument(
        "--sha",
        help="If specified, include the SHA in the wheel name as a build tag.",
    )
    return parser.parse_args()


@contextlib.contextmanager
def restore_changes():
    try:
        yield
    finally:
        subprocess.check_call(
            [
                "git",
                "restore",
                "README.md",
                "pyproject.toml",
            ]
        )


def main():
    args = parse_args()

    # Clean up build artifacts generated by previous builds
    paths_to_clean_up = ["build"]
    for pkg in PACKAGES:
        paths_to_clean_up += [
            f"{pkg.build_path}/dist",
            f"{pkg.build_path}/{pkg.pypi_name}.egg_info",
        ]
    for path in map(Path, paths_to_clean_up):
        if not path.exists():
            continue
        if path.is_file():
            path.unlink()
        else:
            shutil.rmtree(path)

    package = next(p for p in PACKAGES if p.type == args.package_type)

    with restore_changes():
        pyproject = Path("pyproject.toml")
        if package == RELEASE:
            pyproject.write_text(Path("pyproject.release.toml").read_text())

        subprocess.check_call(
            [
                sys.executable,
                "-m",
                "build",
                package.build_path,
            ]
        )

        DIST_DIR = Path("dist")
        DIST_DIR.mkdir(exist_ok=True)
        if package in (SKINNY, TRACING):
            # Move `libs/xyz/dist/*` to `dist/`
            for src in (Path(package.build_path) / "dist").glob("*"):
                print(src)
                dst = DIST_DIR / src.name
                if dst.exists():
                    dst.unlink()
                src.rename(dst)

    if args.sha:
        # If build succeeds, there should be one wheel in the dist directory
        wheel = next(DIST_DIR.glob("mlflow*.whl"))
        name, version, rest = wheel.name.split("-", 2)
        build_tag = f"0.sha.{args.sha}"  # build tag must start with a digit
        wheel.rename(wheel.with_name(f"{name}-{version}-{build_tag}-{rest}"))


if __name__ == "__main__":
    main()


--- dev/check_function_signatures.py ---
from __future__ import annotations

import argparse
import ast
import os
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path


def is_github_actions() -> bool:
    return os.environ.get("GITHUB_ACTIONS") == "true"


@dataclass
class Error:
    file_path: Path
    line: int
    column: int
    lines: list[str]

    def format(self, github: bool = False) -> str:
        message = " ".join(self.lines)
        if github:
            return f"::warning file={self.file_path},line={self.line},col={self.column}::{message}"
        else:
            return f"{self.file_path}:{self.line}:{self.column}: {message}"


@dataclass
class Parameter:
    name: str
    position: int | None  # None for keyword-only
    is_required: bool
    is_positional_only: bool
    is_keyword_only: bool
    lineno: int
    col_offset: int


@dataclass
class Signature:
    positional: list[Parameter]  # Includes positional-only and regular positional
    keyword_only: list[Parameter]
    has_var_positional: bool  # *args
    has_var_keyword: bool  # **kwargs


@dataclass
class ParameterError:
    message: str
    param_name: str
    lineno: int
    col_offset: int


def parse_signature(args: ast.arguments) -> Signature:
    """Convert ast.arguments to a Signature dataclass for easier processing."""
    parameters_positional: list[Parameter] = []
    parameters_keyword_only: list[Parameter] = []

    # Process positional-only parameters
    for i, arg in enumerate(args.posonlyargs):
        parameters_positional.append(
            Parameter(
                name=arg.arg,
                position=i,
                is_required=True,  # All positional-only are required
                is_positional_only=True,
                is_keyword_only=False,
                lineno=arg.lineno,
                col_offset=arg.col_offset,
            )
        )

    # Process regular positional parameters
    offset = len(args.posonlyargs)
    first_optional_idx = len(args.posonlyargs + args.args) - len(args.defaults)

    for i, arg in enumerate(args.args):
        pos = offset + i
        parameters_positional.append(
            Parameter(
                name=arg.arg,
                position=pos,
                is_required=pos < first_optional_idx,
                is_positional_only=False,
                is_keyword_only=False,
                lineno=arg.lineno,
                col_offset=arg.col_offset,
            )
        )

    # Process keyword-only parameters
    for arg, default in zip(args.kwonlyargs, args.kw_defaults):
        parameters_keyword_only.append(
            Parameter(
                name=arg.arg,
                position=None,
                is_required=default is None,
                is_positional_only=False,
                is_keyword_only=True,
                lineno=arg.lineno,
                col_offset=arg.col_offset,
            )
        )

    return Signature(
        positional=parameters_positional,
        keyword_only=parameters_keyword_only,
        has_var_positional=args.vararg is not None,
        has_var_keyword=args.kwarg is not None,
    )


def check_signature_compatibility(
    old_fn: ast.FunctionDef | ast.AsyncFunctionDef,
    new_fn: ast.FunctionDef | ast.AsyncFunctionDef,
) -> list[ParameterError]:
    """
    Return list of error messages when *new_fn* is not backward-compatible with *old_fn*,
    or None if compatible.

    Compatibility rules
    -------------------
    • Positional / positional-only parameters
        - Cannot be reordered, renamed, or removed.
        - Adding **required** ones is breaking.
        - Adding **optional** ones is allowed only at the end.
        - Making an optional parameter required is breaking.

    • Keyword-only parameters (order does not matter)
        - Cannot be renamed or removed.
        - Making an optional parameter required is breaking.
        - Adding a required parameter is breaking; adding an optional parameter is fine.
    """
    old_sig = parse_signature(old_fn.args)
    new_sig = parse_signature(new_fn.args)
    errors: list[ParameterError] = []

    # ------------------------------------------------------------------ #
    # 1. Positional / pos-only parameters
    # ------------------------------------------------------------------ #

    # (a) existing parameters must line up
    for idx, old_param in enumerate(old_sig.positional):
        if idx >= len(new_sig.positional):
            errors.append(
                ParameterError(
                    message=f"Positional param '{old_param.name}' was removed.",
                    param_name=old_param.name,
                    lineno=old_param.lineno,
                    col_offset=old_param.col_offset,
                )
            )
            continue

        new_param = new_sig.positional[idx]
        if old_param.name != new_param.name:
            errors.append(
                ParameterError(
                    message=(
                        f"Positional param order/name changed: "
                        f"'{old_param.name}' -> '{new_param.name}'."
                    ),
                    param_name=new_param.name,
                    lineno=new_param.lineno,
                    col_offset=new_param.col_offset,
                )
            )
            # Stop checking further positional params after first order/name mismatch
            break

        if (not old_param.is_required) and new_param.is_required:
            errors.append(
                ParameterError(
                    message=f"Optional positional param '{old_param.name}' became required.",
                    param_name=new_param.name,
                    lineno=new_param.lineno,
                    col_offset=new_param.col_offset,
                )
            )

    # (b) any extra new positional params must be optional and appended
    if len(new_sig.positional) > len(old_sig.positional):
        for idx in range(len(old_sig.positional), len(new_sig.positional)):
            new_param = new_sig.positional[idx]
            if new_param.is_required:
                errors.append(
                    ParameterError(
                        message=f"New required positional param '{new_param.name}' added.",
                        param_name=new_param.name,
                        lineno=new_param.lineno,
                        col_offset=new_param.col_offset,
                    )
                )

    # ------------------------------------------------------------------ #
    # 2. Keyword-only parameters (order-agnostic)
    # ------------------------------------------------------------------ #
    old_kw_names = {p.name for p in old_sig.keyword_only}
    new_kw_names = {p.name for p in new_sig.keyword_only}

    # Build mappings for easier lookup
    old_kw_by_name = {p.name: p for p in old_sig.keyword_only}
    new_kw_by_name = {p.name: p for p in new_sig.keyword_only}

    # removed or renamed
    for name in old_kw_names - new_kw_names:
        old_param = old_kw_by_name[name]
        errors.append(
            ParameterError(
                message=f"Keyword-only param '{name}' was removed.",
                param_name=name,
                lineno=old_param.lineno,
                col_offset=old_param.col_offset,
            )
        )

    # optional -> required upgrades
    for name in old_kw_names & new_kw_names:
        if not old_kw_by_name[name].is_required and new_kw_by_name[name].is_required:
            new_param = new_kw_by_name[name]
            errors.append(
                ParameterError(
                    message=f"Keyword-only param '{name}' became required.",
                    param_name=name,
                    lineno=new_param.lineno,
                    col_offset=new_param.col_offset,
                )
            )

    # new required keyword-only params
    for param in new_sig.keyword_only:
        if param.is_required and param.name not in old_kw_names:
            errors.append(
                ParameterError(
                    message=f"New required keyword-only param '{param.name}' added.",
                    param_name=param.name,
                    lineno=param.lineno,
                    col_offset=param.col_offset,
                )
            )

    return errors


def _is_private(n: str) -> bool:
    return n.startswith("_") and not n.startswith("__") and not n.endswith("__")


class FunctionSignatureExtractor(ast.NodeVisitor):
    def __init__(self):
        self.functions: dict[str, ast.FunctionDef | ast.AsyncFunctionDef] = {}
        self.stack: list[ast.ClassDef] = []

    def visit_ClassDef(self, node: ast.ClassDef) -> None:
        self.stack.append(node)
        self.generic_visit(node)
        self.stack.pop()

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        # Is this a private function or a function in a private class?
        # If so, skip it.
        if _is_private(node.name) or (self.stack and _is_private(self.stack[-1].name)):
            return

        names = [*(c.name for c in self.stack), node.name]
        self.functions[".".join(names)] = node

    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> None:
        if _is_private(node.name) or (self.stack and _is_private(self.stack[-1].name)):
            return

        names = [*(c.name for c in self.stack), node.name]
        self.functions[".".join(names)] = node


def get_changed_python_files(base_branch: str = "master") -> list[Path]:
    # In GitHub Actions PR context, we need to fetch the base branch first
    if is_github_actions():
        # Fetch the base branch to ensure we have it locally
        subprocess.check_call(
            ["git", "fetch", "origin", f"{base_branch}:{base_branch}"],
        )

    result = subprocess.check_output(
        ["git", "diff", "--name-only", f"{base_branch}...HEAD"], text=True
    )
    files = [s.strip() for s in result.splitlines()]
    return [Path(f) for f in files if f]


def parse_functions(content: str) -> dict[str, ast.FunctionDef | ast.AsyncFunctionDef]:
    tree = ast.parse(content)
    extractor = FunctionSignatureExtractor()
    extractor.visit(tree)
    return extractor.functions


def get_file_content_at_revision(file_path: Path, revision: str) -> str | None:
    try:
        return subprocess.check_output(["git", "show", f"{revision}:{file_path}"], text=True)
    except subprocess.CalledProcessError as e:
        print(f"Warning: Failed to get file content at revision: {e}", file=sys.stderr)
        return None


def compare_signatures(base_branch: str = "master") -> list[Error]:
    errors: list[Error] = []
    for file_path in get_changed_python_files(base_branch):
        # Ignore non-Python files
        if not file_path.suffix == ".py":
            continue

        # Ignore files not in the mlflow directory
        if file_path.parts[0] != "mlflow":
            continue

        # Ignore private modules
        if any(part.startswith("_") for part in file_path.parts):
            continue

        base_content = get_file_content_at_revision(file_path, base_branch)
        if base_content is None:
            # Find not found in the base branch, likely added in the current branch
            continue

        if not file_path.exists():
            # File not found, likely deleted in the current branch
            continue

        current_content = file_path.read_text()
        base_functions = parse_functions(base_content)
        current_functions = parse_functions(current_content)
        for func_name in set(base_functions.keys()) & set(current_functions.keys()):
            base_func = base_functions[func_name]
            current_func = current_functions[func_name]
            if param_errors := check_signature_compatibility(base_func, current_func):
                # Create individual errors for each problematic parameter
                for param_error in param_errors:
                    errors.append(
                        Error(
                            file_path=file_path,
                            line=param_error.lineno,
                            column=param_error.col_offset + 1,
                            lines=[
                                "[Non-blocking | Ignore if not public API]",
                                param_error.message,
                                f"This change will break existing `{func_name}` calls.",
                                "If this is not intended, please fix it.",
                            ],
                        )
                    )

    return errors


@dataclass
class Args:
    base_branch: str


def parse_args() -> Args:
    parser = argparse.ArgumentParser(
        description="Check for breaking changes in Python function signatures"
    )
    parser.add_argument("--base-branch", default=os.environ.get("GITHUB_BASE_REF", "master"))
    args = parser.parse_args()
    return Args(base_branch=args.base_branch)


def main():
    args = parse_args()
    errors = compare_signatures(args.base_branch)
    for error in errors:
        print(error.format(github=is_github_actions()))


if __name__ == "__main__":
    main()


--- dev/check_init_py.py ---
"""
Pre-commit hook to check for missing `__init__.py` files in mlflow and tests directories.

This script ensures that all directories under the mlflow package and tests directory that contain
Python files also have an `__init__.py` file. This prevents `setuptools` from excluding these
directories during package build and ensures test modules are properly structured.

Usage:
    uv run dev/check_init_py.py

Requirements:
- If `mlflow/foo/bar.py` exists, `mlflow/foo/__init__.py` must exist.
- If `tests/foo/test_bar.py` exists, `tests/foo/__init__.py` must exist.
- Only test files (starting with `test_`) in the tests directory are checked.
- All parent directories of Python files are checked recursively for `__init__.py`.
- Ignore directories that do not contain any Python files (e.g., `mlflow/server/js`).
"""

import subprocess
import sys
from pathlib import Path


def get_tracked_python_files() -> list[Path]:
    try:
        result = subprocess.check_output(
            ["git", "ls-files", "mlflow/**/*.py", "tests/**/*.py"],
            text=True,
        )
        paths = (Path(f) for f in result.splitlines() if f)
        return [p for p in paths if not p.is_relative_to("tests") or p.name.startswith("test_")]
    except subprocess.CalledProcessError as e:
        print(f"Error running git ls-files: {e}", file=sys.stderr)
        sys.exit(1)


def main() -> int:
    python_files = get_tracked_python_files()
    if not python_files:
        return 0

    python_dirs = {p for f in python_files for p in f.parents if p != Path(".")}
    missing_init_files = [d for d in python_dirs if not (d / "__init__.py").exists()]
    if missing_init_files:
        print("Error: The following directories contain Python files but lack __init__.py:")
        for d in sorted(missing_init_files):
            print(f"  {d.as_posix()}/")
        print("Please add __init__.py files to the directories listed above.")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())


--- dev/check_patch_prs.py ---
import argparse
import os
import re
import subprocess
import sys
import tempfile
from dataclasses import dataclass

import requests


def get_release_branch(version):
    major_minor_version = ".".join(version.split(".")[:2])
    return f"branch-{major_minor_version}"


@dataclass(frozen=True)
class Commit:
    sha: str
    pr_num: int


def get_commits(branch: str):
    """
    Get the commits in the release branch.
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        subprocess.check_call(
            [
                "git",
                "clone",
                "--shallow-since=3 months ago",
                "--branch",
                branch,
                "https://github.com/mlflow/mlflow.git",
                tmpdir,
            ],
        )
        log_stdout = subprocess.check_output(
            [
                "git",
                "log",
                "--pretty=format:%H %s",
            ],
            text=True,
            cwd=tmpdir,
        )
        pr_rgx = re.compile(r"([a-z0-9]+) .+\s+\(#(\d+)\)$")
        commits = []
        for commit in log_stdout.splitlines():
            if m := pr_rgx.search(commit.rstrip()):
                commits.append(Commit(sha=m.group(1), pr_num=int(m.group(2))))

    return commits


@dataclass(frozen=True)
class PR:
    pr_num: int
    merged: bool


def is_closed(pr):
    return pr["state"] == "closed" and pr["pull_request"]["merged_at"] is None


def fetch_patch_prs(version):
    """
    Fetch PRs labeled with `v{version}` from the MLflow repository.
    """
    label = f"v{version}"
    per_page = 100
    page = 1
    pulls = []
    while True:
        response = requests.get(
            f'https://api.github.com/search/issues?q=is:pr+repo:mlflow/mlflow+label:"{label}"&per_page={per_page}&page={page}',
        )
        response.raise_for_status()
        data = response.json()
        # Exclude closed PRs that are not merged
        pulls.extend(pr for pr in data["items"] if not is_closed(pr))
        if len(data) < per_page:
            break
        page += 1

    return {pr["number"]: pr["pull_request"].get("merged_at") is not None for pr in pulls}


def main(version, dry_run):
    release_branch = get_release_branch(version)
    commits = get_commits(release_branch)
    patch_prs = fetch_patch_prs(version)
    if not_cherry_picked := set(patch_prs) - {c.pr_num for c in commits}:
        print(f"The following patch PRs are not cherry-picked to {release_branch}:")
        for idx, pr_num in enumerate(sorted(not_cherry_picked)):
            merged = patch_prs[pr_num]
            url = f"https://github.com/mlflow/mlflow/pull/{pr_num} (merged: {merged})"
            line = f"  {idx + 1}. {url}"
            if not merged:
                line = f"\033[91m{line}\033[0m"  # Red color using ANSI escape codes
            print(line)

        master_commits = get_commits("master")
        cherry_picks = [c.sha for c in master_commits if c.pr_num in not_cherry_picked]
        # reverse the order of cherry-picks to maintain the order of PRs
        print("\n# Steps to cherry-pick the patch PRs:")
        print(
            f"1. Make sure your local master and {release_branch} branches are synced with "
            "upstream."
        )
        print(f"2. Cut a new branch from {release_branch} (e.g. {release_branch}-cherry-picks).")
        print("3. Run the following command on the new branch:\n")
        print("git cherry-pick " + " ".join(cherry_picks[::-1]))
        print(f"\n4. File a PR against {release_branch}.")
        sys.exit(0 if dry_run else 1)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--version", required=True, help="The version to release")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=os.environ.get("DRY_RUN", "true").lower() == "true",
        help="Dry run mode (default: True, can be set via DRY_RUN env var)",
    )
    parser.add_argument(
        "--no-dry-run",
        action="store_false",
        dest="dry_run",
        help="Disable dry run mode",
    )
    args = parser.parse_args()
    main(args.version, args.dry_run)


--- dev/extract_deps.py ---
import ast
import re
from pathlib import Path


def parse_dependencies(content: str) -> list[str]:
    pattern = r"dependencies\s*=\s*(\[[\s\S]*?\])"
    match = re.search(pattern, content)
    deps_str = match.group(1)
    return ast.literal_eval(deps_str)


def main():
    content = Path("pyproject.toml").read_text()
    dependencies = parse_dependencies(content)
    print("\n".join(dependencies))


if __name__ == "__main__":
    main()


--- dev/format.py ---
import os
import re
import subprocess
import sys

RUFF_FORMAT = [sys.executable, "-m", "ruff", "format"]
MESSAGE_REGEX = re.compile(r"^Would reformat: (.+)$")


def transform(stdout: str, is_maintainer: bool) -> str:
    if not stdout:
        return stdout
    transformed = []
    for line in stdout.splitlines():
        if m := MESSAGE_REGEX.match(line):
            path = m.group(1)
            command = (
                "`ruff format .` or comment `/autoformat`" if is_maintainer else "`ruff format .`"
            )
            # As a workaround for https://github.com/orgs/community/discussions/165826,
            # add fake line:column numbers (1:1)
            line = f"{path}:1:1: Unformatted file. Run {command} to format."

        transformed.append(line)
    return "\n".join(transformed) + "\n"


def main():
    if "NO_FIX" in os.environ:
        with subprocess.Popen(
            [
                *RUFF_FORMAT,
                "--check",
                *sys.argv[1:],
            ],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        ) as prc:
            stdout, stderr = prc.communicate()
            is_maintainer = os.environ.get("IS_MAINTAINER", "false").lower() == "true"
            sys.stdout.write(transform(stdout, is_maintainer))
            sys.stderr.write(stderr)
            sys.exit(prc.returncode)
    else:
        with subprocess.Popen(
            [
                *RUFF_FORMAT,
                *sys.argv[1:],
            ]
        ) as prc:
            prc.communicate()
            sys.exit(prc.returncode)


if __name__ == "__main__":
    main()


--- dev/generate_protos.py ---
import platform
import shutil
import subprocess
import tempfile
import textwrap
import urllib.request
import zipfile
from pathlib import Path
from typing import Literal

SYSTEM = platform.system()
MACHINE = platform.machine()
CACHE_DIR = Path(".cache/protobuf_cache")
MLFLOW_PROTOS_DIR = Path("mlflow/protos")
TEST_PROTOS_DIR = Path("tests/protos")


def gen_protos(
    proto_dir: Path,
    proto_files: list[Path],
    lang: Literal["python", "java"],
    protoc_bin: Path,
    protoc_include_paths: list[Path],
    out_dir: Path,
) -> None:
    assert lang in ["python", "java"]
    out_dir.mkdir(parents=True, exist_ok=True)

    include_args = []
    for include_path in protoc_include_paths:
        include_args.append(f"-I={include_path}")

    subprocess.check_call(
        [
            protoc_bin,
            *include_args,
            f"-I={proto_dir}",
            f"--{lang}_out={out_dir}",
            *[proto_dir / pf for pf in proto_files],
        ]
    )


def gen_stub_files(
    proto_dir: Path,
    proto_files: list[Path],
    protoc_bin: Path,
    protoc_include_paths: list[Path],
    out_dir: Path,
) -> None:
    include_args = []
    for include_path in protoc_include_paths:
        include_args.append(f"-I={include_path}")

    subprocess.check_call(
        [
            protoc_bin,
            *include_args,
            f"-I={proto_dir}",
            f"--pyi_out={out_dir}",
            *[proto_dir / pf for pf in proto_files],
        ]
    )


def apply_python_gencode_replacement(file_path: Path) -> None:
    content = file_path.read_text()

    for old, new in python_gencode_replacements:
        content = content.replace(old, new)

    file_path.write_text(content, encoding="UTF-8")


def _get_python_output_path(proto_file_path: Path) -> Path:
    return proto_file_path.parent / (proto_file_path.stem + "_pb2.py")


def to_paths(*args: str) -> list[Path]:
    return list(map(Path, args))


basic_proto_files = to_paths(
    "databricks.proto",
    "service.proto",
    "model_registry.proto",
    "databricks_artifacts.proto",
    "mlflow_artifacts.proto",
    "internal.proto",
    "scalapb/scalapb.proto",
    "assessments.proto",
    "datasets.proto",
    "webhooks.proto",
)
uc_proto_files = to_paths(
    "databricks_managed_catalog_messages.proto",
    "databricks_managed_catalog_service.proto",
    "databricks_uc_registry_messages.proto",
    "databricks_uc_registry_service.proto",
    "databricks_filesystem_service.proto",
    "unity_catalog_oss_messages.proto",
    "unity_catalog_oss_service.proto",
    "unity_catalog_prompt_messages.proto",
    "unity_catalog_prompt_service.proto",
)
tracing_proto_files = to_paths("databricks_trace_server.proto", "databricks_tracing.proto")
facet_proto_files = to_paths("facet_feature_statistics.proto")
python_proto_files = basic_proto_files + uc_proto_files + facet_proto_files + tracing_proto_files
test_proto_files = to_paths("test_message.proto")


python_gencode_replacements = [
    (
        "from scalapb import scalapb_pb2 as scalapb_dot_scalapb__pb2",
        "from .scalapb import scalapb_pb2 as scalapb_dot_scalapb__pb2",
    ),
    (
        "import databricks_pb2 as databricks__pb2",
        "from . import databricks_pb2 as databricks__pb2",
    ),
    (
        "import databricks_uc_registry_messages_pb2 as databricks__uc__registry__messages__pb2",
        "from . import databricks_uc_registry_messages_pb2 as databricks_uc_registry_messages_pb2",
    ),
    (
        "import databricks_managed_catalog_messages_pb2 as databricks__managed__catalog__"
        "messages__pb2",
        "from . import databricks_managed_catalog_messages_pb2 as databricks_managed_"
        "catalog_messages_pb2",
    ),
    (
        "import unity_catalog_oss_messages_pb2 as unity__catalog__oss__messages__pb2",
        "from . import unity_catalog_oss_messages_pb2 as unity_catalog_oss_messages_pb2",
    ),
    (
        "import unity_catalog_prompt_messages_pb2 as unity__catalog__prompt__messages__pb2",
        "from . import unity_catalog_prompt_messages_pb2 as unity_catalog_prompt_messages_pb2",
    ),
    (
        "import service_pb2 as service__pb2",
        "from . import service_pb2 as service__pb2",
    ),
    (
        "import assessments_pb2 as assessments__pb2",
        "from . import assessments_pb2 as assessments__pb2",
    ),
    (
        "import datasets_pb2 as datasets__pb2",
        "from . import datasets_pb2 as datasets__pb2",
    ),
    (
        "import webhooks_pb2 as webhooks__pb2",
        "from . import webhooks_pb2 as webhooks__pb2",
    ),
]


def gen_python_protos(protoc_bin: Path, protoc_include_paths: list[Path], out_dir: Path) -> None:
    gen_protos(
        MLFLOW_PROTOS_DIR,
        python_proto_files,
        "python",
        protoc_bin,
        protoc_include_paths,
        out_dir,
    )

    gen_protos(
        TEST_PROTOS_DIR,
        test_proto_files,
        "python",
        protoc_bin,
        protoc_include_paths,
        out_dir,
    )

    for proto_file in python_proto_files:
        apply_python_gencode_replacement(out_dir / _get_python_output_path(proto_file))


def download_file(url: str, output_path: Path) -> None:
    urllib.request.urlretrieve(url, output_path)


def download_opentelemetry_protos(version: str = "v1.7.0") -> Path:
    """
    Download OpenTelemetry proto files from GitHub.
    Returns the path to the opentelemetry-proto directory.
    """
    otel_proto_dir = CACHE_DIR / f"opentelemetry-proto-{version}"

    if not otel_proto_dir.exists():
        print(f"Downloading OpenTelemetry proto files {version}...")
        with tempfile.TemporaryDirectory() as tmpdir:
            zip_path = Path(tmpdir) / "otel-proto.zip"
            download_file(
                f"https://github.com/open-telemetry/opentelemetry-proto/archive/refs/tags/{version}.zip",
                zip_path,
            )
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall(tmpdir)

            # Move the extracted directory to cache
            extracted_dir = Path(tmpdir) / f"opentelemetry-proto-{version[1:]}"  # Remove 'v' prefix
            shutil.move(str(extracted_dir), str(otel_proto_dir))

    return otel_proto_dir


def download_and_extract_protoc(version: Literal["3.19.4", "26.0"]) -> tuple[Path, Path]:
    """
    Download and extract specific version protoc tool for Linux systems,
    return extracted protoc executable file path and include path.
    """
    assert SYSTEM == "Linux", "This script only supports Linux systems."
    assert MACHINE in ["x86_64", "aarch64"], (
        "This script only supports x86_64 or aarch64 CPU architectures."
    )

    cpu_type = "x86_64" if MACHINE == "x86_64" else "aarch_64"
    protoc_zip_filename = f"protoc-{version}-linux-{cpu_type}.zip"

    downloaded_protoc_bin = CACHE_DIR / f"protoc-{version}" / "bin" / "protoc"
    downloaded_protoc_include_path = CACHE_DIR / f"protoc-{version}" / "include"
    if not (downloaded_protoc_bin.is_file() and downloaded_protoc_include_path.is_dir()):
        with tempfile.TemporaryDirectory() as t:
            zip_path = Path(t) / protoc_zip_filename
            download_file(
                f"https://github.com/protocolbuffers/protobuf/releases/download/v{version}/{protoc_zip_filename}",
                zip_path,
            )
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall(CACHE_DIR / f"protoc-{version}")

        # Make protoc executable
        downloaded_protoc_bin.chmod(0o755)
    return downloaded_protoc_bin, downloaded_protoc_include_path


def generate_final_python_gencode(
    gencode3194_path: Path, gencode5260_path: Path, out_path: Path
) -> None:
    gencode3194 = gencode3194_path.read_text()
    gencode5260 = gencode5260_path.read_text()

    merged_code = f"""
import google.protobuf
from packaging.version import Version
if Version(google.protobuf.__version__).major >= 5:
{textwrap.indent(gencode5260, "  ")}
else:
{textwrap.indent(gencode3194, "  ")}
"""
    out_path.write_text(merged_code, encoding="UTF-8")


def main() -> None:
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    with tempfile.TemporaryDirectory() as temp_gencode_dir:
        temp_gencode_path = Path(temp_gencode_dir)
        proto3194_out = temp_gencode_path / "3.19.4"
        proto5260_out = temp_gencode_path / "26.0"
        proto3194_out.mkdir(exist_ok=True)
        proto5260_out.mkdir(exist_ok=True)

        protoc3194, protoc3194_include = download_and_extract_protoc("3.19.4")
        protoc5260, protoc5260_include = download_and_extract_protoc("26.0")

        # Download OpenTelemetry proto files
        otel_proto_dir = download_opentelemetry_protos()

        # Build include paths list
        protoc3194_includes = [protoc3194_include, otel_proto_dir]
        protoc5260_includes = [protoc5260_include, otel_proto_dir]

        gen_python_protos(protoc3194, protoc3194_includes, proto3194_out)
        gen_python_protos(protoc5260, protoc5260_includes, proto5260_out)

        for proto_files, protos_dir in [
            (python_proto_files, MLFLOW_PROTOS_DIR),
            (test_proto_files, TEST_PROTOS_DIR),
        ]:
            for proto_file in proto_files:
                gencode_path = _get_python_output_path(proto_file)

                generate_final_python_gencode(
                    proto3194_out / gencode_path,
                    proto5260_out / gencode_path,
                    protos_dir / gencode_path,
                )

    # generate java gencode using pinned protoc 3.19.4 version.
    gen_protos(
        MLFLOW_PROTOS_DIR,
        basic_proto_files,
        "java",
        protoc3194,
        protoc3194_includes,
        Path("mlflow/java/client/src/main/java"),
    )

    gen_stub_files(
        MLFLOW_PROTOS_DIR,
        python_proto_files,
        protoc5260,
        protoc5260_includes,
        Path("mlflow/protos/"),
    )


if __name__ == "__main__":
    main()


--- libs/skinny/README.md ---
# MLflow Skinny

`mlflow-skinny` a lightweight version of MLflow that is designed to be used in environments where you want to minimize the size of the package.

## Core Files

| File               | Description                                                                     |
| ------------------ | ------------------------------------------------------------------------------- |
| `mlflow`           | A symlink that points to the `mlflow` directory in the root of the repository.  |
| `pyproject.toml`   | The package metadata. Autogenerate by [`dev/pyproject.py`](../dev/pyproject.py) |
| `README_SKINNY.md` | The package description. Autogenerate by [`dev/skinny.py`](../dev/pyproject.py) |

## Installation

```sh
# If you have a local clone of the repository
pip install ./libs/skinny

# If you want to install the latest version from GitHub
pip install git+https://github.com/mlflow/mlflow.git#subdirectory=libs/skinny
```


--- libs/tracing/README.md ---
# MLflow Tracing: An Open-Source SDK for Observability and Monitoring GenAI Applications🔍

[![Latest Docs](https://img.shields.io/badge/docs-latest-success.svg?style=for-the-badge)](https://mlflow.org/docs/latest/index.html)
[![Apache 2 License](https://img.shields.io/badge/license-Apache%202-brightgreen.svg?style=for-the-badge&logo=apache)](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt)
[![Slack](https://img.shields.io/badge/slack-@mlflow--users-CF0E5B.svg?logo=slack&logoColor=white&labelColor=3F0E40&style=for-the-badge)](https://mlflow.org/community/#slack)
[![Twitter](https://img.shields.io/twitter/follow/MLflow?style=for-the-badge&labelColor=00ACEE&logo=twitter&logoColor=white)](https://twitter.com/MLflow)

MLflow Tracing (`mlflow-tracing`) is an open-source, lightweight Python package that only includes the minimum set of dependencies and functionality
to instrument your code/models/agents with [MLflow Tracing Feature](https://mlflow.org/docs/latest/tracing). It is designed to be a perfect fit for production environments where you want:

- **⚡️ Faster Deployment**: The package size and dependencies are significantly smaller than the full MLflow package, allowing for faster deployment times in dynamic environments such as Docker containers, serverless functions, and cloud-based applications.
- **🔧 Simplified Dependency Management**: A smaller set of dependencies means less work keeping up with dependency updates, security patches, and breaking changes from upstream libraries.
- **📦 Portability**: With the less number of dependencies, MLflow Tracing can be easily deployed across different environments and platforms, without worrying about compatibility issues.
- **🔒 Fewer Security Risks**: Each dependency potentially introduces security vulnerabilities. By reducing the number of dependencies, MLflow Tracing minimizes the attack surface and reduces the risk of security breaches.

## ✨ Features

- [Automatic Tracing](https://mlflow.org/docs/latest/tracing/integrations/) for AI libraries (OpenAI, LangChain, DSPy, Anthropic, etc...). Follow the link for the full list of supported libraries.
- [Manual instrumentation APIs](https://mlflow.org/docs/latest/tracing/api/manual-instrumentation) such as `@trace` decorator.
- [Production Monitoring](https://mlflow.org/docs/latest/tracing/production)
- Other tracing APIs such as `mlflow.set_trace_tag`, `mlflow.search_traces`, etc.

## 🌐 Choose Backend

The MLflow Trace package is designed to work with a remote hosted MLflow server as a backend. This allows you to log your traces to a central location, making it easier to manage and analyze your traces. There are several different options for hosting your MLflow server, including:

- [Databricks](https://docs.databricks.com/machine-learning/mlflow/managed-mlflow.html) - Databricks offers a FREE, fully managed MLflow server as a part of their platform. This is the easiest way to get started with MLflow tracing, without having to set up any infrastructure.
- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/) - MLflow on Amazon SageMaker is a fully managed service offered as part of the SageMaker platform by AWS, including tracing and other MLflow features such as model registry.
- [Nebius](https://nebius.com/) - Nebius, a cutting-edge cloud platform for GenAI explorers, offers a fully managed MLflow server.
- [Self-hosting](https://mlflow.org/docs/latest/tracking) - MLflow is a fully open-source project, allowing you to self-host your own MLflow server and keep your data private. This is a great option if you want to have full control over your data and infrastructure.

## 🚀 Getting Started

### Installation

To install the MLflow Python package, run the following command:

```bash
pip install mlflow-tracing
```

To install from the source code, run the following command:

```bash
pip install git+https://github.com/mlflow/mlflow.git#subdirectory=libs/tracing
```

> **NOTE:** It is **not** recommended to co-install this package with the full MLflow package together, as it may cause version mismatches issues.

### Connect to the MLflow Server

To connect to your MLflow server to log your traces, set the `MLFLOW_TRACKING_URI` environment variable or use the `mlflow.set_tracking_uri` function:

```python
import mlflow

mlflow.set_tracking_uri("databricks")
# Specify the experiment to log the traces to
mlflow.set_experiment("/Path/To/Experiment")
```

### Start Logging Traces

```python
import openai

client = openai.OpenAI(api_key="<your-api-key>")

# Enable auto-tracing for OpenAI
mlflow.openai.autolog()

# Call the OpenAI API as usual
response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": "Hello, how are you?"}],
)
```

## 📘 Documentation

Official documentation for MLflow Tracing can be found at [here](https://mlflow.org/docs/latest/tracing).

## 🛑 Features _Not_ Included

The following MLflow features are not included in this package.

- MLflow tracking server and UI.
- MLflow's other tracking capabilities such as Runs, Model Registry, Projects, etc.
- Evaluate models/agents and log evaluation results.

To leverage the full feature set of MLflow, install the full package by running `pip install mlflow`.


--- libs/typescript/README.md ---
<h1 align="center" style="border-bottom: none">
    <div>
        <a href="https://mlflow.org/"><picture>
            <img alt="MLflow Logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />
        </picture></a>
        <br>
        MLflow TypeScript SDK
    </div>
</h1>
<h2 align="center" style="border-bottom: none"></h2>

<p align="center">
  <a href="https://github.com/mlflow/mlflow"><img src="https://img.shields.io/github/stars/mlflow/mlflow?style=social" alt="stars"></a>
  <a href="https://www.npmjs.com/package/mlflow-tracing"><img src="https://img.shields.io/npm/v/mlflow-tracing.svg" alt="version"></a>
  <a href="https://www.npmjs.com/package/mlflow-tracing"><img src="https://img.shields.io/npm/dt/mlflow-tracing.svg" alt="downloads"></a>
  <a href="https://github.com/mlflow/mlflow/blob/main/LICENSE"><img src="https://img.shields.io/github/license/mlflow/mlflow" alt="license"></a>
</p>

MLflow Typescript SDK is a variant of the [MLflow Python SDK](https://github.com/mlflow/mlflow) that provides a TypeScript API for MLflow.

> [!IMPORTANT]
> MLflow Typescript SDK is catching up with the Python SDK. Currently only support [Tracing]() and [Feedback Collection]() features. Please raise an issue in Github if you need a feature that is not supported.

## Packages

| Package                                | NPM                                                                                                                                         | Description                                                |
| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| [mlflow-tracing](./core)               | [![npm package](https://img.shields.io/npm/v/mlflow-tracing?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing)               | The core tracing functionality and manual instrumentation. |
| [mlflow-openai](./integrations/openai) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing-openai?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing-openai) | Auto-instrumentation integration for OpenAI.               |

## Installation

```bash
npm install mlflow-tracing
```

> [!NOTE]
> MLflow Typescript SDK requires Node.js 20 or higher.

## Quickstart

Start MLflow Tracking Server if you don't have one already:

```bash
pip install mlflow
mlflow server --backend-store-uri sqlite:///mlruns.db --port 5000
```

Self-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.

Instantiate MLflow SDK in your application:

```typescript
import * as mlflow from 'mlflow-tracing';

mlflow.init({
  trackingUri: 'http://localhost:5000',
  experimentId: '<experiment-id>'
});
```

### Configure with environment variables

The SDK can also read configuration from environment variables so you can avoid
hard-coding connection details. If `MLFLOW_TRACKING_URI` and
`MLFLOW_EXPERIMENT_ID` are set, you can initialize the client without passing
any arguments:

```bash
export MLFLOW_TRACKING_URI=http://localhost:5000
export MLFLOW_EXPERIMENT_ID=123456789
```

```typescript
import * as mlflow from 'mlflow-tracing';

mlflow.init(); // Uses the values from the environment
```

Create a trace:

```typescript
// Wrap a function with mlflow.trace to generate a span when the function is called.
// MLflow will automatically record the function name, arguments, return value,
// latency, and exception information to the span.
const getWeather = mlflow.trace(
  (city: string) => {
    return `The weather in ${city} is sunny`;
  },
  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk
  // for the full list of options.
  { name: 'get-weather' }
);
getWeather('San Francisco');

// Alternatively, start and end span manually
const span = mlflow.startSpan({ name: 'my-span' });
span.end();
```

View traces in MLflow UI:

![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/openai-tool-calling-trace-detail.png?raw=true)

## Adding New Integrations

The TypeScript SDK supports pluggable auto-instrumentation packages under [`integrations/`](./integrations). To add a new integration:

1. Create a new workspace package (for example, `integrations/<provider>`), modeled after the [OpenAI integration](./integrations/openai).
2. Implement the instrumentation entry points in `src/`, exporting a `register()` helper that configures tracing for the target client library.
3. Add package metadata (`package.json`, `tsconfig.json`, and optional `README.md`) so the integration can be built and published.
4. Add unit and/or integration tests under `tests/` that exercise the new instrumentation.
5. Update the root [`package.json`](./package.json) `build:integrations` and `test:integrations` scripts if your package requires additional build or test commands.

Once your integration package is ready, run the local workflow outlined in [Running the SDK after changes](#running-the-sdk-after-changes) and open a pull request that describes the new provider support.

## Contributing

We welcome contributions of new features, bug fixes, and documentation improvements. To contribute:

1. Review the project-wide [contribution guidelines](../../CONTRIBUTING.md) and follow the MLflow [Code of Conduct](../../CODE_OF_CONDUCT.rst).
2. Discuss larger proposals in a GitHub issue or the MLflow community channels before investing significant effort.
3. Fork the repository (or use a feature branch) and make your changes with clear, well-structured commits.
4. Ensure your code includes tests and documentation updates where appropriate.
5. Submit a pull request that summarizes the motivation, implementation details, and validation steps. The MLflow team will review and provide feedback.

## Running the SDK after Changes

The TypeScript workspace uses npm workspaces. After modifying the core SDK or any integration:

```bash
npm install        # Install or update workspace dependencies
npm run build      # Build the core package and all integrations
npm run test       # Execute the test suites for the core SDK and integrations
```

You can run package-specific scripts from their respective directories (for example, `cd core && npm run test`) when iterating on a particular feature. Remember to rebuild before consuming the SDK from another project so that the latest TypeScript output is emitted to `dist/`.

## Trace Usage

MLflow Tracing empowers you throughout the end-to-end lifecycle of your application. Here's how it helps you at each step of the workflow, click on each section to learn more:

<details>
<summary><strong>🔍 Build & Debug</strong></summary>

<table>
<tr>
<td width="60%">

#### Smooth Debugging Experience

MLflow's tracing capabilities provide deep insights into what happens beneath the abstractions of your application, helping you precisely identify where issues occur.

[Learn more →](https://mlflow.org/docs/latest/genai/tracing/observe-with-traces)

</td>
<td width="40%">

![Trace Debug](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-debug.png)

</td>
</tr>
</table>

</details>

<details>
<summary><strong>💬 Human Feedback</strong></summary>

<table>
<tr>
<td width="60%">

#### Track Annotation and User Feedback Attached to Traces

Collecting and managing feedback is essential for improving your application. MLflow Tracing allows you to attach user feedback and annotations directly to traces, creating a rich dataset for analysis.

This feedback data helps you understand user satisfaction, identify areas for improvement, and build better evaluation datasets based on real user interactions.

[Learn more →](https://mlflow.org/docs/latest/genai/tracing/collect-user-feedback)

</td>
<td width="40%">

![Human Feedback](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-human-feedback.png)

</td>
</tr>
</table>

</details>

<details>
<summary><strong>📊 Evaluation</strong></summary>

<table>
<tr>
<td width="60%">

#### Systematic Quality Assessment Throughout Your Application

Evaluating the performance of your application is crucial, but creating a reliable evaluation process can be challenging. Traces serve as a rich data source, helping you assess quality with precise metrics for all components.

When combined with MLflow's evaluation capabilities, you get a seamless experience for assessing and improving your application's performance.

[Learn more →](https://mlflow.org/docs/latest/genai/eval-monitor)

</td>
<td width="40%">

![Evaluation](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-evaluation.png)

</td>
</tr>
</table>

</details>

<details>
<summary><strong>🚀 Production Monitoring</strong></summary>

<table>
<tr>
<td width="60%">

#### Monitor Applications with Your Favorite Observability Stack

Machine learning projects don't end with the first launch. Continuous monitoring and incremental improvement are critical to long-term success.

Integrated with various observability platforms such as Databricks, Datadog, Grafana, and Prometheus, MLflow Tracing provides a comprehensive solution for monitoring your applications in production.

[Learn more →](https://mlflow.org/docs/latest/genai/tracing/prod-tracing)

</td>
<td width="40%">

![Monitoring](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-monitoring.png)

</td>
</tr>
</table>

</details>

<details>
<summary><strong>📦 Dataset Collection</strong></summary>

<table>
<tr>
<td width="60%">

#### Create High-Quality Evaluation Datasets from Production Traces

Traces from production are invaluable for building comprehensive evaluation datasets. By capturing real user interactions and their outcomes, you can create test cases that truly represent your application's usage patterns.

This comprehensive data capture enables you to create realistic test scenarios, validate model performance on actual usage patterns, and continuously improve your evaluation datasets.

[Learn more →](https://mlflow.org/docs/latest/genai/tracing/search-traces#creating-evaluation-datasets)

</td>
<td width="40%">

![Dataset Collection](https://raw.githubusercontent.com/mlflow/mlflow/master/docs/static/images/llms/tracing/genai-trace-dataset.png)

</td>
</tr>
</table>

</details>

## Documentation 📘

Official documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).

## License

This project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).


--- libs/skinny/README_SKINNY.md ---
<!--  Autogenerated by dev/pyproject.py. Do not edit manually.  -->

📣 This is the `mlflow-skinny` package, a lightweight MLflow package without SQL storage, server, UI, or data science dependencies.
Additional dependencies can be installed to leverage the full feature set of MLflow. For example:

- To use the `mlflow.sklearn` component of MLflow Models, install `scikit-learn`, `numpy` and `pandas`.
- To use SQL-based metadata storage, install `sqlalchemy`, `alembic`, and `sqlparse`.
- To use serving-based features, install `flask` and `pandas`.

---

<br>
<br>

<h1 align="center" style="border-bottom: none">
    <a href="https://mlflow.org/">
        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />
    </a>
</h1>
<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>

MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.

<div align="center">

[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)
[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)
[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)
<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">
<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"
      alt="follow on X(Twitter)"></a>
<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">
<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"
      alt="follow on LinkedIn"></a>
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)

</div>

<div align="center">
   <div>
      <a href="https://mlflow.org/"><strong>Website</strong></a> ·
      <a href="https://mlflow.org/docs/latest"><strong>Docs</strong></a> ·
      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> ·
      <a href="https://mlflow.org/blog"><strong>News</strong></a> ·
      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> ·
      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>
   </div>
</div>

<br>

## 🚀 Installation

To install the MLflow Python package, run the following command:

```
pip install mlflow
```

## 📦 Core Components

MLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.

### 💡 For LLM / GenAI Developers

<table>
  <tr>
    <td>
    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>🔍 Tracing / Observability</strong></a>
        <br><br>
        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>📊 LLM Evaluation</strong></a>
        <br><br>
        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>🤖 Prompt Management</strong></a>
        <br><br>
        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>📦 App Version Tracking</strong></a>
        <br><br>
        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>
        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
</table>

### 🎓 For Data Scientists

<table>
  <tr>
    <td colspan="2" align="center" >
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>📝 Experiment Tracking</strong></a>
        <br><br>
        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
  <tr>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>💾 Model Registry</strong></a>
        <br><br>
        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started →</a>
        <br><br>
    </div>
    </td>
    <td>
      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>
    <div align="center">
        <br>
        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>🚀 Deployment</strong></a>
        <br><br>
        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>
        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started →</a>
        <br><br>
    </div>
    </td>
  </tr>
</table>

## 🌐 Hosting MLflow Anywhere

<div align="center" >
  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>
</div>

You can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.

Trusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:

- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)
- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)
- [Databricks](https://www.databricks.com/product/managed-mlflow)
- [Nebius](https://nebius.com/services/managed-mlflow)

For hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).

## 🗣️ Supported Programming Languages

- [Python](https://pypi.org/project/mlflow/)
- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)
- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)
- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)

## 🔗 Integrations

MLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.

![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)

## Usage Examples

### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/ml/tracking/))

The following examples trains a simple regression model with scikit-learn, while enabling MLflow's [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.

```python
import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# Enable MLflow's automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog()

# Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting
rf.fit(X_train, y_train)
```

Once the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.

```
mlflow ui
```

### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))

The following example runs automatic evaluation for question-answering tasks with several built-in metrics.

```python
import mlflow
import pandas as pd

# Evaluation set contains (1) input question (2) model outputs (3) ground truth
df = pd.DataFrame(
    {
        "inputs": ["What is MLflow?", "What is Spark?"],
        "outputs": [
            "MLflow is an innovative fully self-driving airship powered by AI.",
            "Sparks is an American pop and rock duo formed in Los Angeles.",
        ],
        "ground_truth": [
            "MLflow is an open-source platform for productionizing AI.",
            "Apache Spark is an open-source, distributed computing system.",
        ],
    }
)
eval_dataset = mlflow.data.from_pandas(
    df, predictions="outputs", targets="ground_truth"
)

# Start an MLflow Run to record the evaluation results to
with mlflow.start_run(run_name="evaluate_qa"):
    # Run automatic evaluation with a set of built-in metrics for question-answering models
    results = mlflow.evaluate(
        data=eval_dataset,
        model_type="question-answering",
    )

print(results.tables["eval_results_table"])
```

### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))

MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.

```python
import mlflow
from openai import OpenAI

# Enable tracing for OpenAI
mlflow.openai.autolog()

# Query OpenAI LLM normally
response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hi!"}],
    temperature=0.1,
)
```

Then navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.

## 💭 Support

- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest).
- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.
- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.
- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).
- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)
  or join us on [Slack](https://mlflow.org/slack).

## 🤝 Contributing

We happily welcome contributions to MLflow!

- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)
- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)
- Writing about MLflow and sharing your experience

Please see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.

## ⭐️ Star History

<a href="https://star-history.com/#mlflow/mlflow&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />
 </picture>
</a>

## ✏️ Citation

If you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.

## 👥 Core Members

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- [Ben Wilson](https://github.com/BenWilson2)
- [Corey Zumar](https://github.com/dbczumar)
- [Daniel Lok](https://github.com/daniellok-db)
- [Gabriel Fu](https://github.com/gabrielfu)
- [Harutaka Kawamura](https://github.com/harupy)
- [Serena Ruan](https://github.com/serena-ruan)
- [Tomu Hirata](https://github.com/TomeHirata)
- [Weichen Xu](https://github.com/WeichenXu123)
- [Yuki Watanabe](https://github.com/B-Step62)


--- libs/typescript/core/README.md ---
# MLflow Typescript SDK - Core

This is the core package of the [MLflow Typescript SDK](https://github.com/mlflow/mlflow/tree/main/libs/typescript). It is a skinny package that includes the core tracing functionality and manual instrumentation.

| Package              | NPM                                                                                                                           | Description                                                |
| -------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| [mlflow-tracing](./) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing) | The core tracing functionality and manual instrumentation. |

## Installation

```bash
npm install mlflow-tracing
```

## Quickstart

Start MLflow Tracking Server if you don't have one already:

```bash
pip install mlflow
mlflow server --backend-store-uri sqlite:///mlruns.db --port 5000
```

Self-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.

Instantiate MLflow SDK in your application:

```typescript
import * as mlflow from 'mlflow-tracing';

mlflow.init({
  trackingUri: 'http://localhost:5000',
  experimentId: '<experiment-id>'
});
```

Create a trace:

```typescript
// Wrap a function with mlflow.trace to generate a span when the function is called.
// MLflow will automatically record the function name, arguments, return value,
// latency, and exception information to the span.
const getWeather = mlflow.trace(
  (city: string) => {
    return `The weather in ${city} is sunny`;
  },
  // Pass options to set span name. See https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk
  // for the full list of options.
  { name: 'get-weather' }
);
getWeather('San Francisco');

// Alternatively, start and end span manually
const span = mlflow.startSpan({ name: 'my-span' });
span.end();
```

## Documentation 📘

Official documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).

## License

This project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).


--- libs/typescript/integrations/openai/README.md ---
# MLflow Typescript SDK - OpenAI

Seamlessly integrate [MLflow Tracing](https://github.com/mlflow/mlflow/tree/main/libs/typescript) with OpenAI to automatically trace your OpenAI API calls.

| Package             | NPM                                                                                                                                         | Description                                  |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| [mlflow-openai](./) | [![npm package](https://img.shields.io/npm/v/mlflow-tracing-openai?style=flat-square)](https://www.npmjs.com/package/mlflow-tracing-openai) | Auto-instrumentation integration for OpenAI. |

## Installation

```bash
npm install mlflow-openai
```

The package includes the [`mlflow-tracing`](https://github.com/mlflow/mlflow/tree/main/libs/typescript) package and `openai` package as peer dependencies. Depending on your package manager, you may need to install these two packages separately.

## Quickstart

Start MLflow Tracking Server if you don't have one already:

```bash
pip install mlflow
mlflow server --backend-store-uri sqlite:///mlruns.db --port 5000
```

Self-hosting MLflow server requires Python 3.10 or higher. If you don't have one, you can also use [managed MLflow service](https://mlflow.org/#get-started) for free to get started quickly.

Instantiate MLflow SDK in your application:

```typescript
import * as mlflow from 'mlflow-tracing';

mlflow.init({
  trackingUri: 'http://localhost:5000',
  experimentId: '<experiment-id>'
});
```

Create a trace:

```typescript
import { OpenAI } from 'openai';
import { tracedOpenAI } from 'mlflow-openai';

// Wrap the OpenAI client with the tracedOpenAI function
const client = tracedOpenAI(new OpenAI());

// Invoke the client as usual
const response = await client.chat.completions.create({
  model: 'o4-mini',
  messages: [
    { role: 'system', content: 'You are a helpful weather assistant.' },
    { role: 'user', content: "What's the weather like in Seattle?" }
  ]
});
```

View traces in MLflow UI:

![MLflow Tracing UI](https://github.com/mlflow/mlflow/blob/891fed9a746477f808dd2b82d3abb2382293c564/docs/static/images/llms/tracing/quickstart/single-openai-trace-detail.png?raw=true)

## Documentation 📘

Official documentation for MLflow Typescript SDK can be found [here](https://mlflow.org/docs/latest/genai/tracing/app-instrumentation/typescript-sdk).

## License

This project is licensed under the [Apache License 2.0](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt).


--- libs/typescript/jest.global-server-setup.ts ---
import { spawn } from 'child_process';
import { tmpdir } from 'os';
import { mkdtempSync } from 'fs';
import { join } from 'path';
import { TEST_PORT, TEST_TRACKING_URI } from './core/tests/helper';

/**
 * Start MLflow Python server. This is necessary for testing Typescript SDK because
 * the SDK does not have a server implementation and talks to the Python server instead.
 */
module.exports = async () => {
  const tempDir = mkdtempSync(join(tmpdir(), 'mlflow-test-'));

  const mlflowRoot = join(__dirname, '../..'); // Use the local dev version

  // Only start a server if one is not already running
  try {
    const response = await fetch(TEST_TRACKING_URI);
    if (response.ok) {
      return;
    }
  } catch (error) {
    // Ignore error
  }

  // eslint-disable-next-line no-console
  console.log(`Starting MLflow server on port ${TEST_PORT}. This may take a few seconds...
      To speed up the test, you can manually start the server and keep it running during local development.`);

  const mlflowProcess = spawn(
    'uv',
    ['run', '--directory', mlflowRoot, 'mlflow', 'server', '--port', TEST_PORT.toString()],
    {
      cwd: tempDir,
      stdio: 'inherit',
      // Create a new process group so we can kill the entire group
      detached: true
    }
  );

  try {
    await waitForServer(TEST_PORT);
    // eslint-disable-next-line no-console
    console.log(`MLflow server is ready on port ${TEST_PORT}`);
  } catch (error) {
    console.error('Failed to start MLflow server:', error);
    throw error;
  }

  // Set global variables for cleanup in jest.global-teardown.ts
  const globals = globalThis as any;
  globals.mlflowProcess = mlflowProcess;
  globals.tempDir = tempDir;
};

async function waitForServer(maxAttempts: number = 30): Promise<void> {
  for (let i = 0; i < maxAttempts; i++) {
    try {
      const response = await fetch(TEST_TRACKING_URI);
      if (response.ok) {
        return;
      }
    } catch (error) {
      // Ignore error
    }
    await new Promise((resolve) => setTimeout(resolve, 1000));
  }
  throw new Error('Failed to start MLflow server');
}


--- libs/typescript/jest.global-server-teardown.ts ---
import { rmSync } from 'fs';
import { ChildProcess } from 'child_process';

module.exports = async () => {
  const globals = globalThis as any;
  const mlflowProcess = globals.mlflowProcess as ChildProcess;
  const tempDir = globals.tempDir as string;

  if (mlflowProcess) {
    // Kill the process group to ensure worker processes spawned by uvicorn are terminated
    process.kill(-mlflowProcess.pid!, 'SIGTERM');

    // Wait for 1 second to ensure the process is terminated
    await new Promise((resolve) => setTimeout(resolve, 1000));
  }
  if (tempDir) {
    rmSync(tempDir, { recursive: true, force: true });
  }
};


--- libs/typescript/core/jest.config.js ---
/** @type {import('ts-jest').JestConfigWithTsJest} */
module.exports = {
  preset: 'ts-jest',
  testEnvironment: 'node',
  roots: ['<rootDir>/tests', '<rootDir>/src'],
  testMatch: ['**/*.test.ts'],
  moduleFileExtensions: ['ts', 'js', 'json', 'node'],
  transform: {
    '^.+\\.tsx?$': ['ts-jest', { tsconfig: 'tsconfig.json' }]
  },
  globalSetup: '<rootDir>/../jest.global-server-setup.ts',
  globalTeardown: '<rootDir>/../jest.global-server-teardown.ts',
  testTimeout: 30000,
  forceExit: true,
  detectOpenHandles: true
};


--- libs/typescript/core/tests/helper.ts ---
import { LiveSpan } from '../src/core/entities/span';
import { SpanType } from '../src/core/constants';

/**
 * Port and tracking URI for the local MLflow server used for testing.
 * If the server is not running, jest.global-setup.ts will start it.
 */
export const TEST_PORT = 5000;
export const TEST_TRACKING_URI = `http://localhost:${TEST_PORT}`;

/**
 * Mock OpenTelemetry span class for testing
 */
export class MockOtelSpan {
  name: string;
  attributes: Record<string, any>;
  spanId: string;
  traceId: string;

  constructor(
    name: string = 'test-span',
    spanId: string = 'test-span-id',
    traceId: string = 'test-trace-id'
  ) {
    this.name = name;
    this.spanId = spanId;
    this.traceId = traceId;
    this.attributes = {};
  }

  getAttribute(key: string): any {
    return this.attributes[key];
  }

  setAttribute(key: string, value: any): void {
    this.attributes[key] = value;
  }

  spanContext() {
    return {
      spanId: this.spanId,
      traceId: this.traceId
    };
  }
}

/**
 * Create a mock OpenTelemetry span with the given parameters
 */
export function createMockOtelSpan(
  name: string = 'test-span',
  spanId: string = 'test-span-id',
  traceId: string = 'test-trace-id'
): MockOtelSpan {
  return new MockOtelSpan(name, spanId, traceId);
}

/**
 * Create a test LiveSpan with mock OpenTelemetry span
 */
export function createTestSpan(
  name: string = 'test-span',
  traceId: string = 'test-trace-id',
  spanId: string = 'test-span-id',
  spanType: SpanType = SpanType.UNKNOWN
): LiveSpan {
  const mockOtelSpan = createMockOtelSpan(name, spanId, traceId);
  // eslint-disable-next-line @typescript-eslint/no-unsafe-argument
  return new LiveSpan(mockOtelSpan as any, traceId, spanType);
}


--- mlflow/claude_code/README.md ---
# MLflow Claude Code Integration

This module provides automatic tracing integration between Claude Code and MLflow.

## Module Structure

- **`config.py`** - Configuration management (settings files, environment variables)
- **`hooks.py`** - Claude Code hook setup and management
- **`cli.py`** - MLflow CLI commands (`mlflow autolog claude`)
- **`tracing.py`** - Core tracing logic and processors
- **`hooks/`** - Hook implementation handlers

## Installation

```bash
pip install mlflow
```

## Usage

Set up Claude Code tracing in any project directory:

```bash
# Set up tracing in current directory
mlflow autolog claude

# Set up tracing in specific directory
mlflow autolog claude ~/my-project

# Set up with custom tracking URI
mlflow autolog claude -u file://./custom-mlruns
mlflow autolog claude -u sqlite:///mlflow.db

# Set up with Databricks
mlflow autolog claude -u databricks -e 123456789

# Check status
mlflow autolog claude --status

# Disable tracing
mlflow autolog claude --disable
```

## How it Works

1. **Setup**: The `mlflow autolog claude` command configures Claude Code hooks in a `.claude/settings.json` file
2. **Automatic Tracing**: When you use the `claude` command in the configured directory, your conversations are automatically traced to MLflow
3. **View Traces**: Use `mlflow ui` to view your conversation traces

## Configuration

The setup creates two types of configuration:

### Claude Code Hooks

- **PostToolUse**: Captures tool usage during conversations
- **Stop**: Processes complete conversations into MLflow traces

### Environment Variables

- `MLFLOW_CLAUDE_TRACING_ENABLED=true`: Enables tracing
- `MLFLOW_TRACKING_URI`: Where to store traces (defaults to local `.claude/mlflow/runs`)
- `MLFLOW_EXPERIMENT_ID` or `MLFLOW_EXPERIMENT_NAME`: Which experiment to use

## Examples

### Basic Local Setup

```bash
mlflow autolog claude
cd .
claude "help me write a function"
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

### Databricks Integration

```bash
mlflow autolog claude -u databricks -e 123456789
claude "analyze this data"
# View traces in Databricks
```

### Custom Project Setup

```bash
mlflow autolog claude ~/my-ai-project -u sqlite:///mlflow.db -n "My AI Project"
cd ~/my-ai-project
claude "refactor this code"
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

## Troubleshooting

### Check Status

```bash
mlflow autolog claude --status
```

### Disable Tracing

```bash
mlflow autolog claude --disable
```

### View Raw Configuration

The configuration is stored in `.claude/settings.json`:

```bash
cat .claude/settings.json
```

## Requirements

- Python 3.10+ (required by MLflow)
- MLflow installed (`pip install mlflow`)
- Claude Code CLI installed


--- mlflow/R/mlflow/README.md ---
# mlflow: R interface for MLflow

[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/mlflow)](https://cran.r-project.org/package=mlflow)

- Install [MLflow](https://mlflow.org/) from R to track experiments
  locally.
- Connect to MLflow servers to share experiments with others.
- Use MLflow to export models that can be served locally and remotely.

## Prerequisites

To use the MLflow R API, you must install [the MLflow Python package](https://pypi.org/project/mlflow/).

```bash
pip install mlflow
```

Optionally, you can set the `MLFLOW_PYTHON_BIN` and `MLFLOW_BIN` environment variables to specify
the Python and MLflow binaries to use. By default, the R client automatically finds them using
`Sys.which("python")` and `Sys.which("mlflow")`.

```bash
export MLFLOW_PYTHON_BIN=/path/to/bin/python
export MLFLOW_BIN=/path/to/bin/mlflow
```

## Installation

Install `mlflow` as follows:

```r
devtools::install_github("mlflow/mlflow", subdir = "mlflow/R/mlflow")
```

## Development

Install the `mlflow` package as follows:

```r
devtools::install_github("mlflow/mlflow", subdir = "mlflow/R/mlflow")
```

Then install the latest released `mlflow` runtime.

However, currently, the development runtime of `mlflow` is also
required; which means you also need to download or clone the `mlflow`
GitHub repo:

```bash
git clone https://github.com/mlflow/mlflow
```

And upgrade the runtime to the development version as follows:

```bash
# Upgrade to the latest development version
pip install -e <local github repo>
```

## Tracking

MLflow Tracking allows you to logging parameters, code versions,
metrics, and output files when running R code and for later visualizing
the results.

MLflow allows you to group runs under experiments, which can be useful
for comparing runs intended to tackle a particular task. You can create
and activate a new experiment locally using `mlflow` as follows:

```r
library(mlflow)
mlflow_set_experiment("Test")
```

Then you can list view your experiments from MLflows user interface by
running:

```r
mlflow_ui()
```

<img src="tools/readme/mlflow-user-interface.png" class="screenshot" width=520 />

You can also use a MLflow server to track and share experiments, see
[running a tracking
server](https://www.mlflow.org/docs/latest/tracking.html#running-a-tracking-server),
and then make use of this server by running:

```r
mlflow_set_tracking_uri("http://tracking-server:5000")
```

Once the tracking url is defined, the experiments will be stored and
tracked in the specified server which others will also be able to
access.

## Projects

An MLflow Project is a format for packaging data science code in a
reusable and reproducible way.

MLflow projects can be [explicitly
created](https://www.mlflow.org/docs/latest/projects.html#specifying-projects)
or implicitly used by running `R` with `mlflow` from the terminal as
follows:

```bash
mlflow run examples/r_wine --entry-point train.R
```

Notice that is equivalent to running from `examples/r_wine`,

```bash
Rscript -e "mlflow::mlflow_source('train.R')"
```

and `train.R` performing training and logging as follows:

```r
library(mlflow)

# read parameters
column <- mlflow_log_param("column", 1)

# log total rows
mlflow_log_metric("rows", nrow(iris))

# train model
model <- lm(
  Sepal.Width ~ x,
  data.frame(Sepal.Width = iris$Sepal.Width, x = iris[,column])
)

# log models intercept
mlflow_log_metric("intercept", model$coefficients[["(Intercept)"]])
```

### Parameters

You will often want to parameterize your scripts to support running and
tracking multiple experiments. You can define parameters with type under
a `params_example.R` example as follows:

```r
library(mlflow)

# define parameters
my_int <- mlflow_param("my_int", 1, "integer")
my_num <- mlflow_param("my_num", 1.0, "numeric")

# log parameters
mlflow_log_param("param_int", my_int)
mlflow_log_param("param_num", my_num)
```

Then run `mlflow run` with custom parameters as
follows

    mlflow run tests/testthat/examples/ --entry-point params_example.R -P my_int=10 -P my_num=20.0 -P my_str=XYZ

    === Created directory /var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/tmpi6d2_wzf for downloading remote URIs passed to arguments of type 'path' ===
    === Running command 'source /miniconda2/bin/activate mlflow-da39a3ee5e6b4b0d3255bfef95601890afd80709 && Rscript -e "mlflow::mlflow_source('params_example.R')" --args --my_int 10 --my_num 20.0 --my_str XYZ' in run with ID '191b489b2355450a8c3cc9bf96cb1aa3' ===
    === Run (ID '191b489b2355450a8c3cc9bf96cb1aa3') succeeded ===

Run results that we can view with `mlflow_ui()`.

## Models

An MLflow Model is a standard format for packaging machine learning
models that can be used in a variety of downstream tools—for example,
real-time serving through a REST API or batch inference on Apache Spark.
They provide a convention to save a model in different "flavors" that
can be understood by different downstream tools.

To save a model use `mlflow_save_model()`. For instance, you can add the
following lines to the previous `train.R` script:

```r
# train model (...)

# save model
mlflow_save_model(
  crate(~ stats::predict(model, .x), model)
)
```

And trigger a run with that will also save your model as follows:

```bash
mlflow run train.R
```

Each MLflow Model is simply a directory containing arbitrary files,
together with an MLmodel file in the root of the directory that can
define multiple flavors that the model can be viewed in.

The directory containing the model looks as follows:

```r
dir("model")
```

    ## [1] "crate.bin" "MLmodel"

and the model definition `model/MLmodel` like:

```r
cat(paste(readLines("model/MLmodel"), collapse = "\n"))
```

    ## flavors:
    ##   crate:
    ##     version: 0.1.0
    ##     model: crate.bin
    ## time_created: 18-10-03T22:18:25.25.55
    ## run_id: 4286a3d27974487b95b19e01b7b3caab

Later on, the R model can be deployed which will perform predictions
using
`mlflow_rfunc_predict()`:

```r
mlflow_rfunc_predict("model", data = data.frame(x = c(0.3, 0.2)))
```

    ## Warning in mlflow_snapshot_warning(): Running without restoring the
    ## packages snapshot may not reload the model correctly. Consider running
    ## 'mlflow_restore_snapshot()' or setting the 'restore' parameter to 'TRUE'.

    ## 3.400381396714573.40656987651099

    ##        1        2
    ## 3.400381 3.406570

## Deployment

MLflow provides tools for deployment on a local machine and several
production environments. You can use these tools to easily apply your
models in a production environment.

You can serve a model by running,

```bash
mlflow rfunc serve model
```

which is equivalent to
running,

```bash
Rscript -e "mlflow_rfunc_serve('model')"
```

<img src="tools/readme/mlflow-serve-rfunc.png" class="screenshot" width=520 />

You can also run:

```bash
mlflow rfunc predict model data.json
```

which is equivalent to running,

```bash
Rscript -e "mlflow_rfunc_predict('model', 'data.json')"
```

## Dependencies

When running a project, `mlflow_snapshot()` is automatically called to
generate a `r-dependencies.txt` file which contains a list of required
packages and versions.

However, restoring dependencies is not automatic since it's usually an
expensive operation. To restore dependencies run:

```r
mlflow_restore_snapshot()
```

Notice that the `MLFLOW_SNAPSHOT_CACHE` environment variable can be set
to a cache directory to improve the time required to restore
dependencies.

## RStudio

To enable fast iteration while tracking with MLflow improvements over a
model, [RStudio 1.2.897](https://dailies.rstudio.com/) an be configured
to automatically trigger `mlflow_run()` when sourced. This is enabled by
including a `# !source mlflow::mlflow_run` comment at the top of the R
script as
follows:

<img src="tools/readme/mlflow-source-rstudio.png" class="screenshot" width=520 />

## Contributing

See the [MLflow contribution guidelines](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md).


--- mlflow/java/client/README.md ---
# MLflow Java Client

Java client for [MLflow](https://mlflow.org) REST API.
See also the MLflow [Python API](https://mlflow.org/docs/latest/python_api/index.html)
and [REST API](https://mlflow.org/docs/latest/rest-api.html).

## Requirements

- Java 1.8
- Maven
- Run the [MLflow Tracking Server 0.4.2](https://mlflow.org/docs/latest/tracking.html#running-a-tracking-server)

## Build

### Build with tests

The MLflow Java client tests require that MLflow is on the PATH (to start a local server),
so it is recommended to run them from within a development conda environment.

To build a deployable JAR and run tests:

```
mvn package
```

## Run

To run a simple sample.

```
java -cp target/mlflow-java-client-0.4.2.jar \
  com.databricks.mlflow.client.samples.QuickStartDriver http://localhost:5001
```

## JSON Serialization

MLflow Java client uses [Protobuf](https://developers.google.com/protocol-buffers/) 3.6.0 to serialize the JSON payload.

- [service.proto](../mlflow/protos/service.proto) - Protobuf definition of data objects.
- [com.databricks.api.proto.mlflow.Service.java](src/main/java/com/databricks/api/proto/mlflow/Service.java) - Generated Java classes of all data objects.
- [generate_protos.py](generate_protos.py) - One time script to generate Service.java. If service.proto changes you will need to re-run this script.
- Javadoc can be generated by running `mvn javadoc:javadoc`. The output will be in [target/site/apidocs/index.html](target/site/apidocs/index.html).
  Here is the javadoc for [Service.java](target/site/apidocs/com/databricks/api/proto/mlflow/Service.html).

## Java Client API

See [ApiClient.java](src/main/java/org/mlflow/client/ApiClient.java)
and [Service.java domain objects](src/main/java/org/mlflow/api/proto/mlflow/Service.java).

```
Run getRun(String runId)
RunInfo createRun()
RunInfo createRun(String experimentId)
RunInfo createRun(String experimentId, String appName)
RunInfo createRun(CreateRun request)
List<RunInfo> listRunInfos(String experimentId)


List<Experiment> searchExperiments()
GetExperiment.Response getExperiment(String experimentId)
Optional<Experiment> getExperimentByName(String experimentName)
long createExperiment(String experimentName)

void logParam(String runId, String key, String value)
void logMetric(String runId, String key, float value)
void setTerminated(String runId)
void setTerminated(String runId, RunStatus status)
void setTerminated(String runId, RunStatus status, long endTime)
ListArtifacts.Response listArtifacts(String runId, String path)
```

## Usage

### Java Usage

For a simple example see [QuickStartDriver.java](src/main/java/org/mlflow/tracking/samples/QuickStartDriver.java).
For full examples of API coverage see the [tests](src/test/java/org/mlflow/tracking) such as [MlflowClientTest.java](src/test/java/org/mlflow/tracking/MlflowClientTest.java).

```
package org.mlflow.tracking.samples;

import java.util.List;
import java.util.Optional;

import org.apache.log4j.Level;
import org.apache.log4j.LogManager;

import org.mlflow.api.proto.Service.*;
import org.mlflow.tracking.MlflowClient;

/**
 * This is an example application which uses the MLflow Tracking API to create and manage
 * experiments and runs.
 */
public class QuickStartDriver {
  public static void main(String[] args) throws Exception {
    (new QuickStartDriver()).process(args);
  }

  void process(String[] args) throws Exception {
    MlflowClient client;
    if (args.length < 1) {
      client = new MlflowClient();
    } else {
      client = new MlflowClient(args[0]);
    }

    boolean verbose = args.length >= 2 && "true".equals(args[1]);
    if (verbose) {
      LogManager.getLogger("org.mlflow.client").setLevel(Level.DEBUG);
    }

    System.out.println("====== createExperiment");
    String expName = "Exp_" + System.currentTimeMillis();
    String expId = client.createExperiment(expName);
    System.out.println("createExperiment: expId=" + expId);

    System.out.println("====== getExperiment");
    GetExperiment.Response exp = client.getExperiment(expId);
    System.out.println("getExperiment: " + exp);

    System.out.println("====== searchExperiments");
    List<Experiment> exps = client.searchExperiments();
    System.out.println("#experiments: " + exps.size());
    exps.forEach(e -> System.out.println("  Exp: " + e));

    createRun(client, expId);

    System.out.println("====== getExperiment again");
    GetExperiment.Response exp2 = client.getExperiment(expId);
    System.out.println("getExperiment: " + exp2);

    System.out.println("====== getExperiment by name");
    Optional<Experiment> exp3 = client.getExperimentByName(expName);
    S
[truncated]


--- artifacts/ollama/ollama/ollama-llms.txt ---
# Ollama

> To provide a minimalistic, efficient server for running LLMs locally, enabling seamless integration with other tools, frameworks, and applications. It supports model deployment, API-based interactions, and compatibility across multiple programming languages and platforms, empowering users to build custom AI solutions without cloud dependencies.

**Remember:**
- Local deployment of large language models
- Model management via lightweight server
- API-driven interaction (HTTP/GRPC)
- Multi-language SDK support
- Integration with frameworks like LangChain and LlamaIndex
- Ecosystem of plugins and tools for AI workflows

## Docs
- [README](https://raw.githubusercontent.com/ollama/ollama/main/docs/README.md): install & quickstart.
- [Install](https://raw.githubusercontent.com/ollama/ollama/main/macapp/src/install.ts): install & quickstart.
- [Examples](https://raw.githubusercontent.com/ollama/ollama/main/docs/examples.md): worked example.
- [Api](https://raw.githubusercontent.com/ollama/ollama/main/docs/api.md): API reference.
- [Cloud](https://raw.githubusercontent.com/ollama/ollama/main/docs/cloud.md): docs page.
- [Development](https://raw.githubusercontent.com/ollama/ollama/main/docs/development.md): docs page.
- [Docker](https://raw.githubusercontent.com/ollama/ollama/main/docs/docker.md): docs page.
- [Faq](https://raw.githubusercontent.com/ollama/ollama/main/docs/faq.md): core concept.
- [Gpu](https://raw.githubusercontent.com/ollama/ollama/main/docs/gpu.md): docs page.
- [Import](https://raw.githubusercontent.com/ollama/ollama/main/docs/import.md): docs page.

## Tutorials
- [README](https://raw.githubusercontent.com/ollama/ollama/main/api/examples/README.md): install & quickstart.

## Optional
- [Contributing](https://raw.githubusercontent.com/ollama/ollama/main/CONTRIBUTING.md): docs page.
- [Security](https://raw.githubusercontent.com/ollama/ollama/main/SECURITY.md): security policy.
- [Cmakelists](https://raw.githubusercontent.com/ollama/ollama/main/CMakeLists.txt): docs page.
- [README](https://raw.githubusercontent.com/ollama/ollama/main/README.md): docs page.
- [README](https://raw.githubusercontent.com/ollama/ollama/main/app/README.md): install & quickstart.
- [README](https://raw.githubusercontent.com/ollama/ollama/main/llama/README.md): install & quickstart.
- [War And Peace](https://raw.githubusercontent.com/ollama/ollama/main/model/testdata/war-and-peace.txt): docs page.
- [README](https://raw.githubusercontent.com/ollama/ollama/main/runner/README.md): install & quickstart.
- [Registry](https://raw.githubusercontent.com/ollama/ollama/main/server/internal/registry/testdata/registry.txt): docs page.

## .Github
- [20 Feature Request](https://raw.githubusercontent.com/ollama/ollama/main/.github/ISSUE_TEMPLATE/20_feature_request.md): docs page.
- [30 Model Request](https://raw.githubusercontent.com/ollama/ollama/main/.github/ISSUE_TEMPLATE/30_model_request.md): docs page.

## Integration
- [README](https://raw.githubusercontent.com/ollama/ollama/main/integration/README.md): install & quickstart.
- [Shakespeare](https://raw.githubusercontent.com/ollama/ollama/main/integration/testdata/shakespeare.txt): docs page.

## Macapp
- [README](https://raw.githubusercontent.com/ollama/ollama/main/macapp/README.md): install & quickstart.
- [Src](https://raw.githubusercontent.com/ollama/ollama/main/macapp/src/index.html): docs page.
- [Forge.Config](https://raw.githubusercontent.com/ollama/ollama/main/macapp/forge.config.ts): docs page.
- [Postcss.Config](https://raw.githubusercontent.com/ollama/ollama/main/macapp/postcss.config.js): docs page.
- [Tailwind.Config](https://raw.githubusercontent.com/ollama/ollama/main/macapp/tailwind.config.js): docs page.
- [Webpack.Main.Config](https://raw.githubusercontent.com/ollama/ollama/main/macapp/webpack.main.config.ts): docs page.
- [Webpack.Plugins](https://raw.githubusercontent.com/ollama/ollama/main/macapp/webpack.plugins.ts): docs page.
- [Webpack.Renderer.Config](https://raw.githubusercontent.com/ollama/ollama/main/macapp/webpack.renderer.config.ts): docs page.
- [Webpack.Rules](https://raw.githubusercontent.com/ollama/ollama/main/macapp/webpack.rules.ts): docs page.
- [Declarations.D](https://raw.githubusercontent.com/ollama/ollama/main/macapp/src/declarations.d.ts): docs page.

## Ml
- [Cmakelists](https://raw.githubusercontent.com/ollama/ollama/main/ml/backend/ggml/ggml/src/CMakeLists.txt): docs page.
- [Cmakelists](https://raw.githubusercontent.com/ollama/ollama/main/ml/backend/ggml/ggml/src/ggml-blas/CMakeLists.txt): docs page.
- [Cmakelists](https://raw.githubusercontent.com/ollama/ollama/main/ml/backend/ggml/ggml/src/ggml-cpu/CMakeLists.txt): docs page.
- [Cmakelists](https://raw.githubusercontent.com/ollama/ollama/main/ml/backend/ggml/ggml/src/ggml-cuda/CMakeLists.txt): docs page.
- [Cmakelists](https://raw.githubusercontent.com/ollama/ollama/main/ml/backend/ggml/ggml/src/ggml-hip/CMakeLists.txt): docs page.
- [Cmakelists](https://raw.githubusercontent.com/ollama/ollama/main/ml/backend/ggml/ggml/src/ggml-metal/CMakeLists.txt): docs page.
- [Cmakelists](https://raw.githubusercontent.com/ollama/ollama/main/ml/backend/ggml/ggml/src/ggml-vulkan/CMakeLists.txt): docs page.
- [Cmakelists](https://raw.githubusercontent.com/ollama/ollama/main/ml/backend/ggml/ggml/src/ggml-vulkan/vulkan-shaders/CMakeLists.txt): docs page.


--- artifacts/ollama/ollama/ollama-llms-full.txt ---
# llms-full (private-aware)
> Built by GitHub fetches (raw or authenticated). Large files may be truncated.

--- docs/README.md ---
# Documentation

### Getting Started
* [Quickstart](../README.md#quickstart)
* [Examples](./examples.md)
* [Importing models](./import.md)
* [MacOS Documentation](./macos.md)
* [Linux Documentation](./linux.md)
* [Windows Documentation](./windows.md)
* [Docker Documentation](./docker.md)

### Reference

* [API Reference](./api.md)
* [Modelfile Reference](./modelfile.md)
* [OpenAI Compatibility](./openai.md)

### Resources

* [Troubleshooting Guide](./troubleshooting.md)
* [FAQ](./faq.md)
* [Development guide](./development.md)


--- macapp/src/install.ts ---
import * as fs from 'fs'
import { exec as cbExec } from 'child_process'
import * as path from 'path'
import { promisify } from 'util'

const app = process && process.type === 'renderer' ? require('@electron/remote').app : require('electron').app
const ollama = app.isPackaged ? path.join(process.resourcesPath, 'ollama') : path.resolve(process.cwd(), '..', 'ollama')
const exec = promisify(cbExec)
const symlinkPath = '/usr/local/bin/ollama'

export function installed() {
  return fs.existsSync(symlinkPath) && fs.readlinkSync(symlinkPath) === ollama
}

export async function install() {
  const command = `do shell script "mkdir -p ${path.dirname(
    symlinkPath
  )} && ln -F -s \\"${ollama}\\" \\"${symlinkPath}\\"" with administrator privileges`

  await exec(`osascript -e '${command}'`)
}


--- docs/examples.md ---
# Examples

This directory contains different examples of using Ollama.

## Python examples
Ollama Python examples at [ollama-python/examples](https://github.com/ollama/ollama-python/tree/main/examples)


## JavaScript examples
Ollama JavaScript examples at [ollama-js/examples](https://github.com/ollama/ollama-js/tree/main/examples)


## OpenAI compatibility examples
Ollama OpenAI compatibility examples at [ollama/examples/openai](../docs/openai.md)


## Community examples

- [LangChain Ollama Python](https://python.langchain.com/docs/integrations/chat/ollama/)
- [LangChain Ollama JS](https://js.langchain.com/docs/integrations/chat/ollama/)


--- docs/api.md ---
# API

## Endpoints

- [Generate a completion](#generate-a-completion)
- [Generate a chat completion](#generate-a-chat-completion)
- [Create a Model](#create-a-model)
- [List Local Models](#list-local-models)
- [Show Model Information](#show-model-information)
- [Copy a Model](#copy-a-model)
- [Delete a Model](#delete-a-model)
- [Pull a Model](#pull-a-model)
- [Push a Model](#push-a-model)
- [Generate Embeddings](#generate-embeddings)
- [List Running Models](#list-running-models)
- [Version](#version)

## Conventions

### Model names

Model names follow a `model:tag` format, where `model` can have an optional namespace such as `example/model`. Some examples are `orca-mini:3b-q8_0` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.

### Durations

All durations are returned in nanoseconds.

### Streaming responses

Certain endpoints stream responses as JSON objects. Streaming can be disabled by providing `{"stream": false}` for these endpoints.

## Generate a completion

```
POST /api/generate
```

Generate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.

### Parameters

- `model`: (required) the [model name](#model-names)
- `prompt`: the prompt to generate a response for
- `suffix`: the text after the model response
- `images`: (optional) a list of base64-encoded images (for multimodal models such as `llava`)
- `think`: (for thinking models) should the model think before responding?

Advanced parameters (optional):

- `format`: the format to return a response in. Format can be `json` or a JSON schema
- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `system`: system message to (overrides what is defined in the `Modelfile`)
- `template`: the prompt template to use (overrides what is defined in the `Modelfile`)
- `stream`: if `false` the response will be returned as a single response object, rather than a stream of objects
- `raw`: if `true` no formatting will be applied to the prompt. You may choose to use the `raw` parameter if you are specifying a full templated prompt in your request to the API
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)
- `context` (deprecated): the context parameter returned from a previous request to `/generate`, this can be used to keep a short conversational memory

#### Structured outputs

Structured outputs are supported by providing a JSON schema in the `format` parameter. The model will generate a response that matches the schema. See the [structured outputs](#request-structured-outputs) example below.

#### JSON mode

Enable JSON mode by setting the `format` parameter to `json`. This will structure the response as a valid JSON object. See the JSON mode [example](#request-json-mode) below.

> [!IMPORTANT]
> It's important to instruct the model to use JSON in the `prompt`. Otherwise, the model may generate large amounts whitespace.

### Examples

#### Generate request (Streaming)

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?"
}'
```

##### Response

A stream of JSON objects is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T08:52:19.385406455-07:00",
  "response": "The",
  "done": false
}
```

The final response in the stream also includes additional data about the generation:

- `total_duration`: time spent generating the response
- `load_duration`: time spent in nanoseconds loading the model
- `prompt_eval_count`: number of tokens in the prompt
- `prompt_eval_duration`: time spent in nanoseconds evaluating the prompt
- `eval_count`: number of tokens in the response
- `eval_duration`: time in nanoseconds spent generating the response
- `context`: an encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory
- `response`: empty if the response was streamed, if not streamed, this will contain the full response

To calculate how fast the response is generated in tokens per second (token/s), divide `eval_count` / `eval_duration` * `10^9`.

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "response": "",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 10706818083,
  "load_duration": 6338219291,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 130079000,
  "eval_count": 259,
  "eval_duration": 4232710000
}
```

#### Request (No streaming)

##### Request

A response can be received in one reply when streaming is off.

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
```

##### Response

If `stream` is set to `false`, the response will be a single JSON object:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "response": "The sky is blue because it is the color of the sky.",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 5043500667,
  "load_duration": 5025959,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 325953000,
  "eval_count": 290,
  "eval_duration": 4709213000
}
```

#### Request (with suffix)

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "codellama:code",
  "prompt": "def compute_gcd(a, b):",
  "suffix": "    return result",
  "options": {
    "temperature": 0
  },
  "stream": false
}'
```

##### Response

```json5
{
  "model": "codellama:code",
  "created_at": "2024-07-22T20:47:51.147561Z",
  "response": "\n  if a == 0:\n    return b\n  else:\n    return compute_gcd(b % a, a)\n\ndef compute_lcm(a, b):\n  result = (a * b) / compute_gcd(a, b)\n",
  "done": true,
  "done_reason": "stop",
  "context": [...],
  "total_duration": 1162761250,
  "load_duration": 6683708,
  "prompt_eval_count": 17,
  "prompt_eval_duration": 201222000,
  "eval_count": 63,
  "eval_duration": 953997000
}
```

#### Request (Structured outputs)

##### Request

```shell
curl -X POST http://localhost:11434/api/generate -H "Content-Type: application/json" -d '{
  "model": "llama3.1:8b",
  "prompt": "Ollama is 22 years old and is busy saving the world. Respond using JSON",
  "stream": false,
  "format": {
    "type": "object",
    "properties": {
      "age": {
        "type": "integer"
      },
      "available": {
        "type": "boolean"
      }
    },
    "required": [
      "age",
      "available"
    ]
  }
}'
```

##### Response

```json
{
  "model": "llama3.1:8b",
  "created_at": "2024-12-06T00:48:09.983619Z",
  "response": "{\n  \"age\": 22,\n  \"available\": true\n}",
  "done": true,
  "done_reason": "stop",
  "context": [1, 2, 3],
  "total_duration": 1075509083,
  "load_duration": 567678166,
  "prompt_eval_count": 28,
  "prompt_eval_duration": 236000000,
  "eval_count": 16,
  "eval_duration": 269000000
}
```

#### Request (JSON mode)

> [!IMPORTANT]
> When `format` is set to `json`, the output will always be a well-formed JSON object. It's important to also instruct the model to respond in JSON.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "What color is the sky at different times of the day? Respond using JSON",
  "format": "json",
  "stream": false
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-11-09T21:07:55.186497Z",
  "response": "{\n\"morning\": {\n\"color\": \"blue\"\n},\n\"noon\": {\n\"color\": \"blue-gray\"\n},\n\"afternoon\": {\n\"color\": \"warm gray\"\n},\n\"evening\": {\n\"color\": \"orange\"\n}\n}\n",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 4648158584,
  "load_duration": 4071084,
  "prompt_eval_count": 36,
  "prompt_eval_duration": 439038000,
  "eval_count": 180,
  "eval_duration": 4196918000
}
```

The value of `response` will be a string containing JSON similar to:

```json
{
  "morning": {
    "color": "blue"
  },
  "noon": {
    "color": "blue-gray"
  },
  "afternoon": {
    "color": "warm gray"
  },
  "evening": {
    "color": "orange"
  }
}
```

#### Request (with images)

To submit images to multimodal models such as `llava` or `bakllava`, provide a list of base64-encoded `images`:

#### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llava",
  "prompt":"What is in this picture?",
  "stream": false,
  "images": ["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"]
}'
```

#### Response

```json
{
  "model": "llava",
  "created_at": "2023-11-03T15:36:02.583064Z",
  "response": "A happy cartoon character, which is cute and cheerful.",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 2938432250,
  "load_duration": 2559292,
  "prompt_eval_count": 1,
  "prompt_eval_duration": 2195557000,
  "eval_count": 44,
  "eval_duration": 736432000
}
```

#### Request (Raw Mode)

In some cases, you may wish to bypass the templating system and provide a full prompt. In this case, you can use the `raw` parameter to disable templating. Also note that raw mode will not return a context.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "[INST] why is the sky blue? [/INST]",
  "raw": true,
  "stream": false
}'
```

#### Request (Reproducible outputs)

For reproducible outputs, set `seed` to a number:

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Why is the sky blue?",
  "options": {
    "seed": 123
  }
}'
```

##### Response

```json
{
  "model": "mistral",
  "created_at": "2023-11-03T15:36:02.583064Z",
  "response": " The sky appears blue because of a phenomenon called Rayleigh scattering.",
  "done": true,
  "total_duration": 8493852375,
  "load_duration": 6589624375,
  "prompt_eval_count": 14,
  "prompt_eval_duration": 119039000,
  "eval_count": 110,
  "eval_duration": 1779061000
}
```

#### Generate request (With options)

If you want to set custom options for the model at runtime rather than in the Modelfile, you can do so with the `options` parameter. This example sets every available option, but you can set any of them individually and omit the ones you do not want to override.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "stream": false,
  "options": {
    "num_keep": 5,
    "seed": 42,
    "num_predict": 100,
    "top_k": 20,
    "top_p": 0.9,
    "min_p": 0.0,
    "typical_p": 0.7,
    "repeat_last_n": 33,
    "temperature": 0.8,
    "repeat_penalty": 1.2,
    "presence_penalty": 1.5,
    "frequency_penalty": 1.0,
    "penalize_newline": true,
    "stop": ["\n", "user:"],
    "numa": false,
    "num_ctx": 1024,
    "num_batch": 2,
    "num_gpu": 1,
    "main_gpu": 0,
    "use_mmap": true,
    "num_thread": 8
  }
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "response": "The sky is blue because it is the color of the sky.",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 4935886791,
  "load_duration": 534986708,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 107345000,
  "eval_count": 237,
  "eval_duration": 4289432000
}
```

#### Load a model

If an empty prompt is provided, the model will be loaded into memory.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2"
}'
```

##### Response

A single JSON object is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-12-18T19:52:07.071755Z",
  "response": "",
  "done": true
}
```

#### Unload a model

If an empty prompt is provided and the `keep_alive` parameter is set to `0`, a model will be unloaded from memory.

##### Request

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "keep_alive": 0
}'
```

##### Response

A single JSON object is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2024-09-12T03:54:03.516566Z",
  "response": "",
  "done": true,
  "done_reason": "unload"
}
```

## Generate a chat completion

```
POST /api/chat
```

Generate the next message in a chat with a provided model. This is a streaming endpoint, so there will be a series of responses. Streaming can be disabled using `"stream": false`. The final response object will include statistics and additional data from the request.

### Parameters

- `model`: (required) the [model name](#model-names)
- `messages`: the messages of the chat, this can be used to keep a chat memory
- `tools`: list of tools in JSON for the model to use if supported
- `think`: (for thinking models) should the model think before responding?

The `message` object has the following fields:

- `role`: the role of the message, either `system`, `user`, `assistant`, or `tool`
- `content`: the content of the message
- `thinking`: (for thinking models) the model's thinking process
- `images` (optional): a list of images to include in the message (for multimodal models such as `llava`)
- `tool_calls` (optional): a list of tools in JSON that the model wants to use
- `tool_name` (optional): add the name of the tool that was executed to inform the model of the result

Advanced parameters (optional):

- `format`: the format to return a response in. Format can be `json` or a JSON schema.
- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `stream`: if `false` the response will be returned as a single response object, rather than a stream of objects
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)

### Tool calling

Tool calling is supported by providing a list of tools in the `tools` parameter. The model will generate a response that includes a list of tool calls. See the [Chat request (Streaming with tools)](#chat-request-streaming-with-tools) example below.

Models can also explain the result of the tool call in the response. See the [Chat request (With history, with tools)](#chat-request-with-history-with-tools) example below.

[See models with tool calling capabilities](https://ollama.com/search?c=tool).

### Structured outputs

Structured outputs are supported by providing a JSON schema in the `format` parameter. The model will generate a response that matches the schema. See the [Chat request (Structured outputs)](#chat-request-structured-outputs) example below.

### Examples

#### Chat request (Streaming)

##### Request

Send a chat message with a streaming response.

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    }
  ]
}'
```

##### Response

A stream of JSON objects is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T08:52:19.385406455-07:00",
  "message": {
    "role": "assistant",
    "content": "The",
    "images": null
  },
  "done": false
}
```

Final response:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "message": {
    "role": "assistant",
    "content": ""
  },
  "done": true,
  "total_duration": 4883583458,
  "load_duration": 1334875,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 342546000,
  "eval_count": 282,
  "eval_duration": 4535599000
}
```

#### Chat request (Streaming with tools)

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "what is the weather in tokyo?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get the weather in a given city",
        "parameters": {
          "type": "object",
          "properties": {
            "city": {
              "type": "string",
              "description": "The city to get the weather for"
            }
          },
          "required": ["city"]
        }
      }
    }
  ],
  "stream": true
}'
```

##### Response

A stream of JSON objects is returned:
```json
{
    "model": "llama3.2",
    "created_at": "2025-07-07T20:22:19.184789Z",
    "message": {
        "role": "assistant",
        "content": "",
        "tool_calls": [
            {
                "function": {
                    "name": "get_weather",
                    "arguments": {
                        "city": "Tokyo"
                    }
                },
            }
        ]
    },
    "done": false
}
```

Final response:

```json
{
  "model":"llama3.2",
  "created_at":"2025-07-07T20:22:19.19314Z",
  "message": {
    "role": "assistant",
    "content": ""
  },
  "done_reason": "stop",
  "done": true,
  "total_duration": 182242375,
  "load_duration": 41295167,
  "prompt_eval_count": 169,
  "prompt_eval_duration": 24573166,
  "eval_count": 15,
  "eval_duration": 115959084
}
```

#### Chat request (No streaming)

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    }
  ],
  "stream": false
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-12-12T14:13:43.416799Z",
  "message": {
    "role": "assistant",
    "content": "Hello! How are you today?"
  },
  "done": true,
  "total_duration": 5191566416,
  "load_duration": 2154458,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 383809000,
  "eval_count": 298,
  "eval_duration": 4799921000
}
```

#### Chat request (No streaming, with tools)

##### Request


```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "what is the weather in tokyo?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get the weather in a given city",
        "parameters": {
          "type": "object",
          "properties": {
            "city": {
              "type": "string",
              "description": "The city to get the weather for"
            }
          },
          "required": ["city"]
        }
      }
    }
  ],
  "stream": false 
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2025-07-07T20:32:53.844124Z",
  "message": {
    "role": "assistant",
    "content": "",
    "tool_calls": [
      {
        "function": {
          "name": "get_weather",
          "arguments": {
            "city": "Tokyo"
          }
        },
      }
    ]
  },
  "done_reason": "stop",
  "done": true,
  "total_duration": 3244883583,
  "load_duration": 2969184542,
  "prompt_eval_count": 169,
  "prompt_eval_duration": 141656333,
  "eval_count": 18,
  "eval_duration": 133293625
}
```

#### Chat request (Structured outputs)

##### Request

```shell
curl -X POST http://localhost:11434/api/chat -H "Content-Type: application/json" -d '{
  "model": "llama3.1",
  "messages": [{"role": "user", "content": "Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability."}],
  "stream": false,
  "format": {
    "type": "object",
    "properties": {
      "age": {
        "type": "integer"
      },
      "available": {
        "type": "boolean"
      }
    },
    "required": [
      "age",
      "available"
    ]
  },
  "options": {
    "temperature": 0
  }
}'
```

##### Response

```json
{
  "model": "llama3.1",
  "created_at": "2024-12-06T00:46:58.265747Z",
  "message": { "role": "assistant", "content": "{\"age\": 22, \"available\": false}" },
  "done_reason": "stop",
  "done": true,
  "total_duration": 2254970291,
  "load_duration": 574751416,
  "prompt_eval_count": 34,
  "prompt_eval_duration": 1502000000,
  "eval_count": 12,
  "eval_duration": 175000000
}
```

#### Chat request (With History)

Send a chat message with a conversation history. You can use this same approach to start the conversation using multi-shot or chain-of-thought prompting.

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    },
    {
      "role": "assistant",
      "content": "due to rayleigh scattering."
    },
    {
      "role": "user",
      "content": "how is that different than mie scattering?"
    }
  ]
}'
```

##### Response

A stream of JSON objects is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T08:52:19.385406455-07:00",
  "message": {
    "role": "assistant",
    "content": "The"
  },
  "done": false
}
```

Final response:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T19:22:45.499127Z",
  "done": true,
  "total_duration": 8113331500,
  "load_duration": 6396458,
  "prompt_eval_count": 61,
  "prompt_eval_duration": 398801000,
  "eval_count": 468,
  "eval_duration": 7701267000
}
```


#### Chat request (With history, with tools)

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "what is the weather in Toronto?"
    },
    // the message from the model appended to history
    {
      "role": "assistant",
      "content": "",
      "tool_calls": [
        {
          "function": {
            "name": "get_temperature",
            "arguments": {
              "city": "Toronto"
            }
          },
        }
      ]
    },
    // the tool call result appended to history
    {
      "role": "tool",
      "content": "11 degrees celsius",
      "tool_name": "get_temperature",
    }
  ],
  "stream": false,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get the weather in a given city",
        "parameters": {
          "type": "object",
          "properties": {
            "city": {
              "type": "string",
              "description": "The city to get the weather for"
            }
          },
          "required": ["city"]
        }
      }
    }
  ]
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2025-07-07T20:43:37.688511Z",
  "message": {
    "role": "assistant",
    "content": "The current temperature in Toronto is 11°C."
  },
  "done_reason": "stop",
  "done": true,
  "total_duration": 890771750,
  "load_duration": 707634750,
  "prompt_eval_count": 94,
  "prompt_eval_duration": 91703208,
  "eval_count": 11,
  "eval_duration": 90282125
}

```


#### Chat request (with images)

##### Request

Send a chat message with images. The images should be provided as an array, with the individual images encoded in Base64.

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llava",
  "messages": [
    {
      "role": "user",
      "content": "what is in this image?",
      "images": ["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"]
    }
  ]
}'
```

##### Response

```json
{
  "model": "llava",
  "created_at": "2023-12-13T22:42:50.203334Z",
  "message": {
    "role": "assistant",
    "content": " The image features a cute, little pig with an angry facial expression. It's wearing a heart on its shirt and is waving in the air. This scene appears to be part of a drawing or sketching project.",
    "images": null
  },
  "done": true,
  "total_duration": 1668506709,
  "load_duration": 1986209,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 359682000,
  "eval_count": 83,
  "eval_duration": 1303285000
}
```

#### Chat request (Reproducible outputs)

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "Hello!"
    }
  ],
  "options": {
    "seed": 101,
    "temperature": 0
  }
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2023-12-12T14:13:43.416799Z",
  "message": {
    "role": "assistant",
    "content": "Hello! How are you today?"
  },
  "done": true,
  "total_duration": 5191566416,
  "load_duration": 2154458,
  "prompt_eval_count": 26,
  "prompt_eval_duration": 383809000,
  "eval_count": 298,
  "eval_duration": 4799921000
}
```

#### Chat request (with tools)

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "What is the weather today in Paris?"
    }
  ],
  "stream": false,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "Get the current weather for a location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The location to get the weather for, e.g. San Francisco, CA"
            },
            "format": {
              "type": "string",
              "description": "The format to return the weather in, e.g. 'celsius' or 'fahrenheit'",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location", "format"]
        }
      }
    }
  ]
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at": "2024-07-22T20:33:28.123648Z",
  "message": {
    "role": "assistant",
    "content": "",
    "tool_calls": [
      {
        "function": {
          "name": "get_current_weather",
          "arguments": {
            "format": "celsius",
            "location": "Paris, FR"
          }
        }
      }
    ]
  },
  "done_reason": "stop",
  "done": true,
  "total_duration": 885095291,
  "load_duration": 3753500,
  "prompt_eval_count": 122,
  "prompt_eval_duration": 328493000,
  "eval_count": 33,
  "eval_duration": 552222000
}
```

#### Load a model

If the messages array is empty, the model will be loaded into memory.

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": []
}'
```

##### Response

```json
{
  "model": "llama3.2",
  "created_at":"2024-09-12T21:17:29.110811Z",
  "message": {
    "role": "assistant",
    "content": ""
  },
  "done_reason": "load",
  "done": true
}
```

#### Unload a model

If the messages array is empty and the `keep_alive` parameter is set to `0`, a model will be unloaded from memory.

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [],
  "keep_alive": 0
}'
```

##### Response

A single JSON object is returned:

```json
{
  "model": "llama3.2",
  "created_at":"2024-09-12T21:33:17.547535Z",
  "message": {
    "role": "assistant",
    "content": ""
  },
  "done_reason": "unload",
  "done": true
}
```

## Create a Model

```
POST /api/create
```

Create a model from:
 * another model;
 * a safetensors directory; or
 * a GGUF file.

If you are creating a model from a safetensors directory or from a GGUF file, you must [create a blob](#create-a-blob) for each of the files and then use the file name and SHA256 digest associated with each blob in the `files` field.

### Parameters

- `model`: name of the model to create
- `from`: (optional) name of an existing model to create the new model from
- `files`: (optional) a dictionary of file names to SHA256 digests of blobs to create the model from
- `adapters`: (optional) a dictionary of file names to SHA256 digests of blobs for LORA adapters
- `template`: (optional) the prompt template for the model
- `license`: (optional) a string or list of strings containing the license or licenses for the model
- `system`: (optional) a string containing the system prompt for the model
- `parameters`: (optional) a dictionary of parameters for the model (see [Modelfile](./modelfile.md#valid-parameters-and-values) for a list of parameters)
- `messages`: (optional) a list of message objects used to create a conversation
- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects
- `quantize` (optional): quantize a non-quantized (e.g. float16) model

#### Quantization types

| Type | Recommended |
| --- | :-: |
| q4_K_M | * |
| q4_K_S | |
| q8_0 | * |

### Examples

#### Create a new model

Create a new model from an existing model.

##### Request

```shell
curl http://localhost:11434/api/create -d '{
  "model": "mario",
  "from": "llama3.2",
  "system": "You are Mario from Super Mario Bros."
}'
```

##### Response

A stream of JSON objects is returned:

```json
{"status":"reading model metadata"}
{"status":"creating system layer"}
{"status":"using already created layer sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2"}
{"status":"using already created layer sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b"}
{"status":"using already created layer sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d"}
{"status":"using already created layer sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988"}
{"status":"using already created layer sha256:2759286baa875dc22de5394b4a925701b1896a7e3f8e53275c36f75a877a82c9"}
{"status":"writing layer sha256:df30045fe90f0d750db82a058109cecd6d4de9c90a3d75b19c09e5f64580bb42"}
{"status":"writing layer sha256:f18a68eb09bf925bb1b669490407c1b1251c5db98dc4d3d81f3088498ea55690"}
{"status":"writing manifest"}
{"status":"success"}
```

#### Quantize a model

Quantize a non-quantized model.

##### Request

```shell
curl http://localhost:11434/api/create -d '{
  "model": "llama3.2:quantized",
  "from": "llama3.2:3b-instruct-fp16",
  "quantize": "q4_K_M"
}'
```

##### Response

A stream of JSON objects is returned:

```json
{"status":"quantizing F16 model to Q4_K_M","digest":"0","total":6433687776,"completed":12302}
{"status":"quantizing F16 model to Q4_K_M","digest":"0","total":6433687776,"completed":6433687552}
{"status":"verifying conversion"}
{"status":"creating new layer sha256:fb7f4f211b89c6c4928ff4ddb73db9f9c0cfca3e000c3e40d6cf27ddc6ca72eb"}
{"status":"using existing layer sha256:966de95ca8a62200913e3f8bfbf84c8494536f1b94b49166851e76644e966396"}
{"status":"using existing layer sha256:fcc5a6bec9daf9b561a68827b67ab6088e1dba9d1fa2a50d7bbcc8384e0a265d"}
{"status":"using existing layer sha256:a70ff7e570d97baaf4e62ac6e6ad9975e04caa6d900d3742d37698494479e0cd"}
{"status":"using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb"}
{"status":"writing manifest"}
{"status":"success"}
```

#### Create a model from GGUF

Create a model from a GGUF file. The `files` parameter should be filled out with the file name and SHA256 digest of the GGUF file you wish to use. Use [/api/blobs/:digest](#push-a-blob) to push the GGUF file to the server before calling this API.


##### Request

```shell
curl http://localhost:11434/api/create -d '{
  "model": "my-gguf-model",
  "files": {
    "test.gguf": "sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3"
  }
}'
```

##### Response

A stream of JSON objects is returned:

```json
{"status":"parsing GGUF"}
{"status":"using existing layer sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3"}
{"status":"writing manifest"}
{"status":"success"}
```


#### Create a model from a Safetensors directory

The `files` parameter should include a dictionary of files for the safetensors model which includes the file names and SHA256 digest of each file. Use [/api/blobs/:digest](#push-a-blob) to first push each of the files to the server before calling this API. Files will remain in the cache until the Ollama server is restarted.

##### Request

```shell
curl http://localhost:11434/api/create -d '{
  "model": "fred",
  "files": {
    "config.json": "sha256:dd3443e529fb2290423a0c65c2d633e67b419d273f170259e27297219828e389",
    "generation_config.json": "sha256:88effbb63300dbbc7390143fbbdd9d9fa50587b37e8bfd16c8c90d4970a74a36",
    "special_tokens_map.json": "sha256:b7455f0e8f00539108837bfa586c4fbf424e31f8717819a6798be74bef813d05",
    "tokenizer.json": "sha256:bbc1904d35169c542dffbe1f7589a5994ec7426d9e5b609d07bab876f32e97ab",
    "tokenizer_config.json": "sha256:24e8a6dc2547164b7002e3125f10b415105644fcf02bf9ad8b674c87b1eaaed6",
    "model.safetensors": "sha256:1ff795ff6a07e6a68085d206fb84417da2f083f68391c2843cd2b8ac6df8538f"
  }
}'
```

##### Response

A stream of JSON objects is returned:

```shell
{"status":"converting model"}
{"status":"creating new layer sha256:05ca5b813af4a53d2c2922933936e398958855c44ee534858fcfd830940618b6"}
{"status":"using autodetected template llama3-instruct"}
{"status":"using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb"}
{"status":"writing manifest"}
{"status":"success"}
```

## Check if a Blob Exists

```shell
HEAD /api/blobs/:digest
```

Ensures that the file blob (Binary Large Object) used with create a model exists on the server. This checks your Ollama server and not ollama.com.

### Query Parameters

- `digest`: the SHA256 digest of the blob

### Examples

#### Request

```shell
curl -I http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2
```

#### Response

Return 200 OK if the blob exists, 404 Not Found if it does not.

## Push a Blob

```
POST /api/blobs/:digest
```

Push a file to the Ollama server to create a "blob" (Binary Large Object).

### Query Parameters

- `digest`: the expected SHA256 digest of the file

### Examples

#### Request

```shell
curl -T model.gguf -X POST http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2
```

#### Response

Return 201 Created if the blob was successfully created, 400 Bad Request if the digest used is not expected.

## List Local Models

```
GET /api/tags
```

List models that are available locally.

### Examples

#### Request

```shell
curl http://localhost:11434/api/tags
```

#### Response

A single JSON object will be returned.

```json
{
  "models": [
    {
      "name": "deepseek-r1:latest",
      "model": "deepseek-r1:latest",
      "modified_at": "2025-05-10T08:06:48.639712648-07:00",
      "size": 4683075271,
      "digest": "0a8c266910232fd3291e71e5ba1e058cc5af9d411192cf88b6d30e92b6e73163",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "qwen2",
        "families": [
          "qwen2"
        ],
        "parameter_size": "7.6B",
        "quantization_level": "Q4_K_M"
      }
    },
    {
      "name": "llama3.2:latest",
      "model": "llama3.2:latest",
      "modified_at": "2025-05-04T17:37:44.706015396-07:00",
      "size": 2019393189,
      "digest": "a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "llama",
        "families": [
          "llama"
        ],
        "parameter_size": "3.2B",
        "quantization_level": "Q4_K_M"
      }
    }
  ]
}
```

## Show Model Information

```
POST /api/show
```

Show information about a model including details, modelfile, template, parameters, license, system prompt.

### Parameters

- `model`: name of the model to show
- `verbose`: (optional) if set to `true`, returns full data for verbose response fields

### Examples

#### Request

```shell
curl http://localhost:11434/api/show -d '{
  "model": "llava"
}'
```

#### Response

```json5
{
  "modelfile": "# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llava:latest\n\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\nTEMPLATE \"\"\"{{ .System }}\nUSER: {{ .Prompt }}\nASSISTANT: \"\"\"\nPARAMETER num_ctx 4096\nPARAMETER stop \"\u003c/s\u003e\"\nPARAMETER stop \"USER:\"\nPARAMETER stop \"ASSISTANT:\"",
  "parameters": "num_keep                       24\nstop                           \"<|start_header_id|>\"\nstop                           \"<|end_header_id|>\"\nstop                           \"<|eot_id|>\"",
  "template": "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>",
  "details": {
    "parent_model": "",
    "format": "gguf",
    "family": "llama",
    "families": [
      "llama"
    ],
    "parameter_size": "8.0B",
    "quantization_level": "Q4_0"
  },
  "model_info": {
    "general.architecture": "llama",
    "general.file_type": 2,
    "general.parameter_count": 8030261248,
    "general.quantization_version": 2,
    "llama.attention.head_count": 32,
    "llama.attention.head_count_kv": 8,
    "llama.attention.layer_norm_rms_epsilon": 0.00001,
    "llama.block_count": 32,
    "llama.context_length": 8192,
    "llama.embedding_length": 4096,
    "llama.feed_forward_length": 14336,
    "llama.rope.dimension_count": 128,
    "llama.rope.freq_base": 500000,
    "llama.vocab_size": 128256,
    "tokenizer.ggml.bos_token_id": 128000,
    "tokenizer.ggml.eos_token_id": 128009,
    "tokenizer.ggml.merges": [],            // populates if `verbose=true`
    "tokenizer.ggml.model": "gpt2",
    "tokenizer.ggml.pre": "llama-bpe",
    "tokenizer.ggml.token_type": [],        // populates if `verbose=true`
    "tokenizer.ggml.tokens": []             // populates if `verbose=true`
  },
  "capabilities": [
    "completion",
    "vision"
  ],
}
```

## Copy a Model

```
POST /api/copy
```

Copy a model. Creates a model with another name from an existing model.

### Examples

#### Request

```shell
curl http://localhost:11434/api/copy -d '{
  "source": "llama3.2",
  "destination": "llama3-backup"
}'
```

#### Response

Returns a 200 OK if successful, or a 404 Not Found if the source model doesn't exist.

## Delete a Model

```
DELETE /api/delete
```

Delete a model and its data.

### Parameters

- `model`: model name to delete

### Examples

#### Request

```shell
curl -X DELETE http://localhost:11434/api/delete -d '{
  "model": "llama3:13b"
}'
```

#### Response

Returns a 200 OK if successful, 404 Not Found if the model to be deleted doesn't exist.

## Pull a Model

```
POST /api/pull
```

Download a model from the ollama library. Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.

### Parameters

- `model`: name of the model to pull
- `insecure`: (optional) allow insecure connections to the library. Only use this if you are pulling from your own library during development.
- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects

### Examples

#### Request

```shell
curl http://localhost:11434/api/pull -d '{
  "model": "llama3.2"
}'
```

#### Response

If `stream` is not specified, or set to `true`, a stream of JSON objects is returned:

The first object is the manifest:

```json
{
  "status": "pulling manifest"
}
```

Then there is a series of downloading responses. Until any of the download is completed, the `completed` key may not be included. The number of files to be downloaded depends on the number of layers specified in the manifest.

```json
{
  "status": "pulling digestname",
  "digest": "digestname",
  "total": 2142590208,
  "completed": 241970
}
```

After all the files are downloaded, the final responses are:

```json
{
    "status": "verifying sha256 digest"
}
{
    "status": "writing manifest"
}
{
    "status": "removing any unused layers"
}
{
    "status": "success"
}
```

if `stream` is set to false, then the response is a single JSON object:

```json
{
  "status": "success"
}
```

## Push a Model

```
POST /api/push
```

Upload a model to a model library. Requires registering for ollama.ai and adding a public key first.

### Parameters

- `model`: name of the model to push in the form of `<namespace>/<model>:<tag>`
- `insecure`: (optional) allow insecure connections to the library. Only use this if you are pushing to your library during development.
- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects

### Examples

#### Request

```shell
curl http://localhost:11434/api/push -d '{
  "model": "mattw/pygmalion:latest"
}'
```

#### Response

If `stream` is not specified, or set to `true`, a stream of JSON objects is returned:

```json
{ "status": "retrieving manifest" }
```

and then:

```json
{
  "status": "starting upload",
  "digest": "sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab",
  "total": 1928429856
}
```

Then there is a series of uploading responses:

```json
{
  "status": "starting upload",
  "digest": "sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab",
  "total": 1928429856
}
```

Finally, when the upload is complete:

```json
{"status":"pushing manifest"}
{"status":"success"}
```

If `stream` is set to `false`, then the response is a single JSON object:

```json
{ "status": "success" }
```

## Generate Embeddings

```
POST /api/embed
```

Generate embeddings from a model

### Parameters

- `model`: name of model to generate embeddings from
- `input`: text or list of text to generate embeddings for

Advanced parameters:

- `truncate`: truncates the end of each input to fit within context length. Returns error if `false` and context length is exceeded. Defaults to `true`
- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)
- `dimensions`: number of dimensions for the embedding

### Examples

#### Request

```shell
curl http://localhost:11434/api/embed -d '{
  "model": "all-minilm",
  "input": "Why is the sky blue?"
}'
```

#### Response

```json
{
  "model": "all-minilm",
  "embeddings": [[
    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,
    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348
  ]],
  "total_duration": 14143917,
  "load_duration": 1019500,
  "prompt_eval_count": 8
}
```

#### Request (Multiple input)

```shell
curl http://localhost:11434/api/embed -d '{
  "model": "all-minilm",
  "input": ["Why is the sky blue?", "Why is the grass green?"]
}'
```

#### Response

```json
{
  "model": "all-minilm",
  "embeddings": [[
    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,
    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348
  ],[
    -0.0098027075, 0.06042469, 0.025257962, -0.006364387, 0.07272725,
    0.017194884, 0.09032035, -0.051705178, 0.09951512, 0.09072481
  ]]
}
```

## List Running Models
```
GET /api/ps
```

List models that are currently loaded into memory.

#### Examples

### Request

```shell
curl http://localhost:11434/api/ps
```

#### Response

A single JSON object will be returned.

```json
{
  "models": [
    {
      "name": "mistral:latest",
      "model": "mistral:latest",
      "size": 5137025024,
      "digest": "2ae6f6dd7a3dd734790bbbf58b8909a606e0e7e97e94b7604e0aa7ae4490e6d8",
      "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "llama",
        "families": [
          "llama"
        ],
        "parameter_size": "7.2B",
        "quantization_level": "Q4_0"
      },
      "expires_at": "2024-06-04T14:38:31.83753-07:00",
      "size_vram": 5137025024
    }
  ]
}
```

## Generate Embedding

> Note: this endpoint has been superseded by `/api/embed`

```
POST /api/embeddings
```

Generate embeddings from a model

### Parameters

- `model`: name of model to generate embeddings from
- `prompt`: text to generate embeddings for

Advanced parameters:

- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`
- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)

### Examples

#### Request

```shell
curl http://localhost:11434/api/embeddings -d '{
  "model": "all-minilm",
  "prompt": "Here is an article about llamas..."
}'
```

#### Response

```json
{
  "embedding": [
    0.5670403838157654, 0.009260174818336964, 0.23178744316101074, -0.2916173040866852, -0.8924556970596313,
    0.8785552978515625, -0.34576427936553955, 0.5742510557174683, -0.04222835972905159, -0.137906014919281
  ]
}
```

## Version

```
GET /api/version
```

Retrieve the Ollama version

### Examples

#### Request

```shell
curl http://localhost:11434/api/version
```

#### Response

```json
{
  "version": "0.5.1"
}
```




--- docs/cloud.md ---
# Cloud

| Ollama's cloud is currently in preview. For full documentation, see [Ollama's documentation](https://docs.ollama.com/cloud).

## Cloud Models

[Cloud models](https://ollama.com/cloud) are a new kind of model in Ollama that can run without a powerful GPU. Instead, cloud models are automatically offloaded to Ollama's cloud while offering the same capabilities as local models, making it possible to keep using your local tools while running larger models that wouldn’t fit on a personal computer.

Ollama currently supports the following cloud models, with more coming soon:

- `gpt-oss:20b-cloud`
- `gpt-oss:120b-cloud`
- `deepseek-v3.1:671b-cloud`
- `qwen3-coder:480b-cloud`

### Get started

To run a cloud model, open the terminal and run:

```
ollama run gpt-oss:120b-cloud
```

To run cloud models with integrations that work with Ollama, first download the cloud model:

```
ollama pull qwen3-coder:480b-cloud
```

Then sign in to Ollama:

```
ollama signin
```

Finally, access the model using the model name `qwen3-coder:480b-cloud` via Ollama's local API or tooling.

## Cloud API access

Cloud models can also be accessed directly on ollama.com's API. For more information, see the [docs](https://docs.ollama.com/cloud).


--- docs/development.md ---
# Development

Install prerequisites:

- [Go](https://go.dev/doc/install)
- C/C++ Compiler e.g. Clang on macOS, [TDM-GCC](https://github.com/jmeubank/tdm-gcc/releases/latest) (Windows amd64) or [llvm-mingw](https://github.com/mstorsjo/llvm-mingw) (Windows arm64), GCC/Clang on Linux.

Then build and run Ollama from the root directory of the repository:

```shell
go run . serve
```

> [!NOTE]
> Ollama includes native code compiled with CGO.  From time to time these data structures can change and CGO can get out of sync resulting in unexpected crashes.  You can force a full build of the native code by running `go clean -cache` first. 


## macOS (Apple Silicon)

macOS Apple Silicon supports Metal which is built-in to the Ollama binary. No additional steps are required.

## macOS (Intel)

Install prerequisites:

- [CMake](https://cmake.org/download/) or `brew install cmake`

Then, configure and build the project:

```shell
cmake -B build
cmake --build build
```

Lastly, run Ollama:

```shell
go run . serve
```

## Windows

Install prerequisites:

- [CMake](https://cmake.org/download/)
- [Visual Studio 2022](https://visualstudio.microsoft.com/downloads/) including the Native Desktop Workload
- (Optional) AMD GPU support
    - [ROCm](https://rocm.docs.amd.com/en/latest/)
    - [Ninja](https://github.com/ninja-build/ninja/releases)
- (Optional) NVIDIA GPU support
    - [CUDA SDK](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_network)

Then, configure and build the project:

```shell
cmake -B build
cmake --build build --config Release
```

> [!IMPORTANT]
> Building for ROCm requires additional flags:
> ```
> cmake -B build -G Ninja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++
> cmake --build build --config Release
> ```


Lastly, run Ollama:

```shell
go run . serve
```

## Windows (ARM)

Windows ARM does not support additional acceleration libraries at this time.  Do not use cmake, simply `go run` or `go build`.

## Linux

Install prerequisites:

- [CMake](https://cmake.org/download/) or `sudo apt install cmake` or `sudo dnf install cmake`
- (Optional) AMD GPU support
    - [ROCm](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html)
- (Optional) NVIDIA GPU support
    - [CUDA SDK](https://developer.nvidia.com/cuda-downloads)

> [!IMPORTANT]
> Ensure prerequisites are in `PATH` before running CMake.


Then, configure and build the project:

```shell
cmake -B build
cmake --build build
```

Lastly, run Ollama:

```shell
go run . serve
```

## Docker

```shell
docker build .
```

### ROCm

```shell
docker build --build-arg FLAVOR=rocm .
```

## Running tests

To run tests, use `go test`:

```shell
go test ./...
```

> NOTE: In rare circumstances, you may need to change a package using the new
> "synctest" package in go1.24.
>
> If you do not have the "synctest" package enabled, you will not see build or
> test failures resulting from your change(s), if any, locally, but CI will
> break.
>
> If you see failures in CI, you can either keep pushing changes to see if the
> CI build passes, or you can enable the "synctest" package locally to see the
> failures before pushing.
>
> To enable the "synctest" package for testing, run the following command:
>
> ```shell
> GOEXPERIMENT=synctest go test ./...
> ```
>
> If you wish to enable synctest for all go commands, you can set the
> `GOEXPERIMENT` environment variable in your shell profile or by using:
>
> ```shell
> go env -w GOEXPERIMENT=synctest
> ```
>
> Which will enable the "synctest" package for all go commands without needing
> to set it for all shell sessions.
>
> The synctest package is not required for production builds.

## Library detection

Ollama looks for acceleration libraries in the following paths relative to the `ollama` executable:

* `./lib/ollama` (Windows)
* `../lib/ollama` (Linux)
* `.` (macOS)
* `build/lib/ollama` (for development)

If the libraries are not found, Ollama will not run with any acceleration libraries.


--- docs/docker.md ---
# Ollama Docker image

### CPU only

```shell
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

### Nvidia GPU
Install the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation).

#### Install with Apt
1.  Configure the repository

    ```shell
    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \
        | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
    curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
        | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
        | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
    sudo apt-get update
    ```

2.  Install the NVIDIA Container Toolkit packages

    ```shell
    sudo apt-get install -y nvidia-container-toolkit
    ```

#### Install with Yum or Dnf
1.  Configure the repository

    ```shell
    curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo \
        | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo
    ```

2. Install the NVIDIA Container Toolkit packages

    ```shell
    sudo yum install -y nvidia-container-toolkit
    ```

#### Configure Docker to use Nvidia driver

```shell
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

#### Start the container

```shell
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

> [!NOTE]  
> If you're running on an NVIDIA JetPack system, Ollama can't automatically discover the correct JetPack version. Pass the environment variable JETSON_JETPACK=5 or JETSON_JETPACK=6 to the container to select version 5 or 6.

### AMD GPU

To run Ollama using Docker with AMD GPUs, use the `rocm` tag and the following command:

```shell
docker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm
```

### Run model locally

Now you can run a model:

```shell
docker exec -it ollama ollama run llama3.2
```

### Try different models

More models can be found on the [Ollama library](https://ollama.com/library).


--- docs/faq.md ---
# FAQ

## How can I upgrade Ollama?

Ollama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item and then click "Restart to update" to apply the update. Updates can also be installed by downloading the latest version [manually](https://ollama.com/download/).

On Linux, re-run the install script:

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

## How can I view the logs?

Review the [Troubleshooting](./troubleshooting.md) docs for more about using logs.

## Is my GPU compatible with Ollama?

Please refer to the [GPU docs](./gpu.md).

## How can I specify the context window size?

By default, Ollama uses a context window size of 4096 tokens for most models. The `gpt-oss` model has a default context window size of 8192 tokens.

This can be overridden in Settings in the Windows and macOS App, or with the `OLLAMA_CONTEXT_LENGTH` environment variable. For example, to set the default context window to 8K, use:

```shell
OLLAMA_CONTEXT_LENGTH=8192 ollama serve
```

To change this when using `ollama run`, use `/set parameter`:

```shell
/set parameter num_ctx 4096
```

When using the API, specify the `num_ctx` parameter:

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "options": {
    "num_ctx": 4096
  }
}'
```

Setting the context length higher may cause the model to not be able to fit onto the GPU which make the model run more slowly.

## How can I tell if my model was loaded onto the GPU?

Use the `ollama ps` command to see what models are currently loaded into memory.

```shell
ollama ps
```

> **Output**:
>
> ```
> NAME           ID              SIZE     PROCESSOR    CONTEXT    UNTIL
> gpt-oss:20b    05afbac4bad6    16 GB    100% GPU     8192       4 minutes from now
> ```

The `Processor` column will show which memory the model was loaded in to:
* `100% GPU` means the model was loaded entirely into the GPU
* `100% CPU` means the model was loaded entirely in system memory
* `48%/52% CPU/GPU` means the model was loaded partially onto both the GPU and into system memory

## How do I configure Ollama server?

Ollama server can be configured with environment variables.

### Setting environment variables on Mac

If Ollama is run as a macOS application, environment variables should be set using `launchctl`:

1. For each environment variable, call `launchctl setenv`.

    ```bash
    launchctl setenv OLLAMA_HOST "0.0.0.0:11434"
    ```

2. Restart Ollama application.

### Setting environment variables on Linux

If Ollama is run as a systemd service, environment variables should be set using `systemctl`:

1. Edit the systemd service by calling `systemctl edit ollama.service`. This will open an editor.

2. For each environment variable, add a line `Environment` under section `[Service]`:

    ```ini
    [Service]
    Environment="OLLAMA_HOST=0.0.0.0:11434"
    ```

3. Save and exit.

4. Reload `systemd` and restart Ollama:

   ```shell
   systemctl daemon-reload
   systemctl restart ollama
   ```

### Setting environment variables on Windows

On Windows, Ollama inherits your user and system environment variables.

1. First Quit Ollama by clicking on it in the task bar.

2. Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for _environment variables_.

3. Click on _Edit environment variables for your account_.

4. Edit or create a new variable for your user account for `OLLAMA_HOST`, `OLLAMA_MODELS`, etc.

5. Click OK/Apply to save.

6. Start the Ollama application from the Windows Start menu.

## How do I use Ollama behind a proxy?

Ollama pulls models from the Internet and may require a proxy server to access the models. Use `HTTPS_PROXY` to redirect outbound requests through the proxy. Ensure the proxy certificate is installed as a system certificate. Refer to the section above for how to use environment variables on your platform.

> [!NOTE]
> Avoid setting `HTTP_PROXY`. Ollama does not use HTTP for model pulls, only HTTPS. Setting `HTTP_PROXY` may interrupt client connections to the server.

### How do I use Ollama behind a proxy in Docker?

The Ollama Docker container image can be configured to use a proxy by passing `-e HTTPS_PROXY=https://proxy.example.com` when starting the container.

Alternatively, the Docker daemon can be configured to use a proxy. Instructions are available for Docker Desktop on [macOS](https://docs.docker.com/desktop/settings/mac/#proxies), [Windows](https://docs.docker.com/desktop/settings/windows/#proxies), and [Linux](https://docs.docker.com/desktop/settings/linux/#proxies), and Docker [daemon with systemd](https://docs.docker.com/config/daemon/systemd/#httphttps-proxy).

Ensure the certificate is installed as a system certificate when using HTTPS. This may require a new Docker image when using a self-signed certificate.

```dockerfile
FROM ollama/ollama
COPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt
RUN update-ca-certificates
```

Build and run this image:

```shell
docker build -t ollama-with-ca .
docker run -d -e HTTPS_PROXY=https://my.proxy.example.com -p 11434:11434 ollama-with-ca
```

## Does Ollama send my prompts and responses back to ollama.com?

If you're running a model locally, your prompts and responses will always stay on your machine. Ollama Turbo in the App allows you to run your queries on Ollama's servers if you don't have a powerful enough GPU. Web search lets a model query the web, giving you more accurate and up-to-date information. Both Turbo and web search require sending your prompts and responses to Ollama.com. This data is neither logged nor stored.

If you don't want to see the Turbo and web search options in the app, you can disable them in Settings by turning on Airplane mode. In Airplane mode, all models will run locally, and your prompts and responses will stay on your machine.

## How can I expose Ollama on my network?

Ollama binds 127.0.0.1 port 11434 by default. Change the bind address with the `OLLAMA_HOST` environment variable.

Refer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.

## How can I use Ollama with a proxy server?

Ollama runs an HTTP server and can be exposed using a proxy server such as Nginx. To do so, configure the proxy to forward requests and optionally set required headers (if not exposing Ollama on the network). For example, with Nginx:

```nginx
server {
    listen 80;
    server_name example.com;  # Replace with your domain or IP
    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host localhost:11434;
    }
}
```

## How can I use Ollama with ngrok?

Ollama can be accessed using a range of tools for tunneling tools. For example with Ngrok:

```shell
ngrok http 11434 --host-header="localhost:11434"
```

## How can I use Ollama with Cloudflare Tunnel?

To use Ollama with Cloudflare Tunnel, use the `--url` and `--http-host-header` flags:

```shell
cloudflared tunnel --url http://localhost:11434 --http-host-header="localhost:11434"
```

## How can I allow additional web origins to access Ollama?

Ollama allows cross-origin requests from `127.0.0.1` and `0.0.0.0` by default. Additional origins can be configured with `OLLAMA_ORIGINS`.

For browser extensions, you'll need to explicitly allow the extension's origin pattern. Set `OLLAMA_ORIGINS` to include `chrome-extension://*`, `moz-extension://*`, and `safari-web-extension://*` if you wish to allow all browser extensions access, or specific extensions as needed:

```
# Allow all Chrome, Firefox, and Safari extensions
OLLAMA_ORIGINS=chrome-extension://*,moz-extension://*,safari-web-extension://* ollama serve
```

Refer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.

## Where are models stored?

- macOS: `~/.ollama/models`
- Linux: `/usr/share/ollama/.ollama/models`
- Windows: `C:\Users\%username%\.ollama\models`

### How do I set them to a different location?

If a different directory needs to be used, set the environment variable `OLLAMA_MODELS` to the chosen directory.

> Note: on Linux using the standard installer, the `ollama` user needs read and write access to the specified directory. To assign the directory to the `ollama` user run `sudo chown -R ollama:ollama <directory>`.

Refer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.

## How can I use Ollama in Visual Studio Code?

There is already a large collection of plugins available for VSCode as well as other editors that leverage Ollama. See the list of [extensions & plugins](https://github.com/ollama/ollama#extensions--plugins) at the bottom of the main repository readme.

## How do I use Ollama with GPU acceleration in Docker?

The Ollama Docker container can be configured with GPU acceleration in Linux or Windows (with WSL2). This requires the [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit). See [ollama/ollama](https://hub.docker.com/r/ollama/ollama) for more details.

GPU acceleration is not available for Docker Desktop in macOS due to the lack of GPU passthrough and emulation.

## Why is networking slow in WSL2 on Windows 10?

This can impact both installing Ollama, as well as downloading models.

Open `Control Panel > Networking and Internet > View network status and tasks` and click on `Change adapter settings` on the left panel. Find the `vEthernel (WSL)` adapter, right click and select `Properties`.
Click on `Configure` and open the `Advanced` tab. Search through each of the properties until you find `Large Send Offload Version 2 (IPv4)` and `Large Send Offload Version 2 (IPv6)`. *Disable* both of these
properties.

## How can I preload a model into Ollama to get faster response times?

If you are using the API you can preload a model by sending the Ollama server an empty request. This works with both the `/api/generate` and `/api/chat` API endpoints.

To preload the mistral model using the generate endpoint, use:

```shell
curl http://localhost:11434/api/generate -d '{"model": "mistral"}'
```

To use the chat completions endpoint, use:

```shell
curl http://localhost:11434/api/chat -d '{"model": "mistral"}'
```

To preload a model using the CLI, use the command:

```shell
ollama run llama3.2 ""
```

## How do I keep a model loaded in memory or make it unload immediately?

By default models are kept in memory for 5 minutes before being unloaded. This allows for quicker response times if you're making numerous requests to the LLM. If you want to immediately unload a model from memory, use the `ollama stop` command:

```shell
ollama stop llama3.2
```

If you're using the API, use the `keep_alive` parameter with the `/api/generate` and `/api/chat` endpoints to set the amount of time that a model stays in memory. The `keep_alive` parameter can be set to:
* a duration string (such as "10m" or "24h")
* a number in seconds (such as 3600)
* any negative number which will keep the model loaded in memory (e.g. -1 or "-1m")
* '0' which will unload the model immediately after generating a response

For example, to preload a model and leave it in memory use:

```shell
curl http://localhost:11434/api/generate -d '{"model": "llama3.2", "keep_alive": -1}'
```

To unload the model and free up memory use:

```shell
curl http://localhost:11434/api/generate -d '{"model": "llama3.2", "keep_alive": 0}'
```

Alternatively, you can change the amount of time all models are loaded into memory by setting the `OLLAMA_KEEP_ALIVE` environment variable when starting the Ollama server. The `OLLAMA_KEEP_ALIVE` variable uses the same parameter types as the `keep_alive` parameter types mentioned above. Refer to the section explaining [how to configure the Ollama server](#how-do-i-configure-ollama-server) to correctly set the environment variable.

The `keep_alive` API parameter with the `/api/generate` and `/api/chat` API endpoints will override the `OLLAMA_KEEP_ALIVE` setting.

## How do I manage the maximum number of requests the Ollama server can queue?

If too many requests are sent to the server, it will respond with a 503 error indicating the server is overloaded.  You can adjust how many requests may be queue by setting `OLLAMA_MAX_QUEUE`.

## How does Ollama handle concurrent requests?

Ollama supports two levels of concurrent processing.  If your system has sufficient available memory (system memory when using CPU inference, or VRAM for GPU inference) then multiple models can be loaded at the same time.  For a given model, if there is sufficient available memory when the model is loaded, it can be configured to allow parallel request processing.

If there is insufficient available memory to load a new model request while one or more models are already loaded, all new requests will be queued until the new model can be loaded.  As prior models become idle, one or more will be unloaded to make room for the new model.  Queued requests will be processed in order.  When using GPU inference new models must be able to completely fit in VRAM to allow concurrent model loads.

Parallel request processing for a given model results in increasing the context size by the number of parallel requests.  For example, a 2K context with 4 parallel requests will result in an 8K context and additional memory allocation.

The following server settings may be used to adjust how Ollama handles concurrent requests on most platforms:

- `OLLAMA_MAX_LOADED_MODELS` - The maximum number of models that can be loaded concurrently provided they fit in available memory.  The default is 3 * the number of GPUs or 3 for CPU inference.
- `OLLAMA_NUM_PARALLEL` - The maximum number of parallel requests each model will process at the same time.  The default is 1, and will handle 1 request per model at a time.
- `OLLAMA_MAX_QUEUE` - The maximum number of requests Ollama will queue when busy before rejecting additional requests. The default is 512

Note: Windows with Radeon GPUs currently default to 1 model maximum due to limitations in ROCm v5.7 for available VRAM reporting.  Once ROCm v6.2 is available, Windows Radeon will follow the defaults above.  You may enable concurrent model loads on Radeon on Windows, but ensure you don't load more models than will fit into your GPUs VRAM.

## How does Ollama load models on multiple GPUs?

When loading a new model, Ollama evaluates the required VRAM for the model against what is currently available.  If the model will entirely fit on any single GPU, Ollama will load the model on that GPU.  This typically provides the best performance as it reduces the amount of data transferring across the PCI bus during inference.  If the model does not fit entirely on one GPU, then it will be spread across all the available GPUs.

## How can I enable Flash Attention?

Flash Attention is a feature of most modern models that can significantly reduce memory usage as the context size grows.  To enable Flash Attention, set the `OLLAMA_FLASH_ATTENTION` environment variable to `1` when starting the Ollama server.

## How can I set the quantization type for the K/V cache?

The K/V context cache can be quantized to significantly reduce memory usage when Flash Attention is enabled.

To use quantized K/V cache with Ollama you can set the following environment variable:

- `OLLAMA_KV_CACHE_TYPE` - The quantization type for the K/V cache.  Default is `f16`.

> Note: Currently this is a global option - meaning all models will run with the specified quantization type.

The currently available K/V cache quantization types are:

- `f16` - high precision and memory usage (default).
- `q8_0` - 8-bit quantization, uses approximately 1/2 the memory of `f16` with a very small loss in precision, this usually has no noticeable impact on the model's quality (recommended if not using f16).
- `q4_0` - 4-bit quantization, uses approximately 1/4 the memory of `f16` with a small-medium loss in precision that may be more noticeable at higher context sizes.

How much the cache quantization impacts the model's response quality will depend on the model and the task.  Models that have a high GQA count (e.g. Qwen2) may see a larger impact on precision from quantization than models with a low GQA count.

You may need to experiment with different quantization types to find the best balance between memory usage and quality.

## How can I stop Ollama from starting when I login to my computer

Ollama for Windows and macOS register as a login item during installation.  You can disable this if you prefer not to have Ollama automatically start.  Ollama will respect this setting across upgrades, unless you uninstall the application.

**Windows**
- Remove `%APPDATA%\Microsoft\Windows\Start Menu\Programs\Startup\Ollama.lnk`

**MacOS Monterey (v12)**
- Open `Settings` -> `Users & Groups` -> `Login Items` and find the `Ollama` entry, then click the `-` (minus) to remove

**MacOS Ventura (v13) and later**
- Open `Settings` and search for "Login Items", find the `Ollama` entry under "Allow in the Background`, then click the slider to disable.


--- docs/gpu.md ---
# GPU
## Nvidia
Ollama supports Nvidia GPUs with compute capability 5.0+ and driver version 531 and newer.

Check your compute compatibility to see if your card is supported:
[https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)

| Compute Capability | Family              | Cards                                                                                                       |
| ------------------ | ------------------- | ----------------------------------------------------------------------------------------------------------- |
| 12.0               | GeForce RTX 50xx    | `RTX 5060` `RTX 5060 Ti` `RTX 5070` `RTX 5070 Ti` `RTX 5080` `RTX 5090`                                     |
|                    | NVIDIA Professioal  | `RTX PRO 4000 Blackwell` `RTX PRO 4500 Blackwell` `RTX PRO 5000 Blackwell` `RTX PRO 6000 Blackwell`         |
| 11.0               | Jetson              | `T4000` `T5000` (Requires driver 580 or newer)                                                              |
| 10.3               | NVIDIA Professioal  | `B300` `GB300` (Requires driver 580 or newer)                                                               |
| 10.0               | NVIDIA Professioal  | `B200` `GB200` (Requires driver 580 or newer)                                                               |
| 9.0                | NVIDIA              | `H200` `H100` `GH200`                                                                                       |
| 8.9                | GeForce RTX 40xx    | `RTX 4090` `RTX 4080 SUPER` `RTX 4080` `RTX 4070 Ti SUPER` `RTX 4070 Ti` `RTX 4070 SUPER` `RTX 4070` `RTX 4060 Ti` `RTX 4060`  |
|                    | NVIDIA Professional | `L4` `L40` `RTX 6000`                                                                                       |
| 8.7                | Jetson              | `Orin Nano` `Orin NX` `AGX Orin`                                                                            |
| 8.6                | GeForce RTX 30xx    | `RTX 3090 Ti` `RTX 3090` `RTX 3080 Ti` `RTX 3080` `RTX 3070 Ti` `RTX 3070` `RTX 3060 Ti` `RTX 3060` `RTX 3050 Ti` `RTX 3050`   |
|                    | NVIDIA Professional | `A40` `RTX A6000` `RTX A5000` `RTX A4000` `RTX A3000` `RTX A2000` `A10` `A16` `A2`                          |
| 8.0                | NVIDIA              | `A100` `A30`                                                                                                |
| 7.5                | GeForce GTX/RTX     | `GTX 1650 Ti` `TITAN RTX` `RTX 2080 Ti` `RTX 2080` `RTX 2070` `RTX 2060`                                    |
|                    | NVIDIA Professional | `T4` `RTX 5000` `RTX 4000` `RTX 3000` `T2000` `T1200` `T1000` `T600` `T500`                                 |
|                    | Quadro              | `RTX 8000` `RTX 6000` `RTX 5000` `RTX 4000`                                                                 |
| 7.2                | Jetson              | `Xavier NX` `AGX Xavier` (Jetpack 5)                                                                        |
| 7.0                | NVIDIA              | `TITAN V` `V100` `Quadro GV100`                                                                             |
| 6.1                | NVIDIA TITAN        | `TITAN Xp` `TITAN X`                                                                                        |
|                    | GeForce GTX         | `GTX 1080 Ti` `GTX 1080` `GTX 1070 Ti` `GTX 1070` `GTX 1060` `GTX 1050 Ti` `GTX 1050`                       |
|                    | Quadro              | `P6000` `P5200` `P4200` `P3200` `P5000` `P4000` `P3000` `P2200` `P2000` `P1000` `P620` `P600` `P500` `P520` |
|                    | Tesla               | `P40` `P4`                                                                                                  |
| 6.0                | NVIDIA              | `Tesla P100` `Quadro GP100`                                                                                 |
| 5.2                | GeForce GTX         | `GTX TITAN X` `GTX 980 Ti` `GTX 980` `GTX 970` `GTX 960` `GTX 950`                                          |
|                    | Quadro              | `M6000 24GB` `M6000` `M5000` `M5500M` `M4000` `M2200` `M2000` `M620`                                        |
|                    | Tesla               | `M60` `M40`                                                                                                 |
| 5.0                | GeForce GTX         | `GTX 750 Ti` `GTX 750` `NVS 810`                                                                            |
|                    | Quadro              | `K2200` `K1200` `K620` `M1200` `M520` `M5000M` `M4000M` `M3000M` `M2000M` `M1000M` `K620M` `M600M` `M500M`  |

For building locally to support older GPUs, see [developer.md](./development.md#linux-cuda-nvidia)

### GPU Selection

If you have multiple NVIDIA GPUs in your system and want to limit Ollama to use
a subset, you can set `CUDA_VISIBLE_DEVICES` to a comma separated list of GPUs.
Numeric IDs may be used, however ordering may vary, so UUIDs are more reliable.
You can discover the UUID of your GPUs by running `nvidia-smi -L` If you want to
ignore the GPUs and force CPU usage, use an invalid GPU ID (e.g., "-1")

### Linux Suspend Resume

On linux, after a suspend/resume cycle, sometimes Ollama will fail to discover
your NVIDIA GPU, and fallback to running on the CPU.  You can workaround this
driver bug by reloading the NVIDIA UVM driver with `sudo rmmod nvidia_uvm &&
sudo modprobe nvidia_uvm`

## AMD Radeon
Ollama supports the following AMD GPUs:

### Linux Support
| Family         | Cards and accelerators                                                                                               |
| -------------- | -------------------------------------------------------------------------------------------------------------------- |
| AMD Radeon RX  | `7900 XTX` `7900 XT` `7900 GRE` `7800 XT` `7700 XT` `7600 XT` `7600` `6950 XT` `6900 XTX` `6900XT` `6800 XT` `6800`  |
| AMD Radeon PRO | `W7900` `W7800` `W7700` `W7600` `W7500` `W6900X` `W6800X Duo` `W6800X` `W6800` `V620` `V420` `V340` `V320`           |
| AMD Instinct   | `MI300X` `MI300A` `MI300` `MI250X` `MI250` `MI210` `MI200` `MI100`                                                   |

### Windows Support
With ROCm v6.2, the following GPUs are supported on Windows.

| Family         | Cards and accelerators                                                                                                               |
| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| AMD Radeon RX  | `7900 XTX` `7900 XT` `7900 GRE` `7800 XT` `7700 XT` `7600 XT` `7600` `6950 XT` `6900 XTX` `6900XT` `6800 XT` `6800`    |
| AMD Radeon PRO | `W7900` `W7800` `W7700` `W7600` `W7500` `W6900X` `W6800X Duo` `W6800X` `W6800` `V620` |

### Known Workarounds

- The RX Vega 56 requires `HSA_ENABLE_SDMA=0` to disable SDMA

### Overrides on Linux
Ollama leverages the AMD ROCm library, which does not support all AMD GPUs. In
some cases you can force the system to try to use a similar LLVM target that is
close.  For example The Radeon RX 5400 is `gfx1034` (also known as 10.3.4)
however, ROCm does not currently support this target. The closest support is
`gfx1030`.  You can use the environment variable `HSA_OVERRIDE_GFX_VERSION` with
`x.y.z` syntax.  So for example, to force the system to run on the RX 5400, you
would set `HSA_OVERRIDE_GFX_VERSION="10.3.0"` as an environment variable for the
server.  If you have an unsupported AMD GPU you can experiment using the list of
supported types below.

If you have multiple GPUs with different GFX versions, append the numeric device
number to the environment variable to set them individually.  For example,
`HSA_OVERRIDE_GFX_VERSION_0=10.3.0` and  `HSA_OVERRIDE_GFX_VERSION_1=11.0.0`

At this time, the known supported GPU types on linux are the following LLVM Targets.
This table shows some example GPUs that map to these LLVM targets:
| **LLVM Target** | **An Example GPU** |
|-----------------|---------------------|
| gfx908 | Radeon Instinct MI100 |
| gfx90a | Radeon Instinct MI210 |
| gfx940 | Radeon Instinct MI300 |
| gfx941 | |
| gfx942 | |
| gfx1030 | Radeon PRO V620 |
| gfx1100 | Radeon PRO W7900 |
| gfx1101 | Radeon PRO W7700 |
| gfx1102 | Radeon RX 7600 |

AMD is working on enhancing ROCm v6 to broaden support for families of GPUs in a
future release which should increase support for more GPUs.

Reach out on [Discord](https://discord.gg/ollama) or file an
[issue](https://github.com/ollama/ollama/issues) for additional help.

### GPU Selection

If you have multiple AMD GPUs in your system and want to limit Ollama to use a
subset, you can set `ROCR_VISIBLE_DEVICES` to a comma separated list of GPUs.
You can see the list of devices with `rocminfo`.  If you want to ignore the GPUs
and force CPU usage, use an invalid GPU ID (e.g., "-1").  When available, use the
`Uuid` to uniquely identify the device instead of numeric value.

### Container Permission

In some Linux distributions, SELinux can prevent containers from
accessing the AMD GPU devices.  On the host system you can run 
`sudo setsebool container_use_devices=1` to allow containers to use devices.

### Metal (Apple GPUs)
Ollama supports GPU acceleration on Apple devices via the Metal API.


--- docs/import.md ---
# Importing a model

## Table of Contents

  * [Importing a Safetensors adapter](#Importing-a-fine-tuned-adapter-from-Safetensors-weights)
  * [Importing a Safetensors model](#Importing-a-model-from-Safetensors-weights)
  * [Importing a GGUF file](#Importing-a-GGUF-based-model-or-adapter)
  * [Sharing models on ollama.com](#Sharing-your-model-on-ollamacom)

## Importing a fine tuned adapter from Safetensors weights

First, create a `Modelfile` with a `FROM` command pointing at the base model you used for fine tuning, and an `ADAPTER` command which points to the directory with your Safetensors adapter:

```dockerfile
FROM <base model name>
ADAPTER /path/to/safetensors/adapter/directory
```

Make sure that you use the same base model in the `FROM` command as you used to create the adapter otherwise you will get erratic results. Most frameworks use different quantization methods, so it's best to use non-quantized (i.e. non-QLoRA) adapters. If your adapter is in the same directory as your `Modelfile`, use `ADAPTER .` to specify the adapter path.

Now run `ollama create` from the directory where the `Modelfile` was created:

```shell
ollama create my-model
```

Lastly, test the model:

```shell
ollama run my-model
```

Ollama supports importing adapters based on several different model architectures including:

  * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);
  * Mistral (including Mistral 1, Mistral 2, and Mixtral); and
  * Gemma (including Gemma 1 and Gemma 2)

You can create the adapter using a fine tuning framework or tool which can output adapters in the Safetensors format, such as:

  * Hugging Face [fine tuning framework](https://huggingface.co/docs/transformers/en/training)
  * [Unsloth](https://github.com/unslothai/unsloth)
  * [MLX](https://github.com/ml-explore/mlx)


## Importing a model from Safetensors weights

First, create a `Modelfile` with a `FROM` command which points to the directory containing your Safetensors weights:

```dockerfile
FROM /path/to/safetensors/directory
```

If you create the Modelfile in the same directory as the weights, you can use the command `FROM .`.

If you do not create the Modelfile, ollama will act as if there was a Modelfile with the command `FROM .`.

Now run the `ollama create` command from the directory where you created the `Modelfile`:

```shell
ollama create my-model
```

Lastly, test the model:

```shell
ollama run my-model
```

Ollama supports importing models for several different architectures including:

  * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);
  * Mistral (including Mistral 1, Mistral 2, and Mixtral);
  * Gemma (including Gemma 1 and Gemma 2); and
  * Phi3

This includes importing foundation models as well as any fine tuned models which have been _fused_ with a foundation model.
## Importing a GGUF based model or adapter

If you have a GGUF based model or adapter it is possible to import it into Ollama. You can obtain a GGUF model or adapter by:

  * converting a Safetensors model with the `convert_hf_to_gguf.py` from Llama.cpp; 
  * converting a Safetensors adapter with the `convert_lora_to_gguf.py` from Llama.cpp; or
  * downloading a model or adapter from a place such as HuggingFace

To import a GGUF model, create a `Modelfile` containing:

```dockerfile
FROM /path/to/file.gguf
```

For a GGUF adapter, create the `Modelfile` with:

```dockerfile
FROM <model name>
ADAPTER /path/to/file.gguf
```

When importing a GGUF adapter, it's important to use the same base model as the base model that the adapter was created with. You can use:

 * a model from Ollama
 * a GGUF file
 * a Safetensors based model 

Once you have created your `Modelfile`, use the `ollama create` command to build the model.

```shell
ollama create my-model
```

## Quantizing a Model

Quantizing a model allows you to run models faster and with less memory consumption but at reduced accuracy. This allows you to run a model on more modest hardware.

Ollama can quantize FP16 and FP32 based models into different quantization levels using the `-q/--quantize` flag with the `ollama create` command.

First, create a Modelfile with the FP16 or FP32 based model you wish to quantize.

```dockerfile
FROM /path/to/my/gemma/f16/model
```

Use `ollama create` to then create the quantized model.

```shell
$ ollama create --quantize q4_K_M mymodel
transferring model data
quantizing F16 model to Q4_K_M
creating new layer sha256:735e246cc1abfd06e9cdcf95504d6789a6cd1ad7577108a70d9902fef503c1bd
creating new layer sha256:0853f0ad24e5865173bbf9ffcc7b0f5d56b66fd690ab1009867e45e7d2c4db0f
writing manifest
success
```

### Supported Quantizations

- `q8_0`

#### K-means Quantizations

- `q4_K_S`
- `q4_K_M`


## Sharing your model on ollama.com

You can share any model you have created by pushing it to [ollama.com](https://ollama.com) so that other users can try it out.

First, use your browser to go to the [Ollama Sign-Up](https://ollama.com/signup) page. If you already have an account, you can skip this step.

<img src="images/signup.png" alt="Sign-Up" width="40%">

The `Username` field will be used as part of your model's name (e.g. `jmorganca/mymodel`), so make sure you are comfortable with the username that you have selected.

Now that you have created an account and are signed-in, go to the [Ollama Keys Settings](https://ollama.com/settings/keys) page.

Follow the directions on the page to determine where your Ollama Public Key is located.

<img src="images/ollama-keys.png" alt="Ollama Keys" width="80%">

Click on the `Add Ollama Public Key` button, and copy and paste the contents of your Ollama Public Key into the text field.

To push a model to [ollama.com](https://ollama.com), first make sure that it is named correctly with your username. You may have to use the `ollama cp` command to copy
your model to give it the correct name. Once you're happy with your model's name, use the `ollama push` command to push it to [ollama.com](https://ollama.com).

```shell
ollama cp mymodel myuser/mymodel
ollama push myuser/mymodel
```

Once your model has been pushed, other users can pull and run it by using the command:

```shell
ollama run myuser/mymodel
```



--- api/examples/README.md ---
# Ollama API Examples

Run the examples in this directory with:

```shell
go run example_name/main.go
```

## Chat - Chat with a model
- [chat/main.go](chat/main.go)

## Generate - Generate text from a model
- [generate/main.go](generate/main.go)
- [generate-streaming/main.go](generate-streaming/main.go)

## Pull - Pull a model
- [pull-progress/main.go](pull-progress/main.go)



--- CONTRIBUTING.md ---
# Contributing to Ollama

Thank you for your interest in contributing to Ollama! Here are a few guidelines to help get you started.

## Set up

See the [development documentation](./docs/development.md) for instructions on how to build and run Ollama locally.

### Ideal issues

* [Bugs](https://github.com/ollama/ollama/issues?q=is%3Aissue+is%3Aopen+label%3Abug): issues where Ollama stops working or where it results in an unexpected error.
* [Performance](https://github.com/ollama/ollama/issues?q=is%3Aissue+is%3Aopen+label%3Aperformance): issues to make Ollama faster at model inference, downloading or uploading.
* [Security](https://github.com/ollama/ollama/blob/main/SECURITY.md): issues that could lead to a security vulnerability. As mentioned in [SECURITY.md](https://github.com/ollama/ollama/blob/main/SECURITY.md), please do not disclose security vulnerabilities publicly.

### Issues that are harder to review

* New features: new features (e.g. API fields, environment variables) add surface area to Ollama and make it harder to maintain in the long run as they cannot be removed without potentially breaking users in the future.
* Refactoring: large code improvements are important, but can be harder or take longer to review and merge.
* Documentation: small updates to fill in or correct missing documentation is helpful, however large documentation additions can be hard to maintain over time.

### Issues that may not be accepted

* Changes that break backwards compatibility in Ollama's API (including the OpenAI-compatible API)
* Changes that add significant friction to the user experience
* Changes that create a large future maintenance burden for maintainers and contributors

## Proposing a (non-trivial) change

> By "non-trivial", we mean a change that is not a bug fix or small
> documentation update. If you are unsure, please ask us on our [Discord
> server](https://discord.gg/ollama).

Before opening a non-trivial Pull Request, please open an issue to discuss the change and
get feedback from the maintainers. This helps us understand the context of the
change and how it fits into Ollama's roadmap and prevents us from duplicating
work or you from spending time on a change that we may not be able to accept.

Tips for proposals:

* Explain the problem you are trying to solve, not what you are trying to do.
* Explain why the change is important.
* Explain how the change will be used.
* Explain how the change will be tested.

Additionally, for bonus points: Provide draft documentation you would expect to
see if the change were accepted.

## Pull requests

**Commit messages**

The title should look like:

    <package>: <short description>

The package is the most affected Go package. If the change does not affect Go
code, then use the directory name instead. Changes to a single well-known
file in the root directory may use the file name.

The short description should start with a lowercase letter and be a
continuation of the sentence:

      "This changes Ollama to..."

Examples:

      llm/backend/mlx: support the llama architecture
      CONTRIBUTING: provide clarity on good commit messages, and bad
      docs: simplify manual installation with shorter curl commands

Bad Examples:

      feat: add more emoji
      fix: was not using famous web framework
      chore: generify code

**Tests**

Please include tests. Strive to test behavior, not implementation.

**New dependencies**

Dependencies should be added sparingly. If you are adding a new dependency,
please explain why it is necessary and what other ways you attempted that
did not work without it.

## Need help?

If you need help with anything, feel free to reach out to us on our [Discord server](https://discord.gg/ollama).


--- SECURITY.md ---
# Security

The Ollama maintainer team takes security seriously and will actively work to resolve security issues.

## Reporting a vulnerability

If you discover a security vulnerability, please do not open a public issue. Instead, please report it by emailing hello@ollama.com. We ask that you give us sufficient time to investigate and address the vulnerability before disclosing it publicly.

Please include the following details in your report:
- A description of the vulnerability
- Steps to reproduce the issue
- Your assessment of the potential impact
- Any possible mitigations

## Security best practices

While the maintainer team does their best to secure Ollama, users are encouraged to implement their own security best practices, such as:

- Regularly updating to the latest version of Ollama
- Securing access to hosted instances of Ollama
- Monitoring systems for unusual activity

## Contact

For any other questions or concerns related to security, please contact us at hello@ollama.com


--- CMakeLists.txt ---
cmake_minimum_required(VERSION 3.21)

project(Ollama C CXX)

include(CheckLanguage)
include(GNUInstallDirs)

find_package(Threads REQUIRED)

set(CMAKE_BUILD_TYPE Release)
set(BUILD_SHARED_LIBS ON)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
set(CMAKE_CXX_EXTENSIONS OFF)

set(GGML_BUILD ON)
set(GGML_SHARED ON)
set(GGML_CCACHE ON)
set(GGML_BACKEND_DL ON)
set(GGML_BACKEND_SHARED ON)
set(GGML_SCHED_MAX_COPIES 4)

set(GGML_LLAMAFILE ON)
set(GGML_CUDA_PEER_MAX_BATCH_SIZE 128)
set(GGML_CUDA_GRAPHS ON)
set(GGML_CUDA_FA ON)
set(GGML_CUDA_COMPRESSION_MODE default)

if((CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_OSX_ARCHITECTURES MATCHES "arm64")
    OR (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_SYSTEM_PROCESSOR MATCHES "arm|aarch64|ARM64|ARMv[0-9]+"))
    set(GGML_CPU_ALL_VARIANTS ON)
endif()

if (CMAKE_OSX_ARCHITECTURES MATCHES "x86_64")
    set(CMAKE_BUILD_RPATH "@loader_path")
    set(CMAKE_INSTALL_RPATH "@loader_path")
endif()

set(OLLAMA_BUILD_DIR ${CMAKE_BINARY_DIR}/lib/ollama)
set(OLLAMA_INSTALL_DIR ${CMAKE_INSTALL_PREFIX}/lib/ollama/${OLLAMA_RUNNER_DIR})

set(CMAKE_RUNTIME_OUTPUT_DIRECTORY         ${OLLAMA_BUILD_DIR})
set(CMAKE_RUNTIME_OUTPUT_DIRECTORY_DEBUG   ${OLLAMA_BUILD_DIR})
set(CMAKE_RUNTIME_OUTPUT_DIRECTORY_RELEASE ${OLLAMA_BUILD_DIR})
set(CMAKE_LIBRARY_OUTPUT_DIRECTORY         ${OLLAMA_BUILD_DIR})
set(CMAKE_LIBRARY_OUTPUT_DIRECTORY_DEBUG   ${OLLAMA_BUILD_DIR})
set(CMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE ${OLLAMA_BUILD_DIR})

include_directories(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src)
include_directories(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/include)
include_directories(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/ggml-cpu)
include_directories(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/ggml-cpu/amx)

add_compile_definitions(NDEBUG GGML_VERSION=0x0 GGML_COMMIT=0x0)

set(GGML_CPU ON)
add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src)
set_property(TARGET ggml PROPERTY EXCLUDE_FROM_ALL TRUE)

get_target_property(CPU_VARIANTS ggml-cpu MANUALLY_ADDED_DEPENDENCIES)
if(NOT CPU_VARIANTS)
    set(CPU_VARIANTS "ggml-cpu")
endif()

install(TARGETS ggml-base ${CPU_VARIANTS}
    RUNTIME_DEPENDENCIES
        PRE_EXCLUDE_REGEXES ".*"
    RUNTIME DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT CPU
    LIBRARY DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT CPU
    FRAMEWORK DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT CPU
)

check_language(CUDA)
if(CMAKE_CUDA_COMPILER)
    if(CMAKE_VERSION VERSION_GREATER_EQUAL "3.24" AND NOT CMAKE_CUDA_ARCHITECTURES)
        set(CMAKE_CUDA_ARCHITECTURES "native")
    endif()

    find_package(CUDAToolkit)
    add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/ggml-cuda)
    install(TARGETS ggml-cuda
        RUNTIME_DEPENDENCIES
            DIRECTORIES ${CUDAToolkit_BIN_DIR} ${CUDAToolkit_BIN_DIR}/x64 ${CUDAToolkit_LIBRARY_DIR}
            PRE_INCLUDE_REGEXES cublas cublasLt cudart
            PRE_EXCLUDE_REGEXES ".*"
        RUNTIME DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT CUDA
        LIBRARY DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT CUDA
    )
endif()

set(WINDOWS_AMDGPU_TARGETS_EXCLUDE_REGEX "^gfx(908|90a|1200|1201):xnack[+-]$"
    CACHE STRING
    "Regular expression describing AMDGPU_TARGETS not supported on Windows. Override to force building these targets. Default \"^gfx(908|90a|1200|1201):xnack[+-]$\"."
)

check_language(HIP)
if(CMAKE_HIP_COMPILER)
    set(HIP_PLATFORM "amd")

    if(NOT AMDGPU_TARGETS)
        find_package(hip REQUIRED)
        list(FILTER AMDGPU_TARGETS INCLUDE REGEX "^gfx(94[012]|101[02]|1030|110[012]|120[01])$")
    endif()

    if(WIN32 AND WINDOWS_AMDGPU_TARGETS_EXCLUDE_REGEX)
        list(FILTER AMDGPU_TARGETS EXCLUDE REGEX ${WINDOWS_AMDGPU_TARGETS_EXCLUDE_REGEX})
    endif()

    if(AMDGPU_TARGETS)
        find_package(hip REQUIRED)
        add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/ggml-hip)

        if (WIN32)
            target_compile_definitions(ggml-hip PRIVATE GGML_CUDA_NO_PEER_COPY)
        endif()

        target_compile_definitions(ggml-hip PRIVATE GGML_HIP_NO_VMM)

        install(TARGETS ggml-hip
            RUNTIME_DEPENDENCY_SET rocm
            RUNTIME DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT HIP
            LIBRARY DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT HIP
        )
        install(RUNTIME_DEPENDENCY_SET rocm
                DIRECTORIES ${HIP_BIN_INSTALL_DIR} ${HIP_LIB_INSTALL_DIR}
                PRE_INCLUDE_REGEXES hipblas rocblas amdhip64 rocsolver amd_comgr hsa-runtime64 rocsparse tinfo rocprofiler-register drm drm_amdgpu numa elf
                PRE_EXCLUDE_REGEXES ".*"
                POST_EXCLUDE_REGEXES "system32"
            RUNTIME DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT HIP
            LIBRARY DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT HIP
        )

        foreach(HIP_LIB_BIN_INSTALL_DIR IN ITEMS ${HIP_BIN_INSTALL_DIR} ${HIP_LIB_INSTALL_DIR})
            if(EXISTS ${HIP_LIB_BIN_INSTALL_DIR}/rocblas)
                install(DIRECTORY ${HIP_LIB_BIN_INSTALL_DIR}/rocblas DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT HIP)
                break()
            endif()
        endforeach()
    endif()
endif()

find_package(Vulkan)
if(Vulkan_FOUND)
    add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/ml/backend/ggml/ggml/src/ggml-vulkan)
    install(TARGETS ggml-vulkan
        RUNTIME_DEPENDENCIES
            PRE_INCLUDE_REGEXES vulkan
            PRE_EXCLUDE_REGEXES ".*"
        RUNTIME DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT Vulkan
        LIBRARY DESTINATION ${OLLAMA_INSTALL_DIR} COMPONENT Vulkan
    )
endif()


--- README.md ---
<div align="center">
  <a href="https://ollama.com">
    <img alt="ollama" width="240" src="https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7">
  </a>
</div>

# Ollama

Get up and running with large language models.

### macOS

[Download](https://ollama.com/download/Ollama.dmg)

### Windows

[Download](https://ollama.com/download/OllamaSetup.exe)

### Linux

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

[Manual install instructions](https://github.com/ollama/ollama/blob/main/docs/linux.md)

### Docker

The official [Ollama Docker image](https://hub.docker.com/r/ollama/ollama) `ollama/ollama` is available on Docker Hub.

### Libraries

- [ollama-python](https://github.com/ollama/ollama-python)
- [ollama-js](https://github.com/ollama/ollama-js)

### Community

- [Discord](https://discord.gg/ollama)
- [Reddit](https://reddit.com/r/ollama)

## Quickstart

To run and chat with [Gemma 3](https://ollama.com/library/gemma3):

```shell
ollama run gemma3
```

## Model library

Ollama supports a list of models available on [ollama.com/library](https://ollama.com/library 'ollama model library')

Here are some example models that can be downloaded:

| Model              | Parameters | Size  | Download                         |
| ------------------ | ---------- | ----- | -------------------------------- |
| Gemma 3            | 1B         | 815MB | `ollama run gemma3:1b`           |
| Gemma 3            | 4B         | 3.3GB | `ollama run gemma3`              |
| Gemma 3            | 12B        | 8.1GB | `ollama run gemma3:12b`          |
| Gemma 3            | 27B        | 17GB  | `ollama run gemma3:27b`          |
| QwQ                | 32B        | 20GB  | `ollama run qwq`                 |
| DeepSeek-R1        | 7B         | 4.7GB | `ollama run deepseek-r1`         |
| DeepSeek-R1        | 671B       | 404GB | `ollama run deepseek-r1:671b`    |
| Llama 4            | 109B       | 67GB  | `ollama run llama4:scout`        |
| Llama 4            | 400B       | 245GB | `ollama run llama4:maverick`     |
| Llama 3.3          | 70B        | 43GB  | `ollama run llama3.3`            |
| Llama 3.2          | 3B         | 2.0GB | `ollama run llama3.2`            |
| Llama 3.2          | 1B         | 1.3GB | `ollama run llama3.2:1b`         |
| Llama 3.2 Vision   | 11B        | 7.9GB | `ollama run llama3.2-vision`     |
| Llama 3.2 Vision   | 90B        | 55GB  | `ollama run llama3.2-vision:90b` |
| Llama 3.1          | 8B         | 4.7GB | `ollama run llama3.1`            |
| Llama 3.1          | 405B       | 231GB | `ollama run llama3.1:405b`       |
| Phi 4              | 14B        | 9.1GB | `ollama run phi4`                |
| Phi 4 Mini         | 3.8B       | 2.5GB | `ollama run phi4-mini`           |
| Mistral            | 7B         | 4.1GB | `ollama run mistral`             |
| Moondream 2        | 1.4B       | 829MB | `ollama run moondream`           |
| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`         |
| Starling           | 7B         | 4.1GB | `ollama run starling-lm`         |
| Code Llama         | 7B         | 3.8GB | `ollama run codellama`           |
| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored`   |
| LLaVA              | 7B         | 4.5GB | `ollama run llava`               |
| Granite-3.3         | 8B         | 4.9GB | `ollama run granite3.3`          |

> [!NOTE]
> You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.

## Customize a model

### Import from GGUF

Ollama supports importing GGUF models in the Modelfile:

1. Create a file named `Modelfile`, with a `FROM` instruction with the local filepath to the model you want to import.

   ```
   FROM ./vicuna-33b.Q4_0.gguf
   ```

2. Create the model in Ollama

   ```shell
   ollama create example -f Modelfile
   ```

3. Run the model

   ```shell
   ollama run example
   ```

### Import from Safetensors

See the [guide](docs/import.md) on importing models for more information.

### Customize a prompt

Models from the Ollama library can be customized with a prompt. For example, to customize the `llama3.2` model:

```shell
ollama pull llama3.2
```

Create a `Modelfile`:

```
FROM llama3.2

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM """
You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.
"""
```

Next, create and run the model:

```
ollama create mario -f ./Modelfile
ollama run mario
>>> hi
Hello! It's your friend Mario.
```

For more information on working with a Modelfile, see the [Modelfile](docs/modelfile.md) documentation.

## CLI Reference

### Create a model

`ollama create` is used to create a model from a Modelfile.

```shell
ollama create mymodel -f ./Modelfile
```

### Pull a model

```shell
ollama pull llama3.2
```

> This command can also be used to update a local model. Only the diff will be pulled.

### Remove a model

```shell
ollama rm llama3.2
```

### Copy a model

```shell
ollama cp llama3.2 my-model
```

### Multiline input

For multiline input, you can wrap text with `"""`:

```
>>> """Hello,
... world!
... """
I'm a basic program that prints the famous "Hello, world!" message to the console.
```

### Multimodal models

```
ollama run llava "What's in this image? /Users/jmorgan/Desktop/smile.png"
```

> **Output**: The image features a yellow smiley face, which is likely the central focus of the picture.

### Pass the prompt as an argument

```shell
ollama run llama3.2 "Summarize this file: $(cat README.md)"
```

> **Output**: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.

### Show model information

```shell
ollama show llama3.2
```

### List models on your computer

```shell
ollama list
```

### List which models are currently loaded

```shell
ollama ps
```

### Stop a model which is currently running

```shell
ollama stop llama3.2
```

### Start Ollama

`ollama serve` is used when you want to start ollama without running the desktop application.

## Building

See the [developer guide](https://github.com/ollama/ollama/blob/main/docs/development.md)

### Running local builds

Next, start the server:

```shell
./ollama serve
```

Finally, in a separate shell, run a model:

```shell
./ollama run llama3.2
```

## REST API

Ollama has a REST API for running and managing models.

### Generate a response

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt":"Why is the sky blue?"
}'
```

### Chat with a model

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'
```

See the [API documentation](./docs/api.md) for all endpoints.

## Community Integrations

### Web & Desktop

- [Open WebUI](https://github.com/open-webui/open-webui)
- [SwiftChat (macOS with ReactNative)](https://github.com/aws-samples/swift-chat)
- [Enchanted (macOS native)](https://github.com/AugustDev/enchanted)
- [Hollama](https://github.com/fmaclen/hollama)
- [Lollms-Webui](https://github.com/ParisNeo/lollms-webui)
- [LibreChat](https://github.com/danny-avila/LibreChat)
- [Bionic GPT](https://github.com/bionic-gpt/bionic-gpt)
- [HTML UI](https://github.com/rtcfirefly/ollama-ui)
- [Saddle](https://github.com/jikkuatwork/saddle)
- [TagSpaces](https://www.tagspaces.org) (A platform for file-based apps, [utilizing Ollama](https://docs.tagspaces.org/ai/) for the generation of tags and descriptions)
- [Chatbot UI](https://github.com/ivanfioravanti/chatbot-ollama)
- [Chatbot UI v2](https://github.com/mckaywrigley/chatbot-ui)
- [Typescript UI](https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file)
- [Minimalistic React UI for Ollama Models](https://github.com/richawo/minimal-llm-ui)
- [Ollamac](https://github.com/kevinhermawan/Ollamac)
- [big-AGI](https://github.com/enricoros/big-AGI)
- [Cheshire Cat assistant framework](https://github.com/cheshire-cat-ai/core)
- [Amica](https://github.com/semperai/amica)
- [chatd](https://github.com/BruceMacD/chatd)
- [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI)
- [Dify.AI](https://github.com/langgenius/dify)
- [MindMac](https://mindmac.app)
- [NextJS Web Interface for Ollama](https://github.com/jakobhoeg/nextjs-ollama-llm-ui)
- [Msty](https://msty.app)
- [Chatbox](https://github.com/Bin-Huang/Chatbox)
- [WinForm Ollama Copilot](https://github.com/tgraupmann/WinForm_Ollama_Copilot)
- [NextChat](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web) with [Get Started Doc](https://docs.nextchat.dev/models/ollama)
- [Alpaca WebUI](https://github.com/mmo80/alpaca-webui)
- [OllamaGUI](https://github.com/enoch1118/ollamaGUI)
- [OpenAOE](https://github.com/InternLM/OpenAOE)
- [Odin Runes](https://github.com/leonid20000/OdinRunes)
- [LLM-X](https://github.com/mrdjohnson/llm-x) (Progressive Web App)
- [AnythingLLM (Docker + MacOs/Windows/Linux native app)](https://github.com/Mintplex-Labs/anything-llm)
- [Ollama Basic Chat: Uses HyperDiv Reactive UI](https://github.com/rapidarchitect/ollama_basic_chat)
- [Ollama-chats RPG](https://github.com/drazdra/ollama-chats)
- [IntelliBar](https://intellibar.app/) (AI-powered assistant for macOS)
- [Jirapt](https://github.com/AliAhmedNada/jirapt) (Jira Integration to generate issues, tasks, epics)
- [ojira](https://github.com/AliAhmedNada/ojira) (Jira chrome plugin to easily generate descriptions for tasks)
- [QA-Pilot](https://github.com/reid41/QA-Pilot) (Interactive chat tool that can leverage Ollama models for rapid understanding and navigation of GitHub code repositories)
- [ChatOllama](https://github.com/sugarforever/chat-ollama) (Open Source Chatbot based on Ollama with Knowledge Bases)
- [CRAG Ollama Chat](https://github.com/Nagi-ovo/CRAG-Ollama-Chat) (Simple Web Search with Corrective RAG)
- [RAGFlow](https://github.com/infiniflow/ragflow) (Open-source Retrieval-Augmented Generation engine based on deep document understanding)
- [StreamDeploy](https://github.com/StreamDeploy-DevRel/streamdeploy-llm-app-scaffold) (LLM Application Scaffold)
- [chat](https://github.com/swuecho/chat) (chat web app for teams)
- [Lobe Chat](https://github.com/lobehub/lobe-chat) with [Integrating Doc](https://lobehub.com/docs/self-hosting/examples/ollama)
- [Ollama RAG Chatbot](https://github.com/datvodinh/rag-chatbot.git) (Local Chat with multiple PDFs using Ollama and RAG)
- [BrainSoup](https://www.nurgo-software.com/products/brainsoup) (Flexible native client with RAG & multi-agent automation)
- [macai](https://github.com/Renset/macai) (macOS client for Ollama, ChatGPT, and other compatible API back-ends)
- [RWKV-Runner](https://github.com/josStorer/RWKV-Runner) (RWKV offline LLM deployment tool, also usable as a client for ChatGPT and Ollama)
- [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) (app to evaluate and compare models)
- [Olpaka](https://github.com/Otacon/olpaka) (User-friendly Flutter Web App for Ollama)
- [Casibase](https://casibase.org) (An open source AI knowledge base and dialogue system combining the latest RAG, SSO, ollama support, and multiple large language models.)
- [OllamaSpring](https://github.com/CrazyNeil/OllamaSpring) (Ollama Client for macOS)
- [LLocal.in](https://github.com/kartikm7/llocal) (Easy to use Electron Desktop Client for Ollama)
- [Shinkai Desktop](https://github.com/dcSpark/shinkai-apps) (Two click install Local AI using Ollama + Files + RAG)
- [AiLama](https://github.com/zeyoyt/ailama) (A Discord User App that allows you to interact with Ollama anywhere in Discord)
- [Ollama with Google Mesop](https://github.com/rapidarchitect/ollama_mesop/) (Mesop Chat Client implementation with Ollama)
- [R2R](https://github.com/SciPhi-AI/R2R) (Open-source RAG engine)
- [Ollama-Kis](https://github.com/elearningshow/ollama-kis) (A simple easy-to-use GUI with sample custom LLM for Drivers Education)
- [OpenGPA](https://opengpa.org) (Open-source offline-first Enterprise Agentic Application)
- [Painting Droid](https://github.com/mateuszmigas/painting-droid) (Painting app with AI integrations)
- [Kerlig AI](https://www.kerlig.com/) (AI writing assistant for macOS)
- [AI Studio](https://github.com/MindWorkAI/AI-Studio)
- [Sidellama](https://github.com/gyopak/sidellama) (browser-based LLM client)
- [LLMStack](https://github.com/trypromptly/LLMStack) (No-code multi-agent framework to build LLM agents and workflows)
- [BoltAI for Mac](https://boltai.com) (AI Chat Client for Mac)
- [Harbor](https://github.com/av/harbor) (Containerized LLM Toolkit with Ollama as default backend)
- [PyGPT](https://github.com/szczyglis-dev/py-gpt) (AI desktop assistant for Linux, Windows, and Mac)
- [Alpaca](https://github.com/Jeffser/Alpaca) (An Ollama client application for Linux and macOS made with GTK4 and Adwaita)
- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT/blob/master/docs/content/platform/ollama.md) (AutoGPT Ollama integration)
- [Go-CREW](https://www.jonathanhecl.com/go-crew/) (Powerful Offline RAG in Golang)
- [PartCAD](https://github.com/openvmp/partcad/) (CAD model generation with OpenSCAD and CadQuery)
- [Ollama4j Web UI](https://github.com/ollama4j/ollama4j-web-ui) - Java-based Web UI for Ollama built with Vaadin, Spring Boot, and Ollama4j
- [PyOllaMx](https://github.com/kspviswa/pyOllaMx) - macOS application capable of chatting with both Ollama and Apple MLX models.
- [Cline](https://github.com/cline/cline) - Formerly known as Claude Dev is a VSCode extension for multi-file/whole-repo coding
- [Cherry Studio](https://github.com/kangfenmao/cherry-studio) (Desktop client with Ollama support)
- [ConfiChat](https://github.com/1runeberg/confichat) (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)
- [Archyve](https://github.com/nickthecook/archyve) (RAG-enabling document library)
- [crewAI with Mesop](https://github.com/rapidarchitect/ollama-crew-mesop) (Mesop Web Interface to run crewAI with Ollama)
- [Tkinter-based client](https://github.com/chyok/ollama-gui) (Python tkinter-based Client for Ollama)
- [LLMChat](https://github.com/trendy-design/llmchat) (Privacy focused, 100% local, intuitive all-in-one chat interface)
- [Local Multimodal AI Chat](https://github.com/Leon-Sander/Local-Multimodal-AI-Chat) (Ollama-based LLM Chat with support for multiple features, including PDF RAG, voice chat, image-based interactions, and integration with OpenAI.)
- [ARGO](https://github.com/xark-argo/argo) (Locally download and run Ollama and Huggingface models with RAG and deep research on Mac/Windows/Linux)
- [OrionChat](https://github.com/EliasPereirah/OrionChat) - OrionChat is a web interface for chatting with different AI providers
- [G1](https://github.com/bklieger-groq/g1) (Prototype of using prompting strategies to improve the LLM's reasoning through o1-like reasoning chains.)
- [Web management](https://github.com/lemonit-eric-mao/ollama-web-management) (Web management page)
- [Promptery](https://github.com/promptery/promptery) (desktop client for Ollama.)
- [Ollama App](https://github.com/JHubi1/ollama-app) (Modern and easy-to-use multi-platform client for Ollama)
- [chat-ollama](https://github.com/annilq/chat-ollama) (a React Native client for Ollama)
- [SpaceLlama](https://github.com/tcsenpai/spacellama) (Firefox and Chrome extension to quickly summarize web pages with ollama in a sidebar)
- [YouLama](https://github.com/tcsenpai/youlama) (Webapp to quickly summarize any YouTube video, supporting Invidious as well)
- [DualMind](https://github.com/tcsenpai/dualmind) (Experimental app allowing two models to talk to each other in the terminal or in a web interface)
- [ollamarama-matrix](https://github.com/h1ddenpr0cess20/ollamarama-matrix) (Ollama chatbot for the Matrix chat protocol)
- [ollama-chat-app](https://github.com/anan1213095357/ollama-chat-app) (Flutter-based chat app)
- [Perfect Memory AI](https://www.perfectmemory.ai/) (Productivity AI assists personalized by what you have seen on your screen, heard, and said in the meetings)
- [Hexabot](https://github.com/hexastack/hexabot) (A conversational AI builder)
- [Reddit Rate](https://github.com/rapidarchitect/reddit_analyzer) (Search and Rate Reddit topics with a weighted summation)
- [OpenTalkGpt](https://github.com/adarshM84/OpenTalkGpt) (Chrome Extension to manage open-source models supported by Ollama, create custom models, and chat with models from a user-friendly UI)
- [VT](https://github.com/vinhnx/vt.ai) (A minimal multimodal AI chat app, with dynamic conversation routing. Supports local models via Ollama)
- [Nosia](https://github.com/nosia-ai/nosia) (Easy to install and use RAG platform based on Ollama)
- [Witsy](https://github.com/nbonamy/witsy) (An AI Desktop application available for Mac/Windows/Linux)
- [Abbey](https://github.com/US-Artificial-Intelligence/abbey) (A configurable AI interface server with notebooks, document storage, and YouTube support)
- [Minima](https://github.com/dmayboroda/minima) (RAG with on-premises or fully local workflow)
- [aidful-ollama-model-delete](https://github.com/AidfulAI/aidful-ollama-model-delete) (User interface for simplified model cleanup)
- [Perplexica](https://github.com/ItzCrazyKns/Perplexica) (An AI-powered search engine & an open-source alternative to Perplexity AI)
- [Ollama Chat WebUI for Docker ](https://github.com/oslook/ollama-webui) (Support for local docker deployment, lightweight ollama webui)
- [AI Toolkit for Visual Studio Code](https://aka.ms/ai-tooklit/ollama-docs) (Microsoft-official VSCode extension to chat, test, evaluate models with Ollama support, and use them in your AI applications.)
- [MinimalNextOllamaChat](https://github.com/anilkay/MinimalNextOllamaChat) (Minimal Web UI for Chat and Model Control)
- [Chipper](https://github.com/TilmanGriesel/chipper) AI interface for tinkerers (Ollama, Haystack RAG, Python)
- [ChibiChat](https://github.com/CosmicEventHorizon/ChibiChat) (Kotlin-based Android app to chat with Ollama and Koboldcpp API endpoints)
- [LocalLLM](https://github.com/qusaismael/localllm) (Minimal Web-App to run ollama models on it with a GUI)
- [Ollamazing](https://github.com/buiducnhat/ollamazing) (Web extension to run Ollama models)
- [OpenDeepResearcher-via-searxng](https://github.com/benhaotang/OpenDeepResearcher-via-searxng) (A Deep Research equivalent endpoint with Ollama support for running locally)
- [AntSK](https://github.com/AIDotNet/AntSK) (Out-of-the-box & Adaptable RAG Chatbot)
- [MaxKB](https://github.com/1Panel-dev/MaxKB/) (Ready-to-use & flexible RAG Chatbot)
- [yla](https://github.com/danielekp/yla) (Web interface to freely interact with your customized models)
- [LangBot](https://github.com/RockChinQ/LangBot) (LLM-based instant messaging bots platform, with Agents, RAG features, supports multiple platforms)
- [1Panel](https://github.com/1Panel-dev/1Panel/) (Web-based Linux Server Management Tool)
- [AstrBot](https://github.com/Soulter/AstrBot/) (User-friendly LLM-based multi-platform chatbot with a WebUI, supporting RAG, LLM agents, and plugins integration)
- [Reins](https://github.com/ibrahimcetin/reins) (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.)
- [Flufy](https://github.com/Aharon-Bensadoun/Flufy) (A beautiful chat interface for interacting with Ollama's API. Built with React, TypeScript, and Material-UI.)
- [Ellama](https://github.com/zeozeozeo/ellama) (Friendly native app to chat with an Ollama instance)
- [screenpipe](https://github.com/mediar-ai/screenpipe) Build agents powered by your screen history
- [Ollamb](https://github.com/hengkysteen/ollamb) (Simple yet rich in features, cross-platform built with Flutter and designed for Ollama. Try the [web demo](https://hengkysteen.github.io/demo/ollamb/).)
- [Writeopia](https://github.com/Writeopia/Writeopia) (Text editor with integration with Ollama)
- [AppFlowy](https://github.com/AppFlowy-IO/AppFlowy) (AI collaborative workspace with Ollama, cross-platform and self-hostable)
- [Lumina](https://github.com/cushydigit/lumina.git) (A lightweight, minimal React.js frontend for interacting with Ollama servers)
- [Tiny Notepad](https://pypi.org/project/tiny-notepad) (A lightweight, notepad-like interface to chat with ollama available on PyPI)
- [macLlama (macOS native)](https://github.com/hellotunamayo/macLlama) (A native macOS GUI application for interacting with Ollama models, featuring a chat interface.) 
- [GPTranslate](https://github.com/philberndt/GPTranslate) (A fast and lightweight, AI powered desktop translation application written with Rust and Tauri. Features real-time translation with OpenAI/Azure/Ollama.)
- [ollama launcher](https://github.com/NGC13009/ollama-launcher) (A launcher for Ollama, aiming to provide users with convenient functions such as ollama server launching, management, or configuration.)
- [ai-hub](https://github.com/Aj-Seven/ai-hub) (AI Hub supports multiple models via API keys and Chat support via Ollama API.)
- [Mayan EDMS](https://gitlab.com/mayan-edms/mayan-edms) (Open source document management system to organize, tag, search, and automate your files with powerful Ollama driven workflows.)
- [Serene Pub](https://github.com/doolijb/serene-pub) (Beginner friendly, open source AI Roleplaying App for Windows, Mac OS and Linux. Search, download and use models with Ollama all inside the app.)
- [Andes](https://github.com/aqerd/andes) (A Visual Studio Code extension that provides a local UI interface for Ollama models)
- [Clueless](https://github.com/KashyapTan/clueless) (Open Source & Local Cluely: A desktop application LLM assistant to help you talk to anything on your screen using locally served Ollama models. Also undetectable to screenshare)
- [ollama-co2](https://github.com/carbonatedWaterOrg/ollama-co2) (FastAPI web interface for monitoring and managing local and remote Ollama servers with real-time model monitoring and concurrent downloads)

### Cloud

- [Google Cloud](https://cloud.google.com/run/docs/tutorials/gpu-gemma2-with-ollama)
- [Fly.io](https://fly.io/docs/python/do-more/add-ollama/)
- [Koyeb](https://www.koyeb.com/deploy/ollama)

### Terminal

- [oterm](https://github.com/ggozad/oterm)
- [Ellama Emacs client](https://github.com/s-kostyaev/ellama)
- [Emacs client](https://github.com/zweifisch/ollama)
- [neollama](https://github.com/paradoxical-dev/neollama) UI client for interacting with models from within Neovim
- [gen.nvim](https://github.com/David-Kunz/gen.nvim)
- [ollama.nvim](https://github.com/nomnivore/ollama.nvim)
- [ollero.nvim](https://github.com/marco-souza/ollero.nvim)
- [ollama-chat.nvim](https://github.com/gerazov/ollama-chat.nvim)
- [ogpt.nvim](https://github.com/huynle/ogpt.nvim)
- [gptel Emacs client](https://github.com/karthink/gptel)
- [Oatmeal](https://github.com/dustinblackman/oatmeal)
- [cmdh](https://github.com/pgibler/cmdh)
- [ooo](https://github.com/npahlfer/ooo)
- [shell-pilot](https://github.com/reid41/shell-pilot)(Interact with models via pure shell scripts on Linux or macOS)
- [tenere](https://github.com/pythops/tenere)
- [llm-ollama](https://github.com/taketwo/llm-ollama) for [Datasette's LLM CLI](https://llm.datasette.io/en/stable/).
- [typechat-cli](https://github.com/anaisbetts/typechat-cli)
- [ShellOracle](https://github.com/djcopley/ShellOracle)
- [tlm](https://github.com/yusufcanb/tlm)
- [podman-ollama](https://github.com/ericcurtin/podman-ollama)
- [gollama](https://github.com/sammcj/gollama)
- [ParLlama](https://github.com/paulrobello/parllama)
- [Ollama eBook Summary](https://github.com/cognitivetech/ollama-ebook-summary/)
- [Ollama Mixture of Experts (MOE) in 50 lines of code](https://github.com/rapidarchitect/ollama_moe)
- [vim-intelligence-bridge](https://github.com/pepo-ec/vim-intelligence-bridge) Simple interaction of "Ollama" with the Vim editor
- [x-cmd ollama](https://x-cmd.com/mod/ollama)
- [bb7](https://github.com/drunkwcodes/bb7)
- [SwollamaCLI](https://github.com/marcusziade/Swollama) bundled with the Swollama Swift package. [Demo](https://github.com/marcusziade/Swollama?tab=readme-ov-file#cli-usage)
- [aichat](https://github.com/sigoden/aichat) All-in-one LLM CLI tool featuring Shell Assistant, Chat-REPL, RAG, AI tools & agents, with access to OpenAI, Claude, Gemini, Ollama, Groq, and more.
- [PowershAI](https://github.com/rrg92/powershai) PowerShell module that brings AI to terminal on Windows, including support for Ollama
- [DeepShell](https://github.com/Abyss-c0re/deepshell) Your self-hosted AI assistant. Interactive Shell, Files and Folders analysis.
- [orbiton](https://github.com/xyproto/orbiton) Configuration-free text editor and IDE with support for tab completion with Ollama.
- [orca-cli](https://github.com/molbal/orca-cli) Ollama Registry CLI Application - Browse, pull, and download models from Ollama Registry in your terminal.
- [GGUF-to-Ollama](https://github.com/jonathanhecl/gguf-to-ollama) - Importing GGUF to Ollama made easy (multiplatform)
- [AWS-Strands-With-Ollama](https://github.com/rapidarchitect/ollama_strands) - AWS Strands Agents with Ollama Examples
- [ollama-multirun](https://github.com/attogram/ollama-multirun) - A bash shell script to run a single prompt against any or all of your locally installed ollama models, saving the output and performance statistics as easily navigable web pages. ([Demo](https://attogram.github.io/ai_test_zone/))
- [ollama-bash-toolshed](https://github.com/attogram/ollama-bash-toolshed) - Bash scripts to chat with tool using models. Add new tools to your shed with ease. Runs on Ollama.

### Apple Vision Pro

- [SwiftChat](https://github.com/aws-samples/swift-chat) (Cross-platform AI chat app supporting Apple Vision Pro via "Designed for iPad")
- [Enchanted](https://github.com/AugustDev/enchanted)

### Database

- [pgai](https://github.com/timescale/pgai) - PostgreSQL as a vector database (Create and search embeddings from Ollama models using pgvector)
   - [Get started guide](https://github.com/timescale/pgai/blob/main/docs/vectorizer-quick-start.md)
- [MindsDB](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/ollama_handler/README.md) (Connects Ollama models with nearly 200 data platforms and apps)
- [chromem-go](https://github.com/philippgille/chromem-go/blob/v0.5.0/embed_ollama.go) with [example](https://github.com/philippgille/chromem-go/tree/v0.5.0/examples/rag-wikipedia-ollama)
- [Kangaroo](https://github.com/dbkangaroo/kangaroo) (AI-powered SQL client and admin tool for popular databases)

### Package managers

- [Pacman](https://archlinux.org/packages/extra/x86_64/ollama/)
- [Gentoo](https://github.com/gentoo/guru/tree/master/app-misc/ollama)
- [Homebrew](https://formulae.brew.sh/formula/ollama)
- [Helm Chart](https://artifacthub.io/packages/helm/ollama-helm/ollama)
- [Guix channel](https://codeberg.org/tusharhero/ollama-guix)
- [Nix package](https://search.nixos.org/packages?show=ollama&from=0&size=50&sort=relevance&type=packages&query=ollama)
- [Flox](https://flox.dev/blog/ollama-part-one)

### Libraries

- [LangChain](https://python.langchain.com/docs/integrations/chat/ollama/) and [LangChain.js](https://js.langchain.com/docs/integrations/chat/ollama/) with [example](https://js.langchain.com/docs/tutorials/local_rag/)
- [Firebase Genkit](https://firebase.google.com/docs/genkit/plugins/ollama)
- [crewAI](https://github.com/crewAIInc/crewAI)
- [Yacana](https://remembersoftwares.github.io/yacana/) (User-friendly multi-agent framework for brainstorming and executing predetermined flows with built-in tool integration)
- [Spring AI](https://github.com/spring-projects/spring-ai) with [reference](https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html) and [example](https://github.com/tzolov/ollama-tools)
- [LangChainGo](https://github.com/tmc/langchaingo/) with [example](https://github.com/tmc/langchaingo/tree/main/examples/ollama-completion-example)
- [LangChain4j](https://github.com/langchain4j/langchain4j) with [example](https://github.com/langchain4j/langchain4j-examples/tree/main/ollama-examples/src/main/java)
- [LangChainRust](https://github.com/Abraxas-365/langchain-rust) with [example](https://github.com/Abraxas-365/langchain-rust/blob/main/examples/llm_ollama.rs)
- [LangChain for .NET](https://github.com/tryAGI/LangChain) with [example](https://github.com/tryAGI/LangChain/blob/main/examples/LangChain.Samples.OpenAI/Program.cs)
- [LLPhant](https://github.com/theodo-group/LLPhant?tab=readme-ov-file#ollama)
- [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/llm/ollama/) and [LlamaIndexTS](https://ts.llamaindex.ai/modules/llms/available_llms/ollama)
- [LiteLLM](https://github.com/BerriAI/litellm)
- [OllamaFarm for Go](https://github.com/presbrey/ollamafarm)
- [OllamaSharp for .NET](https://github.com/awaescher/OllamaSharp)
- [Ollama for Ruby](https://github.com/gbaptista/ollama-ai)
- [Ollama-rs for Rust](https://github.com/pepperoni21/ollama-rs)
- [Ollama-hpp for C++](https://github.com/jmont-dev/ollama-hpp)
- [Ollama4j for Java](https://github.com/ollama4j/ollama4j)
- [ModelFusion Typescript Library](https://modelfusion.dev/integration/model-provider/ollama)
- [OllamaKit for Swift](https://github.com/kevinhermawan/OllamaKit)
- [Ollama for Dart](https://github.com/breitburg/dart-ollama)
- [Ollama for Laravel](https://github.com/cloudstudio/ollama-laravel)
- [LangChainDart](https://github.com/davidmigloz/langchain_dart)
- [Semantic Kernel - Python](https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/ollama)
- [Haystack](https://github.com/deepset-ai/haystack-integrations/blob/main/integrations/ollama.md)
- [Elixir LangChain](https://github.com/brainlid/langchain)
- [Ollama for R - rollama](https://github.com/JBGruber/rollama)
- [Ollama for R - ollama-r](https://github.com/hauselin/ollama-r)
- [Ollama-ex for Elixir](https://github.com/lebrunel/ollama-ex)
- [Ollama Connector for SAP ABAP](https://github.com/b-tocs/abap_btocs_ollama)
- [Testcontainers](https://testcontainers.com/modules/ollama/)
- [Portkey](https://portkey.ai/docs/welcome/integration-guides/ollama)
- [PromptingTools.jl](https://github.com/svilupp/PromptingTools.jl) with an [example](https://svilupp.github.io/PromptingTools.jl/dev/examples/working_with_ollama)
- [LlamaScript](https://github.com/Project-Llama/llamascript)
- [llm-axe](https://github.com/emirsahin1/llm-axe) (Python Toolkit for Building LLM Powered Apps)
- [Gollm](https://docs.gollm.co/examples/ollama-example)
- [Gollama for Golang](https://github.com/jonathanhecl/gollama)
- [Ollamaclient for Golang](https://github.com/xyproto/ollamaclient)
- [High-level function abstraction in Go](https://gitlab.com/tozd/go/fun)
- [Ollama PHP](https://github.com/ArdaGnsrn/ollama-php)
- [Agents-Flex for Java](https://github.com/agents-flex/agents-flex) with [example](https://github.com/agents-flex/agents-flex/tree/main/agents-flex-llm/agents-flex-llm-ollama/src/test/java/com/agentsflex/llm/ollama)
- [Parakeet](https://github.com/parakeet-nest/parakeet) is a GoLang library, made to simplify the development of small generative AI applications with Ollama.
- [Haverscript](https://github.com/andygill/haverscript) with [examples](https://github.com/andygill/haverscript/tree/main/examples)
- [Ollama for Swift](https://github.com/mattt/ollama-swift)
- [Swollama for Swift](https://github.com/marcusziade/Swollama) with [DocC](https://marcusziade.github.io/Swollama/documentation/swollama/)
- [GoLamify](https://github.com/prasad89/golamify)
- [Ollama for Haskell](https://github.com/tusharad/ollama-haskell)
- [multi-llm-ts](https://github.com/nbonamy/multi-llm-ts) (A Typescript/JavaScript library allowing access to different LLM in a unified API)
- [LlmTornado](https://github.com/lofcz/llmtornado) (C# library providing a unified interface for major FOSS & Commercial inference APIs)
- [Ollama for Zig](https://github.com/dravenk/ollama-zig)
- [Abso](https://github.com/lunary-ai/abso) (OpenAI-compatible TypeScript SDK for any LLM provider)
- [Nichey](https://github.com/goodreasonai/nichey) is a Python package for generating custom wikis for your research topic
- [Ollama for D](https://github.com/kassane/ollama-d)
- [OllamaPlusPlus](https://github.com/HardCodeDev777/OllamaPlusPlus) (Very simple C++ library for Ollama)
- [any-llm](https://github.com/mozilla-ai/any-llm) (A single interface to use different llm providers by [mozilla.ai](https://www.mozilla.ai/))
- [any-agent](https://github.com/mozilla-ai/any-agent) (A single interface to use and evaluate different agent frameworks by [mozilla.ai](https://www.mozilla.ai/))
- [Neuro SAN](https://github.com/cognizant-ai-lab/neuro-san-studio) (Data-driven multi-agent orchestration framework) with [example](https://github.com/cognizant-ai-lab/neuro-san-studio/blob/main/docs/user_guide.md#ollama)
- [achatbot-go](https://github.com/ai-bot-pro/achatbot-go) a multimodal(text/audio/image) chatbot.

### Mobile

- [SwiftChat](https://github.com/aws-samples/swift-chat) (Lightning-fast Cross-platform AI chat app with native UI for Android, iOS, and iPad)
- [Enchanted](https://github.com/AugustDev/enchanted)
- [Maid](https://github.com/Mobile-Artificial-Intelligence/maid)
- [Ollama App](https://github.com/JHubi1/ollama-app) (Modern and easy-to-use multi-platform client for Ollama)
- [ConfiChat](https://github.com/1runeberg/confichat) (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)
- [Ollama Android Chat](https://github.com/sunshine0523/OllamaServer) (No need for Termux, start the Ollama service with one click on an Android device)
- [Reins](https://github.com/ibrahimcetin/reins) (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.)

### Extensions & Plugins

- [Raycast extension](https://github.com/MassimilianoPasquini97/raycast_ollama)
- [Discollama](https://github.com/mxyng/discollama) (Discord bot inside the Ollama discord channel)
- [Continue](https://github.com/continuedev/continue)
- [Vibe](https://github.com/thewh1teagle/vibe) (Transcribe and analyze meetings with Ollama)
- [Obsidian Ollama plugin](https://github.com/hinterdupfinger/obsidian-ollama)
- [Logseq Ollama plugin](https://github.com/omagdy7/ollama-logseq)
- [NotesOllama](https://github.com/andersrex/notesollama) (Apple Notes Ollama plugin)
- [Dagger Chatbot](https://github.com/samalba/dagger-chatbot)
- [Discord AI Bot](https://github.com/mekb-turtle/discord-ai-bot)
- [Ollama Telegram Bot](https://github.com/ruecat/ollama-telegram)
- [Hass Ollama Conversation](https://github.com/ej52/hass-ollama-conversation)
- [Rivet plugin](https://github.com/abrenneke/rivet-plugin-ollama)
- [Obsidian BMO Chatbot plugin](https://github.com/longy2k/obsidian-bmo-chatbot)
- [Cliobot](https://github.com/herval/cliobot) (Telegram bot with Ollama support)
- [Copilot for Obsidian plugin](https://github.com/logancyang/obsidian-copilot)
- [Obsidian Local GPT plugin](https://github.com/pfrankov/obsidian-local-gpt)
- [Open Interpreter](https://docs.openinterpreter.com/language-model-setup/local-models/ollama)
- [Llama Coder](https://github.com/ex3ndr/llama-coder) (Copilot alternative using Ollama)
- [Ollama Copilot](https://github.com/bernardo-bruning/ollama-copilot) (Proxy that allows you to use Ollama as a copilot like GitHub Copilot)
- [twinny](https://github.com/rjmacarthy/twinny) (Copilot and Copilot chat alternative using Ollama)
- [Wingman-AI](https://github.com/RussellCanfield/wingman-ai) (Copilot code and chat alternative using Ollama and Hugging Face)
- [Page Assist](https://github.com/n4ze3m/page-assist) (Chrome Extension)
- [Plasmoid Ollama Control](https://github.com/imoize/plasmoid-ollamacontrol) (KDE Plasma extension that allows you to quickly manage/control Ollama model)
- [AI Telegram Bot](https://github.com/tusharhero/aitelegrambot) (Telegram bot using Ollama in backend)
- [AI ST Completion](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (Sublime Text 4 AI assistant plugin with Ollama support)
- [Discord-Ollama Chat Bot](https://github.com/kevinthedang/discord-ollama) (Generalized TypeScript Discord Bot w/ Tuning Documentation)
- [ChatGPTBox: All in one browser extension](https://github.com/josStorer/chatGPTBox) with [Integrating Tutorial](https://github.com/josStorer/chatGPTBox/issues/616#issuecomment-1975186467)
- [Discord AI chat/moderation bot](https://github.com/rapmd73/Companion) Chat/moderation bot written in python. Uses Ollama to create personalities.
- [Headless Ollama](https://github.com/nischalj10/headless-ollama) (Scripts to automatically install ollama client & models on any OS for apps that depend on ollama server)
- [Terraform AWS Ollama & Open WebUI](https://github.com/xuyangbocn/terraform-aws-self-host-llm) (A Terraform module to deploy on AWS a ready-to-use Ollama service, together with its front-end Open WebUI service.)
- [node-red-contrib-ollama](https://github.com/jakubburkiewicz/node-red-contrib-ollama)
- [Local AI Helper](https://github.com/ivostoykov/localAI) (Chrome and Firefox extensions that enable interactions with the active tab and customisable API endpoints. Includes secure storage for user prompts.)
- [vnc-lm](https://github.com/jake83741/vnc-lm) (Discord bot for messaging with LLMs through Ollama and LiteLLM. Seamlessly move between local and flagship models.)
- [LSP-AI](https://github.com/SilasMarvin/lsp-ai) (Open-source language server for AI-powered functionality)
- [QodeAssist](https://github.com/Palm1r/QodeAssist) (AI-powered coding assistant plugin for Qt Creator)
- [Obsidian Quiz Generator plugin](https://github.com/ECuiDev/obsidian-quiz-generator)
- [AI Summmary Helper plugin](https://github.com/philffm/ai-summary-helper)
- [TextCraft](https://github.com/suncloudsmoon/TextCraft) (Copilot in Word alternative using Ollama)
- [Alfred Ollama](https://github.com/zeitlings/alfred-ollama) (Alfred Workflow)
- [TextLLaMA](https://github.com/adarshM84/TextLLaMA) A Chrome Extension that helps you write emails, correct grammar, and translate into any language
- [Simple-Discord-AI](https://github.com/zyphixor/simple-discord-ai)
- [LLM Telegram Bot](https://github.com/innightwolfsleep/llm_telegram_bot) (telegram bot, primary for RP. Oobabooga-like buttons, [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) API integration e.t.c)
- [mcp-llm](https://github.com/sammcj/mcp-llm) (MCP Server to allow LLMs to call other LLMs)
- [SimpleOllamaUnity](https://github.com/HardCodeDev777/SimpleOllamaUnity) (Unity Engine extension for communicating with Ollama in a few lines of code. Also works at runtime)
- [UnityCodeLama](https://github.com/HardCodeDev777/UnityCodeLama) (Unity Edtior tool to analyze scripts via Ollama)
- [NativeMind](https://github.com/NativeMindBrowser/NativeMindExtension) (Private, on-device AI Assistant, no cloud dependencies)
- [GMAI - Gradle Managed AI](https://gmai.premex.se/) (Gradle plugin for automated Ollama lifecycle management during build phases)
- [NOMYO Router](https://github.com/nomyo-ai/nomyo-router) (A transparent Ollama proxy with model deployment aware routing which auto-manages multiple Ollama instances in a given network)

### Supported backends

- [llama.cpp](https://github.com/ggml-org/llama.cpp) project founded by Georgi Gerganov.

### Observability
- [Opik](https://www.comet.com/docs/opik/cookbook/ollama) is an open-source platform to debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards. Opik supports native intergration to Ollama.
- [Lunary](https://lunary.ai/docs/integrations/ollama) is the leading open-source LLM observability platform. It provides a variety of enterprise-grade features such as real-time analytics, prompt templates management, PII masking, and comprehensive agent tracing.
- [OpenLIT](https://github.com/openlit/openlit) is an OpenTelemetry-native tool for monitoring Ollama Applications & GPUs using traces and metrics.
- [HoneyHive](https://docs.honeyhive.ai/integrations/ollama) is an AI observability and evaluation platform for AI agents. Use HoneyHive to evaluate agent performance, interrogate failures, and monitor quality in production.
- [Langfuse](https://langfuse.com/docs/integrations/ollama) is an open source LLM observability platform that enables teams to collaboratively monitor, evaluate and debug AI applications.
- [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing) is an open source LLM observability tool with a convenient API to log and visualize traces, making it easy to debug and evaluate GenAI applications.


--- app/README.md ---
# Ollama App

## Linux

TODO

## MacOS

TODO

## Windows

If you want to build the installer, youll need to install
- https://jrsoftware.org/isinfo.php


In the top directory of this repo, run the following powershell script
to build the ollama CLI, ollama app, and ollama installer.

```powershell
powershell -ExecutionPolicy Bypass -File .\scripts\build_windows.ps1
```


--- llama/README.md ---
# `llama`

This package provides Go bindings to [llama.cpp](https://github.com/ggerganov/llama.cpp).

## Vendoring

Ollama vendors [llama.cpp](https://github.com/ggerganov/llama.cpp/) and [ggml](https://github.com/ggerganov/llama.cpp/tree/master/ggml/src). While we generally strive to contribute changes back upstream to avoid drift, we carry a small set of patches which are applied to the tracking commit.

If you update the vendoring code, start by running the following command to establish the tracking llama.cpp repo in the `./vendor/` directory.

```shell
make -f Makefile.sync apply-patches
```

### Updating Base Commit

**Pin to new base commit**

To change the base commit, update `FETCH_HEAD` in Makefile.sync.

When updating to a newer base commit, the existing patches may not apply cleanly and require manual merge resolution.

Start by applying the patches. If any of the patches have conflicts, the `git am` will stop at the first failure.

```shell
make -f Makefile.sync apply-patches
```

If there are conflicts, you will see an error message. Resolve the conflicts in `./vendor/`, and continue the patch series with `git am --continue` and rerun `make -f Makefile.sync apply-patches`. Repeat until all patches are successfully applied.

Once all patches are applied, commit the changes to the tracking repository.

```shell
make -f Makefile.sync format-patches sync
```

### Generating Patches

When working on new fixes or features that impact vendored code, use the following model. First get a clean tracking repo with all current patches applied:

```shell
make -f Makefile.sync clean apply-patches
```

Iterate until you're ready to submit PRs. Once your code is ready, commit a change in the `./vendor/` directory, then generate the patches for ollama with

```shell
make -f Makefile.sync format-patches
```

In your `./vendor/` directory, create a branch, and cherry-pick the new commit to that branch, then submit a PR upstream to llama.cpp.

Commit the changes in the ollama repo and submit a PR to Ollama, which will include the vendored code update with your change, along with the patches.

After your PR upstream is merged, follow the **Updating Base Commit** instructions above, however first remove your patch before running `apply-patches` since the new base commit contains your change already.


--- model/testdata/war-and-peace.txt ---
CHAPTER I

"Well, Prince, so Genoa and Lucca are now just family estates of the
Buonapartes. But I warn you, if you don't tell me that this means war,
if you still try to defend the infamies and horrors perpetrated by that
Antichrist--I really believe he is Antichrist--I will have nothing more
to do with you and you are no longer my friend, no longer my 'faithful
slave,' as you call yourself! But how do you do? I see I have frightened
you--sit down and tell me all the news."

It was in July, 1805, and the speaker was the well-known Anna Pavlovna
Scherer, maid of honor and favorite of the Empress Marya Fedorovna. With
these words she greeted Prince Vasili Kuragin, a man of high rank and
importance, who was the first to arrive at her reception. Anna Pavlovna
had had a cough for some days. She was, as she said, suffering from la
grippe; grippe being then a new word in St. Petersburg, used only by the
elite.

All her invitations without exception, written in French, and delivered
by a scarlet-liveried footman that morning, ran as follows:

"If you have nothing better to do, Count (or Prince), and if the
prospect of spending an evening with a poor invalid is not too terrible,
I shall be very charmed to see you tonight between 7 and 10--Annette
Scherer."

"Heavens! what a virulent attack!" replied the prince, not in the least
disconcerted by this reception. He had just entered, wearing an
embroidered court uniform, knee breeches, and shoes, and had stars on
his breast and a serene expression on his flat face. He spoke in that
refined French in which our grandfathers not only spoke but thought, and
with the gentle, patronizing intonation natural to a man of importance
who had grown old in society and at court. He went up to Anna Pavlovna,
kissed her hand, presenting to her his bald, scented, and shining head,
and complacently seated himself on the sofa.

"First of all, dear friend, tell me how you are. Set your friend's mind
at rest," said he without altering his tone, beneath the politeness and
affected sympathy of which indifference and even irony could be
discerned.

"Can one be well while suffering morally? Can one be calm in times like
these if one has any feeling?" said Anna Pavlovna. "You are staying the
whole evening, I hope?"

"And the fete at the English ambassador's? Today is Wednesday. I must
put in an appearance there," said the prince. "My daughter is coming for
me to take me there."

"I thought today's fete had been canceled. I confess all these
festivities and fireworks are becoming wearisome."

"If they had known that you wished it, the entertainment would have been
put off," said the prince, who, like a wound-up clock, by force of habit
said things he did not even wish to be believed.

"Don't tease! Well, and what has been decided about Novosiltsev's
dispatch? You know everything."

"What can one say about it?" replied the prince in a cold, listless
tone. "What has been decided? They have decided that Buonaparte has
burnt his boats, and I believe that we are ready to burn ours."

Prince Vasili always spoke languidly, like an actor repeating a stale
part. Anna Pavlovna Scherer on the contrary, despite her forty years,
overflowed with animation and impulsiveness. To be an enthusiast had
become her social vocation and, sometimes even when she did not feel
like it, she became enthusiastic in order not to disappoint the
expectations of those who knew her. The subdued smile which, though it
did not suit her faded features, always played round her lips expressed,
as in a spoiled child, a continual consciousness of her charming defect,
which she neither wished, nor could, nor considered it necessary, to
correct.

In the midst of a conversation on political matters Anna Pavlovna burst
out:

"Oh, don't speak to me of Austria. Perhaps I don't understand things,
but Austria never has wished, and does not wish, for war. She is
betraying us! Russia alone must save Europe. Our gracious sovereign
recognizes his high vocation and will be true to it. That is the one
thing I have faith in! Our good and wonderful sovereign has to perform
the noblest role on earth, and he is so virtuous and noble that God will
not forsake him. He will fulfill his vocation and crush the hydra of
revolution, which has become more terrible than ever in the person of
this murderer and villain! We alone must avenge the blood of the just
one.... Whom, I ask you, can we rely on?... England with her commercial
spirit will not and cannot understand the Emperor Alexander's loftiness
of soul. She has refused to evacuate Malta. She wanted to find, and
still seeks, some secret motive in our actions. What answer did
Novosiltsev get? None. The English have not understood and cannot
understand the self-abnegation of our Emperor who wants nothing for
himself, but only desires the good of mankind. And what have they
promised? Nothing! And what little they have promised they will not
perform! Prussia has always declared that Buonaparte is invincible, and
that all Europe is powerless before him.... And I don't believe a word
that Hardenburg says, or Haugwitz either. This famous Prussian
neutrality is just a trap. I have faith only in God and the lofty
destiny of our adored monarch. He will save Europe!"

She suddenly paused, smiling at her own impetuosity.

"I think," said the prince with a smile, "that if you had been sent
instead of our dear Wintzingerode you would have captured the King of
Prussia's consent by assault. You are so eloquent. Will you give me a
cup of tea?"

"In a moment. A propos," she added, becoming calm again, "I am expecting
two very interesting men tonight, le Vicomte de Mortemart, who is
connected with the Montmorencys through the Rohans, one of the best
French families. He is one of the genuine emigres, the good ones. And
also the Abbe Morio. Do you know that profound thinker? He has been
received by the Emperor. Had you heard?"

"I shall be delighted to meet them," said the prince. "But tell me," he
added with studied carelessness as if it had only just occurred to him,
though the question he was about to ask was the chief motive of his
visit, "is it true that the Dowager Empress wants Baron Funke to be
appointed first secretary at Vienna? The baron by all accounts is a poor
creature."

Prince Vasili wished to obtain this post for his son, but others were
trying through the Dowager Empress Marya Fedorovna to secure it for the
baron.

Anna Pavlovna almost closed her eyes to indicate that neither she nor
anyone else had a right to criticize what the Empress desired or was
pleased with.

"Baron Funke has been recommended to the Dowager Empress by her sister,"
was all she said, in a dry and mournful tone.

As she named the Empress, Anna Pavlovna's face suddenly assumed an
expression of profound and sincere devotion and respect mingled with
sadness, and this occurred every time she mentioned her illustrious
patroness. She added that Her Majesty had deigned to show Baron Funke
beaucoup d'estime, and again her face clouded over with sadness.

The prince was silent and looked indifferent. But, with the womanly and
courtierlike quickness and tact habitual to her, Anna Pavlovna wished
both to rebuke him (for daring to speak as he had done of a man
recommended to the Empress) and at the same time to console him, so she
said:

"Now about your family. Do you know that since your daughter came out
everyone has been enraptured by her? They say she is amazingly
beautiful."

The prince bowed to signify his respect and gratitude.

"I often think," she continued after a short pause, drawing nearer to
the prince and smiling amiably at him as if to show that political and
social topics were ended and the time had come for intimate
conversation--"I often think how unfairly sometimes the joys of life are
distributed. Why has fate given you two such splendid children? I don't
speak of Anatole, your youngest. I don't like him," she added in a tone
admitting of no rejoinder and raising her eyebrows. "Two such charming
children. And really you appreciate them less than anyone, and so you
don't deserve to have them."

And she smiled her ecstatic smile.

"I can't help it," said the prince. "Lavater would have said I lack the
bump of paternity."

"Don't joke; I mean to have a serious talk with you. Do you know I am
dissatisfied with your younger son? Between ourselves" (and her face
assumed its melancholy expression), "he was mentioned at Her Majesty's
and you were pitied...."

The prince answered nothing, but she looked at him significantly,
awaiting a reply. He frowned.

"What would you have me do?" he said at last. "You know I did all a
father could for their education, and they have both turned out fools.
Hippolyte is at least a quiet fool, but Anatole is an active one. That
is the only difference between them." He said this smiling in a way more
natural and animated than usual, so that the wrinkles round his mouth
very clearly revealed something unexpectedly coarse and unpleasant.

"And why are children born to such men as you? If you were not a father
there would be nothing I could reproach you with," said Anna Pavlovna,
looking up pensively.

"I am your faithful slave and to you alone I can confess that my
children are the bane of my life. It is the cross I have to bear. That
is how I explain it to myself. It can't be helped!"

He said no more, but expressed his resignation to cruel fate by a
gesture. Anna Pavlovna meditated.

"Have you never thought of marrying your prodigal son Anatole?" she
asked. "They say old maids have a mania for matchmaking, and though I
don't feel that weakness in myself as yet, I know a little person who is
very unhappy with her father. She is a relation of yours, Princess Mary
Bolkonskaya."

Prince Vasili did not reply, though, with the quickness of memory and
perception befitting a man of the world, he indicated by a movement of
the head that he was considering this information.

"Do you know," he said at last, evidently unable to check the sad
current of his thoughts, "that Anatole is costing me forty thousand
rubles a year? And," he went on after a pause, "what will it be in five
years, if he goes on like this?" Presently he added: "That's what we
fathers have to put up with.... Is this princess of yours rich?"

"Her father is very rich and stingy. He lives in the country. He is the
well-known Prince Bolkonski who had to retire from the army under the
late Emperor, and was nicknamed 'the King of Prussia.' He is very clever
but eccentric, and a bore. The poor girl is very unhappy. She has a
brother; I think you know him, he married Lise Meinen lately. He is an
aide-de-camp of Kutuzov's and will be here tonight."

"Listen, dear Annette," said the prince, suddenly taking Anna Pavlovna's
hand and for some reason drawing it downwards. "Arrange that affair for
me and I shall always be your most devoted slave-slafe with an f, as a
village elder of mine writes in his reports. She is rich and of good
family and that's all I want."

And with the familiarity and easy grace peculiar to him, he raised the
maid of honor's hand to his lips, kissed it, and swung it to and fro as
he lay back in his armchair, looking in another direction.

"Attendez," said Anna Pavlovna, reflecting, "I'll speak to Lise, young
Bolkonski's wife, this very evening, and perhaps the thing can be
arranged. It shall be on your family's behalf that I'll start my
apprenticeship as old maid."




CHAPTER II

Anna Pavlovna's drawing room was gradually filling. The highest
Petersburg society was assembled there: people differing widely in age
and character but alike in the social circle to which they belonged.
Prince Vasili's daughter, the beautiful Helene, came to take her father
to the ambassador's entertainment; she wore a ball dress and her badge
as maid of honor. The youthful little Princess Bolkonskaya, known as la
femme la plus seduisante de Petersbourg, * was also there. She had been
married during the previous winter, and being pregnant did not go to any
large gatherings, but only to small receptions. Prince Vasili's son,
Hippolyte, had come with Mortemart, whom he introduced. The Abbe Morio
and many others had also come.


* The most fascinating woman in Petersburg.

To each new arrival Anna Pavlovna said, "You have not yet seen my aunt,"
or "You do not know my aunt?" and very gravely conducted him or her to a
little old lady, wearing large bows of ribbon in her cap, who had come
sailing in from another room as soon as the guests began to arrive; and
slowly turning her eyes from the visitor to her aunt, Anna Pavlovna
mentioned each one's name and then left them.

Each visitor performed the ceremony of greeting this old aunt whom not
one of them knew, not one of them wanted to know, and not one of them
cared about; Anna Pavlovna observed these greetings with mournful and
solemn interest and silent approval. The aunt spoke to each of them in
the same words, about their health and her own, and the health of Her
Majesty, "who, thank God, was better today." And each visitor, though
politeness prevented his showing impatience, left the old woman with a
sense of relief at having performed a vexatious duty and did not return
to her the whole evening.

The young Princess Bolkonskaya had brought some work in a gold-
embroidered velvet bag. Her pretty little upper lip, on which a delicate
dark down was just perceptible, was too short for her teeth, but it
lifted all the more sweetly, and was especially charming when she
occasionally drew it down to meet the lower lip. As is always the case
with a thoroughly attractive woman, her defect--the shortness of her
upper lip and her half-open mouth--seemed to be her own special and
peculiar form of beauty. Everyone brightened at the sight of this pretty
young woman, so soon to become a mother, so full of life and health, and
carrying her burden so lightly. Old men and dull dispirited young ones
who looked at her, after being in her company and talking to her a
little while, felt as if they too were becoming, like her, full of life
and health. All who talked to her, and at each word saw her bright smile
and the constant gleam of her white teeth, thought that they were in a
specially amiable mood that day.

The little princess went round the table with quick, short, swaying
steps, her workbag on her arm, and gaily spreading out her dress sat
down on a sofa near the silver samovar, as if all she was doing was a
pleasure to herself and to all around her. "I have brought my work,"
said she in French, displaying her bag and addressing all present.
"Mind, Annette, I hope you have not played a wicked trick on me," she
added, turning to her hostess. "You wrote that it was to be quite a
small reception, and just see how badly I am dressed." And she spread
out her arms to show her short-waisted, lace-trimmed, dainty gray dress,
girdled with a broad ribbon just below the breast.

"Soyez tranquille, Lise, you will always be prettier than anyone else,"
replied Anna Pavlovna.

"You know," said the princess in the same tone of voice and still in
French, turning to a general, "my husband is deserting me? He is going
to get himself killed. Tell me what this wretched war is for?" she
added, addressing Prince Vasili, and without waiting for an answer she
turned to speak to his daughter, the beautiful Helene.

"What a delightful woman this little princess is!" said Prince Vasili to
Anna Pavlovna.

One of the next arrivals was a stout, heavily built young man with
close-cropped hair, spectacles, the light-colored breeches fashionable
at that time, a very high ruffle, and a brown dress coat. This stout
young man was an illegitimate son of Count Bezukhov, a well-known
grandee of Catherine's time who now lay dying in Moscow. The young man
had not yet entered either the military or civil service, as he had only
just returned from abroad where he had been educated, and this was his
first appearance in society. Anna Pavlovna greeted him with the nod she
accorded to the lowest hierarchy in her drawing room. But in spite of
this lowest-grade greeting, a look of anxiety and fear, as at the sight
of something too large and unsuited to the place, came over her face
when she saw Pierre enter. Though he was certainly rather bigger than
the other men in the room, her anxiety could only have reference to the
clever though shy, but observant and natural, expression which
distinguished him from everyone else in that drawing room.

"It is very good of you, Monsieur Pierre, to come and visit a poor
invalid," said Anna Pavlovna, exchanging an alarmed glance with her aunt
as she conducted him to her.

Pierre murmured something unintelligible, and continued to look round as
if in search of something. On his way to the aunt he bowed to the little
princess with a pleased smile, as to an intimate acquaintance.

Anna Pavlovna's alarm was justified, for Pierre turned away from the
aunt without waiting to hear her speech about Her Majesty's health. Anna
Pavlovna in dismay detained him with the words: "Do you know the Abbe
Morio? He is a most interesting man."

"Yes, I have heard of his scheme for perpetual peace, and it is very
interesting but hardly feasible."

"You think so?" rejoined Anna Pavlovna in order to say something and get
away to attend to her duties as hostess. But Pierre now committed a
reverse act of impoliteness. First he had left a lady before she had
finished speaking to him, and now he continued to speak to another who
wished to get away. With his head bent, and his big feet spread apart,
he began explaining his reasons for thinking the abbe's plan chimerical.

"We will talk of it later," said Anna Pavlovna with a smile.

And having got rid of this young man who did not know how to behave, she
resumed her duties as hostess and continued to listen and watch, ready
to help at any point where the conversation might happen to flag. As the
foreman of a spinning mill, when he has set the hands to work, goes
round and notices here a spindle that has stopped or there one that
creaks or makes more noise than it should, and hastens to check the
machine or set it in proper motion, so Anna Pavlovna moved about her
drawing room, approaching now a silent, now a too-noisy group, and by a
word or slight rearrangement kept the conversational machine in steady,
proper, and regular motion. But amid these cares her anxiety about
Pierre was evident. She kept an anxious watch on him when he approached
the group round Mortemart to listen to what was being said there, and
again when he passed to another group whose center was the abbe.

Pierre had been educated abroad, and this reception at Anna Pavlovna's
was the first he had attended in Russia. He knew that all the
intellectual lights of Petersburg were gathered there and, like a child
in a toyshop, did not know which way to look, afraid of missing any
clever conversation that was to be heard. Seeing the self-confident and
refined expression on the faces of those present he was always expecting
to hear something very profound. At last he came up to Morio. Here the
conversation seemed interesting and he stood waiting for an opportunity
to express his own views, as young people are fond of doing.




CHAPTER III

Anna Pavlovna's reception was in full swing. The spindles hummed
steadily and ceaselessly on all sides. With the exception of the aunt,
beside whom sat only one elderly lady, who with her thin careworn face
was rather out of place in this brilliant society, the whole company had
settled into three groups. One, chiefly masculine, had formed round the
abbe. Another, of young people, was grouped round the beautiful Princess
Helene, Prince Vasili's daughter, and the little Princess Bolkonskaya,
very pretty and rosy, though rather too plump for her age. The third
group was gathered round Mortemart and Anna Pavlovna.

The vicomte was a nice-looking young man with soft features and polished
manners, who evidently considered himself a celebrity but out of
politeness modestly placed himself at the disposal of the circle in
which he found himself. Anna Pavlovna was obviously serving him up as a
treat to her guests. As a clever maitre d'hotel serves up as a specially
choice delicacy a piece of meat that no one who had seen it in the
kitchen would have cared to eat, so Anna Pavlovna served up to her
guests, first the vicomte and then the abbe, as peculiarly choice
morsels. The group about Mortemart immediately began discussing the
murder of the Duc d'Enghien. The vicomte said that the Duc d'Enghien had
perished by his own magnanimity, and that there were particular reasons
for Buonaparte's hatred of him.

"Ah, yes! Do tell us all about it, Vicomte," said Anna Pavlovna, with a
pleasant feeling that there was something a la Louis XV in the sound of
that sentence: "Contez nous cela, Vicomte."

The vicomte bowed and smiled courteously in token of his willingness to
comply. Anna Pavlovna arranged a group round him, inviting everyone to
listen to his tale.

"The vicomte knew the duc personally," whispered Anna Pavlovna to one of
the guests. "The vicomte is a wonderful raconteur," said she to another.
"How evidently he belongs to the best society," said she to a third; and
the vicomte was served up to the company in the choicest and most
advantageous style, like a well-garnished joint of roast beef on a hot
dish.

The vicomte wished to begin his story and gave a subtle smile.

"Come over here, Helene, dear," said Anna Pavlovna to the beautiful
young princess who was sitting some way off, the center of another
group.

The princess smiled. She rose with the same unchanging smile with which
she had first entered the room--the smile of a perfectly beautiful
woman. With a slight rustle of her white dress trimmed with moss and
ivy, with a gleam of white shoulders, glossy hair, and sparkling
diamonds, she passed between the men who made way for her, not looking
at any of them but smiling on all, as if graciously allowing each the
privilege of admiring her beautiful figure and shapely shoulders, back,
and bosom--which in the fashion of those days were very much exposed--
and she seemed to bring the glamour of a ballroom with her as she moved
toward Anna Pavlovna. Helene was so lovely that not only did she not
show any trace of coquetry, but on the contrary she even appeared shy of
her unquestionable and all too victorious beauty. She seemed to wish,
but to be unable, to diminish its effect.

"How lovely!" said everyone who saw her; and the vicomte lifted his
shoulders and dropped his eyes as if startled by something extraordinary
when she took her seat opposite and beamed upon him also with her
unchanging smile.

"Madame, I doubt my ability before such an audience," said he, smilingly
inclining his head.

The princess rested her bare round arm on a little table and considered
a reply unnecessary. She smilingly waited. All the time the story was
being told she sat upright, glancing now at her beautiful round arm,
altered in shape by its pressure on the table, now at her still more
beautiful bosom, on which she readjusted a diamond necklace. From time
to time she smoothed the folds of her dress, and whenever the story
produced an effect she glanced at Anna Pavlovna, at once adopted just
the expression she saw on the maid of honor's face, and again relapsed
into her radiant smile.

The little princess had also left the tea table and followed Helene.

"Wait a moment, I'll get my work.... Now then, what are you thinking
of?" she went on, turning to Prince Hippolyte. "Fetch me my workbag."

There was a general movement as the princess, smiling and talking
merrily to everyone at once, sat down and gaily arranged herself in her
seat.

"Now I am all right," she said, and asking the vicomte to begin, she
took up her work.

Prince Hippolyte, having brought the workbag, joined the circle and
moving a chair close to hers seated himself beside her.

Le charmant Hippolyte was surprising by his extraordinary resemblance to
his beautiful sister, but yet more by the fact that in spite of this
resemblance he was exceedingly ugly. His features were like his
sister's, but while in her case everything was lit up by a joyous, self-
satisfied, youthful, and constant smile of animation, and by the
wonderful classic beauty of her figure, his face on the contrary was
dulled by imbecility and a constant expression of sullen self-
confidence, while his body was thin and weak. His eyes, nose, and mouth
all seemed puckered into a vacant, wearied grimace, and his arms and
legs always fell into unnatural positions.

"It's not going to be a ghost story?" said he, sitting down beside the
princess and hastily adjusting his lorgnette, as if without this
instrument he could not begin to speak.

"Why no, my dear fellow," said the astonished narrator, shrugging his
shoulders.

"Because I hate ghost stories," said Prince Hippolyte in a tone which
showed that he only understood the meaning of his words after he had
uttered them.

He spoke with such self-confidence that his hearers could not be sure
whether what he said was very witty or very stupid. He was dressed in a
dark-green dress coat, knee breeches of the color of cuisse de nymphe
effrayee, as he called it, shoes, and silk stockings.

The vicomte told his tale very neatly. It was an anecdote, then current,
to the effect that the Duc d'Enghien had gone secretly to Paris to visit
Mademoiselle George; that at her house he came upon Bonaparte, who also
enjoyed the famous actress' favors, and that in his presence Napoleon
happened to fall into one of the fainting fits to which he was subject,
and was thus at the duc's mercy. The latter spared him, and this
magnanimity Bonaparte subsequently repaid by death.

The story was very pretty and interesting, especially at the point where
the rivals suddenly recognized one another; and the ladies looked
agitated.

"Charming!" said Anna Pavlovna with an inquiring glance at the little
princess.

"Charming!" whispered the little princess, sticking the needle into her
work as if to testify that the interest and fascination of the story
prevented her from going on with it.

The vicomte appreciated this silent praise and smiling gratefully
prepared to continue, but just then Anna Pavlovna, who had kept a
watchful eye on the young man who so alarmed her, noticed that he was
talking too loudly and vehemently with the abbe, so she hurried to the
rescue. Pierre had managed to start a conversation with the abbe about
the balance of power, and the latter, evidently interested by the young
man's simple-minded eagerness, was explaining his pet theory. Both were
talking and listening too eagerly and too naturally, which was why Anna
Pavlovna disapproved.

"The means are... the balance of power in Europe and the rights of the
people," the abbe was saying. "It is only necessary for one powerful
nation like Russia--barbaric as she is said to be--to place herself
disinterestedly at the head of an alliance having for its object the
maintenance of the balance of power of Europe, and it would save the
world!"

"But how are you to get that balance?" Pierre was beginning.

At that moment Anna Pavlovna came up and, looking severely at Pierre,
asked the Italian how he stood Russian climate. The Italian's face
instantly changed and assumed an offensively affected, sugary
expression, evidently habitual to him when conversing with women.

"I am so enchanted by the brilliancy of the wit and culture of the
society, more especially of the feminine society, in which I have had
the honor of being received, that I have not yet had time to think of
the climate," said he.

Not letting the abbe and Pierre escape, Anna Pavlovna, the more
conveniently to keep them under observation, brought them into the
larger circle.




CHAPTER IV

Just then another visitor entered the drawing room: Prince Andrew
Bolkonski, the little princess' husband. He was a very handsome young
man, of medium height, with firm, clearcut features. Everything about
him, from his weary, bored expression to his quiet, measured step,
offered a most striking contrast to his quiet, little wife. It was
evident that he not only knew everyone in the drawing room, but had
found them to be so tiresome that it wearied him to look at or listen to
them. And among all these faces that he found so tedious, none seemed to
bore him so much as that of his pretty wife. He turned away from her
with a grimace that distorted his handsome face, kissed Anna Pavlovna's
hand, and screwing up his eyes scanned the whole company.

"You are off to the war, Prince?" said Anna Pavlovna.

"General Kutuzov," said Bolkonski, speaking French and stressing the
last syllable of the general's name like a Frenchman, "has been pleased
to take me as an aide-de-camp...."

"And Lise, your wife?"

"She will go to the country."

"Are you not ashamed to deprive us of your charming wife?"

"Andre," said his wife, addressing her husband in the same coquettish
manner in which she spoke to other men, "the vicomte has been telling us
such a tale about Mademoiselle George and Buonaparte!"

Prince Andrew screwed up his eyes and turned away. Pierre, who from the
moment Prince Andrew entered the room had watched him with glad,
affectionate eyes, now came up and took his arm. Before he looked round
Prince Andrew frowned again, expressing his annoyance with whoever was
touching his arm, but when he saw Pierre's beaming face he gave him an
unexpectedly kind and pleasant smile.

"There now!... So you, too, are in the great world?" said he to Pierre.

"I knew you would be here," replied Pierre. "I will come to supper with
you. May I?" he added in a low voice so as not to disturb the vicomte
who was continuing his story.

"No, impossible!" said Prince Andrew, laughing and pressing Pierre's
hand to show that there was no need to ask the question. He wished to
say something more, but at that moment Prince Vasili and his daughter
got up to go and the two young men rose to let them pass.

"You must excuse me, dear Vicomte," said Prince Vasili to the Frenchman,
holding him down by the sleeve in a friendly way to prevent his rising.
"This unfortunate fete at the ambassador's deprives me of a pleasure,
and obliges me to interrupt you. I am very sorry to leave your
enchanting party," said he, turning to Anna Pavlovna.

His daughter, Princess Helene, passed between the chairs, lightly
holding up the folds of her dress, and the smile shone still more
radiantly on her beautiful face. Pierre gazed at her with rapturous,
almost frightened, eyes as she passed him.

"Very lovely," said Prince Andrew.

"Very," said Pierre.

In passing Prince Vasili seized Pierre's hand and said to Anna Pavlovna:
"Educate this bear for me! He has been staying with me a whole month and
this is the first time I have seen him in society. Nothing is so
necessary for a young man as the society of clever women."

Anna Pavlovna smiled and promised to take Pierre in hand. She knew his
father to be a connection of Prince Vasili's. The elderly lady who had
been sitting with the old aunt rose hurriedly and overtook Prince Vasili
in the anteroom. All the affectation of interest she had assumed had
left her kindly and tear-worn face and it now expressed only anxiety and
fear.

"How about my son Boris, Prince?" said she, hurrying after him into the
anteroom. "I can't remain any longer in Petersburg. Tell me what news I
may take back to my poor boy."

Although Prince Vasili listened reluctantly and not very politely to the
elderly lady, even betraying some impatience, she gave him an
ingratiating and appealing smile, and took his hand that he might not go
away.

"What would it cost you to say a word to the Emperor, and then he would
be transferred to the Guards at once?" said she.

"Believe me, Princess, I am ready to do all I can," answered Prince
Vasili, "but it is difficult for me to ask the Emperor. I should advise
you to appeal to Rumyantsev through Prince Golitsyn. That would be the
best way."

The elderly lady was a Princess Drubetskaya, belonging to one of the
best families in Russia, but she was poor, and having long been out of
society had lost her former influential connections. She had now come to
Petersburg to procure an appointment in the Guards for her only son. It
was, in fact, solely to meet Prince Vasili that she had obtained an
invitation to Anna Pavlovna's reception and had sat listening to the
vicomte's story. Prince Vasili's words frightened her, an embittered
look clouded her once handsome face, but only for a moment; then she
smiled again and clutched Prince Vasili's arm more tightly.

"Listen to me, Prince," said she. "I have never yet asked you for
anything and I never will again, nor have I ever reminded you of my
father's friendship for you; but now I entreat you for God's sake to do
this for my son--and I shall always regard you as a benefactor," she
added hurriedly. "No, don't be angry, but promise! I have asked Golitsyn
and he has refused. Be the kindhearted man you always were," she said,
trying to smile though tears were in her eyes.

"Papa, we shall be late," said Princess Helene, turning her beautiful
head and looking over her classically molded shoulder as she stood
waiting by the door.

Influence in society, however, is a capital which has to be economized
if it is to last. Prince Vasili knew this, and having once realized that
if he asked on behalf of all who begged of him, he would soon be unable
to ask for himself, he became chary of using his influence. But in
Princess Drubetskaya's case he felt, after her second appeal, something
like qualms of conscience. She had reminded him of what was quite true;
he had been indebted to her father for the first steps in his career.
Moreover, he could see by her manners that she was one of those women--
mostly mothers--who, having once made up their minds, will not rest
until they have gained their end, and are prepared if necessary to go on
insisting day after day and hour after hour, and even to make scenes.
This last consideration moved him.

"My dear Anna Mikhaylovna," said he with his usual familiarity and
weariness of tone, "it is almost impossible for me to do what you ask;
but to prove my devotion to you and how I respect your father's memory,
I will do the impossible--your son shall be transferred to the Guards.
Here is my hand on it. Are you satisfied?"

"My dear benefactor! This is what I expected from you--I knew your
kindness!" He turned to go.

"Wait--just a word! When he has been transferred to the Guards..." she
faltered. "You are on good terms with Michael Ilarionovich Kutuzov...
recommend Boris to him as adjutant! Then I shall be at rest, and
then..."

Prince Vasili smiled.

"No, I won't promise that. You don't know how Kutuzov is pestered since
his appointment as Commander in Chief. He told me himself that all the
Moscow ladies have conspired to give him all their sons as adjutants."

"No, but do promise! I won't let you go! My dear benefactor..."

"Papa," said his beautiful daughter in the same tone as before, "we
shall be late."

"Well, au revoir! Good-bye! You hear her?"

"Then tomorrow you will speak to the Emperor?"

"Certainly; but about Kutuzov, I don't promise."

"Do promise, do promise, Vasili!" cried Anna Mikhaylovna as he went,
with the smile of a coquettish girl, which at one time probably came
naturally to her, but was now very ill-suited to her careworn face.

Apparently she had forgotten her age and by force of habit employed all
the old feminine arts. But as soon as the prince had gone her face
resumed its former cold, artificial expression. She returned to the
group where the vicomte was still talking, and again pretended to
listen, while waiting till it would be time to leave. Her task was
accomplished.




CHAPTER V

"And what do you think of this latest comedy, the coronation at Milan?"
asked Anna Pavlovna, "and of the comedy of the people of Genoa and Lucca
laying their petitions before Monsieur Buonaparte, and Monsieur
Buonaparte sitting on a throne and granting the petitions of the
nations? Adorable! It is enough to make one's head whirl! It is as if
the whole world had gone crazy."

Prince Andrew looked Anna Pavlovna straight in the face with a sarcastic
smile.

"'Dieu me la donne, gare a qui la touche!' * They say he was very fine
when he said that," he remarked, repeating the words in Italian: "'Dio
mi l'ha dato. Guai a chi la tocchi!'"


* God has given it to me, let him who touches it beware!

"I hope this will prove the last drop that will make the glass run
over," Anna Pavlovna continued. "The sovereigns will not be able to
endure this man who is a menace to everything."

"The sovereigns? I do not speak of Russia," said the vicomte, polite but
hopeless: "The sovereigns, madame... What have they done for Louis XVII,
for the Queen, or for Madame Elizabeth? Nothing!" and he became more
animated. "And believe me, they are reaping the reward of their betrayal
of the Bourbon cause. The sovereigns! Why, they are sending ambassadors
to compliment the usurper."

And sighing disdainfully, he again changed his position.

Prince Hippolyte, who had been gazing at the vicomte for some time
through his lorgnette, suddenly turned completely round toward the
little princess, and having asked for a needle began tracing the Conde
coat of arms on the table. He explained this to her with as much gravity
as if she had asked him to do it.

"Baton de gueules, engrele de gueules d'azur--maison Conde," said he.

The princess listened, smiling.

"If Buonaparte remains on the throne of France a year longer," the
vicomte continued, with the air of a man who, in a matter with which he
is better acquainted than anyone else, does not listen to others but
follows the current of his own thoughts, "things will have gone too far.
By intrigues, violence, exile, and executions, French society--I mean
good French society--will have been forever destroyed, and then..."

He shrugged his shoulders and spread out his hands. Pierre wished to
make a remark, for the conversation interested him, but Anna Pavlovna,
who had him under observation, interrupted:

"The Emperor Alexander," said she, with the melancholy which always
accompanied any reference of hers to the Imperial family, "has declared
that he will leave it to the French people themselves to choose their
own form of government; and I believe that once free from the usurper,
the whole nation will certainly throw itself into the arms of its
rightful king," she concluded, trying to be amiable to the royalist
emigrant.

"That is doubtful," said Prince Andrew. "Monsieur le Vicomte quite
rightly supposes that matters have already gone too far. I think it will
be difficult to return to the old regime."

"From what I have heard," said Pierre, blushing and breaking into the
conversation, "almost all the aristocracy has already gone over to
Bonaparte's side."

"It is the Buonapartists who say that," replied the vicomte without
looking at Pierre. "At the present time it is difficult to know the real
state of French public opinion."

"Bonaparte has said so," remarked Prince Andrew with a sarcastic smile.

It was evident that he did not like the vicomte and was aiming his
remarks at him, though without looking at him.

"'I showed them the path to glory, but they did not follow it,'" Prince
Andrew continued after a short silence, again quoting Napoleon's words.
"'I opened my antechambers and they crowded in.' I do not know how far
he was justified in saying so."

"Not in the least," replied the vicomte. "After the murder of the duc
even the most partial ceased to regard him as a hero. If to some
people," he went on, turning to Anna Pavlovna, "he ever was a hero,
after the murder of the duc there was one martyr more in heaven and one
hero less on earth."

Before Anna Pavlovna and the others had time to smile their appreciation
of the vicomte's epigram, Pierre again broke into the conversation, and
though Anna Pavlovna felt sure he would say something inappropriate, she
was unable to stop him.

"The execution of the Duc d'Enghien," declared Monsieur Pierre, "was a
political necessity, and it seems to me that Napoleon showed greatness
of soul by not fearing to take on himself the whole responsibility of
that deed."

"Dieu! Mon Dieu!" muttered Anna Pavlovna in a terrified whisper.

"What, Monsieur Pierre... Do you consider that assassination shows
greatness of soul?" said the little princess, smiling and drawing her
work nearer to her.

"Oh! Oh!" exclaimed several voices.

"Capital!" said Prince Hippolyte in English, and began slapping his knee
with the palm of his hand.

The vicomte merely shrugged his shoulders. Pierre looked solemnly at his
audience over his spectacles and continued.

"I say so," he continued desperately, "because the Bourbons fled from
the Revolution leaving the people to anarchy, and Napoleon alone
understood the Revolution and quelled it, and so for the general good,
he could not stop short for the sake of one man's life."

"Won't you come over to the other table?" suggested Anna Pavlovna.

But Pierre continued his speech without heeding her.

"No," cried he, becoming more and more eager, "Napoleon is great because
he rose superior to the Revolution, suppressed its abuses, preserved all
that was good in it--equality of citizenship and freedom of speech and
of the press--and only for that reason did he obtain power."

"Yes, if having obtained power, without availing himself of it to commit
murder he had restored it to the rightful king, I should have called him
a great man," remarked the vicomte.

"He could not do that. The people only gave him power that he might rid
them of the Bourbons and because they saw that he was a great man. The
Revolution was a grand thing!" continued Monsieur Pierre, betraying by
this desperate and provocative proposition his extreme youth and his
wish to express all that was in his mind.

"What? Revolution and regicide a grand thing?... Well, after that... But
won't you come to this other table?" repeated Anna Pavlovna.

"Rousseau's Contrat Social," said the vicomte with a tolerant smile.

"I am not speaking of regicide, I am speaking about ideas."

"Yes: ideas of robbery, murder, and regicide," again interjected an
ironical voice.

"Those were extremes, no doubt, but they are not what is most important.
What is important are the rights of man, emancipation from prejudices,
and equality of citizenship, and all these ideas Napoleon has retained
in full force."

"Liberty and equality," said the vicomte contemptuously, as if at last
deciding seriously to prove to this youth how foolish his words were,
"high-sounding words which have long been discredited. Who does not love
liberty and equality? Even our Saviour preached liberty and equality.
Have people since the Revolution become happier? On the contrary. We
wanted liberty, but Buonaparte has destroyed it."

Prince Andrew kept looking with an amused smile from Pierre to the
vicomte and from the vicomte to their hostess. In the first moment of
Pierre's outburst Anna Pavlovna, despite her social experience, was
horror-struck. But when she saw that Pierre's sacrilegious words had not
exasperated the vicomte, and had convinced herself that it was
impossible to stop him, she rallied her forces and joined the vicomte in
a vigorous attack on the orator.

"But, my dear Monsieur Pierre," said she, "how do you explain the fact
of a great man executing a duc--or even an ordinary man who--is innocent
and untried?"

"I should like," said the vicomte, "to ask how monsieur explains the
18th Brumaire; was not that an imposture? It was a swindle, and not at
all like the conduct of a great man!"

"And the prisoners he killed in Africa? That was horrible!" said the
little princess, shrugging her shoulders.

"He's a low fellow, say what you will," remarked Prince Hippolyte.

Pierre, not knowing whom to answer, looked at them all and smiled. His
smile was unlike the half-smile of other people. When he smiled, his
grave, even rather gloomy, look was instantaneously replaced by another-
-a childlike, kindly, even rather silly look, which seemed to ask
forgiveness.

The vicomte who was meeting him for the first time saw clearly that this
young Jacobin was not so terrible as his words suggested. All were
silent.

"How do you expect him to answer you all at once?" said Prince Andrew.
"Besides, in the actions of a statesman one has to distinguish between
his acts as a private person, as a general, and as an emperor. So it
seems to me."

"Yes, yes, of course!" Pierre chimed in, pleased at the arrival of this
reinforcement.

"One must admit," continued Prince Andrew, "that Napoleon as a man was
great on the bridge of Arcola, and in the hospital at Jaffa where he
gave his hand to the plague-stricken; but... but there are other acts
which it is difficult to justify."

Prince Andrew, who had evidently wished to tone down the awkwardness of
Pierre's remarks, rose and made a sign to his wife that it was time to
go.

Suddenly Prince Hippolyte started up making signs to everyone to attend,
and asking them all to be seated began:

"I was told a charming Moscow story today and must treat you to it.
Excuse me, Vicomte--I must tell it in Russian or the point will be
lost...." And Prince Hippolyte began to tell his story in such Russian
as a Frenchman would speak after spending about a year in Russia.
Everyone waited, so emphatically and eagerly did he demand their
attention to his story.

"There is in Moscow a lady, une dame, and she is very stingy. She must
have two footmen behind her carriage, and very big ones. That was her
taste. And she had a lady's maid, also big. She said..."

Here Prince Hippolyte paused, evidently collecting his ideas with
difficulty.

"She said... Oh yes! She said, 'Girl,' to the maid, 'put on a livery,
get up behind the carriage, and come with me while I make some calls.'"

Here Prince Hippolyte spluttered and burst out laughing long before his
audience, which produced an effect unfavorable to the narrator. Several
persons, among them the elderly lady and Anna Pavlovna, did however
smile.

"She went. Suddenly there was a great wind. The girl lost her hat and
her long hair came down...." Here he could contain himself no longer and
went on, between gasps of laughter: "And the whole world knew...."

And so the anecdote ended. Though it was unintelligible why he had told
it, or why it had to be told in Russian, still Anna Pavlovna and the
others appreciated Prince Hippolyte's social tact in so agreeably ending
Pierre's unpleasant and unamiable outburst. After the anecdote the
conversation broke up into insignificant small talk about the last and
next balls, about theatricals, and who would meet whom, and when and
where.




CHAPTER VI

Having thanked Anna Pavlovna for her charming soiree, the guests began
to take their leave.

Pierre was ungainly. Stout, about the average height, broad, with huge
red hands; he did not know, as the saying is, how to enter a drawing
room and still less how to leave one; that is, how to say something
particularly agreeable before going away. Besides this he was absent-
minded. When he rose to go, he took up instead of his own, the general's
three-cornered hat, and held it, pulling at the plume, till the general
asked him to restore it. All his absent-mindedness and inability to
enter a room and converse in it was, however, redeemed by his kindly,
simple, and modest expression. Anna Pavlovna turned toward him and, with
a Christian mildness that expressed forgiveness of his indiscretion,
nodded and said: "I hope to see you again, but I also hope you will
change your opinions, my dear Monsieur Pierre."

When she said this, he did not reply and only bowed, but again everybody
saw his smile, which said nothing, unless perhaps, "Opinions are
opinions, but you see what a capital, good-natured fellow I am." And
everyone, including Anna Pavlovna, felt this.

Prince Andrew had gone out into the hall, and, turning his shoulders to
the footman who was helping him on with his cloak, listened
indifferently to his wife's chatter with Prince Hippolyte who had also
come into the hall. Prince Hippolyte stood close to the pretty, pregnant
princess, and stared fixedly at her through his eyeglass.

"Go in, Annette, or you will catch cold," said the little princess,
taking leave of Anna Pavlovna. "It is settled," she added in a low
voice.

Anna Pavlovna had already managed to speak to Lise about the match she
contemplated between Anatole and the little princess' sister-in-law.

"I rely on you, my dear," said Anna Pavlovna, also in a low tone. "Write
to her and let me know how her father looks at the matter. Au revoir!"--
and she left the hall.

Prince Hippolyte approached the little princess and, bending his face
close to her, began to whisper something.

Two footmen, the princess' and his own, stood holding a shawl and a
cloak, waiting for the conversation to finish. They listened to the
French sentences which to them were meaningless, with an air of
understanding but not wishing to appear to do so. The princess as usual
spoke smilingly and listened with a laugh.

"I am very glad I did not go to the ambassador's," said Prince Hippolyte
"-so dull-. It has been a delightful evening, has it not? Delightful!"

"They say the ball will be very good," replied the princess, drawing up
her downy little lip. "All the pretty women in society will be there."

"Not all, for you will not be there; not all," said Prince Hippolyte
smiling joyfully; and snatching the shawl from the footman, whom he even
pushed aside, he began wrapping it round the princess. Either from
awkwardness or intentionally (no one could have said which) after the
shawl had been adjusted he kept his arm around her for a long time, as
though embracing her.

Still smiling, she gracefully moved away, turning and glancing at her
husband. Prince Andrew's eyes were closed, so weary and sleepy did he
seem.

"Are you ready?" he asked his wife, looking past her.

Prince Hippolyte hurriedly put on his cloak, which in the latest fashion
reached to his very heels, and, stumbling in it, ran out into the porch
following the princess, whom a footman was helping into the carriage.

"Princesse, au revoir," cried he, stumbling with his tongue as well as
with his feet.

The princess, picking up her dress, was taking her seat in the dark
carriage, her husband was adjusting his saber; Prince Hippolyte, under
pretense of helping, was in everyone's way.

"Allow me, sir," said Prince Andrew in Russian in a cold, disagreeable
tone to Prince Hippolyte who was blocking his path.

"I am expecting you, Pierre," said the same voice, but gently and
affectionately.

The postilion started, the carriage wheels rattled. Prince Hippolyte
laughed spasmodically as he stood in the porch waiting for the vicomte
whom he had promised to take home.

"Well, mon cher," said the vicomte, having seated himself beside
Hippolyte in the carriage, "your little princess is very nice, very nice
indeed, quite French," and he kissed the tips of his fingers. Hippolyte
burst out laughing.

"Do you know, you are a terrible chap for all your innocent airs,"
continued the vicomte. "I pity the poor husband, that little officer who
gives himself the airs of a monarch."

Hippolyte spluttered again, and amid his laughter said, "And you were
saying that the Russian ladies are not equal to the French? One has to
know how to deal with them."

Pierre reaching the house first went into Prince Andrew's study like one
quite at home, and from habit immediately lay down on the sofa, took
from the shelf the first book that came to his hand (it was Caesar's
Commentaries), and resting on his elbow, began reading it in the middle.

"What have you done to Mlle Scherer? She will be quite ill now," said
Prince Andrew, as he entered the study, rubbing his small white hands.

Pierre turned his whole body, making the sofa creak. He lifted his eager
face to Prince Andrew, smiled, and waved his hand.

"That abbe is very interesting but he does not see the thing in the
right light.... In my opinion perpetual peace is possible but--I do not
know how to express it... not by a balance of political power...."

It was evident that Prince Andrew was not interested in such abstract
conversation.

"One can't everywhere say all one thinks, mon cher. Well, have you at
last decided on anything? Are you going to be a guardsman or a
diplomatist?" asked Prince Andrew after a momentary silence.

Pierre sat up on the sofa, with his legs tucked under him.

"Really, I don't yet know. I don't like either the one or the other."

"But you must decide on something! Your father expects it."

Pierre at the age of ten had been sent abroad with an abbe as tutor, and
had remained away till he was twenty. When he returned to Moscow his
father dismissed the abbe and said to the young man, "Now go to
Petersburg, look round, and choose your profession. I will agree to
anything. Here is a letter to Prince Vasili, and here is money. Write to
me all about it, and I will help you in everything." Pierre had already
been choosing a career for three months, and had not decided on
anything. It was about this choice that Prince Andrew was speaking.
Pierre rubbed his forehead.

"But he must be a Freemason," said he, referring to the abbe whom he had
met that evening.

"That is all nonsense." Prince Andrew again interrupted him, "let us
talk business. Have you been to the Horse Guards?"

"No, I have not; but this is what I have been thinking and wanted to
tell you. There is a war now against Napoleon. If it were a war for
freedom I could understand it and should be the first to enter the army;
but to help England and Austria against the greatest man in the world is
not right."

Prince Andrew only shrugged his shoulders at Pierre's childish words. He
put on the air of one who finds it impossible to reply to such nonsense,
but it would in fact have been difficult to give any other answer than
the one Prince Andrew gave to this naive question.

"If no one fought except on his own conviction, there would be no wars,"
he said.

"And that would be splendid," said Pierre.

Prince Andrew smiled ironically.

"Very likely it would be splendid, but it will never come about..."

"Well, why are you going to the war?" asked Pierre.

"What for? I don't know. I must. Besides that I am going..." He paused.
"I am going because the life I am leading here does not suit me!"




CHAPTER VII

The rustle of a woman's dress was heard in the next room. Prince Andrew
shook himself as if waking up, and his face assumed the look it had had
in Anna Pavlovna's drawing room. Pierre removed his feet from the sofa.
The princess came in. She had changed her gown for a house dress as
fresh and elegant as the other. Prince Andrew rose and politely placed a
chair for her.

"How is it," she began, as usual in French, settling down briskly and
fussily in the easy chair, "how is it Annette never got married? How
stupid you men all are not to have married her! Excuse me for saying so,
but you have no sense about women. What an argumentative fellow you are,
Monsieur Pierre!"

"And I am still arguing with your husband. I can't understand why he
wants to go to the war," replied Pierre, addressing the princess with
none of the embarrassment so commonly shown by young men in their
intercourse with young women.

The princess started. Evidently Pierre's words touched her to the quick.

"Ah, that is just what I tell him!" said she. "I don't understand it; I
don't in the least understand why men can't live without wars. How is it
that we women don't want anything of the kind, don't need it? Now you
shall judge between us. I always tell him: Here he is Uncle's aide-de-
camp, a most brilliant position. He is so well known, so much
appreciated by everyone. The other day at the Apraksins' I heard a lady
asking, 'Is that the famous Prince Andrew?' I did indeed." She laughed.
"He is so well received everywhere. He might easily become aide-de-camp
to the Emperor. You know the Emperor spoke to him most graciously.
Annette and I were speaking of how to arrange it. What do you think?"

Pierre looked at his friend and, noticing that he did not like the
conversation, gave no reply.

"When are you starting?" he asked.

"Oh, don't speak of his going, don't! I won't hear it spoken of," said
the princess in the same petulantly playful tone in which she had spoken
to Hippolyte in the drawing room and which was so plainly ill-suited to
the family circle of which Pierre was almost a member. "Today when I
remembered that all these delightful associations must be broken off...
and then you know, Andre..." (she looked significantly at her husband)
"I'm afraid, I'm afraid!" she whispered, and a shudder ran down her
back.

Her husband looked at her as if surprised to notice that someone besides
Pierre and himself was in the room, and addressed her in a tone of
frigid politeness.

"What is it you are afraid of, Lise? I don't understand," said he.

"There, what egotists men all are: all, all egotists! Just for a whim of
his own, goodness only knows why, he leaves me and locks me up alone in
the country."

"With my father and sister, remember," said Prince Andrew gently.

"Alone all the same, without my friends.... And he expects me not to be
afraid."

Her tone was now querulous and her lip drawn up, giving her not a
joyful, but an animal, squirrel-like expression. She paused as if she
felt it indecorous to speak of her pregnancy before Pierre, though the
gist of the matter lay in that.

"I still can't understand what you are afraid of," said Prince Andrew
slowly, not taking his eyes off his wife.

The princess blushed, and raised her arms with a gesture of despair.

"No, Andrew, I must say you have changed. Oh, how you have..."

"Your doctor tells you to go to bed earlier," said Prince Andrew. "You
had better go."

The princess said nothing, but suddenly her short downy lip quivered.
Prince Andrew rose, shrugged his shoulders, and walked about the room.

Pierre looked over his spectacles with naive surprise, now at him and
now at her, moved as if about to rise too, but changed his mind.

"Why should I mind Monsieur Pierre being here?" exclaimed the little
princess suddenly, her pretty face all at once distorted by a tearful
grimace. "I have long wanted to ask you, Andrew, why you have changed so
to me? What have I done to you? You are going to the war and have no
pity for me. Why is it?"

"Lise!" was all Prince Andrew said. But that one word expressed an
entreaty, a threat, and above all conviction that she would herself
regret her words. But she went on hurriedly:

"You treat me like an invalid or a child. I see it all! Did you behave
like that six months ago?"

"Lise, I beg you to desist," said Prince Andrew still more emphatically.

Pierre, who had been growing more and more agitated as he listened to
all this, rose and approached the princess. He seemed unable to bear the
sight of tears and was ready to cry himself.

"Calm yourself, Princess! It seems so to you because... I assure you I
myself have experienced... and so... because... No, excuse me! An
outsider is out of place here... No, don't distress yourself... Good-
bye!"

Prince Andrew caught him by the hand.

"No, wait, Pierre! The princess is too kind to wish to deprive me of the
pleasure of spending the evening with you."

"No, he thinks only of himself," muttered the princess without
restraining her angry tears.

"Lise!" said Prince Andrew dryly, raising his voice to the pitch which
indicates that patience is exhausted.

Suddenly the angry, squirrel-like expression of the princess' pretty
face changed into a winning and piteous look of fear. Her beautiful eyes
glanced askance at her husband's face, and her own assumed the timid,
deprecating expression of a dog when it rapidly but feebly wags its
drooping tail.

"Mon Dieu, mon Dieu!" she muttered, and lifting her dress with one hand
she went up to her husband and kissed him on the forehead.

"Good night, Lise," said he, rising and courteously kissing her hand as
he would have done to a stranger.




CHAPTER VIII

The friends were silent. Neither cared to begin talking. Pierre
continually glanced at Prince Andrew; Prince Andrew rubbed his forehead
with his small hand.

"Let us go and have supper," he said with a sigh, going to the door.

They entered the elegant, newly decorated, and luxurious dining room.
Everything from the table napkins to the silver, china, and glass bore
that imprint of newness found in the households of the newly married.
Halfway through supper Prince Andrew leaned his elbows on the table and,
with a look of nervous agitation such as Pierre had never before seen on
his face, began to talk--as one who has long had something on his mind
and suddenly determines to speak out.

"Never, never marry, my dear fellow! That's my advice: never marry till
you can say to yourself that you have done all you are capable of, and
until you have ceased to love the woman of your choice and have seen her
plainly as she is, or else you will make a cruel and irrevocable
mistake. Marry when you are old and good for nothing--or all that is
good and noble in you will be lost. It will all be wasted on trifles.
Yes! Yes! Yes! Don't look at me with such surprise. If you marry
expecting anything from yourself in the future, you will feel at every
step that for you all is ended, all is closed except the drawing room,
where you will be ranged side by side with a court lackey and an
idiot!... But what's the good?..." and he waved his arm.

Pierre took off his spectacles, which made his face seem different and
the good-natured expression still more apparent, and gazed at his friend
in amazement.

"My wife," continued Prince Andrew, "is an excellent woman, one of those
rare women with whom a man's honor is safe; but, O God, what would I not
give now to be unmarried! You are the first and only one to whom I
mention this, because I like you."

As he said this Prince Andrew was less than ever like that Bolkonski who
had lolled in Anna Pavlovna's easy chairs and with half-closed eyes had
uttered French phrases between his teeth. Every muscle of his thin face
was now quivering with nervous excitement; his eyes, in which the fire
of life had seemed extinguished, now flashed with brilliant light. It
was evident that the more lifeless he seemed at ordinary times, the more
impassioned he became in these moments of almost morbid irritation.

"You don't understand why I say this," he continued, "but it is the
whole story of life. You talk of Bonaparte and his career," said he
(though Pierre had not mentioned Bonaparte), "but Bonaparte when he
worked went step by step toward his goal. He was free, he had nothing
but his aim to consider, and he reached it. But tie yourself up with a
woman and, like a chained convict, you lose all freedom! And all you
have of hope and strength merely weighs you down and torments you with
regret. Drawing rooms, gossip, balls, vanity, and triviality--these are
the enchanted circle I cannot escape from. I am now going to the war,
the greatest war there ever was, and I know nothing and am fit for
nothing. I am very amiable and have a caustic wit," continued Prince
Andrew, "and at Anna Pavlovna's they listen to me. And that stupid set
without whom my wife cannot exist, and those women... If you only knew
what those society women are, and women in general! My father is right.
Selfish, vain, stupid, trivial in everything--that's what women are when
you see them in their true colors! When you meet them in society it
seems as if there were something in them, but there's nothing, nothing,
nothing! No, don't marry, my dear fellow; don't marry!" concluded Prince
Andrew.

"It seems funny to me," said Pierre, "that you, you should consider
yourself incapable and your life a spoiled life. You have everything
before you, everything. And you..."

He did not finish his sentence, but his tone showed how highly he
thought of his friend and how much he expected of him in the future.

"How can he talk like that?" thought Pierre. He considered his friend a
model of perfection because Prince Andrew possessed in the highest
degree just the very qualities Pierre lacked, and which might be best
described as strength of will. Pierre was always astonished at Prince
Andrew's calm manner of treating everybody, his extraordinary memory,
his extensive reading (he had read everything, knew everything, and had
an opinion about everything), but above all at his capacity for work and
study. And if Pierre was often struck by Andrew's lack of capacity for
philosophical meditation (to which he himself was particularly
addicted), he regarded even this not as a defect but as a sign of
strength.

Even in the best, most friendly and simplest relations of life, praise
and commendation are essential, just as grease is necessary to wheels
that they may run smoothly.

"My part is played out," said Prince Andrew. "What's the use of talking
about me? Let us talk about you," he added after a silence, smiling at
his reassuring thoughts.

That smile was immediately reflected on Pierre's face.

"But what is there to say about me?" said Pierre, his face relaxing into
a careless, merry smile. "What am I? An illegitimate son!" He suddenly
blushed crimson, and it was plain that he had made a great effort to say
this. "Without a name and without means... And it really..." But he did
not say what "it really" was. "For the present I am free and am all
right. Only I haven't the least idea what I am to do; I wanted to
consult you seriously."

Prince Andrew looked kindly at him, yet his glance--friendly and
affectionate as it was--expressed a sense of his own superiority.

"I am fond of you, especially as you are the one live man among our
whole set. Yes, you're all right! Choose what you will; it's all the
same. You'll be all right anywhere. But look here: give up visiting
those Kuragins and leading that sort of life. It suits you so badly--all
this debauchery, dissipation, and the rest of it!"

"What would you have, my dear fellow?" answered Pierre, shrugging his
shoulders. "Women, my dear fellow; women!"

"I don't understand it," replied Prince Andrew. "Women who are comme il
faut, that's a different matter; but the Kuragins' set of women, 'women
and wine' I don't understand!"

Pierre was staying at Prince Vasili Kuragin's and sharing the dissipated
life of his son Anatole, the son whom they were planning to reform by
marrying him to Prince Andrew's sister.

"Do you know?" said Pierre, as if suddenly struck by a happy thought,
"seriously, I have long been thinking of it.... Leading such a life I
can't decide or think properly about anything. One's head aches, and one
spends all one's money. He asked me for tonight, but I won't go."

"You give me your word of honor not to go?"

"On my honor!"




CHAPTER IX

It was past one o'clock when Pierre left his friend. It was a cloudless,
northern, summer night. Pierre took an open cab intending to drive
straight home. But the nearer he drew to the house the more he felt the
impossibility of going to sleep on such a night. It was light enough to
see a long way in the deserted street and it seemed more like morning or
evening than night. On the way Pierre remembered that Anatole Kuragin
was expecting the usual set for cards that evening, after which there
was generally a drinking bout, finishing with visits of a kind Pierre
was very fond of.

"I should like to go to Kuragin's," thought he.

But he immediately recalled his promise to Prince Andrew not to go
there. Then, as happens to people of weak character, he desired so
passionately once more to enjoy that dissipation he was so accustomed to
that he decided to go. The thought immediately occurred to him that his
promise to Prince Andrew was of no account, because before he gave it he
had already promised Prince Anatole to come to his gathering; "besides,"
thought he, "all such 'words of honor' are conventional things with no
definite meaning, especially if one considers that by tomorrow one may
be dead, or something so extraordinary may happen to one that honor and
dishonor will be all the same!" Pierre often indulged in reflections of
this sort, nullifying all his decisions and intentions. He went to
Kuragin's.

Reaching the large house near the Horse Guards' barracks, in which
Anatole lived, Pierre entered the lighted porch, ascended the stairs,
and went in at the open door. There was no one in the anteroom; empty
bottles, cloaks, and overshoes were lying about; there was a smell of
alcohol, and sounds of voices and shouting in the distance.

Cards and supper were over, but the visitors had not yet dispersed.
Pierre threw off his cloak and entered the first room, in which were the
remains of supper. A footman, thinking no one saw him, was drinking on
the sly what was left in the glasses. From the third room came sounds of
laughter, the shouting of familiar voices, the growling of a bear, and
general commotion. Some eight or nine young men were crowding anxiously
round an open window. Three others were romping with a young bear, one
pulling him by the chain and trying to set him at the others.

"I bet a hundred on Stevens!" shouted one.

"Mind, no holding on!" cried another.

"I bet on Dolokhov!" cried a third. "Kuragin, you part our hands."

"There, leave Bruin alone; here's a bet on."

"At one draught, or he loses!" shouted a fourth.

"Jacob, bring a bottle!" shouted the host, a tall, handsome fellow who
stood in the midst of the group, without a coat, and with his fine linen
shirt unfastened in front. "Wait a bit, you fellows.... Here is Petya!
Good man!" cried he, addressing Pierre.

Another voice, from a man of medium height with clear blue eyes,
particularly striking among all these drunken voices by its sober ring,
cried from the window: "Come here; part the bets!" This was Dolokhov, an
officer of the Semenov regiment, a notorious gambler and duelist, who
was living with Anatole. Pierre smiled, looking about him merrily.

"I don't understand. What's it all about?"

"Wait a bit, he is not drunk yet! A bottle here," said Anatole, taking a
glass from the table he went up to Pierre.

"First of all you must drink!"

Pierre drank one glass after another, looking from under his brows at
the tipsy guests who were again crowding round the window, and listening
to their chatter. Anatole kept on refilling Pierre's glass while
explaining that Dolokhov was betting with Stevens, an English naval
officer, that he would drink a bottle of rum sitting on the outer ledge
of the third floor window with his legs hanging out.

"Go on, you must drink it all," said Anatole, giving Pierre the last
glass, "or I won't let you go!"

"No, I won't," said Pierre, pushing Anatole aside, and he went up to the
window.

Dolokhov was holding the Englishman's hand and clearly and distinctly
repeating the terms of the bet, addressing himself particularly to
Anatole and Pierre.

Dolokhov was of medium height, with curly hair and light-blue eyes. He
was about twenty-five. Like all infantry officers he wore no mustache,
so that his mouth, the most striking feature of his face, was clearly
seen. The lines of that mouth were remarkably finely curved. The middle
of the upper lip formed a sharp wedge and closed firmly on the firm
lower one, and something like two distinct smiles played continually
round the two corners of the mouth; this, together with the resolute,
insolent intelligence of his eyes, produced an effect which made it
impossible not to notice his face. Dolokhov was a man of small means and
no connections. Yet, though Anatole spent tens of thousands of rubles,
Dolokhov lived with him and had placed himself on such a footing that
all who knew them, including Anatole himself, respected him more than
they did Anatole. Dolokhov could play all games and nearly always won.
However much he drank, he never lost his clearheadedness. Both Kuragin
and Dolokhov were at that time notorious among the rakes and scapegraces
of Petersburg.

The bottle of rum was brought. The window frame which prevented anyone
from sitting on the outer sill was being forced out by two footmen, who
were evidently flurried and intimidated by the directions and shouts of
the gentlemen around.

Anatole with his swaggering air strode up to the window. He wanted to
smash something. Pushing away the footmen he tugged at the frame, but
could not move it. He smashed a pane.

"You have a try, Hercules," said he, turning to Pierre.

Pierre seized the crossbeam, tugged, and wrenched the oak frame out with
a crash.

"Take it right out, or they'll think I'm holding on," said Dolokhov.

"Is the Englishman bragging?... Eh? Is it all right?" said Anatole.

"First-rate," said Pierre, looking at Dolokhov, who with a bottle of rum
in his hand was approaching the window, from which the light of the sky,
the dawn merging with the afterglow of sunset, was visible.

Dolokhov, the bottle of rum still in his hand, jumped onto the window
sill. "Listen!" cried he, standing there and addressing those in the
room. All were silent.

"I bet fifty imperials"--he spoke French that the Englishman might
understand him, but he did not speak it very well--"I bet fifty
imperials... or do you wish to make it a hundred?" added he, addressing
the Englishman.

"No, fifty," replied the latter.

"All right. Fifty imperials... that I will drink a whole bottle of rum
without taking it from my mouth, sitting outside the window on this
spot" (he stooped and pointed to the sloping ledge outside the window)
"and without holding on to anything. Is that right?"

"Quite right," said the Englishman.

Anatole turned to the Englishman and taking him by one of the buttons of
his coat and looking down at him--the Englishman was short--began
repeating the terms of the wager to him in English.

"Wait!" cried Dolokhov, hammering with the bottle on the window sill to
attract attention. "Wait a bit, Kuragin. Listen! If anyone else does the
same, I will pay him a hundred imperials. Do you understand?"

The Englishman nodded, but gave no indication whether he intended to
accept this challenge or not. Anatole did not release him, and though he
kept nodding to show that he understood, Anatole went on translating
Dolokhov's words into English. A thin young lad, an hussar of the Life
Guards, who had been losing that evening, climbed on the window sill,
leaned over, and looked down.

"Oh! Oh! Oh!" he muttered, looking down from the window at the stones of
the pavement.

"Shut up!" cried Dolokhov, pushing him away from the window. The lad
jumped awkwardly back into the room, tripping over his spurs.

Placing the bottle on the window sill where he could reach it easily,
Dolokhov climbed carefully and slowly through the window and lowered his
legs. Pressing against both sides of the window, he adjusted himself on
his seat, lowered his hands, moved a little to the right and then to the
left, and took up the bottle. Anatole brought two candles and placed
them on the window sill, though it was already quite light. Dolokhov's
back in his white shirt, and his curly head, were lit up from both
sides. Everyone crowded to the window, the Englishman in front. Pierre
stood smiling but silent. One man, older than the others present,
suddenly pushed forward with a scared and angry look and wanted to seize
hold of Dolokhov's shirt.

"I say, this is folly! He'll be killed," said this more sensible man.

Anatole stopped him.

"Don't touch him! You'll startle him and then he'll be killed. Eh?...
What then?... Eh?"

Dolokhov turned round and, again holding on with both hands, arranged
himself on his seat.

"If anyone comes meddling again," said he, emitting the words separately
through his thin compressed lips, "I will throw him down there. Now
then!"

Saying this he again turned round, dropped his hands, took the bottle
and lifted it to his lips, threw back his head, and raised his free hand
to balance himself. One of the footmen who had stooped to pick up some
broken glass remained in that position without taking his eyes from the
window and from Dolokhov's back. Anatole stood erect with staring eyes.
The Englishman looked on sideways, pursing up his lips. The man who had
wished to stop the affair ran to a corner of the room and threw himself
on a sofa with his face to the wall. Pierre hid his face, from which a
faint smile forgot to fade though his features now expressed horror and
fear. All were still. Pierre took his hands from his eyes. Dolokhov
still sat in the same position, only his head was thrown further back
till his curly hair touched his shirt collar, and the hand holding the
bottle was lifted higher and higher and trembled with the effort. The
bottle was emptying perceptibly and rising still higher and his head
tilting yet further back. "Why is it so long?" thought Pierre. It seemed
to him that more than half an hour had elapsed. Suddenly Dolokhov made a
backward movement with his spine, and his arm trembled nervously; this
was sufficient to cause his whole body to slip as he sat on the sloping
ledge. As he began slipping down, his head and arm wavered still more
with the strain. One hand moved as if to clutch the window sill, but
refrained from touching it. Pierre again covered his eyes and thought he
would never open them again. Suddenly he was aware of a stir all around.
He looked up: Dolokhov was standing on the window sill, with a pale but
radiant face.

"It's empty."

He threw the bottle to the Englishman, who caught it neatly. Dolokhov
jumped down. He smelt strongly of rum.

"Well done!... Fine fellow!... There's a bet for you!... Devil take
you!" came from different sides.

The Englishman took out his purse and began counting out the money.
Dolokhov stood frowning and did not speak. Pierre jumped upon the window
sill.

"Gentlemen, who wishes to bet with me? I'll do the same thing!" he
suddenly cried. "Even without a bet, there! Tell them to bring me a
bottle. I'll do it.... Bring a bottle!"

"Let him do it, let him do it," said Dolokhov, smiling.

"What next? Have you gone mad?... No one would let you!... Why, you go
giddy even on a staircase," exclaimed several voices.

"I'll drink it! Let's have a bottle of rum!" shouted Pierre, banging the
table with a determined and drunken gesture and preparing to climb out
of the window.

They seized him by his arms; but he was so strong that everyone who
touched him was sent flying.

"No, you'll never manage him that way," said Anatole. "Wait a bit and
I'll get round him.... Listen! I'll take your bet tomorrow, but now we
are all going to ----'s."

"Come on then," cried Pierre. "Come on!... And we'll take Bruin with
us."

And he caught the bear, took it in his arms, lifted it from the ground,
and began dancing round the room with it.




CHAPTER X

Prince Vasili kept the promise he had given to Princess Drubetskaya who
had spoken to him on behalf of her only son Boris on the evening of Anna
Pavlovna's soiree. The matter was mentioned to the Emperor, an exception
made, and Boris transferred into the regiment of Semenov Guards with the
rank of cornet. He received, however, no appointment to Kutuzov's staff
despite all Anna Mikhaylovna's endeavors and entreaties. Soon after Anna
Pavlovna's reception Anna Mikhaylovna returned to Moscow and went
straight to her rich relations, the Rostovs, with whom she stayed when
in the town and where her darling Bory, who had only just entered a
regiment of the line and was being at once transferred to the Guards as
a cornet, had been educated from childhood and lived for years at a
time. The Guards had already left Petersburg on the tenth of August, and
her son, who had remained in Moscow for his equipment, was to join them
on the march to Radzivilov.

It was St. Natalia's day and the name day of two of the Rostovs--the
mother and the youngest daughter--both named Nataly. Ever since the
morning, carriages with six horses had been coming and going
continually, bringing visitors to the Countess Rostova's big house on
the Povarskaya, so well known to all Moscow. The countess herself and
her handsome eldest daughter were in the drawing-room with the visitors
who came to congratulate, and who constantly succeeded one another in
relays.

The countess was a woman of about forty-five, with a thin Oriental type
of face, evidently worn out with childbearing--she had had twelve. A
languor of motion and speech, resulting from weakness, gave her a
distinguished air which inspired respect. Princess Anna Mikhaylovna
Drubetskaya, who as a member of the household was also seated in the
drawing room, helped to receive and entertain the visitors. The young
people were in one of the inner rooms, not considering it necessary to
take part in receiving the visitors. The count met the guests and saw
them off, inviting them all to dinner.

"I am very, very grateful to you, mon cher," or "ma chere"--he called
everyone without exception and without the slightest variation in his
tone, "my dear," whether they were above or below him in rank--"I thank
you for myself and for our two dear ones whose name day we are keeping.
But mind you come to dinner or I shall be offended, ma chere! On behalf
of the whole family I beg you to come, mon cher!" These words he
repeated to everyone without exception or variation, and with the same
expression on his full, cheerful, clean-shaven face, the same firm
pressure of the hand and the same quick, repeated bows. As soon as he
had seen a visitor off he returned to one of those who were still in the
drawing room, drew a chair toward him or her, and jauntily spreading out
his legs and putting his hands on his knees with the air of a man who
enjoys life and knows how to live, he swayed to and fro with dignity,
offered surmises about the weather, or touched on questions of health,
sometimes in Russian and sometimes in very bad but self-confident
French; then again, like a man weary but unflinching in the fulfillment
of duty, he rose to see some visitors off and, stroking his scanty gray
hairs over his bald patch, also asked them to dinner. Sometimes on his
way back from the anteroom he would pass through the conservatory and
pantry into the large marble dining hall, where tables were being set
out for eighty people; and looking at the footmen, who were bringing in
silver and china, moving tables, and unfolding damask table linen, he
would call Dmitri Vasilevich, a man of good family and the manager of
all his affairs, and while looking with pleasure at the enormous table
would say: "Well, Dmitri, you'll see that things are all as they should
be? That's right! The great thing is the serving, that's it." And with a
complacent sigh he would return to the drawing room.

"Marya Lvovna Karagina and her daughter!" announced the countess'
gigantic footman in his bass voice, entering the drawing room. The
countess reflected a moment and took a pinch from a gold snuffbox with
her husband's portrait on it.

"I'm quite worn out by these callers. However, I'll see her and no more.
She is so affected. Ask her in," she said to the footman in a sad voice,
as if saying: "Very well, finish me off."

A tall, stout, and proud-looking woman, with a round-faced smiling
daughter, entered the drawing room, their dresses rustling.

"Dear Countess, what an age... She has been laid up, poor child... at
the Razumovski's ball... and Countess Apraksina... I was so
delighted..." came the sounds of animated feminine voices, interrupting
one another and mingling with the rustling of dresses and the scraping
of chairs. Then one of those conversations began which last out until,
at the first pause, the guests rise with a rustle of dresses and say, "I
am so delighted... Mamma's health... and Countess Apraksina..." and
then, again rustling, pass into the anteroom, put on cloaks or mantles,
and drive away. The conversation was on the chief topic of the day: the
illness of the wealthy and celebrated beau of Catherine's day, Count
Bezukhov, and about his illegitimate son Pierre, the one who had behaved
so improperly at Anna Pavlovna's reception.

"I am so sorry for the poor count," said the visitor. "He is in such bad
health, and now this vexation about his son is enough to kill him!"

"What is that?" asked the countess as if she did not know what the
visitor alluded to, though she had already heard about the cause of
Count Bezukhov's distress some fifteen times.

"That's what comes of a modern education," exclaimed the visitor. "It
seems that while he was abroad this young man was allowed to do as he
liked, now in Petersburg I hear he has been doing such terrible things
that he has been expelled by the police."

"You don't say so!" replied the countess.

"He chose his friends badly," interposed Anna Mikhaylovna. "Prince
Vasili's son, he, and a certain Dolokhov have, it is said, been up to
heaven only knows what! And they have had to suffer for it. Dolokhov has
been degraded to the ranks and Bezukhov's son sent back to Moscow.
Anatole Kuragin's father managed somehow to get his son's affair hushed
up, but even he was ordered out of Petersburg."

"But what have they been up to?" asked the countess.

"They are regular brigands, especially Dolokhov," replied the visitor.
"He is a son of Marya Ivanovna Dolokhova, such a worthy woman, but
there, just fancy! Those three got hold of a bear somewhere, put it in a
carriage, and set off with it to visit some actresses! The police tried
to interfere, and what did the young men do? They tied a policeman and
the bear back to back and put the bear into the Moyka Canal. And there
was the bear swimming about with the policeman on his back!"

"What a nice figure the policeman must have cut, my dear!" shouted the
count, dying with laughter.

"Oh, how dreadful! How can you laugh at it, Count?"

Yet the ladies themselves could not help laughing.

"It was all they could do to rescue the poor man," continued the
visitor. "And to think it is Cyril Vladimirovich Bezukhov's son who
amuses himself in this sensible manner! And he was said to be so well
educated and clever. This is all that his foreign education has done for
him! I hope that here in Moscow no one will receive him, in spite of his
money. They wanted to introduce him to me, but I quite declined: I have
my daughters to consider."

"Why do you say this young man is so rich?" asked the countess, turning
away from the girls, who at once assumed an air of inattention. "His
children are all illegitimate. I think Pierre also is illegitimate."

The visitor made a gesture with her hand.

"I should think he has a score of them."

Princess Anna Mikhaylovna intervened in the conversation, evidently
wishing to show her connections and knowledge of what went on in
society.

"The fact of the matter is," said she significantly, and also in a half
whisper, "everyone knows Count Cyril's reputation.... He has lost count
of his children, but this Pierre was his favorite."

"How handsome the old man still was only a year ago!" remarked the
countess. "I have never seen a handsomer man."

"He is very much altered now," said Anna Mikhaylovna. "Well, as I was
saying, Prince Vasili is the next heir through his wife, but the count
is very fond of Pierre, looked after his education, and wrote to the
Emperor about him; so that in the case of his death--and he is so ill
that he may die at any moment, and Dr. Lorrain has come from Petersburg-
-no one knows who will inherit his immense fortune, Pierre or Prince
Vasili. Forty thousand serfs and millions of rubles! I know it all very
well for Prince Vasili told me himself. Besides, Cyril Vladimirovich is
my mother's second cousin. He's also my Bory's godfather," she added, as
if she attached no importance at all to the fact.

"Prince Vasili arrived in Moscow yesterday. I hear he has come on some
inspection business," remarked the visitor.

"Yes, but between ourselves," said the princess, "that is a pretext. The
fact is he has come to see Count Cyril Vladimirovich, hearing how ill he
is."

"But do you know, my dear, that was a capital joke," said the count; and
seeing that the elder visitor was not listening, he turned to the young
ladies. "I can just imagine what a funny figure that policeman cut!"

And as he waved his arms to impersonate the policeman, his portly form
again shook with a deep ringing laugh, the laugh of one who always eats
well and, in particular, drinks well. "So do come and dine with us!" he
said.




CHAPTER XI

Silence ensued. The countess looked at her callers, smiling affably, but
not concealing the fact that she would not be distressed if they now
rose and took their leave. The visitor's daughter was already smoothing
down her dress with an inquiring look at her mother, when suddenly from
the next room were heard the footsteps of boys and girls running to the
door and the noise of a chair falling over, and a girl of thirteen,
hiding something in the folds of her short muslin frock, darted in and
stopped short in the middle of the room. It was evident that she had not
intended her flight to bring her so far. Behind her in the doorway
appeared a student with a crimson coat collar, an officer of the Guards,
a girl of fifteen, and a plump rosy-faced boy in a short jacket.

The count jumped up and, swaying from side to side, spread his arms wide
and threw them round the little girl who had run in.

"Ah, here she is!" he exclaimed laughing. "My pet, whose name day it is.
My dear pet!"

"Ma chere, there is a time for everything," said the countess with
feigned severity. "You spoil her, Ilya," she added, turning to her
husband.

"How do you do, my dear? I wish you many happy returns of your name
day," said the visitor. "What a charming child," she added, addressing
the mother.

This black-eyed, wide-mouthed girl, not pretty but full of life--with
childish bare shoulders which after her run heaved and shook her bodice,
with black curls tossed backward, thin bare arms, little legs in lace-
frilled drawers, and feet in low slippers--was just at that charming age
when a girl is no longer a child, though the child is not yet a young
woman. Escaping from her father she ran to hide her flushed face in the
lace of her mother's mantilla--not paying the least attention to her
severe remark--and began to laugh. She laughed, and in fragmentary
sentences tried to explain about a doll which she produced from the
folds of her frock.

"Do you see?... My doll... Mimi... You see..." was all Natasha managed
to utter (to her everything seemed funny). She leaned against her mother
and burst into such a loud, ringing fit of laughter that even the prim
visitor could not help joining in.

"Now then, go away and take your monstrosity with you," said the mother,
pushing away her daughter with pretended sternness, and turning to the
visitor she added: "She is my youngest girl."

Natasha, raising her face for a moment from her mother's mantilla,
glanced up at her through tears of laughter, and again hid her face.

The visitor, compelled to look on at this family scene, thought it
necessary to take some part in it.

"Tell me, my dear," said she to Natasha, "is Mimi a relation of yours? A
daughter, I suppose?"

Natasha did not like the visitor's tone of condescension to childish
things. She did not reply, but looked at her seriously.

Meanwhile the younger generation: Boris, the officer, Anna Mikhaylovna's
son; Nicholas, the undergraduate, the count's eldest son; Sonya, the
count's fifteen-year-old niece, and little Petya, his youngest boy, had
all settled down in the drawing room and were obviously trying to
restrain within the bounds of decorum the excitement and mirth that
shone in all their faces. Evidently in the back rooms, from which they
had dashed out so impetuously, the conversation had been more amusing
than the drawing-room talk of society scandals, the weather, and
Countess Apraksina. Now and then they glanced at one another, hardly
able to suppress their laughter.

The two young men, the student and the officer, friends from childhood,
were of the same age and both handsome fellows, though not alike. Boris
was tall and fair, and his calm and handsome face had regular, delicate
features. Nicholas was short with curly hair and an open expression.
Dark hairs were already showing on his upper lip, and his whole face
expressed impetuosity and enthusiasm. Nicholas blushed when he entered
the drawing room. He evidently tried to find something to say, but
failed. Boris on the contrary at once found his footing, and related
quietly and humorously how he had known that doll Mimi when she was
still quite a young lady, before her nose was broken; how she had aged
during the five years he had known her, and how her head had cracked
right across the skull. Having said this he glanced at Natasha. She
turned away from him and glanced at her younger brother, who was
screwing up his eyes and shaking with suppressed laughter, and unable to
control herself any longer, she jumped up and rushed from the room as
fast as her nimble little feet would carry her. Boris did not laugh.

"You were meaning to go out, weren't you, Mamma? Do you want the
carriage?" he asked his mother with a smile.

"Yes, yes, go and tell them to get it ready," she answered, returning
his smile.

Boris quietly left the room and went in search of Natasha. The plump boy
ran after them angrily, as if vexed that their program had been
disturbed.




CHAPTER XII

The only young people remaining in the drawing room, not counting the
young lady visitor and the countess' eldest daughter (who was four years
older than her sister and behaved already like a grown-up person), were
Nicholas and Sonya, the niece. Sonya was a slender little brunette with
a tender look in her eyes which were veiled by long lashes, thick black
plaits coiling twice round her head, and a tawny tint in her complexion
and especially in the color of her slender but graceful and muscular
arms and neck. By the grace of her movements, by the softness and
flexibility of her small limbs, and by a certain coyness and reserve of
manner, she reminded one of a pretty, half-grown kitten which promises
to become a beautiful little cat. She evidently considered it proper to
show an interest in the general conversation by smiling, but in spite of
herself her eyes under their thick long lashes watched her cousin who
was going to join the army, with such passionate girlish adoration that
her smile could not for a single instant impose upon anyone, and it was
clear that the kitten had settled down only to spring up with more
energy and again play with her cousin as soon as they too could, like
Natasha and Boris, escape from the drawing room.

"Ah yes, my dear," said the count, addressing the visitor and pointing
to Nicholas, "his friend Boris has become an officer, and so for
friendship's sake he is leaving the university and me, his old father,
and entering the military service, my dear. And there was a place and
everything waiting for him in the Archives Department! Isn't that
friendship?" remarked the count in an inquiring tone.

"But they say that war has been declared," replied the visitor.

"They've been saying so a long while," said the count, "and they'll say
so again and again, and that will be the end of it. My dear, there's
friendship for you," he repeated. "He's joining the hussars."

The visitor, not knowing what to say, shook her head.

"It's not at all from friendship," declared Nicholas, flaring up and
turning away as if from a shameful aspersion. "It is not from friendship
at all; I simply feel that the army is my vocation."

He glanced at his cousin and the young lady visitor; and they were both
regarding him with a smile of approbation.

"Schubert, the colonel of the Pavlograd Hussars, is dining with us
today. He has been here on leave and is taking Nicholas back with him.
It can't be helped!" said the count, shrugging his shoulders and
speaking playfully of a matter that evidently distressed him.

"I have already told you, Papa," said his son, "that if you don't wish
to let me go, I'll stay. But I know I am no use anywhere except in the
army; I am not a diplomat or a government clerk.--I don't know how to
hide what I feel." As he spoke he kept glancing with the flirtatiousness
of a handsome youth at Sonya and the young lady visitor.

The little kitten, feasting her eyes on him, seemed ready at any moment
to start her gambols again and display her kittenish nature.

"All right, all right!" said the old count. "He always flares up! This
Buonaparte has turned all their heads; they all think of how he rose
from an ensign and became Emperor. Well, well, God grant it," he added,
not noticing his visitor's sarcastic smile.

The elders began talking about Bonaparte. Julie Karagina turned to young
Rostov.

"What a pity you weren't at the Arkharovs' on Thursday. It was so dull
without you," said she, giving him a tender smile.

The young man, flattered, sat down nearer to her with a coquettish
smile, and engaged the smiling Julie in a confidential conversation
without at all noticing that his involuntary smile had stabbed the heart
of Sonya, who blushed and smiled unnaturally. In the midst of his talk
he glanced round at her. She gave him a passionately angry glance, and
hardly able to restrain her tears and maintain the artificial smile on
her lips, she got up and left the room. All Nicholas' animation
vanished. He waited for the first pause in the conversation, and then
with a distressed face left the room to find Sonya.

"How plainly all these young people wear their hearts on their sleeves!"
said Anna Mikhaylovna, pointing to Nicholas as he went out. "Cousinage--
dangereux voisinage;" * she added.


* Cousinhood is a dangerous neighborhood.

"Yes," said the countess when the brightness these young people had
brought into the room had vanished; and as if answering a question no
one had put but which was always in her mind, "and how much suffering,
how much anxiety one has had to go through that we might rejoice in them
now! And yet really the anxiety is greater now than the joy. One is
always, always anxious! Especially just at this age, so dangerous both
for girls and boys."

"It all depends on the bringing up," remarked the visitor.

"Yes, you're quite right," continued the countess. "Till now I have
always, thank God, been my children's friend and had their full
confidence," said she, repeating the mistake of so many parents who
imagine that their children have no secrets from them. "I know I shall
always be my daughters' first confidante, and that if Nicholas, with his
impulsive nature, does get into mischief (a boy can't help it), he will
all the same never be like those Petersburg young men."

"Yes, they are splendid, splendid youngsters," chimed in the count, who
always solved questions that seemed to him perplexing by deciding that
everything was splendid. "Just fancy: wants to be an hussar. What's one
to do, my dear?"

"What a charming creature your younger girl is," said the visitor; "a
little volcano!"

"Yes, a regular volcano," said the count. "Takes after me! And what a
voice she has; though she's my daughter, I tell the truth when I say
she'll be a singer, a second Salomoni! We have engaged an Italian to
give her lessons."

"Isn't she too young? I have heard that it harms the voice to train it
at that age."

"Oh no, not at all too young!" replied the count. "Why, our mothers used
to be married at twelve or thirteen."

"And she's in love with Boris already. Just fancy!" said the countess
with a gentle smile, looking at Boris and went on, evidently concerned
with a thought that always occupied her: "Now you see if I were to be
severe with her and to forbid it... goodness knows what they might be up
to on the sly" (she meant that they would be kissing), "but as it is, I
know every word she utters. She will come running to me of her own
accord in the evening and tell me everything. Perhaps I spoil her, but
really that seems the best plan. With her elder sister I was stricter."

"Yes, I was brought up quite differently," remarked the handsome elder
daughter, Countess Vera, with a smile.

But the smile did not enhance Vera's beauty as smiles generally do; on
the contrary it gave her an unnatural, and therefore unpleasant,
expression. Vera was good-looking, not at all stupid, quick at learning,
was well-brought up, and had a pleasant voice; what she said was true
and appropriate, yet, strange to say, everyone--the visitors and
countess alike--turned to look at her as if wondering why she had said
it, and they all felt awkward.

"People are always too clever with their eldest children and try to make
something exceptional of them," said the visitor.

"What's the good of denying it, my dear? Our dear countess was too
clever with Vera," said the count. "Well, what of that? She's turned out
splendidly all the same," he added, winking at Vera.

The guests got up and took their leave, promising to return to dinner.

"What manners! I thought they would never go," said the countess, when
she had seen her guests out.




CHAPTER XIII

When Natasha ran out of the drawing room she only went as far as the
conservatory. There she paused and stood listening to the conversation
in the drawing room, waiting for Boris to come out. She was already
growing impatient, and stamped her foot, ready to cry at his not coming
at once, when she heard the young man's discreet steps approaching
neither quickly nor slowly. At this Natasha dashed swiftly among the
flower tubs and hid there.

Boris paused in the middle of the room, looked round, brushed a little
dust from the sleeve of his uniform, and going up to a mirror examined
his handsome face. Natasha, very still, peered out from her ambush,
waiting to see what he would do. He stood a little while before the
glass, smiled, and walked toward the other door. Natasha was about to
call him but changed her mind. "Let him look for me," thought she.
Hardly had Boris gone than Sonya, flushed, in tears, and muttering
angrily, came in at the other door. Natasha checked her first impulse to
run out to her, and remained in her hiding place, watching--as under an
invisible cap--to see what went on in the world. She was experiencing a
new and peculiar pleasure. Sonya, muttering to herself, kept looking
round toward the drawing-room door. It opened and Nicholas came in.

"Sonya, what is the matter with you? How can you?" said he, running up
to her.

"It's nothing, nothing; leave me alone!" sobbed Sonya.

"Ah, I know what it is."

"Well, if you do, so much the better, and you can go back to her!"

"So-o-onya! Look here! How can you torture me and yourself like that,
for a mere fancy?" said Nicholas taking her hand.

Sonya did not pull it away, and left off crying. Natasha, not stirring
and scarcely breathing, watched from her ambush with sparkling eyes.
"What will happen now?" thought she.

"Sonya! What is anyone in the world to me? You alone are everything!"
said Nicholas. "And I will prove it to you."

"I don't like you to talk like that."

"Well, then, I won't; only forgive me, Sonya!" He drew her to him and
kissed her.

"Oh, how nice," thought Natasha; and when Sonya and Nicholas had gone
out of the conservatory she followed and called Boris to her.

"Boris, come here," said she with a sly and significant look. "I have
something to tell you. Here, here!" and she led him into the
conservatory to the place among the tubs where she had been hiding.

Boris followed her, smiling.

"What is the something?" asked he.

She grew confused, glanced round, and, seeing the doll she had thrown
down on one of the tubs, picked it up.

"Kiss the doll," said she.

Boris looked attentively and kindly at her eager face, but did not
reply.

"Don't you want to? Well, then, come here," said she, and went further
in among the plants and threw down the doll. "Closer, closer!" she
whispered.

She caught the young officer by his cuffs, and a look of solemnity and
fear appeared on her flushed face.

"And me? Would you like to kiss me?" she whispered almost inaudibly,
glancing up at him from under her brows, smiling, and almost crying from
excitement.

Boris blushed.

"How funny you are!" he said, bending down to her and blushing still
more, but he waited and did nothing.

Suddenly she jumped up onto a tub to be higher than he, embraced him so
that both her slender bare arms clasped him above his neck, and, tossing
back her hair, kissed him full on the lips.

Then she slipped down among the flowerpots on the other side of the tubs
and stood, hanging her head.

"Natasha," he said, "you know that I love you, but..."

"You are in love with me?" Natasha broke in.

"Yes, I am, but please don't let us do like that.... In another four
years... then I will ask for your hand."

Natasha considered.

"Thirteen, fourteen, fifteen, sixteen," she counted on her slender
little fingers. "All right! Then it's settled?"

A smile of joy and satisfaction lit up her eager face.

"Settled!" replied Boris.

"Forever?" said the little girl. "Till death itself?"

She took his arm and with a happy face went with him into the adjoining
sitting room.




CHAPTER XIV

After receiving her visitors, the countess was so tired that she gave
orders to admit no more, but the porter was told to be sure to invite to
dinner all who came "to congratulate." The countess wished to have a
tête-à-tête talk with the friend of her childhood, Princess Anna
Mikhaylovna, whom she had not seen properly since she returned from
Petersburg. Anna Mikhaylovna, with her tear-worn but pleasant face, drew
her chair nearer to that of the countess.

"With you I will be quite frank," said Anna Mikhaylovna. "There are not
many left of us old friends! That's why I so value your friendship."

Anna Mikhaylovna looked at Vera and paused. The countess pressed her
friend's hand.

"Vera," she said to her eldest daughter who was evidently not a
favorite, "how is it you have so little tact? Don't you see you are not
wanted here? Go to the other girls, or..."

The handsome Vera smiled contemptuously but did not seem at all hurt.

"If you had told me sooner, Mamma, I would have gone," she replied as
she rose to go to her own room.

But as she passed the sitting room she noticed two couples sitting, one
pair at each window. She stopped and smiled scornfully. Sonya was
sitting close to Nicholas who was copying out some verses for her, the
first he had ever written. Boris and Natasha were at the other window
and ceased talking when Vera entered. Sonya and Natasha looked at Vera
with guilty, happy faces.

It was pleasant and touching to see these little girls in love; but
apparently the sight of them roused no pleasant feeling in Vera.

"How often have I asked you not to take my things?" she said. "You have
a room of your own," and she took the inkstand from Nicholas.

"In a minute, in a minute," he said, dipping his pen.

"You always manage to do things at the wrong time," continued Vera. "You
came rushing into the drawing room so that everyone felt ashamed of
you."

Though what she said was quite just, perhaps for that very reason no one
replied, and the four simply looked at one another. She lingered in the
room with the inkstand in her hand.

"And at your age what secrets can there be between Natasha and Boris, or
between you two? It's all nonsense!"

"Now, Vera, what does it matter to you?" said Natasha in defense,
speaking very gently.

She seemed that day to be more than ever kind and affectionate to
everyone.

"Very silly," said Vera. "I am ashamed of you. Secrets indeed!"

"All have secrets of their own," answered Natasha, getting warmer. "We
don't interfere with you and Berg."

"I should think not," said Vera, "because there can never be anything
wrong in my behavior. But I'll just tell Mamma how you are behaving with
Boris."

"Natalya Ilynichna behaves very well to me," remarked Boris. "I have
nothing to complain of."

"Don't, Boris! You are such a diplomat that it is really tiresome," said
Natasha in a mortified voice that trembled slightly. (She used the word
"diplomat," which was just then much in vogue among the children, in the
special sense they attached to it.) "Why does she bother me?" And she
added, turning to Vera, "You'll never understand it, because you've
never loved anyone. You have no heart! You are a Madame de Genlis and
nothing more" (this nickname, bestowed on Vera by Nicholas, was
considered very stinging), "and your greatest pleasure is to be
unpleasant to people! Go and flirt with Berg as much as you please," she
finished quickly.

"I shall at any rate not run after a young man before visitors..."

"Well, now you've done what you wanted," put in Nicholas--"said
unpleasant things to everyone and upset them. Let's go to the nursery."

All four, like a flock of scared birds, got up and left the room.

"The unpleasant things were said to me," remarked Vera, "I said none to
anyone."

"Madame de Genlis! Madame de Genlis!" shouted laughing voices through
the door.

The handsome Vera, who produced such an irritating and unpleasant effect
on everyone, smiled and, evidently unmoved by what had been said to her,
went to the looking glass and arranged her hair and scarf. Looking at
her own handsome face she seemed to become still colder and calmer.

In the drawing room the conversation was still going on.

"Ah, my dear," said the countess, "my life is not all roses either.
Don't I know that at the rate we are living our means won't last long?
It's all the Club and his easygoing nature. Even in the country do we
get any rest? Theatricals, hunting, and heaven knows what besides! But
don't let's talk about me; tell me how you managed everything. I often
wonder at you, Annette--how at your age you can rush off alone in a
carriage to Moscow, to Petersburg, to those ministers and great people,
and know how to deal with them all! It's quite astonishing. How did you
get things settled? I couldn't possibly do it."

"Ah, my love," answered Anna Mikhaylovna, "God grant you never know what
it is to be left a widow without means and with a son you love to
distraction! One learns many things then," she added with a certain
pride. "That lawsuit taught me much. When I want to see one of those big
people I write a note: 'Princess So-and-So desires an interview with So
and-So,' and then I take a cab and go myself two, three, or four times--
till I get what I want. I don't mind what they think of me."

"Well, and to whom did you apply about Bory?" asked the countess. "You
see yours is already an officer in the Guards, while my Nicholas is
going as a cadet. There's no one to interest himself for him. To whom
did you apply?"

"To Prince Vasili. He was so kind. He at once agreed to everything, and
put the matter before the Emperor," said Princess Anna Mikhaylovna
enthusiastically, quite forgetting all the humiliation she had endured
to gain her end.

"Has Prince Vasili aged much?" asked the countess. "I have not seen him
since we acted together at the Rumyantsovs' theatricals. I expect he has
forgotten me. He paid me attentions in those days," said the countess,
with a smile.

"He is just the same as ever," replied Anna Mikhaylovna, "overflowing
with amiability. His position has not turned his head at all. He said to
me, 'I am sorry I can do so little for you, dear Princess. I am at your
command.' Yes, he is a fine fellow and a very kind relation. But,
Nataly, you know my love for my son: I would do anything for his
happiness! And my affairs are in such a bad way that my position is now
a terrible one," continued Anna Mikhaylovna, sadly, dropping her voice.
"My wretched lawsuit takes all I have and makes no progress. Would you
believe it, I have literally not a penny and don't know how to equip
Boris." She took out her handkerchief and began to cry. "I need five
hundred rubles, and have only one twenty-five-ruble note. I am in such a
state.... My only hope now is in Count Cyril Vladimirovich Bezukhov. If
he will not assist his godson--you know he is Bory's godfather--and
allow him something for his maintenance, all my trouble will have been
thrown away.... I shall not be able to equip him."

The countess' eyes filled with tears and she pondered in silence.

"I often think, though, perhaps it's a sin," said the princess, "that
here lives Count Cyril Vladimirovich Bezukhov so rich, all alone... that
tremendous fortune... and what is his life worth? It's a burden to him,
and Bory's life is only just beginning...."

"Surely he will leave something to Boris," said the countess.

"Heaven only knows, my dear! These rich grandees are so selfish. Still,
I will take Boris and go to see him at once, and I shall speak to him
straight out. Let people think what they will of me, it's really all the
same to me when my son's fate is at stake." The princess rose. "It's now
two o'clock and you dine at four. There will just be time."

And like a practical Petersburg lady who knows how to make the most of
time, Anna Mikhaylovna sent someone to call her son, and went into the
anteroom with him.

"Good-bye, my dear," said she to the countess who saw her to the door,
and added in a whisper so that her son should not hear, "Wish me good
luck."

"Are you going to Count Cyril Vladimirovich, my dear?" said the count
coming out from the dining hall into the anteroom, and he added: "If he
is better, ask Pierre to dine with us. He has been to the house, you
know, and danced with the children. Be sure to invite him, my dear. We
will see how Taras distinguishes himself today. He says Count Orlov
never gave such a dinner as ours will be!"




CHAPTER XV

"My dear Boris," said Princess Anna Mikhaylovna to her son as Countess
Rostova's carriage in which they were seated drove over the straw
covered street and turned into the wide courtyard of Count Cyril
Vladimirovich Bezukhov's house. "My dear Boris," said the mother,
drawing her hand from beneath her old mantle and laying it timidly and
tenderly on her son's arm, "be affectionate and attentive to him. Count
Cyril Vladimirovich is your godfather after all, your future depends on
him. Remember that, my dear, and be nice to him, as you so well know how
to be."

"If only I knew that anything besides humiliation would come of it..."
answered her son coldly. "But I have promised and will do it for your
sake."

Although the hall porter saw someone's carriage standing at the
entrance, after scrutinizing the mother and son (who without asking to
be announced had passed straight through the glass porch between the
rows of statues in niches) and looking significantly at the lady's old
cloak, he asked whether they wanted the count or the princesses, and,
hearing that they wished to see the count, said his excellency was worse
today, and that his excellency was not receiving anyone.

"We may as well go back," said the son in French.

"My dear!" exclaimed his mother imploringly, again laying her hand on
his arm as if that touch might soothe or rouse him.

Boris said no more, but looked inquiringly at his mother without taking
off his cloak.

"My friend," said Anna Mikhaylovna in gentle tones, addressing the hall
porter, "I know Count Cyril Vladimirovich is very ill... that's why I
have come... I am a relation. I shall not disturb him, my friend... I
only need see Prince Vasili Sergeevich: he is staying here, is he not?
Please announce me."

The hall porter sullenly pulled a bell that rang upstairs, and turned
away.

"Princess Drubetskaya to see Prince Vasili Sergeevich," he called to a
footman dressed in knee breeches, shoes, and a swallow-tail coat, who
ran downstairs and looked over from the halfway landing.

The mother smoothed the folds of her dyed silk dress before a large
Venetian mirror in the wall, and in her trodden-down shoes briskly
ascended the carpeted stairs.

"My dear," she said to her son, once more stimulating him by a touch,
"you promised me!"

The son, lowering his eyes, followed her quietly.

They entered the large hall, from which one of the doors led to the
apartments assigned to Prince Vasili.

Just as the mother and son, having reached the middle of the hall, were
about to ask their way of an elderly footman who had sprung up as they
entered, the bronze handle of one of the doors turned and Prince Vasili
came out--wearing a velvet coat with a single star on his breast, as was
his custom when at home--taking leave of a good-looking, dark-haired
man. This was the celebrated Petersburg doctor, Lorrain.

"Then it is certain?" said the prince.

"Prince, humanum est errare, * but..." replied the doctor, swallowing
his r's, and pronouncing the Latin words with a French accent.


* To err is human.

"Very well, very well..."

Seeing Anna Mikhaylovna and her son, Prince Vasili dismissed the doctor
with a bow and approached them silently and with a look of inquiry. The
son noticed that an expression of profound sorrow suddenly clouded his
mother's face, and he smiled slightly.

"Ah, Prince! In what sad circumstances we meet again! And how is our
dear invalid?" said she, as though unaware of the cold offensive look
fixed on her.

Prince Vasili stared at her and at Boris questioningly and perplexed.
Boris bowed politely. Prince Vasili without acknowledging the bow turned
to Anna Mikhaylovna, answering her query by a movement of the head and
lips indicating very little hope for the patient.

"Is it possible?" exclaimed Anna Mikhaylovna. "Oh, how awful! It is
terrible to think.... This is my son," she added, indicating Boris. "He
wanted to thank you himself."

Boris bowed again politely.

"Believe me, Prince, a mother's heart will never forget what you have
done for us."

"I am glad I was able to do you a service, my dear Anna Mikhaylovna,"
said Prince Vasili, arranging his lace frill, and in tone and manner,
here in Moscow to Anna Mikhaylovna whom he had placed under an
obligation, assuming an air of much greater importance than he had done
in Petersburg at Anna Scherer's reception.

"Try to serve well and show yourself worthy," added he, addressing Boris
with severity. "I am glad.... Are you here on leave?" he went on in his
usual tone of indifference.

"I am awaiting orders to join my new regiment, your excellency," replied
Boris, betraying neither annoyance at the prince's brusque manner nor a
desire to enter into conversation, but speaking so quietly and
respectfully that the prince gave him a searching glance.

"Are you living with your mother?"

"I am living at Countess Rostova's," replied Boris, again adding, "your
excellency."

"That is, with Ilya Rostov who married Nataly Shinshina," said Anna
Mikhaylovna.

"I know, I know," answered Prince Vasili in his monotonous voice. "I
never could understand how Nataly made up her mind to marry that
unlicked bear! A perfectly absurd and stupid fellow, and a gambler too,
I am told."

"But a very kind man, Prince," said Anna Mikhaylovna with a pathetic
smile, as though she too knew that Count Rostov deserved this censure,
but asked him not to be too hard on the poor old man. "What do the
doctors say?" asked the princess after a pause, her worn face again
expressing deep sorrow.

"They give little hope," replied the prince.

"And I should so like to thank Uncle once for all his kindness to me and
Boris. He is his godson," she added, her tone suggesting that this fact
ought to give Prince Vasili much satisfaction.

Prince Vasili became thoughtful and frowned. Anna Mikhaylovna saw that
he was afraid of finding in her a rival for Count Bezukhov's fortune,
and hastened to reassure him.

"If it were not for my sincere affection and devotion to Uncle," said
she, uttering the word with peculiar assurance and unconcern, "I know
his character: noble, upright... but you see he has no one with him
except the young princesses.... They are still young...." She bent her
head and continued in a whisper: "Has he performed his final duty,
Prince? How priceless are those last moments! It can make things no
worse, and it is absolutely necessary to prepare him if he is so ill. We
women, Prince," and she smiled tenderly, "always know how to say these
things. I absolutely must see him, however painful it may be for me. I
am used to suffering."

Evidently the prince understood her, and also understood, as he had done
at Anna Pavlovna's, that it would be difficult to get rid of Anna
Mikhaylovna.

"Would not such a meeting be too trying for him, dear Anna Mikhaylovna?"
said he. "Let us wait until evening. The doctors are expecting a
crisis."

"But one cannot delay, Prince, at such a moment! Consider that the
welfare of his soul is at stake. Ah, it is awful: the duties of a
Christian..."

A door of one of the inner rooms opened and one of the princesses, the
count's niece, entered with a cold, stern face. The length of her body
was strikingly out of proportion to her short legs. Prince Vasili turned
to her.

"Well, how is he?"

"Still the same; but what can you expect, this noise..." said the
princess, looking at Anna Mikhaylovna as at a stranger.

"Ah, my dear, I hardly knew you," said Anna Mikhaylovna with a happy
smile, ambling lightly up to the count's niece. "I have come, and am at
your service to help you nurse my uncle. I imagine what you have gone
through," and she sympathetically turned up her eyes.

The princess gave no reply and did not even smile, but left the room as
Anna Mikhaylovna took off her gloves and, occupying the position she had
conquered, settled down in an armchair, inviting Prince Vasili to take a
seat beside her.

"Boris," she said to her son with a smile, "I shall go in to see the
count, my uncle; but you, my dear, had better go to Pierre meanwhile and
don't forget to give him the Rostovs' invitation. They ask him to
dinner. I suppose he won't go?" she continued, turning to the prince.

"On the contrary," replied the prince, who had plainly become depressed,
"I shall be only too glad if you relieve me of that young man.... Here
he is, and the count has not once asked for him."

He shrugged his shoulders. A footman conducted Boris down one flight of
stairs and up another, to Pierre's rooms.




CHAPTER XVI

Pierre, after all, had not managed to choose a career for himself in
Petersburg, and had been expelled from there for riotous conduct and
sent to Moscow. The story told about him at Count Rostov's was true.
Pierre had taken part in tying a policeman to a bear. He had now been
for some days in Moscow and was staying as usual at his father's house.
Though he expected that the story of his escapade would be already known
in Moscow and that the ladies about his father--who were never favorably
disposed toward him--would have used it to turn the count against him,
he nevertheless on the day of his arrival went to his father's part of
the house. Entering the drawing room, where the princesses spent most of
their time, he greeted the ladies, two of whom were sitting at
embroidery frames while a third read aloud. It was the eldest who was
reading--the one who had met Anna Mikhaylovna. The two younger ones were
embroidering: both were rosy and pretty and they differed only in that
one had a little mole on her lip which made her much prettier. Pierre
was received as if he were a corpse or a leper. The eldest princess
paused in her reading and silently stared at him with frightened eyes;
the second assumed precisely the same expression; while the youngest,
the one with the mole, who was of a cheerful and lively disposition,
bent over her frame to hide a smile probably evoked by the amusing scene
she foresaw. She drew her wool down through the canvas and, scarcely
able to refrain from laughing, stooped as if trying to make out the
pattern.

"How do you do, cousin?" said Pierre. "You don't recognize me?"

"I recognize you only too well, too well."

"How is the count? Can I see him?" asked Pierre, awkwardly as usual, but
unabashed.

"The count is suffering physically and mentally, and apparently you have
done your best to increase his mental sufferings."

"Can I see the count?" Pierre again asked.

"Hm.... If you wish to kill him, to kill him outright, you can see
him... Olga, go and see whether Uncle's beef tea is ready--it is almost
time," she added, giving Pierre to understand that they were busy, and
busy making his father comfortable, while evidently he, Pierre, was only
busy causing him annoyance.

Olga went out. Pierre stood looking at the sisters; then he bowed and
said: "Then I will go to my rooms. You will let me know when I can see
him."

And he left the room, followed by the low but ringing laughter of the
sister with the mole.

Next day Prince Vasili had arrived and settled in the count's house. He
sent for Pierre and said to him: "My dear fellow, if you are going to
behave here as you did in Petersburg, you will end very badly; that is
all I have to say to you. The count is very, very ill, and you must not
see him at all."

Since then Pierre had not been disturbed and had spent the whole time in
his rooms upstairs.

When Boris appeared at his door Pierre was pacing up and down his room,
stopping occasionally at a corner to make menacing gestures at the wall,
as if running a sword through an invisible foe, and glaring savagely
over his spectacles, and then again resuming his walk, muttering
indistinct words, shrugging his shoulders and gesticulating.

"England is done for," said he, scowling and pointing his finger at
someone unseen. "Mr. Pitt, as a traitor to the nation and to the rights
of man, is sentenced to..." But before Pierre--who at that moment
imagined himself to be Napoleon in person and to have just effected the
dangerous crossing of the Straits of Dover and captured London--could
pronounce Pitt's sentence, he saw a well-built and handsome young
officer entering his room. Pierre paused. He had left Moscow when Boris
was a boy of fourteen, and had quite forgotten him, but in his usual
impulsive and hearty way he took Boris by the hand with a friendly
smile.

"Do you remember me?" asked Boris quietly with a pleasant smile. "I have
come with my mother to see the count, but it seems he is not well."

"Yes, it seems he is ill. People are always disturbing him," answered
Pierre, trying to remember who this young man was.

Boris felt that Pierre did not recognize him but did not consider it
necessary to introduce himself, and without experiencing the least
embarrassment looked Pierre straight in the face.

"Count Rostov asks you to come to dinner today," said he, after a
considerable pause which made Pierre feel uncomfortable.

"Ah, Count Rostov!" exclaimed Pierre joyfully. "Then you are his son,
Ilya? Only fancy, I didn't know you at first. Do you remember how we
went to the Sparrow Hills with Madame Jacquot?... It's such an age..."

"You are mistaken," said Boris deliberately, with a bold and slightly
sarcastic smile. "I am Boris, son of Princess Anna Mikhaylovna
Drubetskaya. Rostov, the father, is Ilya, and his son is Nicholas. I
never knew any Madame Jacquot."

Pierre shook his head and arms as if attacked by mosquitoes or bees.

"Oh dear, what am I thinking about? I've mixed everything up. One has so
many relatives in Moscow! So you are Boris? Of course. Well, now we know
where we are. And what do you think of the Boulogne expedition? The
English will come off badly, you know, if Napoleon gets across the
Channel. I think the expedition is quite feasible. If only Villeneuve
doesn't make a mess of things!"

Boris knew nothing about the Boulogne expedition; he did not read the
papers and it was the first time he had heard Villeneuve's name.

"We here in Moscow are more occupied with dinner parties and scandal
than with politics," said he in his quiet ironical tone. "I know nothing
about it and have not thought about it. Moscow is chiefly busy with
gossip," he continued. "Just now they are talking about you and your
father."

Pierre smiled in his good-natured way as if afraid for his companion's
sake that the latter might say something he would afterwards regret. But
Boris spoke distinctly, clearly, and dryly, looking straight into
Pierre's eyes.

"Moscow has nothing else to do but gossip," Boris went on. "Everybody is
wondering to whom the count will leave his fortune, though he may
perhaps outlive us all, as I sincerely hope he will..."

"Yes, it is all very horrid," interrupted Pierre, "very horrid."

Pierre was still afraid that this officer might inadvertently say
something disconcerting to himself.

"And it must seem to you," said Boris flushing slightly, but not
changing his tone or attitude, "it must seem to you that everyone is
trying to get something out of the rich man?"

"So it does," thought Pierre.

"But I just wish to say, to avoid misunderstandings, that you are quite
mistaken if you reckon me or my mother among such people. We are very
poor, but for my own part at any rate, for the very reason that your
father is rich, I don't regard myself as a relation of his, and neither
I nor my mother would ever ask or take anything from him."

For a long time Pierre could not understand, but when he did, he jumped
up from the sofa, seized Boris under the elbow in his quick, clumsy way,
and, blushing far more than Boris, began to speak with a feeling of
mingled shame and vexation.

"Well, this is strange! Do you suppose I... who could think?... I know
very well..."

But Boris again interrupted him.

"I am glad I have spoken out fully. Perhaps you did not like it? You
must excuse me," said he, putting Pierre at ease instead of being put at
ease by him, "but I hope I have not offended you. I always make it a
rule to speak out... Well, what answer am I to take? Will you come to
dinner at the Rostovs'?"

And Boris, having apparently relieved himself of an onerous duty and
extricated himself from an awkward situation and placed another in it,
became quite pleasant again.

"No, but I say," said Pierre, calming down, "you are a wonderful fellow!
What you have just said is good, very good. Of course you don't know me.
We have not met for such a long time... not since we were children. You
might think that I... I understand, quite understand. I could not have
done it myself, I should not have had the courage, but it's splendid. I
am very glad to have made your acquaintance. It's queer," he added after
a pause, "that you should have suspected me!" He began to laugh. "Well,
what of it! I hope we'll get better acquainted," and he pressed Boris'
hand. "Do you know, I have not once been in to see the count. He has not
sent for me.... I am sorry for him as a man, but what can one do?"

"And so you think Napoleon will manage to get an army across?" asked
Boris with a smile.

Pierre saw that Boris wished to change the subject, and being of the
same mind he began explaining the advantages and disadvantages of the
Boulogne expedition.

A footman came in to summon Boris--the princess was going. Pierre, in
order to make Boris' better acquaintance, promised to come to dinner,
and warmly pressing his hand looked affectionately over his spectacles
into Boris' eyes. After he had gone Pierre continued pacing up and down
the room for a long time, no longer piercing an imaginary foe with his
imaginary sword, but smiling at the remembrance of that pleasant,
intelligent, and resolute young man.

As often happens in early youth, especially to one who leads a lonely
life, he felt an unaccountable tenderness for this young man and made up
his mind that they would be friends.

Prince Vasili saw the princess off. She held a handkerchief to her eyes
and her face was tearful.

"It is dreadful, dreadful!" she was saying, "but cost me what it may I
shall do my duty. I will come and spend the night. He must not be left
like this. Every moment is precious. I can't think why his nieces put it
off. Perhaps God will help me to find a way to prepare him!... Adieu,
Prince! May God support you..."

"Adieu, ma bonne," answered Prince Vasili turning away from her.

"Oh, he is in a dreadful state," said the mother to her son when they
were in the carriage. "He hardly recognizes anybody."

"I don't understand, Mamma--what is his attitude to Pierre?" asked the
son.

"The will will show that, my dear; our fate also depends on it."

"But why do you expect that he will leave us anything?"

"Ah, my dear! He is so rich, and we are so poor!"

"Well, that is hardly a sufficient reason, Mamma..."

"Oh, Heaven! How ill he is!" exclaimed the mother.




CHAPTER XVII

After Anna Mikhaylovna had driven off with her son to visit Count Cyril
Vladimirovich Bezukhov, Countess Rostova sat for a long time all alone
applying her handkerchief to her eyes. At last she rang.

"What is the matter with you, my dear?" she said crossly to the maid who
kept her waiting some minutes. "Don't you wish to serve me? Then I'll
find you another place."

The countess was upset by her friend's sorrow and humiliating poverty,
and was therefore out of sorts, a state of mind which with her always
found expression in calling her maid "my dear" and speaking to her with
exaggerated politeness.

"I am very sorry, ma'am," answered the maid.

"Ask the count to come to me."

The count came waddling in to see his wife with a rather guilty look as
usual.

"Well, little countess? What a saute of game au madere we are to have,
my dear! I tasted it. The thousand rubles I paid for Taras were not ill-
spent. He is worth it!"

He sat down by his wife, his elbows on his knees and his hands ruffling
his gray hair.

"What are your commands, little countess?"

"You see, my dear... What's that mess?" she said, pointing to his
waistcoat. "It's the saute, most likely," she added with a smile. "Well,
you see, Count, I want some money."

Her face became sad.

"Oh, little countess!"... and the count began bustling to get out his
pocketbook.

"I want a great deal, Count! I want five hundred rubles," and taking out
her cambric handkerchief she began wiping her husband's waistcoat.

"Yes, immediately, immediately! Hey, who's there?" he called out in a
tone only used by persons who are certain that those they call will rush
to obey the summons. "Send Dmitri to me!"

Dmitri, a man of good family who had been brought up in the count's
house and now managed all his affairs, stepped softly into the room.

"This is what I want, my dear fellow," said the count to the deferential
young man who had entered. "Bring me..." he reflected a moment, "yes,
bring me seven hundred rubles, yes! But mind, don't bring me such
tattered and dirty notes as last time, but nice clean ones for the
countess."

"Yes, Dmitri, clean ones, please," said the countess, sighing deeply.

"When would you like them, your excellency?" asked Dmitri. "Allow me to
inform you... But, don't be uneasy," he added, noticing that the count
was beginning to breathe heavily and quickly which was always a sign of
approaching anger. "I was forgetting... Do you wish it brought at once?"

"Yes, yes; just so! Bring it. Give it to the countess."

"What a treasure that Dmitri is," added the count with a smile when the
young man had departed. "There is never any 'impossible' with him.
That's a thing I hate! Everything is possible."

"Ah, money, Count, money! How much sorrow it causes in the world," said
the countess. "But I am in great need of this sum."

"You, my little countess, are a notorious spendthrift," said the count,
and having kissed his wife's hand he went back to his study.

When Anna Mikhaylovna returned from Count Bezukhov's the money, all in
clean notes, was lying ready under a handkerchief on the countess'
little table, and Anna Mikhaylovna noticed that something was agitating
her.

"Well, my dear?" asked the countess.

"Oh, what a terrible state he is in! One would not know him, he is so
ill! I was only there a few moments and hardly said a word..."

"Annette, for heaven's sake don't refuse me," the countess began, with a
blush that looked very strange on her thin, dignified, elderly face, and
she took the money from under the handkerchief.

Anna Mikhaylovna instantly guessed her intention and stooped to be ready
to embrace the countess at the appropriate moment.

"This is for Boris from me, for his outfit."

Anna Mikhaylovna was already embracing her and weeping. The countess
wept too. They wept because they were friends, and because they were
kindhearted, and because they--friends from childhood--had to think
about such a base thing as money, and because their youth was over....
But those tears were pleasant to them both.




CHAPTER XVIII

Countess Rostova, with her daughters and a large number of guests, was
already seated in the drawing room. The count took the gentlemen into
his study and showed them his choice collection of Turkish pipes. From
time to time he went out to ask: "Hasn't she come yet?" They were
expecting Marya Dmitrievna Akhrosimova, known in society as le terrible
dragon, a lady distinguished not for wealth or rank, but for common
sense and frank plainness of speech. Marya Dmitrievna was known to the
Imperial family as well as to all Moscow and Petersburg, and both cities
wondered at her, laughed privately at her rudenesses, and told good
stories about her, while none the less all without exception respected
and feared her.

In the count's room, which was full of tobacco smoke, they talked of war
that had been announced in a manifesto, and about the recruiting. None
of them had yet seen the manifesto, but they all knew it had appeared.
The count sat on the sofa between two guests who were smoking and
talking. He neither smoked nor talked, but bending his head first to one
side and then to the other watched the smokers with evident pleasure and
listened to the conversation of his two neighbors, whom he egged on
against each other.

One of them was a sallow, clean-shaven civilian with a thin and wrinkled
face, already growing old, though he was dressed like a most fashionable
young man. He sat with his legs up on the sofa as if quite at home and,
having stuck an amber mouthpiece far into his mouth, was inhaling the
smoke spasmodically and screwing up his eyes. This was an old bachelor,
Shinshin, a cousin of the countess', a man with "a sharp tongue" as they
said in Moscow society. He seemed to be condescending to his companion.
The latter, a fresh, rosy officer of the Guards, irreproachably washed,
brushed, and buttoned, held his pipe in the middle of his mouth and with
red lips gently inhaled the smoke, letting it escape from his handsome
mouth in rings. This was Lieutenant Berg, an officer in the Semenov
regiment with whom Boris was to travel to join the army, and about whom
Natasha had teased her elder sister Vera, speaking of Berg as her
"intended." The count sat between them and listened attentively. His
favorite occupation when not playing boston, a card game he was very
fond of, was that of listener, especially when he succeeded in setting
two loquacious talkers at one another.

"Well, then, old chap, mon tres honorable Alphonse Karlovich," said
Shinshin, laughing ironically and mixing the most ordinary Russian
expressions with the choicest French phrases--which was a peculiarity of
his speech. "Vous comptez vous faire des rentes sur l'etat; * you want
to make something out of your company?"


* You expect to make an income out of the government.

"No, Peter Nikolaevich; I only want to show that in the cavalry the
advantages are far less than in the infantry. Just consider my own
position now, Peter Nikolaevich..."

Berg always spoke quietly, politely, and with great precision. His
conversation always related entirely to himself; he would remain calm
and silent when the talk related to any topic that had no direct bearing
on himself. He could remain silent for hours without being at all put
out of countenance himself or making others uncomfortable, but as soon
as the conversation concerned himself he would begin to talk
circumstantially and with evident satisfaction.

"Consider my position, Peter Nikolaevich. Were I in the cavalry I should
get not more than two hundred rubles every four months, even with the
rank of lieutenant; but as it is I receive two hundred and thirty," said
he, looking at Shinshin and the count with a joyful, pleasant smile, as
if it were obvious to him that his success must always be the chief
desire of everyone else.

"Besides that, Peter Nikolaevich, by exchanging into the Guards I shall
be in a more prominent position," continued Berg, "and vacancies occur
much more frequently in the Foot Guards. Then just think what can be
done with two hundred and thirty rubles! I even manage to put a little
aside and to send something to my father," he went on, emitting a smoke
ring.

"La balance y est... * A German knows how to skin a flint, as the
proverb says," remarked Shinshin, moving his pipe to the other side of
his mouth and winking at the count.


* So that squares matters.

The count burst out laughing. The other guests seeing that Shinshin was
talking came up to listen. Berg, oblivious of irony or indifference,
continued to explain how by exchanging into the Guards he had already
gained a step on his old comrades of the Cadet Corps; how in wartime the
company commander might get killed and he, as senior in the company,
might easily succeed to the post; how popular he was with everyone in
the regiment, and how satisfied his father was with him. Berg evidently
enjoyed narrating all this, and did not seem to suspect that others,
too, might have their own interests. But all he said was so prettily
sedate, and the naivete of his youthful egotism was so obvious, that he
disarmed his hearers.

"Well, my boy, you'll get along wherever you go--foot or horse--that
I'll warrant," said Shinshin, patting him on the shoulder and taking his
feet off the sofa.

Berg smiled joyously. The count, by his guests, went into the drawing
room.

It was just the moment before a big dinner when the assembled guests,
expecting the summons to zakuska, * avoid engaging in any long
conversation but think it necessary to move about and talk, in order to
show that they are not at all impatient for their food. The host and
hostess look toward the door, and now and then glance at one another,
and the visitors try to guess from these glances who, or what, they are
waiting for--some important relation who has not yet arrived, or a dish
that is not yet ready.


* Hors d'oeuvres.

Pierre had come just at dinnertime and was sitting awkwardly in the
middle of the drawing room on the first chair he had come across,
blocking the way for everyone. The countess tried to make him talk, but
he went on naively looking around through his spectacles as if in search
of somebody and answered all her questions in monosyllables. He was in
the way and was the only one who did not notice the fact. Most of the
guests, knowing of the affair with the bear, looked with curiosity at
this big, stout, quiet man, wondering how such a clumsy, modest fellow
could have played such a prank on a policeman.

"You have only lately arrived?" the countess asked him.

"Oui, madame," replied he, looking around him.

"You have not yet seen my husband?"

"Non, madame." He smiled quite inappropriately.

"You have been in Paris recently, I believe? I suppose it's very
interesting."

"Very interesting."

The countess exchanged glances with Anna Mikhaylovna. The latter
understood that she was being asked to entertain this young man, and
sitting down beside him she began to speak about his father; but he
answered her, as he had the countess, only in monosyllables. The other
guests were all conversing with one another. "The Razumovskis... It was
charming... You are very kind... Countess Apraksina..." was heard on all
sides. The countess rose and went into the ballroom.

"Marya Dmitrievna?" came her voice from there.

"Herself," came the answer in a rough voice, and Marya Dmitrievna
entered the room.

All the unmarried ladies and even the married ones except the very
oldest rose. Marya Dmitrievna paused at the door. Tall and stout,
holding high her fifty-year-old head with its gray curls, she stood
surveying the guests, and leisurely arranged her wide sleeves as if
rolling them up. Marya Dmitrievna always spoke in Russian.

"Health and happiness to her whose name day we are keeping and to her
children," she said, in her loud, full-toned voice which drowned all
others. "Well, you old sinner," she went on, turning to the count who
was kissing her hand, "you're feeling dull in Moscow, I daresay? Nowhere
to hunt with your dogs? But what is to be done, old man? Just see how
these nestlings are growing up," and she pointed to the girls. "You must
look for husbands for them whether you like it or not...."

"Well," said she, "how's my Cossack?" (Marya Dmitrievna always called
Natasha a Cossack) and she stroked the child's arm as she came up
fearless and gay to kiss her hand. "I know she's a scamp of a girl, but
I like her."

She took a pair of pear-shaped ruby earrings from her huge reticule and,
having given them to the rosy Natasha, who beamed with the pleasure of
her saint's-day fete, turned away at once and addressed herself to
Pierre.

"Eh, eh, friend! Come here a bit," said she, assuming a soft high tone
of voice. "Come here, my friend..." and she ominously tucked up her
sleeves still higher. Pierre approached, looking at her in a childlike
way through his spectacles.

"Come nearer, come nearer, friend! I used to be the only one to tell
your father the truth when he was in favor, and in your case it's my
evident duty." She paused. All were silent, expectant of what was to
follow, for this was clearly only a prelude.

"A fine lad! My word! A fine lad!... His father lies on his deathbed and
he amuses himself setting a policeman astride a bear! For shame, sir,
for shame! It would be better if you went to the war."

She turned away and gave her hand to the count, who could hardly keep
from laughing.

"Well, I suppose it is time we were at table?" said Marya Dmitrievna.

The count went in first with Marya Dmitrievna, the countess followed on
the arm of a colonel of hussars, a man of importance to them because
Nicholas was to go with him to the regiment; then came Anna Mikhaylovna
with Shinshin. Berg gave his arm to Vera. The smiling Julie Karagina
went in with Nicholas. After them other couples followed, filling the
whole dining hall, and last of all the children, tutors, and governesses
followed singly. The footmen began moving about, chairs scraped, the
band struck up in the gallery, and the guests settled down in their
places. Then the strains of the count's household band were replaced by
the clatter of knives and forks, the voices of visitors, and the soft
steps of the footmen. At one end of the table sat the countess with
Marya Dmitrievna on her right and Anna Mikhaylovna on her left, the
other lady visitors were farther down. At the other end sat the count,
with the hussar colonel on his left and Shinshin and the other male
visitors on his right. Midway down the long table on one side sat the
grownup young people: Vera beside Berg, and Pierre beside Boris; and on
the other side, the children, tutors, and governesses. From behind the
crystal decanters and fruit vases, the count kept glancing at his wife
and her tall cap with its light-blue ribbons, and busily filled his
neighbors' glasses, not neglecting his own. The countess in turn,
without omitting her duties as hostess, threw significant glances from
behind the pineapples at her husband whose face and bald head seemed by
their redness to contrast more than usual with his gray hair. At the
ladies' end an even chatter of voices was heard all the time, at the
men's end the voices sounded louder and louder, especially that of the
colonel of hussars who, growing more and more flushed, ate and drank so
much that the count held him up as a pattern to the other guests. Berg
with tender smiles was saying to Vera that love is not an earthly but a
heavenly feeling. Boris was telling his new friend Pierre who the guests
were and exchanging glances with Natasha, who was sitting opposite.
Pierre spoke little but examined the new faces, and ate a great deal. Of
the two soups he chose turtle with savory patties and went on to the
game without omitting a single dish or one of the wines. These latter
the butler thrust mysteriously forward, wrapped in a napkin, from behind
the next man's shoulders and whispered: "Dry Madeira"... "Hungarian"...
or "Rhine wine" as the case might be. Of the four crystal glasses
engraved with the count's monogram that stood before his plate, Pierre
held out one at random and drank with enjoyment, gazing with ever-
increasing amiability at the other guests. Natasha, who sat opposite,
was looking at Boris as girls of thirteen look at the boy they are in
love with and have just kissed for the first time. Sometimes that same
look fell on Pierre, and that funny lively little girl's look made him
inclined to laugh without knowing why.

Nicholas sat at some distance from Sonya, beside Julie Karagina, to whom
he was again talking with the same involuntary smile. Sonya wore a
company smile but was evidently tormented by jealousy; now she turned
pale, now blushed and strained every nerve to overhear what Nicholas and
Julie were saying to one another. The governess kept looking round
uneasily as if preparing to resent any slight that might be put upon the
children. The German tutor was trying to remember all the dishes, wines,
and kinds of dessert, in order to send a full description of the dinner
to his people in Germany; and he felt greatly offended when the butler
with a bottle wrapped in a napkin passed him by. He frowned, trying to
appear as if he did not want any of that wine, but was mortified because
no one would understand that it was not to quench his thirst or from
greediness that he wanted it, but simply from a conscientious desire for
knowledge.




CHAPTER XIX

At the men's end of the table the talk grew more and more animated. The
colonel told them that the declaration of war had already appeared in
Petersburg and that a copy, which he had himself seen, had that day been
forwarded by courier to the commander-in-chief.

"And why the deuce are we going to fight Bonaparte?" remarked Shinshin.
"He has stopped Austria's cackle and I fear it will be our turn next."

The colonel was a stout, tall, plethoric German, evidently devoted to
the service and patriotically Russian. He resented Shinshin's remark.

"It is for the reasson, my goot sir," said he, speaking with a German
accent, "for the reasson zat ze Emperor knows zat. He declares in ze
manifessto zat he cannot fiew wiz indifference ze danger vreatening
Russia and zat ze safety and dignity of ze Empire as vell as ze sanctity
of its alliances..." he spoke this last word with particular emphasis as
if in it lay the gist of the matter.

Then with the unerring official memory that characterized him he
repeated from the opening words of the manifesto:

... and the wish, which constitutes the Emperor's sole and absolute aim-
-to establish peace in Europe on firm foundations--has now decided him
to despatch part of the army abroad and to create a new condition for
the attainment of that purpose.

"Zat, my dear sir, is vy..." he concluded, drinking a tumbler of wine
with dignity and looking to the count for approval.

"Connaissez-vous le Proverbe: * 'Jerome, Jerome, do not roam, but turn
spindles at home!'?" said Shinshin, puckering his brows and smiling.
"Cela nous convient a merveille.*(2) Suvorov now--he knew what he was
about; yet they beat him a plate couture,*(3) and where are we to find
Suvorovs now? Je vous demande un peu,"*(4) said he, continually changing
from French to Russian.


*Do you know the proverb?

*(2) That suits us down to the ground.

*(3) Hollow.

*(4) I just ask you that.

"Ve must vight to the last tr-r-op of our plood!" said the colonel,
thumping the table; "and ve must tie for our Emperor, and zen all vill
pe vell. And ve must discuss it as little as po-o-ossible"... he dwelt
particularly on the word possible... "as po-o-ossible," he ended, again
turning to the count. "Zat is how ve old hussars look at it, and zere's
an end of it! And how do you, a young man and a young hussar, how do you
judge of it?" he added, addressing Nicholas, who when he heard that the
war was being discussed had turned from his partner with eyes and ears
intent on the colonel.

"I am quite of your opinion," replied Nicholas, flaming up, turning his
plate round and moving his wineglasses about with as much decision and
desperation as though he were at that moment facing some great danger.
"I am convinced that we Russians must die or conquer," he concluded,
conscious--as were others--after the words were uttered that his remarks
were too enthusiastic and emphatic for the occasion and were therefore
awkward.

"What you said just now was splendid!" said his partner Julie.

Sonya trembled all over and blushed to her ears and behind them and down
to her neck and shoulders while Nicholas was speaking.

Pierre listened to the colonel's speech and nodded approvingly.

"That's fine," said he.

"The young man's a real hussar!" shouted the colonel, again thumping the
table.

"What are you making such a noise about over there?" Marya Dmitrievna's
deep voice suddenly inquired from the other end of the table. "What are
you thumping the table for?" she demanded of the hussar, "and why are
you exciting yourself? Do you think the French are here?"

"I am speaking ze truce," replied the hussar with a smile.

"It's all about the war," the count shouted down the table. "You know my
son's going, Marya Dmitrievna? My son is going."

"I have four sons in the army but still I don't fret. It is all in God's
hands. You may die in your bed or God may spare you in a battle,"
replied Marya Dmitrievna's deep voice, which easily carried the whole
length of the table.

"That's true!"

Once more the conversations concentrated, the ladies' at the one end and
the men's at the other.

"You won't ask," Natasha's little brother was saying; "I know you won't
ask!"

"I will," replied Natasha.

Her face suddenly flushed with reckless and joyous resolution. She half
rose, by a glance inviting Pierre, who sat opposite, to listen to what
was coming, and turning to her mother:

"Mamma!" rang out the clear contralto notes of her childish voice,
audible the whole length of the table.

"What is it?" asked the countess, startled; but seeing by her daughter's
face that it was only mischief, she shook a finger at her sternly with a
threatening and forbidding movement of her head.

The conversation was hushed.

"Mamma! What sweets are we going to have?" and Natasha's voice sounded
still more firm and resolute.

The countess tried to frown, but could not. Marya Dmitrievna shook her
fat finger.

"Cossack!" she said threateningly.

Most of the guests, uncertain how to regard this sally, looked at the
elders.

"You had better take care!" said the countess.

"Mamma! What sweets are we going to have?" Natasha again cried boldly,
with saucy gaiety, confident that her prank would be taken in good part.

Sonya and fat little Petya doubled up with laughter.

"You see! I have asked," whispered Natasha to her little brother and to
Pierre, glancing at him again.

"Ice pudding, but you won't get any," said Marya Dmitrievna.

Natasha saw there was nothing to be afraid of and so she braved even
Marya Dmitrievna.

"Marya Dmitrievna! What kind of ice pudding? I don't like ice cream."

"Carrot ices."

"No! What kind, Marya Dmitrievna? What kind?" she almost screamed; "I
want to know!"

Marya Dmitrievna and the countess burst out laughing, and all the guests
joined in. Everyone laughed, not at Marya Dmitrievna's answer but at the
incredible boldness and smartness of this little girl who had dared to
treat Marya Dmitrievna in this fashion.

Natasha only desisted when she had been told that there would be
pineapple ice. Before the ices, champagne was served round. The band
again struck up, the count and countess kissed, and the guests, leaving
their seats, went up to "congratulate" the countess, and reached across
the table to clink glasses with the count, with the children, and with
one another. Again the footmen rushed about, chairs scraped, and in the
same order in which they had entered but with redder faces, the guests
returned to the drawing room and to the count's study.




CHAPTER XX

The card tables were drawn out, sets made up for boston, and the count's
visitors settled themselves, some in the two drawing rooms, some in the
sitting room, some in the library.

The count, holding his cards fanwise, kept himself with difficulty from
dropping into his usual after-dinner nap, and laughed at everything. The
young people, at the countess' instigation, gathered round the
clavichord and harp. Julie by general request played first. After she
had played a little air with variations on the harp, she joined the
other young ladies in begging Natasha and Nicholas, who were noted for
their musical talent, to sing something. Natasha, who was treated as
though she were grown up, was evidently very proud of this but at the
same time felt shy.

"What shall we sing?" she said.

"'The Brook,'" suggested Nicholas.

"Well, then, let's be quick. Boris, come here," said Natasha. "But where
is Sonya?"

She looked round and seeing that her friend was not in the room ran to
look for her.

Running into Sonya's room and not finding her there, Natasha ran to the
nursery, but Sonya was not there either. Natasha concluded that she must
be on the chest in the passage. The chest in the passage was the place
of mourning for the younger female generation in the Rostov household.
And there in fact was Sonya lying face downward on Nurse's dirty feather
bed on the top of the chest, crumpling her gauzy pink dress under her,
hiding her face with her slender fingers, and sobbing so convulsively
that her bare little shoulders shook. Natasha's face, which had been so
radiantly happy all that saint's day, suddenly changed: her eyes became
fixed, and then a shiver passed down her broad neck and the corners of
her mouth drooped.

"Sonya! What is it? What is the matter?... Oo... Oo... Oo...!" And
Natasha's large mouth widened, making her look quite ugly, and she began
to wail like a baby without knowing why, except that Sonya was crying.
Sonya tried to lift her head to answer but could not, and hid her face
still deeper in the bed. Natasha wept, sitting on the blue-striped
feather bed and hugging her friend. With an effort Sonya sat up and
began wiping her eyes and explaining.

"Nicholas is going away in a week's time, his... papers... have come...
he told me himself... but still I should not cry," and she showed a
paper she held in her hand--with the verses Nicholas had written,
"still, I should not cry, but you can't... no one can understand... what
a soul he has!"

And she began to cry again because he had such a noble soul.

"It's all very well for you... I am not envious... I love you and Boris
also," she went on, gaining a little strength; "he is nice... there are
no difficulties in your way.... But Nicholas is my cousin... one would
have to... the Metropolitan himself... and even then it can't be done.
And besides, if she tells Mamma" (Sonya looked upon the countess as her
mother and called her so) "that I am spoiling Nicholas' career and am
heartless and ungrateful, while truly... God is my witness," and she
made the sign of the cross, "I love her so much, and all of you, only
Vera... And what for? What have I done to her? I am so grateful to you
that I would willingly sacrifice everything, only I have nothing...."

Sonya could not continue, and again hid her face in her hands and in the
feather bed. Natasha began consoling her, but her face showed that she
understood all the gravity of her friend's trouble.

"Sonya," she suddenly exclaimed, as if she had guessed the true reason
of her friend's sorrow, "I'm sure Vera has said something to you since
dinner? Hasn't she?"

"Yes, these verses Nicholas wrote himself and I copied some others, and
she found them on my table and said she'd show them to Mamma, and that I
was ungrateful, and that Mamma would never allow him to marry me, but
that he'll marry Julie. You see how he's been with her all day...
Natasha, what have I done to deserve it?..."

And again she began to sob, more bitterly than before. Natasha lifted
her up, hugged her, and, smiling through her tears, began comforting
her.

"Sonya, don't believe her, darling! Don't believe her! Do you remember
how we and Nicholas, all three of us, talked in the sitting room after
supper? Why, we settled how everything was to be. I don't quite remember
how, but don't you remember that it could all be arranged and how nice
it all was? There's Uncle Shinshin's brother has married his first
cousin. And we are only second cousins, you know. And Boris says it is
quite possible. You know I have told him all about it. And he is so
clever and so good!" said Natasha. "Don't you cry, Sonya, dear love,
darling Sonya!" and she kissed her and laughed. "Vera's spiteful; never
mind her! And all will come right and she won't say anything to Mamma.
Nicholas will tell her himself, and he doesn't care at all for Julie."

Natasha kissed her on the hair.

Sonya sat up. The little kitten brightened, its eyes shone, and it
seemed ready to lift its tail, jump down on its soft paws, and begin
playing with the ball of worsted as a kitten should.

"Do you think so?... Really? Truly?" she said, quickly smoothing her
frock and hair.

"Really, truly!" answered Natasha, pushing in a crisp lock that had
strayed from under her friend's plaits.

Both laughed.

"Well, let's go and sing 'The Brook.'"

"Come along!"

"Do you know, that fat Pierre who sat opposite me is so funny!" said
Natasha, stopping suddenly. "I feel so happy!"

And she set off at a run along the passage.

Sonya, shaking off some down which clung to her and tucking away the
verses in the bosom of her dress close to her bony little chest, ran
after Natasha down the passage into the sitting room with flushed face
and light, joyous steps. At the visitors' request the young people sang
the quartette, "The Brook," with which everyone was delighted. Then
Nicholas sang a song he had just learned:


At nighttime in the moon's fair glow How sweet, as fancies wander free,
To feel that in this world there's one Who still is thinking but of
thee!

That while her fingers touch the harp Wafting sweet music o'er the lea,
It is for thee thus swells her heart, Sighing its message out to thee...

A day or two, then bliss unspoilt, But oh! till then I cannot live!...

He had not finished the last verse before the young people began to get
ready to dance in the large hall, and the sound of the feet and the
coughing of the musicians were heard from the gallery.

Pierre was sitting in the drawing-room where Shinshin had engaged him,
as a man recently returned from abroad, in a political conversation in
which several others joined but which bored Pierre. When the music began
Natasha came in and walking straight up to Pierre said, laughing and
blushing:

"Mamma told me to ask you to join the dancers."

"I am afraid of mixing the figures," Pierre replied; "but if you will be
my teacher..." And lowering his big arm he offered it to the slender
little girl.

While the couples were arranging themselves and the musicians tuning up,
Pierre sat down with his little partner. Natasha was perfectly happy;
she was dancing with a grown-up man, who had been abroad. She was
sitting in a conspicuous place and talking to him like a grown-up lady.
She had a fan in her hand that one of the ladies had given her to hold.
Assuming quite the pose of a society woman (heaven knows when and where
she had learned it) she talked with her partner, fanning herself and
smiling over the fan.

"Dear, dear! Just look at her!" exclaimed the countess as she crossed
the ballroom, pointing to Natasha.

Natasha blushed and laughed.

"Well, really, Mamma! Why should you? What is there to be surprised at?"

In the midst of the third ecossaise there was a clatter of chairs being
pushed back in the sitting room where the count and Marya Dmitrievna had
been playing cards with the majority of the more distinguished and older
visitors. They now, stretching themselves after sitting so long, and
replacing their purses and pocketbooks, entered the ballroom. First came
Marya Dmitrievna and the count, both with merry countenances. The count,
with playful ceremony somewhat in ballet style, offered his bent arm to
Marya Dmitrievna. He drew himself up, a smile of debonair gallantry lit
up his face and as soon as the last figure of the ecossaise was ended,
he clapped his hands to the musicians and shouted up to their gallery,
addressing the first violin:

"Semen! Do you know the Daniel Cooper?"

This was the count's favorite dance, which he had danced in his youth.
(Strictly speaking, Daniel Cooper was one figure of the anglaise.)

"Look at Papa!" shouted Natasha to the whole company, and quite
forgetting that she was dancing with a grown-up partner she bent her
curly head to her knees and made the whole room ring with her laughter.

And indeed everybody in the room looked with a smile of pleasure at the
jovial old gentleman, who standing beside his tall and stout partner,
Marya Dmitrievna, curved his arms, beat time, straightened his
shoulders, turned out his toes, tapped gently with his foot, and, by a
smile that broadened his round face more and more, prepared the
onlookers for what was to follow. As soon as the provocatively gay
strains of Daniel Cooper (somewhat resembling those of a merry peasant
dance) began to sound, all the doorways of the ballroom were suddenly
filled by the domestic serfs--the men on one side and the women on the
other--who with beaming faces had come to see their master making merry.

"Just look at the master! A regular eagle he is!" loudly remarked the
nurse, as she stood in one of the doorways.

The count danced well and knew it. But his partner could not and did not
want to dance well. Her enormous figure stood erect, her powerful arms
hanging down (she had handed her reticule to the countess), and only her
stern but handsome face really joined in the dance. What was expressed
by the whole of the count's plump figure, in Marya Dmitrievna found
expression only in her more and more beaming face and quivering nose.
But if the count, getting more and more into the swing of it, charmed
the spectators by the unexpectedness of his adroit maneuvers and the
agility with which he capered about on his light feet, Marya Dmitrievna
produced no less impression by slight exertions--the least effort to
move her shoulders or bend her arms when turning, or stamp her foot--
which everyone appreciated in view of her size and habitual severity.
The dance grew livelier and livelier. The other couples could not
attract a moment's attention to their own evolutions and did not even
try to do so. All were watching the count and Marya Dmitrievna. Natasha
kept pulling everyone by sleeve or dress, urging them to "look at Papa!"
though as it was they never took their eyes off the couple. In the
intervals of the dance the count, breathing deeply, waved and shouted to
the musicians to play faster. Faster, faster, and faster; lightly, more
lightly, and yet more lightly whirled the count, flying round Marya
Dmitrievna, now on his toes, now on his heels; until, turning his
partner round to her seat, he executed the final pas, raising his soft
foot backwards, bowing his perspiring head, smiling and making a wide
sweep with his arm, amid a thunder of applause and laughter led by
Natasha. Both partners stood still, breathing heavily and wiping their
faces with their cambric handkerchiefs.

"That's how we used to dance in our time, ma chere," said the count.

"That was a Daniel Cooper!" exclaimed Marya Dmitrievna, tucking up her
sleeves and puffing heavily.




CHAPTER XXI

While in the Rostovs' ballroom the sixth anglaise was being danced, to a
tune in which the weary musicians blundered, and while tired footmen and
cooks were getting the supper, Count Bezukhov had a sixth stroke. The
doctors pronounced recovery impossible. After a mute confession,
communion was administered to the dying man, preparations made for the
sacrament of unction, and in his house there was the bustle and thrill
of suspense usual at such moments. Outside the house, beyond the gates,
a group of undertakers, who hid whenever a carriage drove up, waited in
expectation of an important order for an expensive funeral. The Military
Governor of Moscow, who had been assiduous in sending aides-de-camp to
inquire after the count's health, came himself that evening to bid a
last farewell to the celebrated grandee of Catherine's court, Count
Bezukhov.

The magnificent reception room was crowded. Everyone stood up
respectfully when the Military Governor, having stayed about half an
hour alone with the dying man, passed out, slightly acknowledging their
bows and trying to escape as quickly as possible from the glances fixed
on him by the doctors, clergy, and relatives of the family. Prince
Vasili, who had grown thinner and paler during the last few days,
escorted him to the door, repeating something to him several times in
low tones.

When the Military Governor had gone, Prince Vasili sat down all alone on
a chair in the ballroom, crossing one leg high over the other, leaning
his elbow on his knee and covering his face with his hand. After sitting
so for a while he rose, and, looking about him with frightened eyes,
went with unusually hurried steps down the long corridor leading to the
back of the house, to the room of the eldest princess.

Those who were in the dimly lit reception room spoke in nervous
whispers, and, whenever anyone went into or came from the dying man's
room, grew silent and gazed with eyes full of curiosity or expectancy at
his door, which creaked slightly when opened.

"The limits of human life... are fixed and may not be o'erpassed," said
an old priest to a lady who had taken a seat beside him and was
listening naively to his words.

"I wonder, is it not too late to administer unction?" asked the lady,
adding the priest's clerical title, as if she had no opinion of her own
on the subject.

"Ah, madam, it is a great sacrament," replied the priest, passing his
hand over the thin grizzled strands of hair combed back across his bald
head.

"Who was that? The Military Governor himself?" was being asked at the
other side of the room. "How young-looking he is!"

"Yes, and he is over sixty. I hear the count no longer recognizes
anyone. They wished to administer the sacrament of unction."

"I knew someone who received that sacrament seven times."

The second princess had just come from the sickroom with her eyes red
from weeping and sat down beside Dr. Lorrain, who was sitting in a
graceful pose under a portrait of Catherine, leaning his elbow on a
table.

"Beautiful," said the doctor in answer to a remark about the weather.
"The weather is beautiful, Princess; and besides, in Moscow one feels as
if one were in the country."

"Yes, indeed," replied the princess with a sigh. "So he may have
something to drink?"

Lorrain considered.

"Has he taken his medicine?"

"Yes."

The doctor glanced at his watch.

"Take a glass of boiled water and put a pinch of cream of tartar," and
he indicated with his delicate fingers what he meant by a pinch.

"Dere has neffer been a gase," a German doctor was saying to an aide-de-
camp, "dat one liffs after de sird stroke."

"And what a well-preserved man he was!" remarked the aide-de-camp. "And
who will inherit his wealth?" he added in a whisper.

"It von't go begging," replied the German with a smile.

Everyone again looked toward the door, which creaked as the second
princess went in with the drink she had prepared according to Lorrain's
instructions. The German doctor went up to Lorrain.

"Do you think he can last till morning?" asked the German, addressing
Lorrain in French which he pronounced badly.

Lorrain, pursing up his lips, waved a severely negative finger before
his nose.

"Tonight, not later," said he in a low voice, and he moved away with a
decorous smile of self-satisfaction at being able clearly to understand
and state the patient's condition.

Meanwhile Prince Vasili had opened the door into the princess' room.

In this room it was almost dark; only two tiny lamps were burning before
the icons and there was a pleasant scent of flowers and burnt pastilles.
The room was crowded with small pieces of furniture, whatnots,
cupboards, and little tables. The quilt of a high, white feather bed was
just visible behind a screen. A small dog began to bark.

"Ah, is it you, cousin?"

She rose and smoothed her hair, which was as usual so extremely smooth
that it seemed to be made of one piece with her head and covered with
varnish.

"Has anything happened?" she asked. "I am so terrified."

"No, there is no change. I only came to have a talk about business,
Catiche," * muttered the prince, seating himself wearily on the chair
she had just vacated. "You have made the place warm, I must say," he
remarked. "Well, sit down: let's have a talk."


*Catherine.

"I thought perhaps something had happened," she said with her unchanging
stonily severe expression; and, sitting down opposite the prince, she
prepared to listen.

"I wished to get a nap, mon cousin, but I can't."

"Well, my dear?" said Prince Vasili, taking her hand and bending it
downwards as was his habit.

It was plain that this "well?" referred to much that they both
understood without naming.

The princess, who had a straight, rigid body, abnormally long for her
legs, looked directly at Prince Vasili with no sign of emotion in her
prominent gray eyes. Then she shook her head and glanced up at the icons
with a sigh. This might have been taken as an expression of sorrow and
devotion, or of weariness and hope of resting before long. Prince Vasili
understood it as an expression of weariness.

"And I?" he said; "do you think it is easier for me? I am as worn out as
a post horse, but still I must have a talk with you, Catiche, a very
serious talk."

Prince Vasili said no more and his cheeks began to twitch nervously, now
on one side, now on the other, giving his face an unpleasant expression
which was never to be seen on it in a drawing room. His eyes too seemed
strange; at one moment they looked impudently sly and at the next
glanced round in alarm.

The princess, holding her little dog on her lap with her thin bony
hands, looked attentively into Prince Vasili's eyes evidently resolved
not to be the first to break silence, if she had to wait till morning.

"Well, you see, my dear princess and cousin, Catherine Semenovna,"
continued Prince Vasili, returning to his theme, apparently not without
an inner struggle; "at such a moment as this one must think of
everything. One must think of the future, of all of you... I love you
all, like children of my own, as you know."

The princess continued to look at him without moving, and with the same
dull expression.

"And then of course my family has also to be considered," Prince Vasili
went on, testily pushing away a little table without looking at her.
"You know, Catiche, that we--you three sisters, Mamontov, and my wife--
are the count's only direct heirs. I know, I know how hard it is for you
to talk or think of such matters. It is no easier for me; but, my dear,
I am getting on for sixty and must be prepared for anything. Do you know
I have sent for Pierre? The count," pointing to his portrait,
"definitely demanded that he should be called."

Prince Vasili looked questioningly at the princess, but could not make
out whether she was considering what he had just said or whether she was
simply looking at him.

"There is one thing I constantly pray God to grant, mon cousin," she
replied, "and it is that He would be merciful to him and would allow his
noble soul peacefully to leave this..."

"Yes, yes, of course," interrupted Prince Vasili impatiently, rubbing
his bald head and angrily pulling back toward him the little table that
he had pushed away. "But... in short, the fact is... you know yourself
that last winter the count made a will by which he left all his
property, not to us his direct heirs, but to Pierre."

"He has made wills enough!" quietly remarked the princess. "But he
cannot leave the estate to Pierre. Pierre is illegitimate."

"But, my dear," said Prince Vasili suddenly, clutching the little table
and becoming more animated and talking more rapidly: "what if a letter
has been written to the Emperor in which the count asks for Pierre's
legitimation? Do you understand that in consideration of the count's
services, his request would be granted?..."

The princess smiled as people do who think they know more about the
subject under discussion than those they are talking with.

"I can tell you more," continued Prince Vasili, seizing her hand, "that
letter was written, though it was not sent, and the Emperor knew of it.
The only question is, has it been destroyed or not? If not, then as soon
as all is over," and Prince Vasili sighed to intimate what he meant by
the words all is over, "and the count's papers are opened, the will and
letter will be delivered to the Emperor, and the petition will certainly
be granted. Pierre will get everything as the legitimate son."

"And our share?" asked the princess smiling ironically, as if anything
might happen, only not that.

"But, my poor Catiche, it is as clear as daylight! He will then be the
legal heir to everything and you won't get anything. You must know, my
dear, whether the will and letter were written, and whether they have
been destroyed or not. And if they have somehow been overlooked, you
ought to know where they are, and must find them, because..."

"What next?" the princess interrupted, smiling sardonically and not
changing the expression of her eyes. "I am a woman, and you think we are
all stupid; but I know this: an illegitimate son cannot inherit... un
batard!" * she added, as if supposing that this translation of the word
would effectively prove to Prince Vasili the invalidity of his
contention.


* A bastard.

"Well, really, Catiche! Can't you understand! You are so intelligent,
how is it you don't see that if the count has written a letter to the
Emperor begging him to recognize Pierre as legitimate, it follows that
Pierre will not be Pierre but will become Count Bezukhov, and will then
inherit everything under the will? And if the will and letter are not
destroyed, then you will have nothing but the consolation of having been
dutiful et tout ce qui s'ensuit! * That's certain."


* And all that follows therefrom.

"I know the will was made, but I also know that it is invalid; and you,
mon cousin, seem to consider me a perfect fool," said the princess with
the expression women assume when they suppose they are saying something
witty and stinging.

"My dear Princess Catherine Semenovna," began Prince Vasili impatiently,
"I came here not to wrangle with you, but to talk about your interests
as with a kinswoman, a good, kind, true relation. And I tell you for the
tenth time that if the letter to the Emperor and the will in Pierre's
favor are among the count's papers, then, my dear girl, you and your
sisters are not heiresses! If you don't believe me, then believe an
expert. I have just been talking to Dmitri Onufrich" (the family
solicitor) "and he says the same."

At this a sudden change evidently took place in the princess' ideas; her
thin lips grew white, though her eyes did not change, and her voice when
she began to speak passed through such transitions as she herself
evidently did not expect.

"That would be a fine thing!" said she. "I never wanted anything and I
don't now."

She pushed the little dog off her lap and smoothed her dress.

"And this is gratitude--this is recognition for those who have
sacrificed everything for his sake!" she cried. "It's splendid! Fine! I
don't want anything, Prince."

"Yes, but you are not the only one. There are your sisters..." replied
Prince Vasili.

But the princess did not listen to him.

"Yes, I knew it long ago but had forgotten. I knew that I could expect
nothing but meanness, deceit, envy, intrigue, and ingratitude--the
blackest ingratitude--in this house..."

"Do you or do you not know where that will is?" insisted Prince Vasili,
his cheeks twitching more than ever.

"Yes, I was a fool! I still believed in people, loved them, and
sacrificed myself. But only the base, the vile succeed! I know who has
been intriguing!"

The princess wished to rise, but the prince held her by the hand. She
had the air of one who has suddenly lost faith in the whole human race.
She gave her companion an angry glance.

"There is still time, my dear. You must remember, Catiche, that it was
all done casually in a moment of anger, of illness, and was afterwards
forgotten. Our duty, my dear, is to rectify his mistake, to ease his
last moments by not letting him commit this injustice, and not to let
him die feeling that he is rendering unhappy those who..."

"Who sacrificed everything for him," chimed in the princess, who would
again have risen had not the prince still held her fast, "though he
never could appreciate it. No, mon cousin," she added with a sigh, "I
shall always remember that in this world one must expect no reward, that
in this world there is neither honor nor justice. In this world one has
to be cunning and cruel."

"Now come, come! Be reasonable. I know your excellent heart."

"No, I have a wicked heart."

"I know your heart," repeated the prince. "I value your friendship and
wish you to have as good an opinion of me. Don't upset yourself, and let
us talk sensibly while there is still time, be it a day or be it but an
hour.... Tell me all you know about the will, and above all where it is.
You must know. We will take it at once and show it to the count. He has,
no doubt, forgotten it and will wish to destroy it. You understand that
my sole desire is conscientiously to carry out his wishes; that is my
only reason for being here. I came simply to help him and you."

"Now I see it all! I know who has been intriguing--I know!" cried the
princess.

"That's not the point, my dear."

"It's that protege of yours, that sweet Princess Drubetskaya, that Anna
Mikhaylovna whom I would not take for a housemaid... the infamous, vile
woman!"

"Do not let us lose any time..."

"Ah, don't talk to me! Last winter she wheedled herself in here and told
the count such vile, disgraceful things about us, especially about
Sophie--I can't repeat them--that it made the count quite ill and he
would not see us for a whole fortnight. I know it was then he wrote this
vile, infamous paper, but I thought the thing was invalid."

"We've got to it at last--why did you not tell me about it sooner?"

"It's in the inlaid portfolio that he keeps under his pillow," said the
princess, ignoring his question. "Now I know! Yes; if I have a sin, a
great sin, it is hatred of that vile woman!" almost shrieked the
princess, now quite changed. "And what does she come worming herself in
here for? But I will give her a piece of my mind. The time will come!"




CHAPTER XXII

While these conversations were going on in the reception room and the
princess' room, a carriage containing Pierre (who had been sent for) and
Anna Mikhaylovna (who found it necessary to accompany him) was driving
into the court of Count Bezukhov's house. As the wheels rolled softly
over the straw beneath the windows, Anna Mikhaylovna, having turned with
words of comfort to her companion, realized that he was asleep in his
corner and woke him up. Rousing himself, Pierre followed Anna
Mikhaylovna out of the carriage, and only then began to think of the
interview with his dying father which awaited him. He noticed that they
had not come to the front entrance but to the back door. While he was
getting down from the carriage steps two men, who looked like
tradespeople, ran hurriedly from the entrance and hid in the shadow of
the wall. Pausing for a moment, Pierre noticed several other men of the
same kind hiding in the shadow of the house on both sides. But neither
Anna Mikhaylovna nor the footman nor the coachman, who could not help
seeing these people, took any notice of them. "It seems to be all
right," Pierre concluded, and followed Anna Mikhaylovna. She hurriedly
ascended the narrow dimly lit stone staircase, calling to Pierre, who
was lagging behind, to follow. Though he did not see why it was
necessary for him to go to the count at all, still less why he had to go
by the back stairs, yet judging by Anna Mikhaylovna's air of assurance
and haste, Pierre concluded that it was all absolutely necessary.
Halfway up the stairs they were almost knocked over by some men who,
carrying pails, came running downstairs, their boots clattering. These
men pressed close to the wall to let Pierre and Anna Mikhaylovna pass
and did not evince the least surprise at seeing them there.

"Is this the way to the princesses' apartments?" asked Anna Mikhaylovna
of one of them.

"Yes," replied a footman in a bold loud voice, as if anything were now
permissible; "the door to the left, ma'am."

"Perhaps the count did not ask for me," said Pierre when he reached the
landing. "I'd better go to my own room."

Anna Mikhaylovna paused and waited for him to come up.

"Ah, my friend!" she said, touching his arm as she had done her son's
when speaking to him that afternoon, "believe me I suffer no less than
you do, but be a man!"

"But really, hadn't I better go away?" he asked, looking kindly at her
over his spectacles.

"Ah, my dear friend! Forget the wrongs that may have been done you.
Think that he is your father... perhaps in the agony of death." She
sighed. "I have loved you like a son from the first. Trust yourself to
me, Pierre. I shall not forget your interests."

Pierre did not understand a word, but the conviction that all this had
to be grew stronger, and he meekly followed Anna Mikhaylovna who was
already opening a door.

This door led into a back anteroom. An old man, a servant of the
princesses, sat in a corner knitting a stocking. Pierre had never been
in this part of the house and did not even know of the existence of
these rooms. Anna Mikhaylovna, addressing a maid who was hurrying past
with a decanter on a tray as "my dear" and "my sweet," asked about the
princess' health and then led Pierre along a stone passage. The first
door on the left led into the princesses' apartments. The maid with the
decanter in her haste had not closed the door (everything in the house
was done in haste at that time), and Pierre and Anna Mikhaylovna in
passing instinctively glanced into the room, where Prince Vasili and the
eldest princess were sitting close together talking. Seeing them pass,
Prince Vasili drew back with obvious impatience, while the princess
jumped up and with a gesture of desperation slammed the door with all
her might.

This action was so unlike her usual composure and the fear depicted on
Prince Vasili's face so out of keeping with his dignity that Pierre
stopped and glanced inquiringly over his spectacles at his guide. Anna
Mikhaylovna evinced no surprise, she only smiled faintly and sighed, as
if to say that this was no more than she had expected.

"Be a man, my friend. I will look after your interests," said she in
reply to his look, and went still faster along the passage.

Pierre could not make out what it was all about, and still less what
"watching over his interests" meant, but he decided that all these
things had to be. From the passage they went into a large, dimly lit
room adjoining the count's reception room. It was one of those sumptuous
but cold apartments known to Pierre only from the front approach, but
even in this room there now stood an empty bath, and water had been
spilled on the carpet. They were met by a deacon with a censer and by a
servant who passed out on tiptoe without heeding them. They went into
the reception room familiar to Pierre, with two Italian windows opening
into the conservatory, with its large bust and full length portrait of
Catherine the Great. The same people were still sitting here in almost
the same positions as before, whispering to one another. All became
silent and turned to look at the pale tear-worn Anna Mikhaylovna as she
entered, and at the big stout figure of Pierre who, hanging his head,
meekly followed her.

Anna Mikhaylovna's face expressed a consciousness that the decisive
moment had arrived. With the air of a practical Petersburg lady she now,
keeping Pierre close beside her, entered the room even more boldly than
that afternoon. She felt that as she brought with her the person the
dying man wished to see, her own admission was assured. Casting a rapid
glance at all those in the room and noticing the count's confessor
there, she glided up to him with a sort of amble, not exactly bowing yet
seeming to grow suddenly smaller, and respectfully received the blessing
first of one and then of another priest.

"God be thanked that you are in time," said she to one of the priests;
"all we relatives have been in such anxiety. This young man is the
count's son," she added more softly. "What a terrible moment!"

Having said this she went up to the doctor.

"Dear doctor," said she, "this young man is the count's son. Is there
any hope?"

The doctor cast a rapid glance upwards and silently shrugged his
shoulders. Anna Mikhaylovna with just the same movement raised her
shoulders and eyes, almost closing the latter, sighed, and moved away
from the doctor to Pierre. To him, in a particularly respectful and
tenderly sad voice, she said:

"Trust in His mercy!" and pointing out a small sofa for him to sit and
wait for her, she went silently toward the door that everyone was
watching and it creaked very slightly as she disappeared behind it.

Pierre, having made up his mind to obey his monitress implicitly, moved
toward the sofa she had indicated. As soon as Anna Mikhaylovna had
disappeared he noticed that the eyes of all in the room turned to him
with something more than curiosity and sympathy. He noticed that they
whispered to one another, casting significant looks at him with a kind
of awe and even servility. A deference such as he had never before
received was shown him. A strange lady, the one who had been talking to
the priests, rose and offered him her seat; an aide-de-camp picked up
and returned a glove Pierre had dropped; the doctors became respectfully
silent as he passed by, and moved to make way for him. At first Pierre
wished to take another seat so as not to trouble the lady, and also to
pick up the glove himself and to pass round the doctors who were not
even in his way; but all at once he felt that this would not do, and
that tonight he was a person obliged to perform some sort of awful rite
which everyone expected of him, and that he was therefore bound to
accept their services. He took the glove in silence from the aide-de-
camp, and sat down in the lady's chair, placing his huge hands
symmetrically on his knees in the naive attitude of an Egyptian statue,
and decided in his own mind that all was as it should be, and that in
order not to lose his head and do foolish things he must not act on his
own ideas tonight, but must yield himself up entirely to the will of
those who were guiding him.

Not two minutes had passed before Prince Vasili with head erect
majestically entered the room. He was wearing his long coat with three
stars on his breast. He seemed to have grown thinner since the morning;
his eyes seemed larger than usual when he glanced round and noticed
Pierre. He went up to him, took his hand (a thing he never used to do),
and drew it downwards as if wishing to ascertain whether it was firmly
fixed on.

"Courage, courage, my friend! He has asked to see you. That is well!"
and he turned to go.

But Pierre thought it necessary to ask: "How is..." and hesitated, not
knowing whether it would be proper to call the dying man "the count,"
yet ashamed to call him "father."

"He had another stroke about half an hour ago. Courage, my friend..."

Pierre's mind was in such a confused state that the word "stroke"
suggested to him a blow from something. He looked at Prince Vasili in
perplexity, and only later grasped that a stroke was an attack of
illness. Prince Vasili said something to Lorrain in passing and went
through the door on tiptoe. He could not walk well on tiptoe and his
whole body jerked at each step. The eldest princess followed him, and
the priests and deacons and some servants also went in at the door.
Through that door was heard a noise of things being moved about, and at
last Anna Mikhaylovna, still with the same expression, pale but resolute
in the discharge of duty, ran out and touching Pierre lightly on the arm
said:

"The divine mercy is inexhaustible! Unction is about to be administered.
Come."

Pierre went in at the door, stepping on the soft carpet, and noticed
that the strange lady, the aide-de-camp, and some of the servants, all
followed him in, as if there were now no further need for permission to
enter that room.




CHAPTER XXIII

Pierre well knew this large room divided by columns and an arch, its
walls hung round with Persian carpets. The part of the room behind the
columns, with a high silk-curtained mahogany bedstead on one side and on
the other an immense case containing icons, was brightly illuminated
with red light like a Russian church during evening service. Under the
gleaming icons stood a long invalid chair, and in that chair on snowy-
white smooth pillows, evidently freshly changed, Pierre saw--covered to
the waist by a bright green quilt--the familiar, majestic figure of his
father, Count Bezukhov, with that gray mane of hair above his broad
forehead which reminded one of a lion, and the deep characteristically
noble wrinkles of his handsome, ruddy face. He lay just under the icons;
his large thick hands outside the quilt. Into the right hand, which was
lying palm downwards, a wax taper had been thrust between forefinger and
thumb, and an old servant, bending over from behind the chair, held it
in position. By the chair stood the priests, their long hair falling
over their magnificent glittering vestments, with lighted tapers in
their hands, slowly and solemnly conducting the service. A little behind
them stood the two younger princesses holding handkerchiefs to their
eyes, and just in front of them their eldest sister, Catiche, with a
vicious and determined look steadily fixed on the icons, as though
declaring to all that she could not answer for herself should she glance
round. Anna Mikhaylovna, with a meek, sorrowful, and all-forgiving
expression on her face, stood by the door near the strange lady. Prince
Vasili in front of the door, near the invalid chair, a wax taper in his
left hand, was leaning his left arm on the carved back of a velvet chair
he had turned round for the purpose, and was crossing himself with his
right hand, turning his eyes upward each time he touched his forehead.
His face wore a calm look of piety and resignation to the will of God.
"If you do not understand these sentiments," he seemed to be saying, "so
much the worse for you!"

Behind him stood the aide-de-camp, the doctors, and the menservants; the
men and women had separated as in church. All were silently crossing
themselves, and the reading of the church service, the subdued chanting
of deep bass voices, and in the intervals sighs and the shuffling of
feet were the only sounds that could be heard. Anna Mikhaylovna, with an
air of importance that showed that she felt she quite knew what she was
about, went across the room to where Pierre was standing and gave him a
taper. He lit it and, distracted by observing those around him, began
crossing himself with the hand that held the taper.

Sophie, the rosy, laughter-loving, youngest princess with the mole,
watched him. She smiled, hid her face in her handkerchief, and remained
with it hidden for awhile; then looking up and seeing Pierre she again
began to laugh. She evidently felt unable to look at him without
laughing, but could not resist looking at him: so to be out of
temptation she slipped quietly behind one of the columns. In the midst
of the service the voices of the priests suddenly ceased, they whispered
to one another, and the old servant who was holding the count's hand got
up and said something to the ladies. Anna Mikhaylovna stepped forward
and, stooping over the dying man, beckoned to Lorrain from behind her
back. The French doctor held no taper; he was leaning against one of the
columns in a respectful attitude implying that he, a foreigner, in spite
of all differences of faith, understood the full importance of the rite
now being performed and even approved of it. He now approached the sick
man with the noiseless step of one in full vigor of life, with his
delicate white fingers raised from the green quilt the hand that was
free, and turning sideways felt the pulse and reflected a moment. The
sick man was given something to drink, there was a stir around him, then
the people resumed their places and the service continued. During this
interval Pierre noticed that Prince Vasili left the chair on which he
had been leaning, and--with an air which intimated that he knew what he
was about and if others did not understand him it was so much the worse
for them--did not go up to the dying man, but passed by him, joined the
eldest princess, and moved with her to the side of the room where stood
the high bedstead with its silken hangings. On leaving the bed both
Prince Vasili and the princess passed out by a back door, but returned
to their places one after the other before the service was concluded.
Pierre paid no more attention to this occurrence than to the rest of
what went on, having made up his mind once for all that what he saw
happening around him that evening was in some way essential.

The chanting of the service ceased, and the voice of the priest was
heard respectfully congratulating the dying man on having received the
sacrament. The dying man lay as lifeless and immovable as before. Around
him everyone began to stir: steps were audible and whispers, among which
Anna Mikhaylovna's was the most distinct.

Pierre heard her say:

"Certainly he must be moved onto the bed; here it will be impossible..."

The sick man was so surrounded by doctors, princesses, and servants that
Pierre could no longer see the reddish-yellow face with its gray mane--
which, though he saw other faces as well, he had not lost sight of for a
single moment during the whole service. He judged by the cautious
movements of those who crowded round the invalid chair that they had
lifted the dying man and were moving him.

"Catch hold of my arm or you'll drop him!" he heard one of the servants
say in a frightened whisper. "Catch hold from underneath. Here!"
exclaimed different voices; and the heavy breathing of the bearers and
the shuffling of their feet grew more hurried, as if the weight they
were carrying were too much for them.

As the bearers, among whom was Anna Mikhaylovna, passed the young man he
caught a momentary glimpse between their heads and backs of the dying
man's high, stout, uncovered chest and powerful shoulders, raised by
those who were holding him under the armpits, and of his gray, curly,
leonine head. This head, with its remarkably broad brow and cheekbones,
its handsome, sensual mouth, and its cold, majestic expression, was not
disfigured by the approach of death. It was the same as Pierre
remembered it three months before, when the count had sent him to
Petersburg. But now this head was swaying helplessly with the uneven
movements of the bearers, and the cold listless gaze fixed itself upon
nothing.

After a few minutes' bustle beside the high bedstead, those who had
carried the sick man dispersed. Anna Mikhaylovna touched Pierre's hand
and said, "Come." Pierre went with her to the bed on which the sick man
had been laid in a stately pose in keeping with the ceremony just
completed. He lay with his head propped high on the pillows. His hands
were symmetrically placed on the green silk quilt, the palms downward.
When Pierre came up the count was gazing straight at him, but with a
look the significance of which could not be understood by mortal man.
Either this look meant nothing but that as long as one has eyes they
must look somewhere, or it meant too much. Pierre hesitated, not knowing
what to do, and glanced inquiringly at his guide. Anna Mikhaylovna made
a hurried sign with her eyes, glancing at the sick man's hand and moving
her lips as if to send it a kiss. Pierre, carefully stretching his neck
so as not to touch the quilt, followed her suggestion and pressed his
lips to the large boned, fleshy hand. Neither the hand nor a single
muscle of the count's face stirred. Once more Pierre looked
questioningly at Anna Mikhaylovna to see what he was to do next. Anna
Mikhaylovna with her eyes indicated a chair that stood beside the bed.
Pierre obediently sat down, his eyes asking if he were doing right. Anna
Mikhaylovna nodded approvingly. Again Pierre fell into the naively
symmetrical pose of an Egyptian statue, evidently distressed that his
stout and clumsy body took up so much room and doing his utmost to look
as small as possible. He looked at the count, who still gazed at the
spot where Pierre's face had been before he sat down. Anna Mikhaylovna
indicated by her attitude her consciousness of the pathetic importance
of these last moments of meeting between the father and son. This lasted
about two minutes, which to Pierre seemed an hour. Suddenly the broad
muscles and lines of the count's face began to twitch. The twitching
increased, the handsome mouth was drawn to one side (only now did Pierre
realize how near death his father was), and from that distorted mouth
issued an indistinct, hoarse sound. Anna Mikhaylovna looked attentively
at the sick man's eyes, trying to guess what he wanted; she pointed
first to Pierre, then to some drink, then named Prince Vasili in an
inquiring whisper, then pointed to the quilt. The eyes and face of the
sick man showed impatience. He made an effort to look at the servant who
stood constantly at the head of the bed.

"Wants to turn on the other side," whispered the servant, and got up to
turn the count's heavy body toward the wall.

Pierre rose to help him.

While the count was being turned over, one of his arms fell back
helplessly and he made a fruitless effort to pull it forward. Whether he
noticed the look of terror with which Pierre regarded that lifeless arm,
or whether some other thought flitted across his dying brain, at any
rate he glanced at the refractory arm, at Pierre's terror-stricken face,
and again at the arm, and on his face a feeble, piteous smile appeared,
quite out of keeping with his features, that seemed to deride his own
helplessness. At sight of this smile Pierre felt an unexpected quivering
in his breast and a tickling in his nose, and tears dimmed his eyes. The
sick man was turned on to his side with his face to the wall. He sighed.

"He is dozing," said Anna Mikhaylovna, observing that one of the
princesses was coming to take her turn at watching. "Let us go."

Pierre went out.




CHAPTER XXIV

There was now no one in the reception room except Prince Vasili and the
eldest princess, who were sitting under the portrait of Catherine the
Great and talking eagerly. As soon as they saw Pierre and his companion
they became silent, and Pierre thought he saw the princess hide
something as she whispered:

"I can't bear the sight of that woman."

"Catiche has had tea served in the small drawing room," said Prince
Vasili to Anna Mikhaylovna. "Go and take something, my poor Anna
Mikhaylovna, or you will not hold out."

To Pierre he said nothing, merely giving his arm a sympathetic squeeze
below the shoulder. Pierre went with Anna Mikhaylovna into the small
drawing room.

"There is nothing so refreshing after a sleepless night as a cup of this
delicious Russian tea," Lorrain was saying with an air of restrained
animation as he stood sipping tea from a delicate Chinese handleless cup
before a table on which tea and a cold supper were laid in the small
circular room. Around the table all who were at Count Bezukhov's house
that night had gathered to fortify themselves. Pierre well remembered
this small circular drawing room with its mirrors and little tables.
During balls given at the house Pierre, who did not know how to dance,
had liked sitting in this room to watch the ladies who, as they passed
through in their ball dresses with diamonds and pearls on their bare
shoulders, looked at themselves in the brilliantly lighted mirrors which
repeated their reflections several times. Now this same room was dimly
lighted by two candles. On one small table tea things and supper dishes
stood in disorder, and in the middle of the night a motley throng of
people sat there, not merrymaking, but somberly whispering, and
betraying by every word and movement that they none of them forgot what
was happening and what was about to happen in the bedroom. Pierre did
not eat anything though he would very much have liked to. He looked
inquiringly at his monitress and saw that she was again going on tiptoe
to the reception room where they had left Prince Vasili and the eldest
princess. Pierre concluded that this also was essential, and after a
short interval followed her. Anna Mikhaylovna was standing beside the
princess, and they were both speaking in excited whispers.

"Permit me, Princess, to know what is necessary and what is not
necessary," said the younger of the two speakers, evidently in the same
state of excitement as when she had slammed the door of her room.

"But, my dear princess," answered Anna Mikhaylovna blandly but
impressively, blocking the way to the bedroom and preventing the other
from passing, "won't this be too much for poor Uncle at a moment when he
needs repose? Worldly conversation at a moment when his soul is already
prepared..."

Prince Vasili was seated in an easy chair in his familiar attitude, with
one leg crossed high above the other. His cheeks, which were so flabby
that they looked heavier below, were twitching violently; but he wore
the air of a man little concerned in what the two ladies were saying.

"Come, my dear Anna Mikhaylovna, let Catiche do as she pleases. You know
how fond the count is of her."

"I don't even know what is in this paper," said the younger of the two
ladies, addressing Prince Vasili and pointing to an inlaid portfolio she
held in her hand. "All I know is that his real will is in his writing
table, and this is a paper he has forgotten...."

She tried to pass Anna Mikhaylovna, but the latter sprang so as to bar
her path.

"I know, my dear, kind princess," said Anna Mikhaylovna, seizing the
portfolio so firmly that it was plain she would not let go easily. "Dear
princess, I beg and implore you, have some pity on him! Je vous en
conjure..."

The princess did not reply. Their efforts in the struggle for the
portfolio were the only sounds audible, but it was evident that if the
princess did speak, her words would not be flattering to Anna
Mikhaylovna. Though the latter held on tenaciously, her voice lost none
of its honeyed firmness and softness.

"Pierre, my dear, come here. I think he will not be out of place in a
family consultation; is it not so, Prince?"

"Why don't you speak, cousin?" suddenly shrieked the princess so loud
that those in the drawing room heard her and were startled. "Why do you
remain silent when heaven knows who permits herself to interfere, making
a scene on the very threshold of a dying man's room? Intriguer!" she
hissed viciously, and tugged with all her might at the portfolio.

But Anna Mikhaylovna went forward a step or two to keep her hold on the
portfolio, and changed her grip.

Prince Vasili rose. "Oh!" said he with reproach and surprise, "this is
absurd! Come, let go I tell you."

The princess let go.

"And you too!"

But Anna Mikhaylovna did not obey him.

"Let go, I tell you! I will take the responsibility. I myself will go
and ask him, I!... does that satisfy you?"

"But, Prince," said Anna Mikhaylovna, "after such a solemn sacrament,
allow him a moment's peace! Here, Pierre, tell them your opinion," said
she, turning to the young man who, having come quite close, was gazing
with astonishment at the angry face of the princess which had lost all
dignity, and at the twitching cheeks of Prince Vasili.

"Remember that you will answer for the consequences," said Prince Vasili
severely. "You don't know what you are doing."

"Vile woman!" shouted the princess, darting unexpectedly at Anna
Mikhaylovna and snatching the portfolio from her.

Prince Vasili bent his head and spread out his hands.

At this moment that terrible door, which Pierre had watched so long and
which had always opened so quietly, burst noisily open and banged
against the wall, and the second of the three sisters rushed out
wringing her hands.

"What are you doing!" she cried vehemently. "He is dying and you leave
me alone with him!"

Her sister dropped the portfolio. Anna Mikhaylovna, stooping, quickly
caught up the object of contention and ran into the bedroom. The eldest
princess and Prince Vasili, recovering themselves, followed her. A few
minutes later the eldest sister came out with a pale hard face, again
biting her underlip. At sight of Pierre her expression showed an
irrepressible hatred.

"Yes, now you may be glad!" said she; "this is what you have been
waiting for." And bursting into tears she hid her face in her
handkerchief and rushed from the room.

Prince Vasili came next. He staggered to the sofa on which Pierre was
sitting and dropped onto it, covering his face with his hand. Pierre
noticed that he was pale and that his jaw quivered and shook as if in an
ague.

"Ah, my friend!" said he, taking Pierre by the elbow; and there was in
his voice a sincerity and weakness Pierre had never observed in it
before. "How often we sin, how much we deceive, and all for what? I am
near sixty, dear friend... I too... All will end in death, all! Death is
awful..." and he burst into tears.

Anna Mikhaylovna came out last. She approached Pierre with slow, quiet
steps.

"Pierre!" she said.

Pierre gave her an inquiring look. She kissed the young man on his
forehead, wetting him with her tears. Then after a pause she said:

"He is no more...."

Pierre looked at her over his spectacles.

"Come, I will go with you. Try to weep, nothing gives such relief as
tears."

She led him into the dark drawing room and Pierre was glad no one could
see his face. Anna Mikhaylovna left him, and when she returned he was
fast asleep with his head on his arm.

In the morning Anna Mikhaylovna said to Pierre:

"Yes, my dear, this is a great loss for us all, not to speak of you. But
God will support you: you are young, and are now, I hope, in command of
an immense fortune. The will has not yet been opened. I know you well
enough to be sure that this will not turn your head, but it imposes
duties on you, and you must be a man."

Pierre was silent.

"Perhaps later on I may tell you, my dear boy, that if I had not been
there, God only knows what would have happened! You know, Uncle promised
me only the day before yesterday not to forget Boris. But he had no
time. I hope, my dear friend, you will carry out your father's wish?"

Pierre understood nothing of all this and coloring shyly looked in
silence at Princess Anna Mikhaylovna. After her talk with Pierre, Anna
Mikhaylovna returned to the Rostovs' and went to bed. On waking in the
morning she told the Rostovs and all her acquaintances the details of
Count Bezukhov's death. She said the count had died as she would herself
wish to die, that his end was not only touching but edifying. As to the
last meeting between father and son, it was so touching that she could
not think of it without tears, and did not know which had behaved better
during those awful moments--the father who so remembered everything and
everybody at last and had spoken such pathetic words to the son, or
Pierre, whom it had been pitiful to see, so stricken was he with grief,
though he tried hard to hide it in order not to sadden his dying father.
"It is painful, but it does one good. It uplifts the soul to see such
men as the old count and his worthy son," said she. Of the behavior of
the eldest princess and Prince Vasili she spoke disapprovingly, but in
whispers and as a great secret.




CHAPTER XXV

At Bald Hills, Prince Nicholas Andreevich Bolkonski's estate, the
arrival of young Prince Andrew and his wife was daily expected, but this
expectation did not upset the regular routine of life in the old
prince's household. General in Chief Prince Nicholas Andreevich
(nicknamed in society, "the King of Prussia") ever since the Emperor
Paul had exiled him to his country estate had lived there continuously
with his daughter, Princess Mary, and her companion, Mademoiselle
Bourienne. Though in the new reign he was free to return to the
capitals, he still continued to live in the country, remarking that
anyone who wanted to see him could come the hundred miles from Moscow to
Bald Hills, while he himself needed no one and nothing. He used to say
that there are only two sources of human vice--idleness and
superstition, and only two virtues--activity and intelligence. He
himself undertook his daughter's education, and to develop these two
cardinal virtues in her gave her lessons in algebra and geometry till
she was twenty, and arranged her life so that her whole time was
occupied. He was himself always occupied: writing his memoirs, solving
problems in higher mathematics, turning snuffboxes on a lathe, working
in the garden, or superintending the building that was always going on
at his estate. As regularity is a prime condition facilitating activity,
regularity in his household was carried to the highest point of
exactitude. He always came to table under precisely the same conditions,
and not only at the same hour but at the same minute. With those about
him, from his daughter to his serfs, the prince was sharp and invariably
exacting, so that without being a hardhearted man he inspired such fear
and respect as few hardhearted men would have aroused. Although he was
in retirement and had now no influence in political affairs, every high
official appointed to the province in which the prince's estate lay
considered it his duty to visit him and waited in the lofty antechamber
just as the architect, gardener, or Princess Mary did, till the prince
appeared punctually to the appointed hour. Everyone sitting in this
antechamber experienced the same feeling of respect and even fear when
the enormously high study door opened and showed the figure of a rather
small old man, with powdered wig, small withered hands, and bushy gray
eyebrows which, when he frowned, sometimes hid the gleam of his shrewd,
youthfully glittering eyes.

On the morning of the day that the young couple were to arrive, Princess
Mary entered the antechamber as usual at the time appointed for the
morning greeting, crossing herself with trepidation and repeating a
silent prayer. Every morning she came in like that, and every morning
prayed that the daily interview might pass off well.

An old powdered manservant who was sitting in the antechamber rose
quietly and said in a whisper: "Please walk in."

Through the door came the regular hum of a lathe. The princess timidly
opened the door which moved noiselessly and easily. She paused at the
entrance. The prince was working at the lathe and after glancing round
continued his work.

The enormous study was full of things evidently in constant use. The
large table covered with books and plans, the tall glass-fronted
bookcases with keys in the locks, the high desk for writing while
standing up, on which lay an open exercise book, and the lathe with
tools laid ready to hand and shavings scattered around--all indicated
continuous, varied, and orderly activity. The motion of the small foot
shod in a Tartar boot embroidered with silver, and the firm pressure of
the lean sinewy hand, showed that the prince still possessed the
tenacious endurance and vigor of hardy old age. After a few more turns
of the lathe he removed his foot from the pedal, wiped his chisel,
dropped it into a leather pouch attached to the lathe, and, approaching
the table, summoned his daughter. He never gave his children a blessing,
so he simply held out his bristly cheek (as yet unshaven) and, regarding
her tenderly and attentively, said severely:

"Quite well? All right then, sit down." He took the exercise book
containing lessons in geometry written by himself and drew up a chair
with his foot.

"For tomorrow!" said he, quickly finding the page and making a scratch
from one paragraph to another with his hard nail.

The princess bent over the exercise book on the table.

"Wait a bit, here's a letter for you," said the old man suddenly, taking
a letter addressed in a woman's hand from a bag hanging above the table,
onto which he threw it.

At the sight of the letter red patches showed themselves on the
princess' face. She took it quickly and bent her head over it.

"From Heloise?" asked the prince with a cold smile that showed his still
sound, yellowish teeth.

"Yes, it's from Julie," replied the princess with a timid glance and a
timid smile.

"I'll let two more letters pass, but the third I'll read," said the
prince sternly; "I'm afraid you write much nonsense. I'll read the
third!"

"Read this if you like, Father," said the princess, blushing still more
and holding out the letter.

"The third, I said the third!" cried the prince abruptly, pushing the
letter away, and leaning his elbows on the table he drew toward him the
exercise book containing geometrical figures.

"Well, madam," he began, stooping over the book close to his daughter
and placing an arm on the back of the chair on which she sat, so that
she felt herself surrounded on all sides by the acrid scent of old age
and tobacco, which she had known so long. "Now, madam, these triangles
are equal; please note that the angle ABC..."

The princess looked in a scared way at her father's eyes glittering
close to her; the red patches on her face came and went, and it was
plain that she understood nothing and was so frightened that her fear
would prevent her understanding any of her father's further
explanations, however clear they might be. Whether it was the teacher's
fault or the pupil's, this same thing happened every day: the princess'
eyes grew dim, she could not see and could not hear anything, but was
only conscious of her stern father's withered face close to her, of his
breath and the smell of him, and could think only of how to get away
quickly to her own room to make out the problem in peace. The old man
was beside himself: moved the chair on which he was sitting noisily
backward and forward, made efforts to control himself and not become
vehement, but almost always did become vehement, scolded, and sometimes
flung the exercise book away.

The princess gave a wrong answer.

"Well now, isn't she a fool!" shouted the prince, pushing the book aside
and turning sharply away; but rising immediately, he paced up and down,
lightly touched his daughter's hair and sat down again.

He drew up his chair, and continued to explain.

"This won't do, Princess; it won't do," said he, when Princess Mary,
having taken and closed the exercise book with the next day's lesson,
was about to leave: "Mathematics are most important, madam! I don't want
to have you like our silly ladies. Get used to it and you'll like it,"
and he patted her cheek. "It will drive all the nonsense out of your
head."

She turned to go, but he stopped her with a gesture and took an uncut
book from the high desk.

"Here is some sort of Key to the Mysteries that your Heloise has sent
you. Religious! I don't interfere with anyone's belief... I have looked
at it. Take it. Well, now go. Go."

He patted her on the shoulder and himself closed the door after her.

Princess Mary went back to her room with the sad, scared expression that
rarely left her and which made her plain, sickly face yet plainer. She
sat down at her writing table, on which stood miniature portraits and
which was littered with books and papers. The princess was as untidy as
her father was tidy. She put down the geometry book and eagerly broke
the seal of her letter. It was from her most intimate friend from
childhood; that same Julie Karagina who had been at the Rostovs' name-
day party.

Julie wrote in French:

Dear and precious Friend, How terrible and frightful a thing is
separation! Though I tell myself that half my life and half my happiness
are wrapped up in you, and that in spite of the distance separating us
our hearts are united by indissoluble bonds, my heart rebels against
fate and in spite of the pleasures and distractions around me I cannot
overcome a certain secret sorrow that has been in my heart ever since we
parted. Why are we not together as we were last summer, in your big
study, on the blue sofa, the confidential sofa? Why cannot I now, as
three months ago, draw fresh moral strength from your look, so gentle,
calm, and penetrating, a look I loved so well and seem to see before me
as I write?

Having read thus far, Princess Mary sighed and glanced into the mirror
which stood on her right. It reflected a weak, ungraceful figure and
thin face. Her eyes, always sad, now looked with particular hopelessness
at her reflection in the glass. "She flatters me," thought the princess,
turning away and continuing to read. But Julie did not flatter her
friend, the princess' eyes--large, deep and luminous (it seemed as if at
times there radiated from them shafts of warm light)--were so beautiful
that very often in spite of the plainness of her face they gave her an
attraction more powerful than that of beauty. But the princess never saw
the beautiful expression of her own eyes--the look they had when she was
not thinking of herself. As with everyone, her face assumed a forced
unnatural expression as soon as she looked in a glass. She went on
reading:

All Moscow talks of nothing but war. One of my two brothers is already
abroad, the other is with the Guards, who are starting on their march to
the frontier. Our dear Emperor has left Petersburg and it is thought
intends to expose his precious person to the chances of war. God grant
that the Corsican monster who is destroying the peace of Europe may be
overthrown by the angel whom it has pleased the Almighty, in His
goodness, to give us as sovereign! To say nothing of my brothers, this
war has deprived me of one of the associations nearest my heart. I mean
young Nicholas Rostov, who with his enthusiasm could not bear to remain
inactive and has left the university to join the army. I will confess to
you, dear Mary, that in spite of his extreme youth his departure for the
army was a great grief to me. This young man, of whom I spoke to you
last summer, is so noble-minded and full of that real youthfulness which
one seldom finds nowadays among our old men of twenty and, particularly,
he is so frank and has so much heart. He is so pure and poetic that my
relations with him, transient as they were, have been one of the
sweetest comforts to my poor heart, which has already suffered so much.
Someday I will tell you about our parting and all that was said then.
That is still too fresh. Ah, dear friend, you are happy not to know
these poignant joys and sorrows. You are fortunate, for the latter are
generally the stronger! I know very well that Count Nicholas is too
young ever to be more to me than a friend, but this sweet friendship,
this poetic and pure intimacy, were what my heart needed. But enough of
this! The chief news, about which all Moscow gossips, is the death of
old Count Bezukhov, and his inheritance. Fancy! The three princesses
have received very little, Prince Vasili nothing, and it is Monsieur
Pierre who has inherited all the property and has besides been
recognized as legitimate; so that he is now Count Bezukhov and possessor
of the finest fortune in Russia. It is rumored that Prince Vasili played
a very despicable part in this affair and that he returned to Petersburg
quite crestfallen.

I confess I understand very little about all these matters of wills and
inheritance; but I do know that since this young man, whom we all used
to know as plain Monsieur Pierre, has become Count Bezukhov and the
owner of one of the largest fortunes in Russia, I am much amused to
watch the change in the tone and manners of the mammas burdened by
marriageable daughters, and of the young ladies themselves, toward him,
though, between you and me, he always seemed to me a poor sort of
fellow. As for the past two years people have amused themselves by
finding husbands for me (most of whom I don't even know), the
matchmaking chronicles of Moscow now speak of me as the future Countess
Bezukhova. But you will understand that I have no desire for the post. A
propos of marriages: do you know that a while ago that universal auntie
Anna Mikhaylovna told me, under the seal of strict secrecy, of a plan of
marriage for you. It is neither more nor less than with Prince Vasili's
son Anatole, whom they wish to reform by marrying him to someone rich
and distinguee, and it is on you that his relations' choice has fallen.
I don't know what you will think of it, but I consider it my duty to let
you know of it. He is said to be very handsome and a terrible
scapegrace. That is all I have been able to find out about him.

But enough of gossip. I am at the end of my second sheet of paper, and
Mamma has sent for me to go and dine at the Apraksins'. Read the
mystical book I am sending you; it has an enormous success here. Though
there are things in it difficult for the feeble human mind to grasp, it
is an admirable book which calms and elevates the soul. Adieu! Give my
respects to monsieur your father and my compliments to Mademoiselle
Bourienne. I embrace you as I love you.

JULIE

P.S. Let me have news of your brother and his charming little wife.

The princess pondered awhile with a thoughtful smile and her luminous
eyes lit up so that her face was entirely transformed. Then she suddenly
rose and with her heavy tread went up to the table. She took a sheet of
paper and her hand moved rapidly over it. This is the reply she wrote,
also in French:

Dear and precious Friend, Your letter of the 13th has given me great
delight. So you still love me, my romantic Julie? Separation, of which
you say so much that is bad, does not seem to have had its usual effect
on you. You complain of our separation. What then should I say, if I
dared complain, I who am deprived of all who are dear to me? Ah, if we
had not religion to console us life would be very sad. Why do you
suppose that I should look severely on your affection for that young
man? On such matters I am only severe with myself. I understand such
feelings in others, and if never having felt them I cannot approve of
them, neither do I condemn them. Only it seems to me that Christian
love, love of one's neighbor, love of one's enemy, is worthier, sweeter,
and better than the feelings which the beautiful eyes of a young man can
inspire in a romantic and loving young girl like yourself.

The news of Count Bezukhov's death reached us before your letter and my
father was much affected by it. He says the count was the last
representative but one of the great century, and that it is his own turn
now, but that he will do all he can to let his turn come as late as
possible. God preserve us from that terrible misfortune!

I cannot agree with you about Pierre, whom I knew as a child. He always
seemed to me to have an excellent heart, and that is the quality I value
most in people. As to his inheritance and the part played by Prince
Vasili, it is very sad for both. Ah, my dear friend, our divine
Saviour's words, that it is easier for a camel to go through the eye of
a needle than for a rich man to enter the Kingdom of God, are terribly
true. I pity Prince Vasili but am still more sorry for Pierre. So young,
and burdened with such riches--to what temptations he will be exposed!
If I were asked what I desire most on earth, it would be to be poorer
than the poorest beggar. A thousand thanks, dear friend, for the volume
you have sent me and which has such success in Moscow. Yet since you
tell me that among some good things it contains others which our weak
human understanding cannot grasp, it seems to me rather useless to spend
time in reading what is unintelligible and can therefore bear no fruit.
I never could understand the fondness some people have for confusing
their minds by dwelling on mystical books that merely awaken their
doubts and excite their imagination, giving them a bent for exaggeration
quite contrary to Christian simplicity. Let us rather read the Epistles
and Gospels. Let us not seek to penetrate what mysteries they contain;
for how can we, miserable sinners that we are, know the terrible and
holy secrets of Providence while we remain in this flesh which forms an
impenetrable veil between us and the Eternal? Let us rather confine
ourselves to studying those sublime rules which our divine Saviour has
left for our guidance here below. Let us try to conform to them and
follow them, and let us be persuaded that the less we let our feeble
human minds roam, the better we shall please God, who rejects all
knowledge that does not come from Him; and the less we seek to fathom
what He has been pleased to conceal from us, the sooner will He
vouchsafe its revelation to us through His divine Spirit.

My father has not spoken to me of a suitor, but has only told me that he
has received a letter and is expecting a visit from Prince Vasili. In
regard to this project of marriage for me, I will tell you, dear sweet
friend, that I look on marriage as a divine institution to which we must
conform. However painful it may be to me, should the Almighty lay the
duties of wife and mother upon me I shall try to perform them as
faithfully as I can, without disquieting myself by examining my feelings
toward him whom He may give me for husband.

I have had a letter from my brother, who announces his speedy arrival at
Bald Hills with his wife. This pleasure will be but a brief one,
however, for he will leave us again to take part in this unhappy war
into which we have been drawn, God knows how or why. Not only where you
are--at the heart of affairs and of the world--is the talk all of war,
even here amid fieldwork and the calm of nature--which townsfolk
consider characteristic of the country--rumors of war are heard and
painfully felt. My father talks of nothing but marches and
countermarches, things of which I understand nothing; and the day before
yesterday during my daily walk through the village I witnessed a
heartrending scene.... It was a convoy of conscripts enrolled from our
people and starting to join the army. You should have seen the state of
the mothers, wives, and children of the men who were going and should
have heard the sobs. It seems as though mankind has forgotten the laws
of its divine Saviour, Who preached love and forgiveness of injuries--
and that men attribute the greatest merit to skill in killing one
another.

Adieu, dear and kind friend; may our divine Saviour and His most Holy
Mother keep you in their holy and all-powerful care!

MARY

"Ah, you are sending off a letter, Princess? I have already dispatched
mine. I have written to my poor mother," said the smiling Mademoiselle
Bourienne rapidly, in her pleasant mellow tones and with guttural r's.
She brought into Princess Mary's strenuous, mournful, and gloomy world a
quite different atmosphere, careless, lighthearted, and self-satisfied.

"Princess, I must warn you," she added, lowering her voice and evidently
listening to herself with pleasure, and speaking with exaggerated
grasseyement, "the prince has been scolding Michael Ivanovich. He is in
a very bad humor, very morose. Be prepared."

"Ah, dear friend," replied Princess Mary, "I have asked you never to
warn me of the humor my father is in. I do not allow myself to judge him
and would not have others do so."

The princess glanced at her watch and, seeing that she was five minutes
late in starting her practice on the clavichord, went into the sitting
room with a look of alarm. Between twelve and two o'clock, as the day
was mapped out, the prince rested and the princess played the
clavichord.




CHAPTER XXVI

The gray-haired valet was sitting drowsily listening to the snoring of
the prince, who was in his large study. From the far side of the house
through the closed doors came the sound of difficult passages--twenty
times repeated--of a sonata by Dussek.

Just then a closed carriage and another with a hood drove up to the
porch. Prince Andrew got out of the carriage, helped his little wife to
alight, and let her pass into the house before him. Old Tikhon, wearing
a wig, put his head out of the door of the antechamber, reported in a
whisper that the prince was sleeping, and hastily closed the door.
Tikhon knew that neither the son's arrival nor any other unusual event
must be allowed to disturb the appointed order of the day. Prince Andrew
apparently knew this as well as Tikhon; he looked at his watch as if to
ascertain whether his father's habits had changed since he was at home
last, and, having assured himself that they had not, he turned to his
wife.

"He will get up in twenty minutes. Let us go across to Mary's room," he
said.

The little princess had grown stouter during this time, but her eyes and
her short, downy, smiling lip lifted when she began to speak just as
merrily and prettily as ever.

"Why, this is a palace!" she said to her husband, looking around with
the expression with which people compliment their host at a ball. "Let's
come, quick, quick!" And with a glance round, she smiled at Tikhon, at
her husband, and at the footman who accompanied them.

"Is that Mary practicing? Let's go quietly and take her by surprise."

Prince Andrew followed her with a courteous but sad expression.

"You've grown older, Tikhon," he said in passing to the old man, who
kissed his hand.

Before they reached the room from which the sounds of the clavichord
came, the pretty, fair haired Frenchwoman, Mademoiselle Bourienne,
rushed out apparently beside herself with delight.

"Ah! what joy for the princess!" exclaimed she: "At last! I must let her
know."

"No, no, please not... You are Mademoiselle Bourienne," said the little
princess, kissing her. "I know you already through my sister-in-law's
friendship for you. She was not expecting us?"

They went up to the door of the sitting room from which came the sound
of the oft-repeated passage of the sonata. Prince Andrew stopped and
made a grimace, as if expecting something unpleasant.

The little princess entered the room. The passage broke off in the
middle, a cry was heard, then Princess Mary's heavy tread and the sound
of kissing. When Prince Andrew went in the two princesses, who had only
met once before for a short time at his wedding, were in each other's
arms warmly pressing their lips to whatever place they happened to
touch. Mademoiselle Bourienne stood near them pressing her hand to her
heart, with a beatific smile and obviously equally ready to cry or to
laugh. Prince Andrew shrugged his shoulders and frowned, as lovers of
music do when they hear a false note. The two women let go of one
another, and then, as if afraid of being too late, seized each other's
hands, kissing them and pulling them away, and again began kissing each
other on the face, and then to Prince Andrew's surprise both began to
cry and kissed again. Mademoiselle Bourienne also began to cry. Prince
Andrew evidently felt ill at ease, but to the two women it seemed quite
natural that they should cry, and apparently it never entered their
heads that it could have been otherwise at this meeting.

"Ah! my dear!... Ah! Mary!" they suddenly exclaimed, and then laughed.
"I dreamed last night..."--"You were not expecting us?..." "Ah! Mary,
you have got thinner?..." "And you have grown stouter!..."

"I knew the princess at once," put in Mademoiselle Bourienne.

"And I had no idea!..." exclaimed Princess Mary. "Ah, Andrew, I did not
see you."

Prince Andrew and his sister, hand in hand, kissed one another, and he
told her she was still the same crybaby as ever. Princess Mary had
turned toward her brother, and through her tears the loving, warm,
gentle look of her large luminous eyes, very beautiful at that moment,
rested on Prince Andrew's face.

The little princess talked incessantly, her short, downy upper lip
continually and rapidly touching her rosy nether lip when necessary and
drawing up again next moment when her face broke into a smile of
glittering teeth and sparkling eyes. She told of an accident they had
had on the Spasski Hill which might have been serious for her in her
condition, and immediately after that informed them that she had left
all her clothes in Petersburg and that heaven knew what she would have
to dress in here; and that Andrew had quite changed, and that Kitty
Odyntsova had married an old man, and that there was a suitor for Mary,
a real one, but that they would talk of that later. Princess Mary was
still looking silently at her brother and her beautiful eyes were full
of love and sadness. It was plain that she was following a train of
thought independent of her sister-in-law's words. In the midst of a
description of the last Petersburg fete she addressed her brother:

"So you are really going to the war, Andrew?" she said sighing.

Lise sighed too.

"Yes, and even tomorrow," replied her brother.

"He is leaving me here, God knows why, when he might have had
promotion..."

Princess Mary did not listen to the end, but continuing her train of
thought turned to her sister-in-law with a tender glance at her figure.

"Is it certain?" she said.

The face of the little princess changed. She sighed and said: "Yes,
quite certain. Ah! it is very dreadful..."

Her lip descended. She brought her face close to her sister-in-law's and
unexpectedly again began to cry.

"She needs rest," said Prince Andrew with a frown. "Don't you, Lise?
Take her to your room and I'll go to Father. How is he? Just the same?"

"Yes, just the same. Though I don't know what your opinion will be,"
answered the princess joyfully.

"And are the hours the same? And the walks in the avenues? And the
lathe?" asked Prince Andrew with a scarcely perceptible smile which
showed that, in spite of all his love and respect for his father, he was
aware of his weaknesses.

"The hours are the same, and the lathe, and also the mathematics and my
geometry lessons," said Princess Mary gleefully, as if her lessons in
geometry were among the greatest delights of her life.

When the twenty minutes had elapsed and the time had come for the old
prince to get up, Tikhon came to call the young prince to his father.
The old man made a departure from his usual routine in honor of his
son's arrival: he gave orders to admit him to his apartments while he
dressed for dinner. The old prince always dressed in old-fashioned
style, wearing an antique coat and powdered hair; and when Prince Andrew
entered his father's dressing room (not with the contemptuous look and
manner he wore in drawing rooms, but with the animated face with which
he talked to Pierre), the old man was sitting on a large leather-covered
chair, wrapped in a powdering mantle, entrusting his head to Tikhon.

"Ah! here's the warrior! Wants to vanquish Buonaparte?" said the old
man, shaking his powdered head as much as the tail, which Tikhon was
holding fast to plait, would allow.

"You at least must tackle him properly, or else if he goes on like this
he'll soon have us, too, for his subjects! How are you?" And he held out
his cheek.

The old man was in a good temper after his nap before dinner. (He used
to say that a nap "after dinner was silver--before dinner, golden.") He
cast happy, sidelong glances at his son from under his thick, bushy
eyebrows. Prince Andrew went up and kissed his father on the spot
indicated to him. He made no reply on his father's favorite topic--
making fun of the military men of the day, and more particularly of
Bonaparte.

"Yes, Father, I have come to you and brought my wife who is pregnant,"
said Prince Andrew, following every movement of his father's face with
an eager and respectful look. "How is your health?"

"Only fools and rakes fall ill, my boy. You know me: I am busy from
morning till night and abstemious, so of course I am well."

"Thank God," said his son smiling.

"God has nothing to do with it! Well, go on," he continued, returning to
his hobby; "tell me how the Germans have taught you to fight Bonaparte
by this new science you call 'strategy.'"

Prince Andrew smiled.

"Give me time to collect my wits, Father," said he, with a smile that
showed that his father's foibles did not prevent his son from loving and
honoring him. "Why, I have not yet had time to settle down!"

"Nonsense, nonsense!" cried the old man, shaking his pigtail to see
whether it was firmly plaited, and grasping his by the hand. "The house
for your wife is ready. Princess Mary will take her there and show her
over, and they'll talk nineteen to the dozen. That's their woman's way!
I am glad to have her. Sit down and talk. About Mikhelson's army I
understand--Tolstoy's too... a simultaneous expedition.... But what's
the southern army to do? Prussia is neutral... I know that. What about
Austria?" said he, rising from his chair and pacing up and down the room
followed by Tikhon, who ran after him, handing him different articles of
clothing. "What of Sweden? How will they cross Pomerania?"

Prince Andrew, seeing that his father insisted, began--at first
reluctantly, but gradually with more and more animation, and from habit
changing unconsciously from Russian to French as he went on--to explain
the plan of operation for the coming campaign. He explained how an army,
ninety thousand strong, was to threaten Prussia so as to bring her out
of her neutrality and draw her into the war; how part of that army was
to join some Swedish forces at Stralsund; how two hundred and twenty
thousand Austrians, with a hundred thousand Russians, were to operate in
Italy and on the Rhine; how fifty thousand Russians and as many English
were to land at Naples, and how a total force of five hundred thousand
men was to attack the French from different sides. The old prince did
not evince the least interest during this explanation, but as if he were
not listening to it continued to dress while walking about, and three
times unexpectedly interrupted. Once he stopped it by shouting: "The
white one, the white one!"

This meant that Tikhon was not handing him the waistcoat he wanted.
Another time he interrupted, saying:

"And will she soon be confined?" and shaking his head reproachfully
said: "That's bad! Go on, go on."

The third interruption came when Prince Andrew was finishing his
description. The old man began to sing, in the cracked voice of old age:
"Malbrook s'en va-t-en guerre. Dieu sait quand reviendra." *


* "Marlborough is going to the wars; God knows when he'll return."

His son only smiled.

"I don't say it's a plan I approve of," said the son; "I am only telling
you what it is. Napoleon has also formed his plan by now, not worse than
this one."

"Well, you've told me nothing new," and the old man repeated,
meditatively and rapidly:

"Dieu sait quand reviendra. Go to the dining room."




CHAPTER XXVII

At the appointed hour the prince, powdered and shaven, entered the
dining room where his daughter-in-law, Princess Mary, and Mademoiselle
Bourienne were already awaiting him together with his architect, who by
a strange caprice of his employer's was admitted to table though the
position of that insignificant individual was such as could certainly
not have caused him to expect that honor. The prince, who generally kept
very strictly to social distinctions and rarely admitted even important
government officials to his table, had unexpectedly selected Michael
Ivanovich (who always went into a corner to blow his nose on his checked
handkerchief) to illustrate the theory that all men are equals, and had
more than once impressed on his daughter that Michael Ivanovich was "not
a whit worse than you or I." At dinner the prince usually spoke to the
taciturn Michael Ivanovich more often than to anyone else.

In the dining room, which like all the rooms in the house was
exceedingly lofty, the members of the household and the footmen--one
behind each chair--stood waiting for the prince to enter. The head
butler, napkin on arm, was scanning the setting of the table, making
signs to the footmen, and anxiously glancing from the clock to the door
by which the prince was to enter. Prince Andrew was looking at a large
gilt frame, new to him, containing the genealogical tree of the Princes
Bolkonski, opposite which hung another such frame with a badly painted
portrait (evidently by the hand of the artist belonging to the estate)
of a ruling prince, in a crown--an alleged descendant of Rurik and
ancestor of the Bolkonskis. Prince Andrew, looking again at that
genealogical tree, shook his head, laughing as a man laughs who looks at
a portrait so characteristic of the original as to be amusing.

"How thoroughly like him that is!" he said to Princess Mary, who had
come up to him.

Princess Mary looked at her brother in surprise. She did not understand
what he was laughing at. Everything her father did inspired her with
reverence and was beyond question.

"Everyone has his Achilles' heel," continued Prince Andrew. "Fancy, with
his powerful mind, indulging in such nonsense!"

Princess Mary could not understand the boldness of her brother's
criticism and was about to reply, when the expected footsteps were heard
coming from the study. The prince walked in quickly and jauntily as was
his wont, as if intentionally contrasting the briskness of his manners
with the strict formality of his house. At that moment the great clock
struck two and another with a shrill tone joined in from the drawing
room. The prince stood still; his lively glittering eyes from under
their thick, bushy eyebrows sternly scanned all present and rested on
the little princess. She felt, as courtiers do when the Tsar enters, the
sensation of fear and respect which the old man inspired in all around
him. He stroked her hair and then patted her awkwardly on the back of
her neck.

"I'm glad, glad, to see you," he said, looking attentively into her
eyes, and then quickly went to his place and sat down. "Sit down, sit
down! Sit down, Michael Ianovich!"

He indicated a place beside him to his daughter-in-law. A footman moved
the chair for her.

"Ho, ho!" said the old man, casting his eyes on her rounded figure.
"You've been in a hurry. That's bad!"

He laughed in his usual dry, cold, unpleasant way, with his lips only
and not with his eyes.

"You must walk, walk as much as possible, as much as possible," he said.

The little princess did not, or did not wish to, hear his words. She was
silent and seemed confused. The prince asked her about her father, and
she began to smile and talk. He asked about mutual acquaintances, and
she became still more animated and chattered away giving him greetings
from various people and retelling the town gossip.

"Countess Apraksina, poor thing, has lost her husband and she has cried
her eyes out," she said, growing more and more lively.

As she became animated the prince looked at her more and more sternly,
and suddenly, as if he had studied her sufficiently and had formed a
definite idea of her, he turned away and addressed Michael Ivanovich.

"Well, Michael Ivanovich, our Bonaparte will be having a bad time of it.
Prince Andrew" (he always spoke thus of his son) "has been telling me
what forces are being collected against him! While you and I never
thought much of him."

Michael Ivanovich did not at all know when "you and I" had said such
things about Bonaparte, but understanding that he was wanted as a peg on
which to hang the prince's favorite topic, he looked inquiringly at the
young prince, wondering what would follow.

"He is a great tactician!" said the prince to his son, pointing to the
architect.

And the conversation again turned on the war, on Bonaparte, and the
generals and statesmen of the day. The old prince seemed convinced not
only that all the men of the day were mere babies who did not know the A
B C of war or of politics, and that Bonaparte was an insignificant
little Frenchy, successful only because there were no longer any
Potemkins or Suvorovs left to oppose him; but he was also convinced that
there were no political difficulties in Europe and no real war, but only
a sort of puppet show at which the men of the day were playing,
pretending to do something real. Prince Andrew gaily bore with his
father's ridicule of the new men, and drew him on and listened to him
with evident pleasure.

"The past always seems good," said he, "but did not Suvorov himself fall
into a trap Moreau set him, and from which he did not know how to
escape?"

"Who told you that? Who?" cried the prince. "Suvorov!" And he jerked
away his plate, which Tikhon briskly caught. "Suvorov!... Consider,
Prince Andrew. Two... Frederick and Suvorov; Moreau!... Moreau would
have been a prisoner if Suvorov had had a free hand; but he had the
Hofs-kriegs-wurst-schnapps-Rath on his hands. It would have puzzled the
devil himself! When you get there you'll find out what those Hofs-
kriegs-wurst-Raths are! Suvorov couldn't manage them so what chance has
Michael Kutuzov? No, my dear boy," he continued, "you and your generals
won't get on against Buonaparte; you'll have to call in the French, so
that birds of a feather may fight together. The German, Pahlen, has been
sent to New York in America, to fetch the Frenchman, Moreau," he said,
alluding to the invitation made that year to Moreau to enter the Russian
service.... "Wonderful!... Were the Potemkins, Suvorovs, and Orlovs
Germans? No, lad, either you fellows have all lost your wits, or I have
outlived mine. May God help you, but we'll see what will happen.
Buonaparte has become a great commander among them! Hm!..."

"I don't at all say that all the plans are good," said Prince Andrew, "I
am only surprised at your opinion of Bonaparte. You may laugh as much as
you like, but all the same Bonaparte is a great general!"

"Michael Ivanovich!" cried the old prince to the architect who, busy
with his roast meat, hoped he had been forgotten: "Didn't I tell you
Buonaparte was a great tactician? Here, he says the same thing."

"To be sure, your excellency," replied the architect.

The prince again laughed his frigid laugh.

"Buonaparte was born with a silver spoon in his mouth. He has got
splendid soldiers. Besides he began by attacking Germans. And only
idlers have failed to beat the Germans. Since the world began everybody
has beaten the Germans. They beat no one--except one another. He made
his reputation fighting them."

And the prince began explaining all the blunders which, according to
him, Bonaparte had made in his campaigns and even in politics. His son
made no rejoinder, but it was evident that whatever arguments were
presented he was as little able as his father to change his opinion. He
listened, refraining from a reply, and involuntarily wondered how this
old man, living alone in the country for so many years, could know and
discuss so minutely and acutely all the recent European military and
political events.

"You think I'm an old man and don't understand the present state of
affairs?" concluded his father. "But it troubles me. I don't sleep at
night. Come now, where has this great commander of yours shown his
skill?" he concluded.

"That would take too long to tell," answered the son.

"Well, then go to your Buonaparte! Mademoiselle Bourienne, here's
another admirer of that powder-monkey emperor of yours," he exclaimed in
excellent French.

"You know, Prince, I am not a Bonapartist!"

"Dieu sait quand reviendra..." hummed the prince out of tune and, with a
laugh still more so, he quitted the table.

The little princess during the whole discussion and the rest of the
dinner sat silent, glancing with a frightened look now at her father-in-
law and now at Princess Mary. When they left the table she took her
sister-in-law's arm and drew her into another room.

"What a clever man your father is," said she; "perhaps that is why I am
afraid of him."

"Oh, he is so kind!" answered Princess Mary.




CHAPTER XXVIII

Prince Andrew was to leave next evening. The old prince, not altering
his routine, retired as usual after dinner. The little princess was in
her sister-in-law's room. Prince Andrew in a traveling coat without
epaulettes had been packing with his valet in the rooms assigned to him.
After inspecting the carriage himself and seeing the trunks put in, he
ordered the horses to be harnessed. Only those things he always kept
with him remained in his room; a small box, a large canteen fitted with
silver plate, two Turkish pistols and a saber--a present from his father
who had brought it from the siege of Ochakov. All these traveling
effects of Prince Andrew's were in very good order: new, clean, and in
cloth covers carefully tied with tapes.

When starting on a journey or changing their mode of life, men capable
of reflection are generally in a serious frame of mind. At such moments
one reviews the past and plans for the future. Prince Andrew's face
looked very thoughtful and tender. With his hands behind him he paced
briskly from corner to corner of the room, looking straight before him
and thoughtfully shaking his head. Did he fear going to the war, or was
he sad at leaving his wife?--perhaps both, but evidently he did not wish
to be seen in that mood, for hearing footsteps in the passage he
hurriedly unclasped his hands, stopped at a table as if tying the cover
of the small box, and assumed his usual tranquil and impenetrable
expression. It was the heavy tread of Princess Mary that he heard.

"I hear you have given orders to harness," she cried, panting (she had
apparently been running), "and I did so wish to have another talk with
you alone! God knows how long we may again be parted. You are not angry
with me for coming? You have changed so, Andrusha," she added, as if to
explain such a question.

She smiled as she uttered his pet name, "Andrusha." It was obviously
strange to her to think that this stern handsome man should be Andrusha-
-the slender mischievous boy who had been her playfellow in childhood.

"And where is Lise?" he asked, answering her question only by a smile.

"She was so tired that she has fallen asleep on the sofa in my room. Oh,
Andrew! What a treasure of a wife you have," said she, sitting down on
the sofa, facing her brother. "She is quite a child: such a dear, merry
child. I have grown so fond of her."

Prince Andrew was silent, but the princess noticed the ironical and
contemptuous look that showed itself on his face.

"One must be indulgent to little weaknesses; who is free from them,
Andrew? Don't forget that she has grown up and been educated in society,
and so her position now is not a rosy one. We should enter into
everyone's situation. Tout comprendre, c'est tout pardonner. * Think
what it must be for her, poor thing, after what she has been used to, to
be parted from her husband and be left alone in the country, in her
condition! It's very hard."


* To understand all is to forgive all.

Prince Andrew smiled as he looked at his sister, as we smile at those we
think we thoroughly understand.

"You live in the country and don't think the life terrible," he replied.

"I... that's different. Why speak of me? I don't want any other life,
and can't, for I know no other. But think, Andrew: for a young society
woman to be buried in the country during the best years of her life, all
alone--for Papa is always busy, and I... well, you know what poor
resources I have for entertaining a woman used to the best society.
There is only Mademoiselle Bourienne...."

"I don't like your Mademoiselle Bourienne at all," said Prince Andrew.

"No? She is very nice and kind and, above all, she's much to be pitied.
She has no one, no one. To tell the truth, I don't need her, and she's
even in my way. You know I always was a savage, and now am even more so.
I like being alone.... Father likes her very much. She and Michael
Ivanovich are the two people to whom he is always gentle and kind,
because he has been a benefactor to them both. As Sterne says: 'We don't
love people so much for the good they have done us, as for the good we
have done them.' Father took her when she was homeless after losing her
own father. She is very good-natured, and my father likes her way of
reading. She reads to him in the evenings and reads splendidly."

"To be quite frank, Mary, I expect Father's character sometimes makes
things trying for you, doesn't it?" Prince Andrew asked suddenly.

Princess Mary was first surprised and then aghast at this question.

"For me? For me?... Trying for me!..." said she.

"He always was rather harsh; and now I should think he's getting very
trying," said Prince Andrew, apparently speaking lightly of their father
in order to puzzle or test his sister.

"You are good in every way, Andrew, but you have a kind of intellectual
pride," said the princess, following the train of her own thoughts
rather than the trend of the conversation--"and that's a great sin. How
can one judge Father? But even if one might, what feeling except
veneration could such a man as my father evoke? And I am so contented
and happy with him. I only wish you were all as happy as I am."

Her brother shook his head incredulously.

"The only thing that is hard for me... I will tell you the truth,
Andrew... is Father's way of treating religious subjects. I don't
understand how a man of his immense intellect can fail to see what is as
clear as day, and can go so far astray. That is the only thing that
makes me unhappy. But even in this I can see lately a shade of
improvement. His satire has been less bitter of late, and there was a
monk he received and had a long talk with."

"Ah! my dear, I am afraid you and your monk are wasting your powder,"
said Prince Andrew banteringly yet tenderly.

"Ah! mon ami, I only pray, and hope that God will hear me. Andrew..."
she said timidly after a moment's silence, "I have a great favor to ask
of you."

"What is it, dear?"

"No--promise that you will not refuse! It will give you no trouble and
is nothing unworthy of you, but it will comfort me. Promise,
Andrusha!..." said she, putting her hand in her reticule but not yet
taking out what she was holding inside it, as if what she held were the
subject of her request and must not be shown before the request was
granted.

She looked timidly at her brother.

"Even if it were a great deal of trouble..." answered Prince Andrew, as
if guessing what it was about.

"Think what you please! I know you are just like Father. Think as you
please, but do this for my sake! Please do! Father's father, our
grandfather, wore it in all his wars." (She still did not take out what
she was holding in her reticule.) "So you promise?"

"Of course. What is it?"

"Andrew, I bless you with this icon and you must promise me you will
never take it off. Do you promise?"

"If it does not weigh a hundredweight and won't break my neck... To
please you..." said Prince Andrew. But immediately, noticing the pained
expression his joke had brought to his sister's face, he repented and
added: "I am glad; really, dear, I am very glad."

"Against your will He will save and have mercy on you and bring you to
Himself, for in Him alone is truth and peace," said she in a voice
trembling with emotion, solemnly holding up in both hands before her
brother a small, oval, antique, dark-faced icon of the Saviour in a gold
setting, on a finely wrought silver chain.

She crossed herself, kissed the icon, and handed it to Andrew.

"Please, Andrew, for my sake!..."

Rays of gentle light shone from her large, timid eyes. Those eyes lit up
the whole of her thin, sickly face and made it beautiful. Her brother
would have taken the icon, but she stopped him. Andrew understood,
crossed himself and kissed the icon. There was a look of tenderness, for
he was touched, but also a gleam of irony on his face.

"Thank you, my dear." She kissed him on the forehead and sat down again
on the sofa. They were silent for a while.

"As I was saying to you, Andrew, be kind and generous as you always used
to be. Don't judge Lise harshly," she began. "She is so sweet, so good-
natured, and her position now is a very hard one."

"I do not think I have complained of my wife to you, Masha, or blamed
her. Why do you say all this to me?"

Red patches appeared on Princess Mary's face and she was silent as if
she felt guilty.

"I have said nothing to you, but you have already been talked to. And I
am sorry for that," he went on.

The patches grew deeper on her forehead, neck, and cheeks. She tried to
say something but could not. Her brother had guessed right: the little
princess had been crying after dinner and had spoken of her forebodings
about her confinement, and how she dreaded it, and had complained of her
fate, her father-in-law, and her husband. After crying she had fallen
asleep. Prince Andrew felt sorry for his sister.

"Know this, Masha: I can't reproach, have not reproached, and never
shall reproach my wife with anything, and I cannot reproach myself with
anything in regard to her; and that always will be so in whatever
circumstances I may be placed. But if you want to know the truth... if
you want to know whether I am happy? No! Is she happy? No! But why this
is so I don't know..."

As he said this he rose, went to his sister, and, stooping, kissed her
forehead. His fine eyes lit up with a thoughtful, kindly, and
unaccustomed brightness, but he was looking not at his sister but over
her head toward the darkness of the open doorway.

"Let us go to her, I must say good-by. Or--go and wake and I'll come in
a moment. Petrushka!" he called to his valet: "Come here, take these
away. Put this on the seat and this to the right."

Princess Mary rose and moved to the door, then stopped and said:
"Andrew, if you had faith you would have turned to God and asked Him to
give you the love you do not feel, and your prayer would have been
answered."

"Well, may be!" said Prince Andrew. "Go, Masha; I'll come immediately."

On the way to his sister's room, in the passage which connected one wing
with the other, Prince Andrew met Mademoiselle Bourienne smiling
sweetly. It was the third time that day that, with an ecstatic and
artless smile, she had met him in secluded passages.

"Oh! I thought you were in your room," she said, for some reason
blushing and dropping her eyes.

Prince Andrew looked sternly at her and an expression of anger suddenly
came over his face. He said nothing to her but looked at her forehead
and hair, without looking at her eyes, with such contempt that the
Frenchwoman blushed and went away without a word. When he reached his
sister's room his wife was already awake and her merry voice, hurrying
one word after another, came through the open door. She was speaking as
usual in French, and as if after long self-restraint she wished to make
up for lost time.

"No, but imagine the old Countess Zubova, with false curls and her mouth
full of false teeth, as if she were trying to cheat old age.... Ha, ha,
ha! Mary!"

This very sentence about Countess Zubova and this same laugh Prince
Andrew had already heard from his wife in the presence of others some
five times. He entered the room softly. The little princess, plump and
rosy, was sitting in an easy chair with her work in her hands, talking
incessantly, repeating Petersburg reminiscences and even phrases. Prince
Andrew came up, stroked her hair, and asked if she felt rested after
their journey. She answered him and continued her chatter.

The coach with six horses was waiting at the porch. It was an autumn
night, so dark that the coachman could not see the carriage pole.
Servants with lanterns were bustling about in the porch. The immense
house was brilliant with lights shining through its lofty windows. The
domestic serfs were crowding in the hall, waiting to bid good-by to the
young prince. The members of the household were all gathered in the
reception hall: Michael Ivanovich, Mademoiselle Bourienne, Princess
Mary, and the little princess. Prince Andrew had been called to his
father's study as the latter wished to say good-by to him alone. All
were waiting for them to come out.

When Prince Andrew entered the study the old man in his old-age
spectacles and white dressing gown, in which he received no one but his
son, sat at the table writing. He glanced round.

"Going?" And he went on writing.

"I've come to say good-by."

"Kiss me here," and he touched his cheek: "Thanks, thanks!"

"What do you thank me for?"

"For not dilly-dallying and not hanging to a woman's apron strings. The
Service before everything. Thanks, thanks!" And he went on writing, so
that his quill spluttered and squeaked. "If you have anything to say,
say it. These two things can be done together," he added.

"About my wife... I am ashamed as it is to leave her on your hands..."

"Why talk nonsense? Say what you want."

"When her confinement is due, send to Moscow for an accoucheur.... Let
him be here...."

The old prince stopped writing and, as if not understanding, fixed his
stern eyes on his son.

"I know that no one can help if nature does not do her work," said
Prince Andrew, evidently confused. "I know that out of a million cases
only one goes wrong, but it is her fancy and mine. They have been
telling her things. She has had a dream and is frightened."

"Hm... Hm..." muttered the old prince to himself, finishing what he was
writing. "I'll do it."

He signed with a flourish and suddenly turning to his son began to
laugh.

"It's a bad business, eh?"

"What is bad, Father?"

"The wife!" said the old prince, briefly and significantly.

"I don't understand!" said Prince Andrew.

"No, it can't be helped, lad," said the prince. "They're all like that;
one can't unmarry. Don't be afraid; I won't tell anyone, but you know it
yourself."

He seized his son by the hand with small bony fingers, shook it, looked
straight into his son's face with keen eyes which seemed to see through
him, and again laughed his frigid laugh.

The son sighed, thus admitting that his father had understood him. The
old man continued to fold and seal his letter, snatching up and throwing
down the wax, the seal, and the paper, with his accustomed rapidity.

"What's to be done? She's pretty! I will do everything. Make your mind
easy," said he in abrupt sentences while sealing his letter.

Andrew did not speak; he was both pleased and displeased that his father
understood him. The old man got up and gave the letter to his son.

"Listen!" said he; "don't worry about your wife: what can be done shall
be. Now listen! Give this letter to Michael Ilarionovich. * I have
written that he should make use of you in proper places and not keep you
long as an adjutant: a bad position! Tell him I remember and like him.
Write and tell me how he receives you. If he is all right--serve him.
Nicholas Bolkonski's son need not serve under anyone if he is in
disfavor. Now come here."


*Kutuzov.

He spoke so rapidly that he did not finish half his words, but his son
was accustomed to understand him. He led him to the desk, raised the
lid, drew out a drawer, and took out an exercise book filled with his
bold, tall, close handwriting.

"I shall probably die before you. So remember, these are my memoirs;
hand them to the Emperor after my death. Now here is a Lombard bond and
a letter; it is a premium for the man who writes a history of Suvorov's
wars. Send it to the Academy. Here are some jottings for you to read
when I am gone. You will find them useful."

Andrew did not tell his father that he would no doubt live a long time
yet. He felt that he must not say it.

"I will do it all, Father," he said.

"Well, now, good-by!" He gave his son his hand to kiss, and embraced
him. "Remember this, Prince Andrew, if they kill you it will hurt me,
your old father..." he paused unexpectedly, and then in a querulous
voice suddenly shrieked: "but if I hear that you have not behaved like a
son of Nicholas Bolkonski, I shall be ashamed!"

"You need not have said that to me, Father," said the son with a smile.

The old man was silent.

"I also wanted to ask you," continued Prince Andrew, "if I'm killed and
if I have a son, do not let him be taken away from you--as I said
yesterday... let him grow up with you.... Please."

"Not let the wife have him?" said the old man, and laughed.

They stood silent, facing one another. The old man's sharp eyes were
fixed straight on his son's. Something twitched in the lower part of the
old prince's face.

"We've said good-by. Go!" he suddenly shouted in a loud, angry voice,
opening his door.

"What is it? What?" asked both princesses when they saw for a moment at
the door Prince Andrew and the figure of the old man in a white dressing
gown, spectacled and wigless, shouting in an angry voice.

Prince Andrew sighed and made no reply.

"Well!" he said, turning to his wife.

And this "Well!" sounded coldly ironic, as if he were saying,: "Now go
through your performance."

"Andrew, already!" said the little princess, turning pale and looking
with dismay at her husband.

He embraced her. She screamed and fell unconscious on his shoulder.

He cautiously released the shoulder she leaned on, looked into her face,
and carefully placed her in an easy chair.

"Adieu, Mary," said he gently to his sister, taking her by the hand and
kissing her, and then he left the room with rapid steps.

The little princess lay in the armchair, Mademoiselle Bourienne chafing
her temples. Princess Mary, supporting her sister-in-law, still looked
with her beautiful eyes full of tears at the door through which Prince
Andrew had gone and made the sign of the cross in his direction. From
the study, like pistol shots, came the frequent sound of the old man
angrily blowing his nose. Hardly had Prince Andrew gone when the study
door opened quickly and the stern figure of the old man in the white
dressing gown looked out.

"Gone? That's all right!" said he; and looking angrily at the
unconscious little princess, he shook his head reprovingly and slammed
the door.

BOOK TWO: 1805




CHAPTER I

In October, 1805, a Russian army was occupying the villages and towns of
the Archduchy of Austria, and yet other regiments freshly arriving from
Russia were settling near the fortress of Braunau and burdening the
inhabitants on whom they were quartered. Braunau was the headquarters of
the commander-in-chief, Kutuzov.

On October 11, 1805, one of the infantry regiments that had just reached
Braunau had halted half a mile from the town, waiting to be inspected by
the commander-in-chief. Despite the un-Russian appearance of the
locality and surroundings--fruit gardens, stone fences, tiled roofs, and
hills in the distance--and despite the fact that the inhabitants (who
gazed with curiosity at the soldiers) were not Russians, the regiment
had just the appearance of any Russian regiment preparing for an
inspection anywhere in the heart of Russia.

On the evening of the last day's march an order had been received that
the commander-in-chief would inspect the regiment on the march. Though
the words of the order were not clear to the regimental commander, and
the question arose whether the troops were to be in marching order or
not, it was decided at a consultation between the battalion commanders
to present the regiment in parade order, on the principle that it is
always better to "bow too low than not bow low enough." So the soldiers,
after a twenty-mile march, were kept mending and cleaning all night long
without closing their eyes, while the adjutants and company commanders
calculated and reckoned, and by morning the regiment--instead of the
straggling, disorderly crowd it had been on its last march the day
before--presented a well-ordered array of two thousand men each of whom
knew his place and his duty, had every button and every strap in place,
and shone with cleanliness. And not only externally was all in order,
but had it pleased the commander-in-chief to look under the uniforms he
would have found on every man a clean shirt, and in every knapsack the
appointed number of articles, "awl, soap, and all," as the soldiers say.
There was only one circumstance concerning which no one could be at
ease. It was the state of the soldiers' boots. More than half the men's
boots were in holes. But this defect was not due to any fault of the
regimental commander, for in spite of repeated demands boots had not
been issued by the Austrian commissariat, and the regiment had marched
some seven hundred miles.

The commander of the regiment was an elderly, choleric, stout, and
thick-set general with grizzled eyebrows and whiskers, and wider from
chest to back than across the shoulders. He had on a brand-new uniform
showing the creases where it had been folded and thick gold epaulettes
which seemed to stand rather than lie down on his massive shoulders. He
had the air of a man happily performing one of the most solemn duties of
his life. He walked about in front of the line and at every step pulled
himself up, slightly arching his back. It was plain that the commander
admired his regiment, rejoiced in it, and that his whole mind was
engrossed by it, yet his strut seemed to indicate that, besides military
matters, social interests and the fair sex occupied no small part of his
thoughts.

"Well, Michael Mitrich, sir?" he said, addressing one of the battalion
commanders who smilingly pressed forward (it was plain that they both
felt happy). "We had our hands full last night. However, I think the
regiment is not a bad one, eh?"

The battalion commander perceived the jovial irony and laughed.

"It would not be turned off the field even on the Tsaritsin Meadow."

"What?" asked the commander.

At that moment, on the road from the town on which signalers had been
posted, two men appeared on horse back. They were an aide-de-camp
followed by a Cossack.

The aide-de-camp was sent to confirm the order which had not been
clearly worded the day before, namely, that the commander-in-chief
wished to see the regiment just in the state in which it had been on the
march: in their greatcoats, and packs, and without any preparation
whatever.

A member of the Hofkriegsrath from Vienna had come to Kutuzov the day
before with proposals and demands for him to join up with the army of
the Archduke Ferdinand and Mack, and Kutuzov, not considering this
junction advisable, meant, among other arguments in support of his view,
to show the Austrian general the wretched state in which the troops
arrived from Russia. With this object he intended to meet the regiment;
so the worse the condition it was in, the better pleased the commander-
in-chief would be. Though the aide-de-camp did not know these
circumstances, he nevertheless delivered the definite order that the men
should be in their greatcoats and in marching order, and that the
commander-in-chief would otherwise be dissatisfied. On hearing this the
regimental commander hung his head, silently shrugged his shoulders, and
spread out his arms with a choleric gesture.

"A fine mess we've made of it!" he remarked.

"There now! Didn't I tell you, Michael Mitrich, that if it was said 'on
the march' it meant in greatcoats?" said he reproachfully to the
battalion commander. "Oh, my God!" he added, stepping resolutely
forward. "Company commanders!" he shouted in a voice accustomed to
command. "Sergeants major!... How soon will he be here?" he asked the
aide-de-camp with a respectful politeness evidently relating to the
personage he was referring to.

"In an hour's time, I should say."

"Shall we have time to change clothes?"

"I don't know, General...."

The regimental commander, going up to the line himself, ordered the
soldiers to change into their greatcoats. The company commanders ran off
to their companies, the sergeants major began bustling (the greatcoats
were not in very good condition), and instantly the squares that had up
to then been in regular order and silent began to sway and stretch and
hum with voices. On all sides soldiers were running to and fro, throwing
up their knapsacks with a jerk of their shoulders and pulling the straps
over their heads, unstrapping their overcoats and drawing the sleeves on
with upraised arms.

In half an hour all was again in order, only the squares had become gray
instead of black. The regimental commander walked with his jerky steps
to the front of the regiment and examined it from a distance.

"Whatever is this? This!" he shouted and stood still. "Commander of the
third company!"

"Commander of the third company wanted by the general!... commander to
the general... third company to the commander." The words passed along
the lines and an adjutant ran to look for the missing officer.

When the eager but misrepeated words had reached their destination in a
cry of: "The general to the third company," the missing officer appeared
from behind his company and, though he was a middle-aged man and not in
the habit of running, trotted awkwardly stumbling on his toes toward the
general. The captain's face showed the uneasiness of a schoolboy who is
told to repeat a lesson he has not learned. Spots appeared on his nose,
the redness of which was evidently due to intemperance, and his mouth
twitched nervously. The general looked the captain up and down as he
came up panting, slackening his pace as he approached.

"You will soon be dressing your men in petticoats! What is this?"
shouted the regimental commander, thrusting forward his jaw and pointing
at a soldier in the ranks of the third company in a greatcoat of bluish
cloth, which contrasted with the others. "What have you been after? The
commander in chief is expected and you leave your place? Eh? I'll teach
you to dress the men in fancy coats for a parade.... Eh...?"

The commander of the company, with his eyes fixed on his superior,
pressed two fingers more and more rigidly to his cap, as if in this
pressure lay his only hope of salvation.

"Well, why don't you speak? Whom have you got there dressed up as a
Hungarian?" said the commander with an austere gibe.

"Your excellency..."

"Well, your excellency, what? Your excellency! But what about your
excellency?... nobody knows."

"Your excellency, it's the officer Dolokhov, who has been reduced to the
ranks," said the captain softly.

"Well? Has he been degraded into a field marshal, or into a soldier? If
a soldier, he should be dressed in regulation uniform like the others."

"Your excellency, you gave him leave yourself, on the march."

"Gave him leave? Leave? That's just like you young men," said the
regimental commander cooling down a little. "Leave indeed.... One says a
word to you and you... What?" he added with renewed irritation, "I beg
you to dress your men decently."

And the commander, turning to look at the adjutant, directed his jerky
steps down the line. He was evidently pleased at his own display of
anger and walking up to the regiment wished to find a further excuse for
wrath. Having snapped at an officer for an unpolished badge, at another
because his line was not straight, he reached the third company.

"H-o-o-w are you standing? Where's your leg? Your leg?" shouted the
commander with a tone of suffering in his voice, while there were still
five men between him and Dolokhov with his bluish-gray uniform.

Dolokhov slowly straightened his bent knee, looking straight with his
clear, insolent eyes in the general's face.

"Why a blue coat? Off with it... Sergeant major! Change his coat... the
ras..." he did not finish.

"General, I must obey orders, but I am not bound to endure..." Dolokhov
hurriedly interrupted.

"No talking in the ranks!... No talking, no talking!"

"Not bound to endure insults," Dolokhov concluded in loud, ringing
tones.

The eyes of the general and the soldier met. The general became silent,
angrily pulling down his tight scarf.

"I request you to have the goodness to change your coat," he said as he
turned away.




CHAPTER II

"He's coming!" shouted the signaler at that moment.

The regimental commander, flushing, ran to his horse, seized the stirrup
with trembling hands, threw his body across the saddle, righted himself,
drew his saber, and with a happy and resolute countenance, opening his
mouth awry, prepared to shout. The regiment fluttered like a bird
preening its plumage and became motionless.

"Att-ention!" shouted the regimental commander in a soul-shaking voice
which expressed joy for himself, severity for the regiment, and welcome
for the approaching chief.

Along the broad country road, edged on both sides by trees, came a high,
light blue Viennese caleche, slightly creaking on its springs and drawn
by six horses at a smart trot. Behind the caleche galloped the suite and
a convoy of Croats. Beside Kutuzov sat an Austrian general, in a white
uniform that looked strange among the Russian black ones. The caleche
stopped in front of the regiment. Kutuzov and the Austrian general were
talking in low voices and Kutuzov smiled slightly as treading heavily he
stepped down from the carriage just as if those two thousand men
breathlessly gazing at him and the regimental commander did not exist.

The word of command rang out, and again the regiment quivered, as with a
jingling sound it presented arms. Then amidst a dead silence the feeble
voice of the commander-in-chief was heard. The regiment roared, "Health
to your ex... len... len... lency!" and again all became silent. At
first Kutuzov stood still while the regiment moved; then he and the
general in white, accompanied by the suite, walked between the ranks.

From the way the regimental commander saluted the commander-in-chief and
devoured him with his eyes, drawing himself up obsequiously, and from
the way he walked through the ranks behind the generals, bending forward
and hardly able to restrain his jerky movements, and from the way he
darted forward at every word or gesture of the commander-in-chief, it
was evident that he performed his duty as a subordinate with even
greater zeal than his duty as a commander. Thanks to the strictness and
assiduity of its commander the regiment, in comparison with others that
had reached Braunau at the same time, was in splendid condition. There
were only 217 sick and stragglers. Everything was in good order except
the boots.

Kutuzov walked through the ranks, sometimes stopping to say a few
friendly words to officers he had known in the Turkish war, sometimes
also to the soldiers. Looking at their boots he several times shook his
head sadly, pointing them out to the Austrian general with an expression
which seemed to say that he was not blaming anyone, but could not help
noticing what a bad state of things it was. The regimental commander ran
forward on each such occasion, fearing to miss a single word of the
commander-in-chief's regarding the regiment. Behind Kutuzov, at a
distance that allowed every softly spoken word to be heard, followed
some twenty men of his suite. These gentlemen talked among themselves
and sometimes laughed. Nearest of all to the commander-in-chief walked a
handsome adjutant. This was Prince Bolkonski. Beside him was his comrade
Nesvitski, a tall staff officer, extremely stout, with a kindly,
smiling, handsome face and moist eyes. Nesvitski could hardly keep from
laughter provoked by a swarthy hussar officer who walked beside him.
This hussar, with a grave face and without a smile or a change in the
expression of his fixed eyes, watched the regimental commander's back
and mimicked his every movement. Each time the commander started and
bent forward, the hussar started and bent forward in exactly the same
manner. Nesvitski laughed and nudged the others to make them look at the
wag.

Kutuzov walked slowly and languidly past thousands of eyes which were
starting from their sockets to watch their chief. On reaching the third
company he suddenly stopped. His suite, not having expected this,
involuntarily came closer to him.

"Ah, Timokhin!" said he, recognizing the red-nosed captain who had been
reprimanded on account of the blue greatcoat.

One would have thought it impossible for a man to stretch himself more
than Timokhin had done when he was reprimanded by the regimental
commander, but now that the commander-in-chief addressed him he drew
himself up to such an extent that it seemed he could not have sustained
it had the commander-in-chief continued to look at him, and so Kutuzov,
who evidently understood his case and wished him nothing but good,
quickly turned away, a scarcely perceptible smile flitting over his
scarred and puffy face.

"Another Ismail comrade," said he. "A brave officer! Are you satisfied
with him?" he asked the regimental commander.

And the latter--unconscious that he was being reflected in the hussar
officer as in a looking glass--started, moved forward, and answered:
"Highly satisfied, your excellency!"

"We all have our weaknesses," said Kutuzov smiling and walking away from
him. "He used to have a predilection for Bacchus."

The regimental commander was afraid he might be blamed for this and did
not answer. The hussar at that moment noticed the face of the red-nosed
captain and his drawn-in stomach, and mimicked his expression and pose
with such exactitude that Nesvitski could not help laughing. Kutuzov
turned round. The officer evidently had complete control of his face,
and while Kutuzov was turning managed to make a grimace and then assume
a most serious, deferential, and innocent expression.

The third company was the last, and Kutuzov pondered, apparently trying
to recollect something. Prince Andrew stepped forward from among the
suite and said in French:

"You told me to remind you of the officer Dolokhov, reduced to the ranks
in this regiment."

"Where is Dolokhov?" asked Kutuzov.

Dolokhov, who had already changed into a soldier's gray greatcoat, did
not wait to be called. The shapely figure of the fair-haired soldier,
with his clear blue eyes, stepped forward from the ranks, went up to the
commander in chief, and presented arms.

"Have you a complaint to make?" Kutuzov asked with a slight frown.

"This is Dolokhov," said Prince Andrew.

"Ah!" said Kutuzov. "I hope this will be a lesson to you. Do your duty.
The Emperor is gracious, and I shan't forget you if you deserve well."

The clear blue eyes looked at the commander-in-chief just as boldly as
they had looked at the regimental commander, seeming by their expression
to tear open the veil of convention that separates a commander-in-chief
so widely from a private.

"One thing I ask of your excellency," Dolokhov said in his firm,
ringing, deliberate voice. "I ask an opportunity to atone for my fault
and prove my devotion to His Majesty the Emperor and to Russia!"

Kutuzov turned away. The same smile of the eyes with which he had turned
from Captain Timokhin again flitted over his face. He turned away with a
grimace as if to say that everything Dolokhov had said to him and
everything he could say had long been known to him, that he was weary of
it and it was not at all what he wanted. He turned away and went to the
carriage.

The regiment broke up into companies, which went to their appointed
quarters near Braunau, where they hoped to receive boots and clothes and
to rest after their hard marches.

"You won't bear me a grudge, Prokhor Ignatych?" said the regimental
commander, overtaking the third company on its way to its quarters and
riding up to Captain Timokhin who was walking in front. (The regimental
commander's face now that the inspection was happily over beamed with
irrepressible delight.) "It's in the Emperor's service... it can't be
helped... one is sometimes a bit hasty on parade... I am the first to
apologize, you know me!... He was very pleased!" And he held out his
hand to the captain.

"Don't mention it, General, as if I'd be so bold!" replied the captain,
his nose growing redder as he gave a smile which showed where two front
teeth were missing that had been knocked out by the butt end of a gun at
Ismail.

"And tell Mr. Dolokhov that I won't forget him--he may be quite easy.
And tell me, please--I've been meaning to ask--how is he behaving
himself, and in general..."

"As far as the service goes he is quite punctilious, your excellency;
but his character..." said Timokhin.

"And what about his character?" asked the regimental commander.

"It's different on different days," answered the captain. "One day he is
sensible, well educated, and good-natured, and the next he's a wild
beast.... In Poland, if you please, he nearly killed a Jew."

"Oh, well, well!" remarked the regimental commander. "Still, one must
have pity on a young man in misfortune. You know he has important
connections... Well, then, you just..."

"I will, your excellency," said Timokhin, showing by his smile that he
understood his commander's wish.

"Well, of course, of course!"

The regimental commander sought out Dolokhov in the ranks and, reining
in his horse, said to him:

"After the next affair... epaulettes."

Dolokhov looked round but did not say anything, nor did the mocking
smile on his lips change.

"Well, that's all right," continued the regimental commander. "A cup of
vodka for the men from me," he added so that the soldiers could hear. "I
thank you all! God be praised!" and he rode past that company and
overtook the next one.

"Well, he's really a good fellow, one can serve under him," said
Timokhin to the subaltern beside him.

"In a word, a hearty one..." said the subaltern, laughing (the
regimental commander was nicknamed King of Hearts).

The cheerful mood of their officers after the inspection infected the
soldiers. The company marched on gaily. The soldiers' voices could be
heard on every side.

"And they said Kutuzov was blind of one eye?"

"And so he is! Quite blind!"

"No, friend, he is sharper-eyed than you are. Boots and leg bands... he
noticed everything..."

"When he looked at my feet, friend... well, thinks I..."

"And that other one with him, the Austrian, looked as if he were smeared
with chalk--as white as flour! I suppose they polish him up as they do
the guns."

"I say, Fedeshon!... Did he say when the battles are to begin? You were
near him. Everybody said that Buonaparte himself was at Braunau."

"Buonaparte himself!... Just listen to the fool, what he doesn't know!
The Prussians are up in arms now. The Austrians, you see, are putting
them down. When they've been put down, the war with Buonaparte will
begin. And he says Buonaparte is in Braunau! Shows you're a fool. You'd
better listen more carefully!"

"What devils these quartermasters are! See, the fifth company is turning
into the village already... they will have their buckwheat cooked before
we reach our quarters."

"Give me a biscuit, you devil!"

"And did you give me tobacco yesterday? That's just it, friend! Ah,
well, never mind, here you are."

"They might call a halt here or we'll have to do another four miles
without eating."

"Wasn't it fine when those Germans gave us lifts! You just sit still and
are drawn along."

"And here, friend, the people are quite beggarly. There they all seemed
to be Poles--all under the Russian crown--but here they're all regular
Germans."

"Singers to the front" came the captain's order.

And from the different ranks some twenty men ran to the front. A
drummer, their leader, turned round facing the singers, and flourishing
his arm, began a long-drawn-out soldiers' song, commencing with the
words: "Morning dawned, the sun was rising," and concluding: "On then,
brothers, on to glory, led by Father Kamenski." This song had been
composed in the Turkish campaign and now being sung in Austria, the only
change being that the words "Father Kamenski" were replaced by "Father
Kutuzov."

Having jerked out these last words as soldiers do and waved his arms as
if flinging something to the ground, the drummer--a lean, handsome
soldier of forty--looked sternly at the singers and screwed up his eyes.
Then having satisfied himself that all eyes were fixed on him, he raised
both arms as if carefully lifting some invisible but precious object
above his head and, holding it there for some seconds, suddenly flung it
down and began:

"Oh, my bower, oh, my bower...!"

"Oh, my bower new...!" chimed in twenty voices, and the castanet player,
in spite of the burden of his equipment, rushed out to the front and,
walking backwards before the company, jerked his shoulders and
flourished his castanets as if threatening someone. The soldiers,
swinging their arms and keeping time spontaneously, marched with long
steps. Behind the company the sound of wheels, the creaking of springs,
and the tramp of horses' hoofs were heard. Kutuzov and his suite were
returning to the town. The commander-in-chief made a sign that the men
should continue to march at ease, and he and all his suite showed
pleasure at the sound of the singing and the sight of the dancing
soldier and the gay and smartly marching men. In the second file from
the right flank, beside which the carriage passed the company, a blue-
eyed soldier involuntarily attracted notice. It was Dolokhov marching
with particular grace and boldness in time to the song and looking at
those driving past as if he pitied all who were not at that moment
marching with the company. The hussar cornet of Kutuzov's suite who had
mimicked the regimental commander, fell back from the carriage and rode
up to Dolokhov.

Hussar cornet Zherkov had at one time, in Petersburg, belonged to the
wild set led by Dolokhov. Zherkov had met Dolokhov abroad as a private
and had not seen fit to recognize him. But now that Kutuzov had spoken
to the gentleman ranker, he addressed him with the cordiality of an old
friend.

"My dear fellow, how are you?" said he through the singing, making his
horse keep pace with the company.

"How am I?" Dolokhov answered coldly. "I am as you see."

The lively song gave a special flavor to the tone of free and easy
gaiety with which Zherkov spoke, and to the intentional coldness of
Dolokhov's reply.

"And how do you get on with the officers?" inquired Zherkov.

"All right. They are good fellows. And how have you wriggled onto the
staff?"

"I was attached; I'm on duty."

Both were silent.

"She let the hawk fly upward from her wide right sleeve," went the song,
arousing an involuntary sensation of courage and cheerfulness. Their
conversation would probably have been different but for the effect of
that song.

"Is it true that Austrians have been beaten?" asked Dolokhov.

"The devil only knows! They say so."

"I'm glad," answered Dolokhov briefly and clearly, as the song demanded.

"I say, come round some evening and we'll have a game of faro!" said
Zherkov.

"Why, have you too much money?"

"Do come."

"I can't. I've sworn not to. I won't drink and won't play till I get
reinstated."

"Well, that's only till the first engagement."

"We shall see."

They were again silent.

"Come if you need anything. One can at least be of use on the staff..."

Dolokhov smiled. "Don't trouble. If I want anything, I won't beg--I'll
take it!"

"Well, never mind; I only..."

"And I only..."

"Good-bye."

"Good health..."


"It's a long, long way. To my native land..."

Zherkov touched his horse with the spurs; it pranced excitedly from foot
to foot uncertain with which to start, then settled down, galloped past
the company, and overtook the carriage, still keeping time to the song.




CHAPTER III

On returning from the review, Kutuzov took the Austrian general into his
private room and, calling his adjutant, asked for some papers relating
to the condition of the troops on their arrival, and the letters that
had come from the Archduke Ferdinand, who was in command of the advanced
army. Prince Andrew Bolkonski came into the room with the required
papers. Kutuzov and the Austrian member of the Hofkriegsrath were
sitting at the table on which a plan was spread out.

"Ah!..." said Kutuzov glancing at Bolkonski as if by this exclamation he
was asking the adjutant to wait, and he went on with the conversation in
French.

"All I can say, General," said he with a pleasant elegance of expression
and intonation that obliged one to listen to each deliberately spoken
word. It was evident that Kutuzov himself listened with pleasure to his
own voice. "All I can say, General, is that if the matter depended on my
personal wishes, the will of His Majesty the Emperor Francis would have
been fulfilled long ago. I should long ago have joined the archduke. And
believe me on my honour that to me personally it would be a pleasure to
hand over the supreme command of the army into the hands of a better
informed and more skillful general--of whom Austria has so many--and to
lay down all this heavy responsibility. But circumstances are sometimes
too strong for us, General."

And Kutuzov smiled in a way that seemed to say, "You are quite at
liberty not to believe me and I don't even care whether you do or not,
but you have no grounds for telling me so. And that is the whole point."

The Austrian general looked dissatisfied, but had no option but to reply
in the same tone.

"On the contrary," he said, in a querulous and angry tone that
contrasted with his flattering words, "on the contrary, your
excellency's participation in the common action is highly valued by His
Majesty; but we think the present delay is depriving the splendid
Russian troops and their commander of the laurels they have been
accustomed to win in their battles," he concluded his evidently
prearranged sentence.

Kutuzov bowed with the same smile.

"But that is my conviction, and judging by the last letter with which
His Highness the Archduke Ferdinand has honored me, I imagine that the
Austrian troops, under the direction of so skillful a leader as General
Mack, have by now already gained a decisive victory and no longer need
our aid," said Kutuzov.

The general frowned. Though there was no definite news of an Austrian
defeat, there were many circumstances confirming the unfavorable rumors
that were afloat, and so Kutuzov's suggestion of an Austrian victory
sounded much like irony. But Kutuzov went on blandly smiling with the
same expression, which seemed to say that he had a right to suppose so.
And, in fact, the last letter he had received from Mack's army informed
him of a victory and stated strategically the position of the army was
very favorable.

"Give me that letter," said Kutuzov turning to Prince Andrew. "Please
have a look at it"--and Kutuzov with an ironical smile about the corners
of his mouth read to the Austrian general the following passage, in
German, from the Archduke Ferdinand's letter:

We have fully concentrated forces of nearly seventy thousand men with
which to attack and defeat the enemy should he cross the Lech. Also, as
we are masters of Ulm, we cannot be deprived of the advantage of
commanding both sides of the Danube, so that should the enemy not cross
the Lech, we can cross the Danube, throw ourselves on his line of
communications, recross the river lower down, and frustrate his
intention should he try to direct his whole force against our faithful
ally. We shall therefore confidently await the moment when the Imperial
Russian army will be fully equipped, and shall then, in conjunction with
it, easily find a way to prepare for the enemy the fate he deserves.

Kutuzov sighed deeply on finishing this paragraph and looked at the
member of the Hofkriegsrath mildly and attentively.

"But you know the wise maxim your excellency, advising one to expect the
worst," said the Austrian general, evidently wishing to have done with
jests and to come to business. He involuntarily looked round at the
aide-de-camp.

"Excuse me, General," interrupted Kutuzov, also turning to Prince
Andrew. "Look here, my dear fellow, get from Kozlovski all the reports
from our scouts. Here are two letters from Count Nostitz and here is one
from His Highness the Archduke Ferdinand and here are these," he said,
handing him several papers, "make a neat memorandum in French out of all
this, showing all the news we have had of the movements of the Austrian
army, and then give it to his excellency."

Prince Andrew bowed his head in token of having understood from the
first not only what had been said but also what Kutuzov would have liked
to tell him. He gathered up the papers and with a bow to both, stepped
softly over the carpet and went out into the waiting room.

Though not much time had passed since Prince Andrew had left Russia, he
had changed greatly during that period. In the expression of his face,
in his movements, in his walk, scarcely a trace was left of his former
affected languor and indolence. He now looked like a man who has time to
think of the impression he makes on others, but is occupied with
agreeable and interesting work. His face expressed more satisfaction
with himself and those around him, his smile and glance were brighter
and more attractive.

Kutuzov, whom he had overtaken in Poland, had received him very kindly,
promised not to forget him, distinguished him above the other adjutants,
and had taken him to Vienna and given him the more serious commissions.
From Vienna Kutuzov wrote to his old comrade, Prince Andrew's father.

Your son bids fair to become an officer distinguished by his industry,
firmness, and expedition. I consider myself fortunate to have such a
subordinate by me.

On Kutuzov's staff, among his fellow officers and in the army generally,
Prince Andrew had, as he had had in Petersburg society, two quite
opposite reputations. Some, a minority, acknowledged him to be different
from themselves and from everyone else, expected great things of him,
listened to him, admired, and imitated him, and with them Prince Andrew
was natural and pleasant. Others, the majority, disliked him and
considered him conceited, cold, and disagreeable. But among these people
Prince Andrew knew how to take his stand so that they respected and even
feared him.

Coming out of Kutuzov's room into the waiting room with the papers in
his hand Prince Andrew came up to his comrade, the aide-de-camp on duty,
Kozlovski, who was sitting at the window with a book.

"Well, Prince?" asked Kozlovski.

"I am ordered to write a memorandum explaining why we are not
advancing."

"And why is it?"

Prince Andrew shrugged his shoulders.

"Any news from Mack?"

"No."

"If it were true that he has been beaten, news would have come."

"Probably," said Prince Andrew moving toward the outer door.

But at that instant a tall Austrian general in a greatcoat, with the
order of Maria Theresa on his neck and a black bandage round his head,
who had evidently just arrived, entered quickly, slamming the door.
Prince Andrew stopped short.

"Commander in Chief Kutuzov?" said the newly arrived general speaking
quickly with a harsh German accent, looking to both sides and advancing
straight toward the inner door.

"The commander-in-chief is engaged," said Kozlovski, going hurriedly up
to the unknown general and blocking his way to the door. "Whom shall I
announce?"

The unknown general looked disdainfully down at Kozlovski, who was
rather short, as if surprised that anyone should not know him.

"The commander-in-chief is engaged," repeated Kozlovski calmly.

The general's face clouded, his lips quivered and trembled. He took out
a notebook, hurriedly scribbled something in pencil, tore out the leaf,
gave it to Kozlovski, stepped quickly to the window, and threw himself
into a chair, gazing at those in the room as if asking, "Why do they
look at me?" Then he lifted his head, stretched his neck as if he
intended to say something, but immediately, with affected indifference,
began to hum to himself, producing a queer sound which immediately broke
off. The door of the private room opened and Kutuzov appeared in the
doorway. The general with the bandaged head bent forward as though
running away from some danger, and, making long, quick strides with his
thin legs, went up to Kutuzov.

"Vous voyez le malheureux Mack," he uttered in a broken voice.

Kutuzov's face as he stood in the open doorway remained perfectly
immobile for a few moments. Then wrinkles ran over his face like a wave
and his forehead became smooth again, he bowed his head respectfully,
closed his eyes, silently let Mack enter his room before him, and closed
the door himself behind him.

The report which had been circulated that the Austrians had been beaten
and that the whole army had surrendered at Ulm proved to be correct.
Within half an hour adjutants had been sent in various directions with
orders which showed that the Russian troops, who had hitherto been
inactive, would also soon have to meet the enemy.

Prince Andrew was one of those rare staff officers whose chief interest
lay in the general progress of the war. When he saw Mack and heard the
details of his disaster he understood that half the campaign was lost,
understood all the difficulties of the Russian army's position, and
vividly imagined what awaited it and the part he would have to play.
Involuntarily he felt a joyful agitation at the thought of the
humiliation of arrogant Austria and that in a week's time he might,
perhaps, see and take part in the first Russian encounter with the
French since Suvorov met them. He feared that Bonaparte's genius might
outweigh all the courage of the Russian troops, and at the same time
could not admit the idea of his hero being disgraced.

Excited and irritated by these thoughts Prince Andrew went toward his
room to write to his father, to whom he wrote every day. In the corridor
he met Nesvitski, with whom he shared a room, and the wag Zherkov; they
were as usual laughing.

"Why are you so glum?" asked Nesvitski noticing Prince Andrew's pale
face and glittering eyes.

"There's nothing to be gay about," answered Bolkonski.

Just as Prince Andrew met Nesvitski and Zherkov, there came toward them
from the other end of the corridor, Strauch, an Austrian general who on
Kutuzov's staff in charge of the provisioning of the Russian army, and
the member of the Hofkriegsrath who had arrived the previous evening.
There was room enough in the wide corridor for the generals to pass the
three officers quite easily, but Zherkov, pushing Nesvitski aside with
his arm, said in a breathless voice,

"They're coming!... they're coming!... Stand aside, make way, please
make way!"

The generals were passing by, looking as if they wished to avoid
embarrassing attentions. On the face of the wag Zherkov there suddenly
appeared a stupid smile of glee which he seemed unable to suppress.

"Your excellency," said he in German, stepping forward and addressing
the Austrian general, "I have the honor to congratulate you."

He bowed his head and scraped first with one foot and then with the
other, awkwardly, like a child at a dancing lesson.

The member of the Hofkriegsrath looked at him severely but, seeing the
seriousness of his stupid smile, could not but give him a moment's
attention. He screwed up his eyes showing that he was listening.

"I have the honor to congratulate you. General Mack has arrived, quite
well, only a little bruised just here," he added, pointing with a
beaming smile to his head.

The general frowned, turned away, and went on.

"Gott, wie naiv!" * said he angrily, after he had gone a few steps.


* "Good God, what simplicity!"

Nesvitski with a laugh threw his arms round Prince Andrew, but
Bolkonski, turning still paler, pushed him away with an angry look and
turned to Zherkov. The nervous irritation aroused by the appearance of
Mack, the news of his defeat, and the thought of what lay before the
Russian army found vent in anger at Zherkov's untimely jest.

"If you, sir, choose to make a buffoon of yourself," he said sharply,
with a slight trembling of the lower jaw, "I can't prevent your doing
so; but I warn you that if you dare to play the fool in my presence, I
will teach you to behave yourself."

Nesvitski and Zherkov were so surprised by this outburst that they gazed
at Bolkonski silently with wide-open eyes.

"What's the matter? I only congratulated them," said Zherkov.

"I am not jesting with you; please be silent!" cried Bolkonski, and
taking Nesvitski's arm he left Zherkov, who did not know what to say.

"Come, what's the matter, old fellow?" said Nesvitski trying to soothe
him.

"What's the matter?" exclaimed Prince Andrew standing still in his
excitement. "Don't you understand that either we are officers serving
our Tsar and our country, rejoicing in the successes and grieving at the
misfortunes of our common cause, or we are merely lackeys who care
nothing for their master's business. Quarante mille hommes massacres et
l'armee de nos allies detruite, et vous trouvez la le mot pour rire," *
he said, as if strengthening his views by this French sentence. "C'est
bien pour un garcon de rien comme cet individu dont vous avez fait un
ami, mais pas pour vous, pas pour vous. *(2) Only a hobbledehoy could
amuse himself in this way," he added in Russian--but pronouncing the
word with a French accent--having noticed that Zherkov could still hear
him.


* "Forty thousand men massacred and the army of our allies destroyed,
and you find that a cause for jesting!"

* (2) "It is all very well for that good-for-nothing fellow of whom you
have made a friend, but not for you, not for you."

He waited a moment to see whether the cornet would answer, but he turned
and went out of the corridor.




CHAPTER IV

The Pavlograd Hussars were stationed two miles from Braunau. The
squadron in which Nicholas Rostov served as a cadet was quartered in the
German village of Salzeneck. The best quarters in the village were
assigned to cavalry-captain Denisov, the squadron commander, known
throughout the whole cavalry division as Vaska Denisov. Cadet Rostov,
ever since he had overtaken the regiment in Poland, had lived with the
squadron commander.

On October 11, the day when all was astir at headquarters over the news
of Mack's defeat, the camp life of the officers of this squadron was
proceeding as usual. Denisov, who had been losing at cards all night,
had not yet come home when Rostov rode back early in the morning from a
foraging expedition. Rostov in his cadet uniform, with a jerk to his
horse, rode up to the porch, swung his leg over the saddle with a supple
youthful movement, stood for a moment in the stirrup as if loathe to
part from his horse, and at last sprang down and called to his orderly.

"Ah, Bondarenko, dear friend!" said he to the hussar who rushed up
headlong to the horse. "Walk him up and down, my dear fellow," he
continued, with that gay brotherly cordiality which goodhearted young
people show to everyone when they are happy.

"Yes, your excellency," answered the Ukrainian gaily, tossing his head.

"Mind, walk him up and down well!"

Another hussar also rushed toward the horse, but Bondarenko had already
thrown the reins of the snaffle bridle over the horse's head. It was
evident that the cadet was liberal with his tips and that it paid to
serve him. Rostov patted the horse's neck and then his flank, and
lingered for a moment.

"Splendid! What a horse he will be!" he thought with a smile, and
holding up his saber, his spurs jingling, he ran up the steps of the
porch. His landlord, who in a waistcoat and a pointed cap, pitchfork in
hand, was clearing manure from the cowhouse, looked out, and his face
immediately brightened on seeing Rostov. "Schon gut Morgen! Schon gut
Morgen!" * he said winking with a merry smile, evidently pleased to
greet the young man.


* "A very good morning! A very good morning!"

"Schon fleissig?" * said Rostov with the same gay brotherly smile which
did not leave his eager face. "Hoch Oestreicher! Hoch Russen! Kaiser
Alexander hoch!" *(2) said he, quoting words often repeated by the
German landlord.


* "Busy already?"

* (2) "Hurrah for the Austrians! Hurrah for the Russians! Hurrah for
Emperor Alexander!"

The German laughed, came out of the cowshed, pulled off his cap, and
waving it above his head cried:

"Und die ganze Welt hoch!" *


* "And hurrah for the whole world!"

Rostov waved his cap above his head like the German and cried laughing,
"Und vivat die ganze Welt!" Though neither the German cleaning his
cowshed nor Rostov back with his platoon from foraging for hay had any
reason for rejoicing, they looked at each other with joyful delight and
brotherly love, wagged their heads in token of their mutual affection,
and parted smiling, the German returning to his cowshed and Rostov going
to the cottage he occupied with Denisov.

"What about your master?" he asked Lavrushka, Denisov's orderly, whom
all the regiment knew for a rogue.

"Hasn't been in since the evening. Must have been losing," answered
Lavrushka. "I know by now, if he wins he comes back early to brag about
it, but if he stays out till morning it means he's lost and will come
back in a rage. Will you have coffee?"

"Yes, bring some."

Ten minutes later Lavrushka brought the coffee. "He's coming!" said he.
"Now for trouble!" Rostov looked out of the window and saw Denisov
coming home. Denisov was a small man with a red face, sparkling black
eyes, and black tousled mustache and hair. He wore an unfastened cloak,
wide breeches hanging down in creases, and a crumpled shako on the back
of his head. He came up to the porch gloomily, hanging his head.

"Lavwuska!" he shouted loudly and angrily, "take it off, blockhead!"

"Well, I am taking it off," replied Lavrushka's voice.

"Ah, you're up already," said Denisov, entering the room.

"Long ago," answered Rostov, "I have already been for the hay, and have
seen Fraulein Mathilde."

"Weally! And I've been losing, bwother. I lost yesterday like a damned
fool!" cried Denisov, not pronouncing his r's. "Such ill luck! Such ill
luck. As soon as you left, it began and went on. Hullo there! Tea!"

Puckering up his face though smiling, and showing his short strong
teeth, he began with stubby fingers of both hands to ruffle up his thick
tangled black hair.

"And what devil made me go to that wat?" (an officer nicknamed "the
rat") he said, rubbing his forehead and whole face with both hands.
"Just fancy, he didn't let me win a single cahd, not one cahd."

He took the lighted pipe that was offered to him, gripped it in his
fist, and tapped it on the floor, making the sparks fly, while he
continued to shout.

"He lets one win the singles and collahs it as soon as one doubles it;
gives the singles and snatches the doubles!"

He scattered the burning tobacco, smashed the pipe, and threw it away.
Then he remained silent for a while, and all at once looked cheerfully
with his glittering, black eyes at Rostov.

"If at least we had some women here; but there's nothing foh one to do
but dwink. If we could only get to fighting soon. Hullo, who's there?"
he said, turning to the door as he heard a tread of heavy boots and the
clinking of spurs that came to a stop, and a respectful cough.

"The squadron quartermaster!" said Lavrushka.

Denisov's face puckered still more.

"Wetched!" he muttered, throwing down a purse with some gold in it.
"Wostov, deah fellow, just see how much there is left and shove the
purse undah the pillow," he said, and went out to the quartermaster.

Rostov took the money and, mechanically arranging the old and new coins
in separate piles, began counting them.

"Ah! Telyanin! How d'ye do? They plucked me last night," came Denisov's
voice from the next room.

"Where? At Bykov's, at the rat's... I knew it," replied a piping voice,
and Lieutenant Telyanin, a small officer of the same squadron, entered
the room.

Rostov thrust the purse under the pillow and shook the damp little hand
which was offered him. Telyanin for some reason had been transferred
from the Guards just before this campaign. He behaved very well in the
regiment but was not liked; Rostov especially detested him and was
unable to overcome or conceal his groundless antipathy to the man.

"Well, young cavalryman, how is my Rook behaving?" he asked. (Rook was a
young horse Telyanin had sold to Rostov.)

The lieutenant never looked the man he was speaking to straight in the
face; his eyes continually wandered from one object to another.

"I saw you riding this morning..." he added.

"Oh, he's all right, a good horse," answered Rostov, though the horse
for which he had paid seven hundred rubbles was not worth half that sum.
"He's begun to go a little lame on the left foreleg," he added.

"The hoof's cracked! That's nothing. I'll teach you what to do and show
you what kind of rivet to use."

"Yes, please do," said Rostov.

"I'll show you, I'll show you! It's not a secret. And it's a horse
you'll thank me for."

"Then I'll have it brought round," said Rostov wishing to avoid
Telyanin, and he went out to give the order.

In the passage Denisov, with a pipe, was squatting on the threshold
facing the quartermaster who was reporting to him. On seeing Rostov,
Denisov screwed up his face and pointing over his shoulder with his
thumb to the room where Telyanin was sitting, he frowned and gave a
shudder of disgust.

"Ugh! I don't like that fellow," he said, regardless of the
quartermaster's presence.

Rostov shrugged his shoulders as much as to say: "Nor do I, but what's
one to do?" and, having given his order, he returned to Telyanin.

Telyanin was sitting in the same indolent pose in which Rostov had left
him, rubbing his small white hands.

"Well there certainly are disgusting people," thought Rostov as he
entered.

"Have you told them to bring the horse?" asked Telyanin, getting up and
looking carelessly about him.

"I have."

"Let us go ourselves. I only came round to ask Denisov about yesterday's
order. Have you got it, Denisov?"

"Not yet. But where are you off to?"

"I want to teach this young man how to shoe a horse," said Telyanin.

They went through the porch and into the stable. The lieutenant
explained how to rivet the hoof and went away to his own quarters.

When Rostov went back there was a bottle of vodka and a sausage on the
table. Denisov was sitting there scratching with his pen on a sheet of
paper. He looked gloomily in Rostov's face and said: "I am witing to
her."

He leaned his elbows on the table with his pen in his hand and,
evidently glad of a chance to say quicker in words what he wanted to
write, told Rostov the contents of his letter.

"You see, my fwiend," he said, "we sleep when we don't love. We are
childwen of the dust... but one falls in love and one is a God, one is
pua' as on the first day of cweation... Who's that now? Send him to the
devil, I'm busy!" he shouted to Lavrushka, who went up to him not in the
least abashed.

"Who should it be? You yourself told him to come. It's the quartermaster
for the money."

Denisov frowned and was about to shout some reply but stopped.

"Wetched business," he muttered to himself. "How much is left in the
puhse?" he asked, turning to Rostov.

"Seven new and three old imperials."

"Oh, it's wetched! Well, what are you standing there for, you sca'cwow?
Call the quahtehmasteh," he shouted to Lavrushka.

"Please, Denisov, let me lend you some: I have some, you know," said
Rostov, blushing.

"Don't like bowwowing from my own fellows, I don't," growled Denisov.

"But if you won't accept money from me like a comrade, you will offend
me. Really I have some," Rostov repeated.

"No, I tell you."

And Denisov went to the bed to get the purse from under the pillow.

"Where have you put it, Wostov?"

"Under the lower pillow."

"It's not there."

Denisov threw both pillows on the floor. The purse was not there.

"That's a miwacle."

"Wait, haven't you dropped it?" said Rostov, picking up the pillows one
at a time and shaking them.

He pulled off the quilt and shook it. The purse was not there.

"Dear me, can I have forgotten? No, I remember thinking that you kept it
under your head like a treasure," said Rostov. "I put it just here.
Where is it?" he asked, turning to Lavrushka.

"I haven't been in the room. It must be where you put it."

"But it isn't?..."

"You're always like that; you thwow a thing down anywhere and forget it.
Feel in your pockets."

"No, if I hadn't thought of it being a treasure," said Rostov, "but I
remember putting it there."

Lavrushka turned all the bedding over, looked under the bed and under
the table, searched everywhere, and stood still in the middle of the
room. Denisov silently watched Lavrushka's movements, and when the
latter threw up his arms in surprise saying it was nowhere to be found
Denisov glanced at Rostov.

"Wostov, you've not been playing schoolboy twicks..."

Rostov felt Denisov's gaze fixed on him, raised his eyes, and instantly
dropped them again. All the blood which had seemed congested somewhere
below his throat rushed to his face and eyes. He could not draw breath.

"And there hasn't been anyone in the room except the lieutenant and
yourselves. It must be here somewhere," said Lavrushka.

"Now then, you devil's puppet, look alive and hunt for it!" shouted
Denisov, suddenly, turning purple and rushing at the man with a
threatening gesture. "If the purse isn't found I'll flog you, I'll flog
you all."

Rostov, his eyes avoiding Denisov, began buttoning his coat, buckled on
his saber, and put on his cap.

"I must have that purse, I tell you," shouted Denisov, shaking his
orderly by the shoulders and knocking him against the wall.

"Denisov, let him alone, I know who has taken it," said Rostov, going
toward the door without raising his eyes. Denisov paused, thought a
moment, and, evidently understanding what Rostov hinted at, seized his
arm.

"Nonsense!" he cried, and the veins on his forehead and neck stood out
like cords. "You are mad, I tell you. I won't allow it. The purse is
here! I'll flay this scoundwel alive, and it will be found."

"I know who has taken it," repeated Rostov in an unsteady voice, and
went to the door.

"And I tell you, don't you dahe to do it!" shouted Denisov, rushing at
the cadet to restrain him.

But Rostov pulled away his arm and, with as much anger as though Denisov
were his worst enemy, firmly fixed his eyes directly on his face.

"Do you understand what you're saying?" he said in a trembling voice.
"There was no one else in the room except myself. So that if it is not
so, then..."

He could not finish, and ran out of the room.

"Ah, may the devil take you and evewybody," were the last words Rostov
heard.

Rostov went to Telyanin's quarters.

"The master is not in, he's gone to headquarters," said Telyanin's
orderly. "Has something happened?" he added, surprised at the cadet's
troubled face.

"No, nothing."

"You've only just missed him," said the orderly.

The headquarters were situated two miles away from Salzeneck, and
Rostov, without returning home, took a horse and rode there. There was
an inn in the village which the officers frequented. Rostov rode up to
it and saw Telyanin's horse at the porch.

In the second room of the inn the lieutenant was sitting over a dish of
sausages and a bottle of wine.

"Ah, you've come here too, young man!" he said, smiling and raising his
eyebrows.

"Yes," said Rostov as if it cost him a great deal to utter the word; and
he sat down at the nearest table.

Both were silent. There were two Germans and a Russian officer in the
room. No one spoke and the only sounds heard were the clatter of knives
and the munching of the lieutenant.

When Telyanin had finished his lunch he took out of his pocket a double
purse and, drawing its rings aside with his small, white, turned-up
fingers, drew out a gold imperial, and lifting his eyebrows gave it to
the waiter.

"Please be quick," he said.

The coin was a new one. Rostov rose and went up to Telyanin.

"Allow me to look at your purse," he said in a low, almost inaudible,
voice.

With shifting eyes but eyebrows still raised, Telyanin handed him the
purse.

"Yes, it's a nice purse. Yes, yes," he said, growing suddenly pale, and
added, "Look at it, young man."

Rostov took the purse in his hand, examined it and the money in it, and
looked at Telyanin. The lieutenant was looking about in his usual way
and suddenly seemed to grow very merry.

"If we get to Vienna I'll get rid of it there but in these wretched
little towns there's nowhere to spend it," said he. "Well, let me have
it, young man, I'm going."

Rostov did not speak.

"And you? Are you going to have lunch too? They feed you quite decently
here," continued Telyanin. "Now then, let me have it."

He stretched out his hand to take hold of the purse. Rostov let go of
it. Telyanin took the purse and began carelessly slipping it into the
pocket of his riding breeches, with his eyebrows lifted and his mouth
slightly open, as if to say, "Yes, yes, I am putting my purse in my
pocket and that's quite simple and is no one else's business."

"Well, young man?" he said with a sigh, and from under his lifted brows
he glanced into Rostov's eyes.

Some flash as of an electric spark shot from Telyanin's eyes to Rostov's
and back, and back again and again in an instant.

"Come here," said Rostov, catching hold of Telyanin's arm and almost
dragging him to the window. "That money is Denisov's; you took it..." he
whispered just above Telyanin's ear.

"What? What? How dare you? What?" said Telyanin.

But these words came like a piteous, despairing cry and an entreaty for
pardon. As soon as Rostov heard them, an enormous load of doubt fell
from him. He was glad, and at the same instant began to pity the
miserable man who stood before him, but the task he had begun had to be
completed.

"Heaven only knows what the people here may imagine," muttered Telyanin,
taking up his cap and moving toward a small empty room. "We must have an
explanation..."

"I know it and shall prove it," said Rostov.

"I..."

Every muscle of Telyanin's pale, terrified face began to quiver, his
eyes still shifted from side to side but with a downward look not rising
to Rostov's face, and his sobs were audible.

"Count!... Don't ruin a young fellow... here is this wretched money,
take it..." He threw it on the table. "I have an old father and
mother!..."

Rostov took the money, avoiding Telyanin's eyes, and went out of the
room without a word. But at the door he stopped and then retraced his
steps. "O God," he said with tears in his eyes, "how could you do it?"

"Count..." said Telyanin drawing nearer to him.

"Don't touch me," said Rostov, drawing back. "If you need it, take the
money," and he threw the purse to him and ran out of the inn.




CHAPTER V

That same evening there was an animated discussion among the squadron's
officers in Denisov's quarters.

"And I tell you, Rostov, that you must apologize to the colonel!" said a
tall, grizzly-haired staff captain, with enormous mustaches and many
wrinkles on his large features, to Rostov who was crimson with
excitement.

The staff captain, Kirsten, had twice been reduced to the ranks for
affairs of honor and had twice regained his commission.

"I will allow no one to call me a liar!" cried Rostov. "He told me I
lied, and I told him he lied. And there it rests. He may keep me on duty
every day, or may place me under arrest, but no one can make me
apologize, because if he, as commander of this regiment, thinks it
beneath his dignity to give me satisfaction, then..."

"You just wait a moment, my dear fellow, and listen," interrupted the
staff captain in his deep bass, calmly stroking his long mustache. "You
tell the colonel in the presence of other officers that an officer has
stolen..."

"I'm not to blame that the conversation began in the presence of other
officers. Perhaps I ought not to have spoken before them, but I am not a
diplomatist. That's why I joined the hussars, thinking that here one
would not need finesse; and he tells me that I am lying--so let him give
me satisfaction..."

"That's all right. No one thinks you a coward, but that's not the point.
Ask Denisov whether it is not out of the question for a cadet to demand
satisfaction of his regimental commander?"

Denisov sat gloomily biting his mustache and listening to the
conversation, evidently with no wish to take part in it. He answered the
staff captain's question by a disapproving shake of his head.

"You speak to the colonel about this nasty business before other
officers," continued the staff captain, "and Bogdanich" (the colonel was
called Bogdanich) "shuts you up."

"He did not shut me up, he said I was telling an untruth."

"Well, have it so, and you talked a lot of nonsense to him and must
apologize."

"Not on any account!" exclaimed Rostov.

"I did not expect this of you," said the staff captain seriously and
severely. "You don't wish to apologize, but, man, it's not only to him
but to the whole regiment--all of us--you're to blame all round. The
case is this: you ought to have thought the matter over and taken
advice; but no, you go and blurt it all straight out before the
officers. Now what was the colonel to do? Have the officer tried and
disgrace the whole regiment? Disgrace the whole regiment because of one
scoundrel? Is that how you look at it? We don't see it like that. And
Bogdanich was a brick: he told you you were saying what was not true.
It's not pleasant, but what's to be done, my dear fellow? You landed
yourself in it. And now, when one wants to smooth the thing over, some
conceit prevents your apologizing, and you wish to make the whole affair
public. You are offended at being put on duty a bit, but why not
apologize to an old and honorable officer? Whatever Bogdanich may be,
anyway he is an honorable and brave old colonel! You're quick at taking
offense, but you don't mind disgracing the whole regiment!" The staff
captain's voice began to tremble. "You have been in the regiment next to
no time, my lad, you're here today and tomorrow you'll be appointed
adjutant somewhere and can snap your fingers when it is said 'There are
thieves among the Pavlograd officers!' But it's not all the same to us!
Am I not right, Denisov? It's not the same!"

Denisov remained silent and did not move, but occasionally looked with
his glittering black eyes at Rostov.

"You value your own pride and don't wish to apologize," continued the
staff captain, "but we old fellows, who have grown up in and, God
willing, are going to die in the regiment, we prize the honor of the
regiment, and Bogdanich knows it. Oh, we do prize it, old fellow! And
all this is not right, it's not right! You may take offense or not but I
always stick to mother truth. It's not right!"

And the staff captain rose and turned away from Rostov.


"That's twue, devil take it!" shouted Denisov, jumping up. "Now then,
Wostov, now then!"

Rostov, growing red and pale alternately, looked first at one officer
and then at the other.

"No, gentlemen, no... you mustn't think... I quite understand. You're
wrong to think that of me... I... for me... for the honor of the
regiment I'd... Ah well, I'll show that in action, and for me the honor
of the flag... Well, never mind, it's true I'm to blame, to blame all
round. Well, what else do you want?..."

"Come, that's right, Count!" cried the staff captain, turning round and
clapping Rostov on the shoulder with his big hand.

"I tell you," shouted Denisov, "he's a fine fellow."

"That's better, Count," said the staff captain, beginning to address
Rostov by his title, as if in recognition of his confession. "Go and
apologize, your excellency. Yes, go!"

"Gentlemen, I'll do anything. No one shall hear a word from me," said
Rostov in an imploring voice, "but I can't apologize, by God I can't, do
what you will! How can I go and apologize like a little boy asking
forgiveness?"

Denisov began to laugh.

"It'll be worse for you. Bogdanich is vindictive and you'll pay for your
obstinacy," said Kirsten.

"No, on my word it's not obstinacy! I can't describe the feeling. I
can't..."

"Well, it's as you like," said the staff captain. "And what has become
of that scoundrel?" he asked Denisov.

"He has weported himself sick, he's to be stwuck off the list tomowwow,"
muttered Denisov.

"It is an illness, there's no other way of explaining it," said the
staff captain.

"Illness or not, he'd better not cwoss my path. I'd kill him!" shouted
Denisov in a bloodthirsty tone.

Just then Zherkov entered the room.

"What brings you here?" cried the officers turning to the newcomer.

"We're to go into action, gentlemen! Mack has surrendered with his whole
army."

"It's not true!"

"I've seen him myself!"

"What? Saw the real Mack? With hands and feet?"

"Into action! Into action! Bring him a bottle for such news! But how did
you come here?"

"I've been sent back to the regiment all on account of that devil, Mack.
An Austrian general complained of me. I congratulated him on Mack's
arrival... What's the matter, Rostov? You look as if you'd just come out
of a hot bath."

"Oh, my dear fellow, we're in such a stew here these last two days."

The regimental adjutant came in and confirmed the news brought by
Zherkov. They were under orders to advance next day.

"We're going into action, gentlemen!"

"Well, thank God! We've been sitting here too long!"




CHAPTER VI

Kutuzov fell back toward Vienna, destroying behind him the bridges over
the rivers Inn (at Braunau) and Traun (near Linz). On October 23 the
Russian troops were crossing the river Enns. At midday the Russian
baggage train, the artillery, and columns of troops were defiling
through the town of Enns on both sides of the bridge.

It was a warm, rainy, autumnal day. The wide expanse that opened out
before the heights on which the Russian batteries stood guarding the
bridge was at times veiled by a diaphanous curtain of slanting rain, and
then, suddenly spread out in the sunlight, far-distant objects could be
clearly seen glittering as though freshly varnished. Down below, the
little town could be seen with its white, red-roofed houses, its
cathedral, and its bridge, on both sides of which streamed jostling
masses of Russian troops. At the bend of the Danube, vessels, an island,
and a castle with a park surrounded by the waters of the confluence of
the Enns and the Danube became visible, and the rocky left bank of the
Danube covered with pine forests, with a mystic background of green
treetops and bluish gorges. The turrets of a convent stood out beyond a
wild virgin pine forest, and far away on the other side of the Enns the
enemy's horse patrols could be discerned.

Among the field guns on the brow of the hill the general in command of
the rearguard stood with a staff officer, scanning the country through
his fieldglass. A little behind them Nesvitski, who had been sent to the
rearguard by the commander-in-chief, was sitting on the trail of a gun
carriage. A Cossack who accompanied him had handed him a knapsack and a
flask, and Nesvitski was treating some officers to pies and real
doppelkummel. The officers gladly gathered round him, some on their
knees, some squatting Turkish fashion on the wet grass.

"Yes, the Austrian prince who built that castle was no fool. It's a fine
place! Why are you not eating anything, gentlemen?" Nesvitski was
saying.

"Thank you very much, Prince," answered one of the officers, pleased to
be talking to a staff officer of such importance. "It's a lovely place!
We passed close to the park and saw two deer... and what a splendid
house!"

"Look, Prince," said another, who would have dearly liked to take
another pie but felt shy, and therefore pretended to be examining the
countryside--"See, our infantrymen have already got there. Look there in
the meadow behind the village, three of them are dragging something.
They'll ransack that castle," he remarked with evident approval.

"So they will," said Nesvitski. "No, but what I should like," added he,
munching a pie in his moist-lipped handsome mouth, "would be to slip in
over there."

He pointed with a smile to a turreted nunnery, and his eyes narrowed and
gleamed.

"That would be fine, gentlemen!"

The officers laughed.

"Just to flutter the nuns a bit. They say there are Italian girls among
them. On my word I'd give five years of my life for it!"

"They must be feeling dull, too," said one of the bolder officers,
laughing.

Meanwhile the staff officer standing in front pointed out something to
the general, who looked through his field glass.

"Yes, so it is, so it is," said the general angrily, lowering the field
glass and shrugging his shoulders, "so it is! They'll be fired on at the
crossing. And why are they dawdling there?"

On the opposite side the enemy could be seen by the naked eye, and from
their battery a milk-white cloud arose. Then came the distant report of
a shot, and our troops could be seen hurrying to the crossing.

Nesvitski rose, puffing, and went up to the general, smiling.

"Would not your excellency like a little refreshment?" he said.

"It's a bad business," said the general without answering him, "our men
have been wasting time."

"Hadn't I better ride over, your excellency?" asked Nesvitski.

"Yes, please do," answered the general, and he repeated the order that
had already once been given in detail: "and tell the hussars that they
are to cross last and to fire the bridge as I ordered; and the
inflammable material on the bridge must be reinspected."

"Very good," answered Nesvitski.

He called the Cossack with his horse, told him to put away the knapsack
and flask, and swung his heavy person easily into the saddle.

"I'll really call in on the nuns," he said to the officers who watched
him smilingly, and he rode off by the winding path down the hill.

"Now then, let's see how far it will carry, Captain. Just try!" said the
general, turning to an artillery officer. "Have a little fun to pass the
time."

"Crew, to your guns!" commanded the officer.

In a moment the men came running gaily from their campfires and began
loading.

"One!" came the command.

Number one jumped briskly aside. The gun rang out with a deafening
metallic roar, and a whistling grenade flew above the heads of our
troops below the hill and fell far short of the enemy, a little smoke
showing the spot where it burst.

The faces of officers and men brightened up at the sound. Everyone got
up and began watching the movements of our troops below, as plainly
visible as if but a stone's throw away, and the movements of the
approaching enemy farther off. At the same instant the sun came fully
out from behind the clouds, and the clear sound of the solitary shot and
the brilliance of the bright sunshine merged in a single joyous and
spirited impression.




CHAPTER VII

Two of the enemy's shots had already flown across the bridge, where
there was a crush. Halfway across stood Prince Nesvitski, who had
alighted from his horse and whose big body was jammed against the
railings. He looked back laughing to the Cossack who stood a few steps
behind him holding two horses by their bridles. Each time Prince
Nesvitski tried to move on, soldiers and carts pushed him back again and
pressed him against the railings, and all he could do was to smile.

"What a fine fellow you are, friend!" said the Cossack to a convoy
soldier with a wagon, who was pressing onto the infantrymen who were
crowded together close to his wheels and his horses. "What a fellow! You
can't wait a moment! Don't you see the general wants to pass?"

But the convoyman took no notice of the word "general" and shouted at
the soldiers who were blocking his way. "Hi there, boys! Keep to the
left! Wait a bit." But the soldiers, crowded together shoulder to
shoulder, their bayonets interlocking, moved over the bridge in a dense
mass. Looking down over the rails Prince Nesvitski saw the rapid, noisy
little waves of the Enns, which rippling and eddying round the piles of
the bridge chased each other along. Looking on the bridge he saw equally
uniform living waves of soldiers, shoulder straps, covered shakos,
knapsacks, bayonets, long muskets, and, under the shakos, faces with
broad cheekbones, sunken cheeks, and listless tired expressions, and
feet that moved through the sticky mud that covered the planks of the
bridge. Sometimes through the monotonous waves of men, like a fleck of
white foam on the waves of the Enns, an officer, in a cloak and with a
type of face different from that of the men, squeezed his way along;
sometimes like a chip of wood whirling in the river, an hussar on foot,
an orderly, or a townsman was carried through the waves of infantry; and
sometimes like a log floating down the river, an officers' or company's
baggage wagon, piled high, leather covered, and hemmed in on all sides,
moved across the bridge.

"It's as if a dam had burst," said the Cossack hopelessly. "Are there
many more of you to come?"

"A million all but one!" replied a waggish soldier in a torn coat, with
a wink, and passed on followed by another, an old man.

"If he" (he meant the enemy) "begins popping at the bridge now," said
the old soldier dismally to a comrade, "you'll forget to scratch
yourself."

That soldier passed on, and after him came another sitting on a cart.

"Where the devil have the leg bands been shoved to?" said an orderly,
running behind the cart and fumbling in the back of it.

And he also passed on with the wagon. Then came some merry soldiers who
had evidently been drinking.

"And then, old fellow, he gives him one in the teeth with the butt end
of his gun..." a soldier whose greatcoat was well tucked up said gaily,
with a wide swing of his arm.

"Yes, the ham was just delicious..." answered another with a loud laugh.
And they, too, passed on, so that Nesvitski did not learn who had been
struck on the teeth, or what the ham had to do with it.

"Bah! How they scurry. He just sends a ball and they think they'll all
be killed," a sergeant was saying angrily and reproachfully.

"As it flies past me, Daddy, the ball I mean," said a young soldier with
an enormous mouth, hardly refraining from laughing, "I felt like dying
of fright. I did, 'pon my word, I got that frightened!" said he, as if
bragging of having been frightened.

That one also passed. Then followed a cart unlike any that had gone
before. It was a German cart with a pair of horses led by a German, and
seemed loaded with a whole houseful of effects. A fine brindled cow with
a large udder was attached to the cart behind. A woman with an unweaned
baby, an old woman, and a healthy German girl with bright red cheeks
were sitting on some feather beds. Evidently these fugitives were
allowed to pass by special permission. The eyes of all the soldiers
turned toward the women, and while the vehicle was passing at foot pace
all the soldiers' remarks related to the two young ones. Every face bore
almost the same smile, expressing unseemly thoughts about the women.

"Just see, the German sausage is making tracks, too!"

"Sell me the missis," said another soldier, addressing the German, who,
angry and frightened, strode energetically along with downcast eyes.

"See how smart she's made herself! Oh, the devils!"

"There, Fedotov, you should be quartered on them!"

"I have seen as much before now, mate!"

"Where are you going?" asked an infantry officer who was eating an
apple, also half smiling as he looked at the handsome girl.

The German closed his eyes, signifying that he did not understand.

"Take it if you like," said the officer, giving the girl an apple.

The girl smiled and took it. Nesvitski like the rest of the men on the
bridge did not take his eyes off the women till they had passed. When
they had gone by, the same stream of soldiers followed, with the same
kind of talk, and at last all stopped. As often happens, the horses of a
convoy wagon became restive at the end of the bridge, and the whole
crowd had to wait.

"And why are they stopping? There's no proper order!" said the soldiers.
"Where are you shoving to? Devil take you! Can't you wait? It'll be
worse if he fires the bridge. See, here's an officer jammed in too"--
different voices were saying in the crowd, as the men looked at one
another, and all pressed toward the exit from the bridge.

Looking down at the waters of the Enns under the bridge, Nesvitski
suddenly heard a sound new to him, of something swiftly approaching...
something big, that splashed into the water.

"Just see where it carries to!" a soldier near by said sternly, looking
round at the sound.

"Encouraging us to get along quicker," said another uneasily.

The crowd moved on again. Nesvitski realized that it was a cannon ball.

"Hey, Cossack, my horse!" he said. "Now, then, you there! get out of the
way! Make way!"

With great difficulty he managed to get to his horse, and shouting
continually he moved on. The soldiers squeezed themselves to make way
for him, but again pressed on him so that they jammed his leg, and those
nearest him were not to blame for they were themselves pressed still
harder from behind.

"Nesvitski, Nesvitski! you numskull!" came a hoarse voice from behind
him.

Nesvitski looked round and saw, some fifteen paces away but separated by
the living mass of moving infantry, Vaska Denisov, red and shaggy, with
his cap on the back of his black head and a cloak hanging jauntily over
his shoulder.

"Tell these devils, these fiends, to let me pass!" shouted Denisov
evidently in a fit of rage, his coal-black eyes with their bloodshot
whites glittering and rolling as he waved his sheathed saber in a small
bare hand as red as his face.

"Ah, Vaska!" joyfully replied Nesvitski. "What's up with you?"

"The squadwon can't pass," shouted Vaska Denisov, showing his white
teeth fiercely and spurring his black thoroughbred Arab, which twitched
its ears as the bayonets touched it, and snorted, spurting white foam
from his bit, tramping the planks of the bridge with his hoofs, and
apparently ready to jump over the railings had his rider let him. "What
is this? They're like sheep! Just like sheep! Out of the way!... Let us
pass!... Stop there, you devil with the cart! I'll hack you with my
saber!" he shouted, actually drawing his saber from its scabbard and
flourishing it.

The soldiers crowded against one another with terrified faces, and
Denisov joined Nesvitski.

"How's it you're not drunk today?" said Nesvitski when the other had
ridden up to him.

"They don't even give one time to dwink!" answered Vaska Denisov. "They
keep dwagging the wegiment to and fwo all day. If they mean to fight,
let's fight. But the devil knows what this is."

"What a dandy you are today!" said Nesvitski, looking at Denisov's new
cloak and saddlecloth.

Denisov smiled, took out of his sabretache a handkerchief that diffused
a smell of perfume, and put it to Nesvitski's nose.

"Of course. I'm going into action! I've shaved, bwushed my teeth, and
scented myself."

The imposing figure of Nesvitski followed by his Cossack, and the
determination of Denisov who flourished his sword and shouted
frantically, had such an effect that they managed to squeeze through to
the farther side of the bridge and stopped the infantry. Beside the
bridge Nesvitski found the colonel to whom he had to deliver the order,
and having done this he rode back.

Having cleared the way Denisov stopped at the end of the bridge.
Carelessly holding in his stallion that was neighing and pawing the
ground, eager to rejoin its fellows, he watched his squadron draw
nearer. Then the clang of hoofs, as of several horses galloping,
resounded on the planks of the bridge, and the squadron, officers in
front and men four abreast, spread across the bridge and began to emerge
on his side of it.

The infantry who had been stopped crowded near the bridge in the
trampled mud and gazed with that particular feeling of ill-will,
estrangement, and ridicule with which troops of different arms usually
encounter one another at the clean, smart hussars who moved past them in
regular order.

"Smart lads! Only fit for a fair!" said one.

"What good are they? They're led about just for show!" remarked another.

"Don't kick up the dust, you infantry!" jested an hussar whose prancing
horse had splashed mud over some foot soldiers.

"I'd like to put you on a two days' march with a knapsack! Your fine
cords would soon get a bit rubbed," said an infantryman, wiping the mud
off his face with his sleeve. "Perched up there, you're more like a bird
than a man."

"There now, Zikin, they ought to put you on a horse. You'd look fine,"
said a corporal, chaffing a thin little soldier who bent under the
weight of his knapsack.

"Take a stick between your legs, that'll suit you for a horse!" the
hussar shouted back.




CHAPTER VIII

The last of the infantry hurriedly crossed the bridge, squeezing
together as they approached it as if passing through a funnel. At last
the baggage wagons had all crossed, the crush was less, and the last
battalion came onto the bridge. Only Denisov's squadron of hussars
remained on the farther side of the bridge facing the enemy, who could
be seen from the hill on the opposite bank but was not yet visible from
the bridge, for the horizon as seen from the valley through which the
river flowed was formed by the rising ground only half a mile away. At
the foot of the hill lay wasteland over which a few groups of our
Cossack scouts were moving. Suddenly on the road at the top of the high
ground, artillery and troops in blue uniform were seen. These were the
French. A group of Cossack scouts retired down the hill at a trot. All
the officers and men of Denisov's squadron, though they tried to talk of
other things and to look in other directions, thought only of what was
there on the hilltop, and kept constantly looking at the patches
appearing on the skyline, which they knew to be the enemy's troops. The
weather had cleared again since noon and the sun was descending brightly
upon the Danube and the dark hills around it. It was calm, and at
intervals the bugle calls and the shouts of the enemy could be heard
from the hill. There was no one now between the squadron and the enemy
except a few scattered skirmishers. An empty space of some seven hundred
yards was all that separated them. The enemy ceased firing, and that
stern, threatening, inaccessible, and intangible line which separates
two hostile armies was all the more clearly felt.

"One step beyond that boundary line which resembles the line dividing
the living from the dead lies uncertainty, suffering, and death. And
what is there? Who is there?--there beyond that field, that tree, that
roof lit up by the sun? No one knows, but one wants to know. You fear
and yet long to cross that line, and know that sooner or later it must
be crossed and you will have to find out what is there, just as you will
inevitably have to learn what lies the other side of death. But you are
strong, healthy, cheerful, and excited, and are surrounded by other such
excitedly animated and healthy men." So thinks, or at any rate feels,
anyone who comes in sight of the enemy, and that feeling gives a
particular glamour and glad keenness of impression to everything that
takes place at such moments.

On the high ground where the enemy was, the smoke of a cannon rose, and
a ball flew whistling over the heads of the hussar squadron. The
officers who had been standing together rode off to their places. The
hussars began carefully aligning their horses. Silence fell on the whole
squadron. All were looking at the enemy in front and at the squadron
commander, awaiting the word of command. A second and a third cannon
ball flew past. Evidently they were firing at the hussars, but the balls
with rapid rhythmic whistle flew over the heads of the horsemen and fell
somewhere beyond them. The hussars did not look round, but at the sound
of each shot, as at the word of command, the whole squadron with its
rows of faces so alike yet so different, holding its breath while the
ball flew past, rose in the stirrups and sank back again. The soldiers
without turning their heads glanced at one another, curious to see their
comrades' impression. Every face, from Denisov's to that of the bugler,
showed one common expression of conflict, irritation, and excitement,
around chin and mouth. The quartermaster frowned, looking at the
soldiers as if threatening to punish them. Cadet Mironov ducked every
time a ball flew past. Rostov on the left flank, mounted on his Rook--a
handsome horse despite its game leg--had the happy air of a schoolboy
called up before a large audience for an examination in which he feels
sure he will distinguish himself. He was glancing at everyone with a
clear, bright expression, as if asking them to notice how calmly he sat
under fire. But despite himself, on his face too that same indication of
something new and stern showed round the mouth.

"Who's that curtseying there? Cadet Miwonov! That's not wight! Look at
me," cried Denisov who, unable to keep still on one spot, kept turning
his horse in front of the squadron.

The black, hairy, snub-nosed face of Vaska Denisov, and his whole short
sturdy figure with the sinewy hairy hand and stumpy fingers in which he
held the hilt of his naked saber, looked just as it usually did,
especially toward evening when he had emptied his second bottle; he was
only redder than usual. With his shaggy head thrown back like birds when
they drink, pressing his spurs mercilessly into the sides of his good
horse, Bedouin, and sitting as though falling backwards in the saddle,
he galloped to the other flank of the squadron and shouted in a hoarse
voice to the men to look to their pistols. He rode up to Kirsten. The
staff captain on his broad-backed, steady mare came at a walk to meet
him. His face with its long mustache was serious as always, only his
eyes were brighter than usual.

"Well, what about it?" said he to Denisov. "It won't come to a fight.
You'll see--we shall retire."

"The devil only knows what they're about!" muttered Denisov. "Ah,
Wostov," he cried noticing the cadet's bright face, "you've got it at
last."

And he smiled approvingly, evidently pleased with the cadet. Rostov felt
perfectly happy. Just then the commander appeared on the bridge. Denisov
galloped up to him.

"Your excellency! Let us attack them! I'll dwive them off."

"Attack indeed!" said the colonel in a bored voice, puckering up his
face as if driving off a troublesome fly. "And why are you stopping
here? Don't you see the skirmishers are retreating? Lead the squadron
back."

The squadron crossed the bridge and drew out of range of fire without
having lost a single man. The second squadron that had been in the front
line followed them across and the last Cossacks quitted the farther side
of the river.

The two Pavlograd squadrons, having crossed the bridge, retired up the
hill one after the other. Their colonel, Karl Bogdanich Schubert, came
up to Denisov's squadron and rode at a footpace not far from Rostov,
without taking any notice of him although they were now meeting for the
first time since their encounter concerning Telyanin. Rostov, feeling
that he was at the front and in the power of a man toward whom he now
admitted that he had been to blame, did not lift his eyes from the
colonel's athletic back, his nape covered with light hair, and his red
neck. It seemed to Rostov that Bogdanich was only pretending not to
notice him, and that his whole aim now was to test the cadet's courage,
so he drew himself up and looked around him merrily; then it seemed to
him that Bogdanich rode so near in order to show him his courage. Next
he thought that his enemy would send the squadron on a desperate attack
just to punish him--Rostov. Then he imagined how, after the attack,
Bogdanich would come up to him as he lay wounded and would magnanimously
extend the hand of reconciliation.

The high-shouldered figure of Zherkov, familiar to the Pavlograds as he
had but recently left their regiment, rode up to the colonel. After his
dismissal from headquarters Zherkov had not remained in the regiment,
saying he was not such a fool as to slave at the front when he could get
more rewards by doing nothing on the staff, and had succeeded in
attaching himself as an orderly officer to Prince Bagration. He now came
to his former chief with an order from the commander of the rear guard.

"Colonel," he said, addressing Rostov's enemy with an air of gloomy
gravity and glancing round at his comrades, "there is an order to stop
and fire the bridge."

"An order to who?" asked the colonel morosely.

"I don't myself know 'to who,'" replied the cornet in a serious tone,
"but the prince told me to 'go and tell the colonel that the hussars
must return quickly and fire the bridge.'"

Zherkov was followed by an officer of the suite who rode up to the
colonel of hussars with the same order. After him the stout Nesvitski
came galloping up on a Cossack horse that could scarcely carry his
weight.

"How's this, Colonel?" he shouted as he approached. "I told you to fire
the bridge, and now someone has gone and blundered; they are all beside
themselves over there and one can't make anything out."

The colonel deliberately stopped the regiment and turned to Nesvitski.

"You spoke to me of inflammable material," said he, "but you said
nothing about firing it."

"But, my dear sir," said Nesvitski as he drew up, taking off his cap and
smoothing his hair wet with perspiration with his plump hand, "wasn't I
telling you to fire the bridge, when inflammable material had been put
in position?"

"I am not your 'dear sir,' Mr. Staff Officer, and you did not tell me to
burn the bridge! I know the service, and it is my habit orders strictly
to obey. You said the bridge would be burned, but who would it burn, I
could not know by the holy spirit!"

"Ah, that's always the way!" said Nesvitski with a wave of the hand.
"How did you get here?" said he, turning to Zherkov.

"On the same business. But you are damp! Let me wring you out!"

"You were saying, Mr. Staff Officer..." continued the colonel in an
offended tone.

"Colonel," interrupted the officer of the suite, "You must be quick or
the enemy will bring up his guns to use grapeshot."

The colonel looked silently at the officer of the suite, at the stout
staff officer, and at Zherkov, and he frowned.

"I will the bridge fire," he said in a solemn tone as if to announce
that in spite of all the unpleasantness he had to endure he would still
do the right thing.

Striking his horse with his long muscular legs as if it were to blame
for everything, the colonel moved forward and ordered the second
squadron, that in which Rostov was serving under Denisov, to return to
the bridge.

"There, it's just as I thought," said Rostov to himself. "He wishes to
test me!" His heart contracted and the blood rushed to his face. "Let
him see whether I am a coward!" he thought.

Again on all the bright faces of the squadron the serious expression
appeared that they had worn when under fire. Rostov watched his enemy,
the colonel, closely--to find in his face confirmation of his own
conjecture, but the colonel did not once glance at Rostov, and looked as
he always did when at the front, solemn and stern. Then came the word of
command.

"Look sharp! Look sharp!" several voices repeated around him.

Their sabers catching in the bridles and their spurs jingling, the
hussars hastily dismounted, not knowing what they were to do. The men
were crossing themselves. Rostov no longer looked at the colonel, he had
no time. He was afraid of falling behind the hussars, so much afraid
that his heart stood still. His hand trembled as he gave his horse into
an orderly's charge, and he felt the blood rush to his heart with a
thud. Denisov rode past him, leaning back and shouting something. Rostov
saw nothing but the hussars running all around him, their spurs catching
and their sabers clattering.

"Stretchers!" shouted someone behind him.

Rostov did not think what this call for stretchers meant; he ran on,
trying only to be ahead of the others; but just at the bridge, not
looking at the ground, he came on some sticky, trodden mud, stumbled,
and fell on his hands. The others outstripped him.

"At boss zides, Captain," he heard the voice of the colonel, who, having
ridden ahead, had pulled up his horse near the bridge, with a
triumphant, cheerful face.

Rostov wiping his muddy hands on his breeches looked at his enemy and
was about to run on, thinking that the farther he went to the front the
better. But Bogdanich, without looking at or recognizing Rostov, shouted
to him:

"Who's that running on the middle of the bridge? To the right! Come
back, Cadet!" he cried angrily; and turning to Denisov, who, showing off
his courage, had ridden on to the planks of the bridge:

"Why run risks, Captain? You should dismount," he said.

"Oh, every bullet has its billet," answered Vaska Denisov, turning in
his saddle.

Meanwhile Nesvitski, Zherkov, and the officer of the suite were standing
together out of range of the shots, watching, now the small group of men
with yellow shakos, dark-green jackets braided with cord, and blue
riding breeches, who were swarming near the bridge, and then at what was
approaching in the distance from the opposite side--the blue uniforms
and groups with horses, easily recognizable as artillery.

"Will they burn the bridge or not? Who'll get there first? Will they get
there and fire the bridge or will the French get within grapeshot range
and wipe them out?" These were the questions each man of the troops on
the high ground above the bridge involuntarily asked himself with a
sinking heart--watching the bridge and the hussars in the bright evening
light and the blue tunics advancing from the other side with their
bayonets and guns.

"Ugh. The hussars will get it hot!" said Nesvitski; "they are within
grapeshot range now."

"He shouldn't have taken so many men," said the officer of the suite.

"True enough," answered Nesvitski; "two smart fellows could have done
the job just as well."

"Ah, your excellency," put in Zherkov, his eyes fixed on the hussars,
but still with that naive air that made it impossible to know whether he
was speaking in jest or in earnest. "Ah, your excellency! How you look
at things! Send two men? And who then would give us the Vladimir medal
and ribbon? But now, even if they do get peppered, the squadron may be
recommended for honors and he may get a ribbon. Our Bogdanich knows how
things are done."

"There now!" said the officer of the suite, "that's grapeshot."

He pointed to the French guns, the limbers of which were being detached
and hurriedly removed.

On the French side, amid the groups with cannon, a cloud of smoke
appeared, then a second and a third almost simultaneously, and at the
moment when the first report was heard a fourth was seen. Then two
reports one after another, and a third.

"Oh! Oh!" groaned Nesvitski as if in fierce pain, seizing the officer of
the suite by the arm. "Look! A man has fallen! Fallen, fallen!"

"Two, I think."

"If I were Tsar I would never go to war," said Nesvitski, turning away.

The French guns were hastily reloaded. The infantry in their blue
uniforms advanced toward the bridge at a run. Smoke appeared again but
at irregular intervals, and grapeshot cracked and rattled onto the
bridge. But this time Nesvitski could not see what was happening there,
as a dense cloud of smoke arose from it. The hussars had succeeded in
setting it on fire and the French batteries were now firing at them, no
longer to hinder them but because the guns were trained and there was
someone to fire at.

The French had time to fire three rounds of grapeshot before the hussars
got back to their horses. Two were misdirected and the shot went too
high, but the last round fell in the midst of a group of hussars and
knocked three of them over.

Rostov, absorbed by his relations with Bogdanich, had paused on the
bridge not knowing what to do. There was no one to hew down (as he had
always imagined battles to himself), nor could he help to fire the
bridge because he had not brought any burning straw with him like the
other soldiers. He stood looking about him, when suddenly he heard a
rattle on the bridge as if nuts were being spilt, and the hussar nearest
to him fell against the rails with a groan. Rostov ran up to him with
the others. Again someone shouted, "Stretchers!" Four men seized the
hussar and began lifting him.

"Oooh! For Christ's sake let me alone!" cried the wounded man, but still
he was lifted and laid on the stretcher.

Nicholas Rostov turned away and, as if searching for something, gazed
into the distance, at the waters of the Danube, at the sky, and at the
sun. How beautiful the sky looked; how blue, how calm, and how deep! How
bright and glorious was the setting sun! With what soft glitter the
waters of the distant Danube shone. And fairer still were the faraway
blue mountains beyond the river, the nunnery, the mysterious gorges, and
the pine forests veiled in the mist of their summits... There was peace
and happiness... "I should wish for nothing else, nothing, if only I
were there," thought Rostov. "In myself alone and in that sunshine there
is so much happiness; but here... groans, suffering, fear, and this
uncertainty and hurry... There--they are shouting again, and again are
all running back somewhere, and I shall run with them, and it, death, is
here above me and around... Another instant and I shall never again see
the sun, this water, that gorge!..."

At that instant the sun began to hide behind the clouds, and other
stretchers came into view before Rostov. And the fear of death and of
the stretchers, and love of the sun and of life, all merged into one
feeling of sickening agitation.

"O Lord God! Thou who art in that heaven, save, forgive, and protect
me!" Rostov whispered.

The hussars ran back to the men who held their horses; their voices
sounded louder and calmer, the stretchers disappeared from sight.

"Well, fwiend? So you've smelt powdah!" shouted Vaska Denisov just above
his ear.

"It's all over; but I am a coward--yes, a coward!" thought Rostov, and
sighing deeply he took Rook, his horse, which stood resting one foot,
from the orderly and began to mount.

"Was that grapeshot?" he asked Denisov.

"Yes and no mistake!" cried Denisov. "You worked like wegular bwicks and
it's nasty work! An attack's pleasant work! Hacking away at the dogs!
But this sort of thing is the very devil, with them shooting at you like
a target."

And Denisov rode up to a group that had stopped near Rostov, composed of
the colonel, Nesvitski, Zherkov, and the officer from the suite.

"Well, it seems that no one has noticed," thought Rostov. And this was
true. No one had taken any notice, for everyone knew the sensation which
the cadet under fire for the first time had experienced.

"Here's something for you to report," said Zherkov. "See if I don't get
promoted to a sublieutenancy."

"Inform the prince that I the bridge fired!" said the colonel
triumphantly and gaily.

"And if he asks about the losses?"

"A trifle," said the colonel in his bass voice: "two hussars wounded,
and one knocked out," he added, unable to restrain a happy smile, and
pronouncing the phrase "knocked out" with ringing distinctness.




CHAPTER IX

Pursued by the French army of a hundred thousand men under the command
of Bonaparte, encountering a population that was unfriendly to it,
losing confidence in its allies, suffering from shortness of supplies,
and compelled to act under conditions of war unlike anything that had
been foreseen, the Russian army of thirty-five thousand men commanded by
Kutuzov was hurriedly retreating along the Danube, stopping where
overtaken by the enemy and fighting rearguard actions only as far as
necessary to enable it to retreat without losing its heavy equipment.
There had been actions at Lambach, Amstetten, and Melk; but despite the
courage and endurance--acknowledged even by the enemy--with which the
Russians fought, the only consequence of these actions was a yet more
rapid retreat. Austrian troops that had escaped capture at Ulm and had
joined Kutuzov at Braunau now separated from the Russian army, and
Kutuzov was left with only his own weak and exhausted forces. The
defense of Vienna was no longer to be thought of. Instead of an
offensive, the plan of which, carefully prepared in accord with the
modern science of strategics, had been handed to Kutuzov when he was in
Vienna by the Austrian Hofkriegsrath, the sole and almost unattainable
aim remaining for him was to effect a junction with the forces that were
advancing from Russia, without losing his army as Mack had done at Ulm.

On the twenty-eighth of October Kutuzov with his army crossed to the
left bank of the Danube and took up a position for the first time with
the river between himself and the main body of the French. On the
thirtieth he attacked Mortier's division, which was on the left bank,
and broke it up. In this action for the first time trophies were taken:
banners, cannon, and two enemy generals. For the first time, after a
fortnight's retreat, the Russian troops had halted and after a fight had
not only held the field but had repulsed the French. Though the troops
were ill-clad, exhausted, and had lost a third of their number in
killed, wounded, sick, and stragglers; though a number of sick and
wounded had been abandoned on the other side of the Danube with a letter
in which Kutuzov entrusted them to the humanity of the enemy; and though
the big hospitals and the houses in Krems converted into military
hospitals could no longer accommodate all the sick and wounded, yet the
stand made at Krems and the victory over Mortier raised the spirits of
the army considerably. Throughout the whole army and at headquarters
most joyful though erroneous rumors were rife of the imaginary approach
of columns from Russia, of some victory gained by the Austrians, and of
the retreat of the frightened Bonaparte.

Prince Andrew during the battle had been in attendance on the Austrian
General Schmidt, who was killed in the action. His horse had been
wounded under him and his own arm slightly grazed by a bullet. As a mark
of the commander-in-chief's special favor he was sent with the news of
this victory to the Austrian court, now no longer at Vienna (which was
threatened by the French) but at Brunn. Despite his apparently delicate
build Prince Andrew could endure physical fatigue far better than many
very muscular men, and on the night of the battle, having arrived at
Krems excited but not weary, with dispatches from Dokhturov to Kutuzov,
he was sent immediately with a special dispatch to Brunn. To be so sent
meant not only a reward but an important step toward promotion.

The night was dark but starry, the road showed black in the snow that
had fallen the previous day--the day of the battle. Reviewing his
impressions of the recent battle, picturing pleasantly to himself the
impression his news of a victory would create, or recalling the send-off
given him by the commander-in-chief and his fellow officers, Prince
Andrew was galloping along in a post chaise enjoying the feelings of a
man who has at length begun to attain a long-desired happiness. As soon
as he closed his eyes his ears seemed filled with the rattle of the
wheels and the sensation of victory. Then he began to imagine that the
Russians were running away and that he himself was killed, but he
quickly roused himself with a feeling of joy, as if learning afresh that
this was not so but that on the contrary the French had run away. He
again recalled all the details of the victory and his own calm courage
during the battle, and feeling reassured he dozed off.... The dark
starry night was followed by a bright cheerful morning. The snow was
thawing in the sunshine, the horses galloped quickly, and on both sides
of the road were forests of different kinds, fields, and villages.

At one of the post stations he overtook a convoy of Russian wounded. The
Russian officer in charge of the transport lolled back in the front
cart, shouting and scolding a soldier with coarse abuse. In each of the
long German carts six or more pale, dirty, bandaged men were being
jolted over the stony road. Some of them were talking (he heard Russian
words), others were eating bread; the more severely wounded looked
silently, with the languid interest of sick children, at the envoy
hurrying past them.

Prince Andrew told his driver to stop, and asked a soldier in what
action they had been wounded. "Day before yesterday, on the Danube,"
answered the soldier. Prince Andrew took out his purse and gave the
soldier three gold pieces.

"That's for them all," he said to the officer who came up.

"Get well soon, lads!" he continued, turning to the soldiers. "There's
plenty to do still."

"What news, sir?" asked the officer, evidently anxious to start a
conversation.

"Good news!... Go on!" he shouted to the driver, and they galloped on.

It was already quite dark when Prince Andrew rattled over the paved
streets of Brunn and found himself surrounded by high buildings, the
lights of shops, houses, and street lamps, fine carriages, and all that
atmosphere of a large and active town which is always so attractive to a
soldier after camp life. Despite his rapid journey and sleepless night,
Prince Andrew when he drove up to the palace felt even more vigorous and
alert than he had done the day before. Only his eyes gleamed feverishly
and his thoughts followed one another with extraordinary clearness and
rapidity. He again vividly recalled the details of the battle, no longer
dim, but definite and in the concise form in which he imagined himself
stating them to the Emperor Francis. He vividly imagined the casual
questions that might be put to him and the answers he would give. He
expected to be at once presented to the Emperor. At the chief entrance
to the palace, however, an official came running out to meet him, and
learning that he was a special messenger led him to another entrance.

"To the right from the corridor, Euer Hochgeboren! There you will find
the adjutant on duty," said the official. "He will conduct you to the
Minister of War."

The adjutant on duty, meeting Prince Andrew, asked him to wait, and went
in to the Minister of War. Five minutes later he returned and bowing
with particular courtesy ushered Prince Andrew before him along a
corridor to the cabinet where the Minister of War was at work. The
adjutant by his elaborate courtesy appeared to wish to ward off any
attempt at familiarity on the part of the Russian messenger.

Prince Andrew's joyous feeling was considerably weakened as he
approached the door of the minister's room. He felt offended, and
without his noticing it the feeling of offense immediately turned into
one of disdain which was quite uncalled for. His fertile mind instantly
suggested to him a point of view which gave him a right to despise the
adjutant and the minister. "Away from the smell of powder, they probably
think it easy to gain victories!" he thought. His eyes narrowed
disdainfully, he entered the room of the Minister of War with peculiarly
deliberate steps. This feeling of disdain was heightened when he saw the
minister seated at a large table reading some papers and making pencil
notes on them, and for the first two or three minutes taking no notice
of his arrival. A wax candle stood at each side of the minister's bent
bald head with its gray temples. He went on reading to the end, without
raising his eyes at the opening of the door and the sound of footsteps.

"Take this and deliver it," said he to his adjutant, handing him the
papers and still taking no notice of the special messenger.

Prince Andrew felt that either the actions of Kutuzov's army interested
the Minister of War less than any of the other matters he was concerned
with, or he wanted to give the Russian special messenger that
impression. "But that is a matter of perfect indifference to me," he
thought. The minister drew the remaining papers together, arranged them
evenly, and then raised his head. He had an intellectual and distinctive
head, but the instant he turned to Prince Andrew the firm, intelligent
expression on his face changed in a way evidently deliberate and
habitual to him. His face took on the stupid artificial smile (which
does not even attempt to hide its artificiality) of a man who is
continually receiving many petitioners one after another.

"From General Field Marshal Kutuzov?" he asked. "I hope it is good news?
There has been an encounter with Mortier? A victory? It was high time!"

He took the dispatch which was addressed to him and began to read it
with a mournful expression.

"Oh, my God! My God! Schmidt!" he exclaimed in German. "What a calamity!
What a calamity!"

Having glanced through the dispatch he laid it on the table and looked
at Prince Andrew, evidently considering something.

"Ah what a calamity! You say the affair was decisive? But Mortier is not
captured." Again he pondered. "I am very glad you have brought good
news, though Schmidt's death is a heavy price to pay for the victory.
His Majesty will no doubt wish to see you, but not today. I thank you!
You must have a rest. Be at the levee tomorrow after the parade.
However, I will let you know."

The stupid smile, which had left his face while he was speaking,
reappeared.

"Au revoir! Thank you very much. His Majesty will probably desire to see
you," he added, bowing his head.

When Prince Andrew left the palace he felt that all the interest and
happiness the victory had afforded him had been now left in the
indifferent hands of the Minister of War and the polite adjutant. The
whole tenor of his thoughts instantaneously changed; the battle seemed
the memory of a remote event long past.




CHAPTER X

Prince Andrew stayed at Brunn with Bilibin, a Russian acquaintance of
his in the diplomatic service.

"Ah, my dear prince! I could not have a more welcome visitor," said
Bilibin as he came out to meet Prince Andrew. "Franz, put the prince's
things in my bedroom," said he to the servant who was ushering Bolkonski
in. "So you're a messenger of victory, eh? Splendid! And I am sitting
here ill, as you see."

After washing and dressing, Prince Andrew came into the diplomat's
luxurious study and sat down to the dinner prepared for him. Bilibin
settled down comfortably beside the fire.

After his journey and the campaign during which he had been deprived of
all the comforts of cleanliness and all the refinements of life, Prince
Andrew felt a pleasant sense of repose among luxurious surroundings such
as he had been accustomed to from childhood. Besides it was pleasant,
after his reception by the Austrians, to speak if not in Russian (for
they were speaking French) at least with a Russian who would, he
supposed, share the general Russian antipathy to the Austrians which was
then particularly strong.

Bilibin was a man of thirty-five, a bachelor, and of the same circle as
Prince Andrew. They had known each other previously in Petersburg, but
had become more intimate when Prince Andrew was in Vienna with Kutuzov.
Just as Prince Andrew was a young man who gave promise of rising high in
the military profession, so to an even greater extent Bilibin gave
promise of rising in his diplomatic career. He still a young man but no
longer a young diplomat, as he had entered the service at the age of
sixteen, had been in Paris and Copenhagen, and now held a rather
important post in Vienna. Both the foreign minister and our ambassador
in Vienna knew him and valued him. He was not one of those many
diplomats who are esteemed because they have certain negative qualities,
avoid doing certain things, and speak French. He was one of those, who,
liking work, knew how to do it, and despite his indolence would
sometimes spend a whole night at his writing table. He worked well
whatever the import of his work. It was not the question "What for?" but
the question "How?" that interested him. What the diplomatic matter
might be he did not care, but it gave him great pleasure to prepare a
circular, memorandum, or report, skillfully, pointedly, and elegantly.
Bilibin's services were valued not only for what he wrote, but also for
his skill in dealing and conversing with those in the highest spheres.

Bilibin liked conversation as he liked work, only when it could be made
elegantly witty. In society he always awaited an opportunity to say
something striking and took part in a conversation only when that was
possible. His conversation was always sprinkled with wittily original,
finished phrases of general interest. These sayings were prepared in the
inner laboratory of his mind in a portable form as if intentionally, so
that insignificant society people might carry them from drawing room to
drawing room. And, in fact, Bilibin's witticisms were hawked about in
the Viennese drawing rooms and often had an influence on matters
considered important.

His thin, worn, sallow face was covered with deep wrinkles, which always
looked as clean and well washed as the tips of one's fingers after a
Russian bath. The movement of these wrinkles formed the principal play
of expression on his face. Now his forehead would pucker into deep folds
and his eyebrows were lifted, then his eyebrows would descend and deep
wrinkles would crease his cheeks. His small, deep-set eyes always
twinkled and looked out straight.

"Well, now tell me about your exploits," said he.

Bolkonski, very modestly without once mentioning himself, described the
engagement and his reception by the Minister of War.

"They received me and my news as one receives a dog in a game of
skittles," said he in conclusion.

Bilibin smiled and the wrinkles on his face disappeared.

"Cependant, mon cher," he remarked, examining his nails from a distance
and puckering the skin above his left eye, "malgre la haute estime que
je professe pour the Orthodox Russian army, j'avoue que votre victoire
n'est pas des plus victorieuses." *


* "But my dear fellow, with all my respect for the Orthodox Russian
army, I must say that your victory was not particularly victorious."

He went on talking in this way in French, uttering only those words in
Russian on which he wished to put a contemptuous emphasis.

"Come now! You with all your forces fall on the unfortunate Mortier and
his one division, and even then Mortier slips through your fingers!
Where's the victory?"

"But seriously," said Prince Andrew, "we can at any rate say without
boasting that it was a little better than at Ulm..."

"Why didn't you capture one, just one, marshal for us?"

"Because not everything happens as one expects or with the smoothness of
a parade. We had expected, as I told you, to get at their rear by seven
in the morning but had not reached it by five in the afternoon."

"And why didn't you do it at seven in the morning? You ought to have
been there at seven in the morning," returned Bilibin with a smile. "You
ought to have been there at seven in the morning."

"Why did you not succeed in impressing on Bonaparte by diplomatic
methods that he had better leave Genoa alone?" retorted Prince Andrew in
the same tone.

"I know," interrupted Bilibin, "you're thinking it's very easy to take
marshals, sitting on a sofa by the fire! That is true, but still why
didn't you capture him? So don't be surprised if not only the Minister
of War but also his Most August Majesty the Emperor and King Francis is
not much delighted by your victory. Even I, a poor secretary of the
Russian Embassy, do not feel any need in token of my joy to give my
Franz a thaler, or let him go with his Liebchen to the Prater... True,
we have no Prater here..."

He looked straight at Prince Andrew and suddenly unwrinkled his
forehead.

"It is now my turn to ask you 'why?' mon cher," said Bolkonski. "I
confess I do not understand: perhaps there are diplomatic subtleties
here beyond my feeble intelligence, but I can't make it out. Mack loses
a whole army, the Archduke Ferdinand and the Archduke Karl give no signs
of life and make blunder after blunder. Kutuzov alone at last gains a
real victory, destroying the spell of the invincibility of the French,
and the Minister of War does not even care to hear the details."

"That's just it, my dear fellow. You see it's hurrah for the Tsar, for
Russia, for the Orthodox Greek faith! All that is beautiful, but what do
we, I mean the Austrian court, care for your victories? Bring us nice
news of a victory by the Archduke Karl or Ferdinand (one archduke's as
good as another, as you know) and even if it is only over a fire brigade
of Bonaparte's, that will be another story and we'll fire off some
cannon! But this sort of thing seems done on purpose to vex us. The
Archduke Karl does nothing, the Archduke Ferdinand disgraces himself.
You abandon Vienna, give up its defense--as much as to say: 'Heaven is
with us, but heaven help you and your capital!' The one general whom we
all loved, Schmidt, you expose to a bullet, and then you congratulate us
on the victory! Admit that more irritating news than yours could not
have been conceived. It's as if it had been done on purpose, on purpose.
Besides, suppose you did gain a brilliant victory, if even the Archduke
Karl gained a victory, what effect would that have on the general course
of events? It's too late now when Vienna is occupied by the French
army!"

"What? Occupied? Vienna occupied?"

"Not only occupied, but Bonaparte is at Schonbrunn, and the count, our
dear Count Vrbna, goes to him for orders."

After the fatigues and impressions of the journey, his reception, and
especially after having dined, Bolkonski felt that he could not take in
the full significance of the words he heard.

"Count Lichtenfels was here this morning," Bilibin continued, "and
showed me a letter in which the parade of the French in Vienna was fully
described: Prince Murat et tout le tremblement... You see that your
victory is not a matter for great rejoicing and that you can't be
received as a savior."

"Really I don't care about that, I don't care at all," said Prince
Andrew, beginning to understand that his news of the battle before Krems
was really of small importance in view of such events as the fall of
Austria's capital. "How is it Vienna was taken? What of the bridge and
its celebrated bridgehead and Prince Auersperg? We heard reports that
Prince Auersperg was defending Vienna?" he said.

"Prince Auersperg is on this, on our side of the river, and is defending
us--doing it very badly, I think, but still he is defending us. But
Vienna is on the other side. No, the bridge has not yet been taken and I
hope it will not be, for it is mined and orders have been given to blow
it up. Otherwise we should long ago have been in the mountains of
Bohemia, and you and your army would have spent a bad quarter of an hour
between two fires."

"But still this does not mean that the campaign is over," said Prince
Andrew.

"Well, I think it is. The bigwigs here think so too, but they daren't
say so. It will be as I said at the beginning of the campaign, it won't
be your skirmishing at Durrenstein, or gunpowder at all, that will
decide the matter, but those who devised it," said Bilibin quoting one
of his own mots, releasing the wrinkles on his forehead, and pausing.
"The only question is what will come of the meeting between the Emperor
Alexander and the King of Prussia in Berlin? If Prussia joins the
Allies, Austria's hand will be forced and there will be war. If not it
is merely a question of settling where the preliminaries of the new
Campo Formio are to be drawn up."

"What an extraordinary genius!" Prince Andrew suddenly exclaimed,
clenching his small hand and striking the table with it, "and what luck
the man has!"

"Buonaparte?" said Bilibin inquiringly, puckering up his forehead to
indicate that he was about to say something witty. "Buonaparte?" he
repeated, accentuating the u: "I think, however, now that he lays down
laws for Austria at Schonbrunn, il faut lui faire grace de l'u! * I
shall certainly adopt an innovation and call him simply Bonaparte!"


* "We must let him off the u!"

"But joking apart," said Prince Andrew, "do you really think the
campaign is over?"

"This is what I think. Austria has been made a fool of, and she is not
used to it. She will retaliate. And she has been fooled in the first
place because her provinces have been pillaged--they say the Holy
Russian army loots terribly--her army is destroyed, her capital taken,
and all this for the beaux yeux * of His Sardinian Majesty. And
therefore--this is between ourselves--I instinctively feel that we are
being deceived, my instinct tells me of negotiations with France and
projects for peace, a secret peace concluded separately."


* Fine eyes.

"Impossible!" cried Prince Andrew. "That would be too base."

"If we live we shall see," replied Bilibin, his face again becoming
smooth as a sign that the conversation was at an end.

When Prince Andrew reached the room prepared for him and lay down in a
clean shirt on the feather bed with its warmed and fragrant pillows, he
felt that the battle of which he had brought tidings was far, far away
from him. The alliance with Prussia, Austria's treachery, Bonaparte's
new triumph, tomorrow's levee and parade, and the audience with the
Emperor Francis occupied his thoughts.

He closed his eyes, and immediately a sound of cannonading, of musketry
and the rattling of carriage wheels seemed to fill his ears, and now
again drawn out in a thin line the musketeers were descending the hill,
the French were firing, and he felt his heart palpitating as he rode
forward beside Schmidt with the bullets merrily whistling all around,
and he experienced tenfold the joy of living, as he had not done since
childhood.

He woke up...

"Yes, that all happened!" he said, and, smiling happily to himself like
a child, he fell into a deep, youthful slumber.




CHAPTER XI

Next day he woke late. Recalling his recent impressions, the first
thought that came into his mind was that today he had to be presented to
the Emperor Francis; he remembered the Minister of War, the polite
Austrian adjutant, Bilibin, and last night's conversation. Having
dressed for his attendance at court in full parade uniform, which he had
not worn for a long time, he went into Bilibin's study fresh, animated,
and handsome, with his hand bandaged. In the study were four gentlemen
of the diplomatic corps. With Prince Hippolyte Kuragin, who was a
secretary to the embassy, Bolkonski was already acquainted. Bilibin
introduced him to the others.

The gentlemen assembled at Bilibin's were young, wealthy, gay society
men, who here, as in Vienna, formed a special set which Bilibin, their
leader, called les notres. * This set, consisting almost exclusively of
diplomats, evidently had its own interests which had nothing to do with
war or politics but related to high society, to certain women, and to
the official side of the service. These gentlemen received Prince Andrew
as one of themselves, an honor they did not extend to many. From
politeness and to start conversation, they asked him a few questions
about the army and the battle, and then the talk went off into merry
jests and gossip.


* Ours.

"But the best of it was," said one, telling of the misfortune of a
fellow diplomat, "that the Chancellor told him flatly that his
appointment to London was a promotion and that he was so to regard it.
Can you fancy the figure he cut?..."

"But the worst of it, gentlemen--I am giving Kuragin away to you--is
that that man suffers, and this Don Juan, wicked fellow, is taking
advantage of it!"

Prince Hippolyte was lolling in a lounge chair with his legs over its
arm. He began to laugh.

"Tell me about that!" he said.

"Oh, you Don Juan! You serpent!" cried several voices.

"You, Bolkonski, don't know," said Bilibin turning to Prince Andrew,
"that all the atrocities of the French army (I nearly said of the
Russian army) are nothing compared to what this man has been doing among
the women!"

"La femme est la compagne de l'homme," * announced Prince Hippolyte, and
began looking through a lorgnette at his elevated legs.


* "Woman is man's companion."

Bilibin and the rest of "ours" burst out laughing in Hippolyte's face,
and Prince Andrew saw that Hippolyte, of whom--he had to admit--he had
almost been jealous on his wife's account, was the butt of this set.

"Oh, I must give you a treat," Bilibin whispered to Bolkonski. "Kuragin
is exquisite when he discusses politics--you should see his gravity!"

He sat down beside Hippolyte and wrinkling his forehead began talking to
him about politics. Prince Andrew and the others gathered round these
two.

"The Berlin cabinet cannot express a feeling of alliance," began
Hippolyte gazing round with importance at the others, "without
expressing... as in its last note... you understand... Besides, unless
His Majesty the Emperor derogates from the principle of our alliance...

"Wait, I have not finished..." he said to Prince Andrew, seizing him by
the arm, "I believe that intervention will be stronger than
nonintervention. And..." he paused. "Finally one cannot impute the
nonreceipt of our dispatch of November 18. That is how it will end." And
he released Bolkonski's arm to indicate that he had now quite finished.

"Demosthenes, I know thee by the pebble thou secretest in thy golden
mouth!" said Bilibin, and the mop of hair on his head moved with
satisfaction.

Everybody laughed, and Hippolyte louder than anyone. He was evidently
distressed, and breathed painfully, but could not restrain the wild
laughter that convulsed his usually impassive features.

"Well now, gentlemen," said Bilibin, "Bolkonski is my guest in this
house and in Brunn itself. I want to entertain him as far as I can, with
all the pleasures of life here. If we were in Vienna it would be easy,
but here, in this wretched Moravian hole, it is more difficult, and I
beg you all to help me. Brunn's attractions must be shown him. You can
undertake the theater, I society, and you, Hippolyte, of course the
women."

"We must let him see Amelie, she's exquisite!" said one of "ours,"
kissing his finger tips.

"In general we must turn this bloodthirsty soldier to more humane
interests," said Bilibin.

"I shall scarcely be able to avail myself of your hospitality,
gentlemen, it is already time for me to go," replied Prince Andrew
looking at his watch.

"Where to?"

"To the Emperor."

"Oh! Oh! Oh! Well, au revoir, Bolkonski! Au revoir, Prince! Come back
early to dinner," cried several voices. "We'll take you in hand."

"When speaking to the Emperor, try as far as you can to praise the way
that provisions are supplied and the routes indicated," said Bilibin,
accompanying him to the hall.

"I should like to speak well of them, but as far as I know the facts, I
can't," replied Bolkonski, smiling.

"Well, talk as much as you can, anyway. He has a passion for giving
audiences, but he does not like talking himself and can't do it, as you
will see."




CHAPTER XII

At the levee Prince Andrew stood among the Austrian officers as he had
been told to, and the Emperor Francis merely looked fixedly into his
face and just nodded to him with his long head. But after it was over,
the adjutant he had seen the previous day ceremoniously informed
Bolkonski that the Emperor desired to give him an audience. The Emperor
Francis received him standing in the middle of the room. Before the
conversation began Prince Andrew was struck by the fact that the Emperor
seemed confused and blushed as if not knowing what to say.

"Tell me, when did the battle begin?" he asked hurriedly.

Prince Andrew replied. Then followed other questions just as simple:
"Was Kutuzov well? When had he left Krems?" and so on. The Emperor spoke
as if his sole aim were to put a given number of questions--the answers
to these questions, as was only too evident, did not interest him.

"At what o'clock did the battle begin?" asked the Emperor.

"I cannot inform Your Majesty at what o'clock the battle began at the
front, but at Durrenstein, where I was, our attack began after five in
the afternoon," replied Bolkonski growing more animated and expecting
that he would have a chance to give a reliable account, which he had
ready in his mind, of all he knew and had seen. But the Emperor smiled
and interrupted him.

"How many miles?"

"From where to where, Your Majesty?"

"From Durrenstein to Krems."

"Three and a half miles, Your Majesty."

"The French have abandoned the left bank?"

"According to the scouts the last of them crossed on rafts during the
night."

"Is there sufficient forage in Krems?"

"Forage has not been supplied to the extent..."

The Emperor interrupted him.

"At what o'clock was General Schmidt killed?"

"At seven o'clock, I believe."

"At seven o'clock? It's very sad, very sad!"

The Emperor thanked Prince Andrew and bowed. Prince Andrew withdrew and
was immediately surrounded by courtiers on all sides. Everywhere he saw
friendly looks and heard friendly words. Yesterday's adjutant reproached
him for not having stayed at the palace, and offered him his own house.
The Minister of War came up and congratulated him on the Maria Theresa
Order of the third grade, which the Emperor was conferring on him. The
Empress' chamberlain invited him to see Her Majesty. The archduchess
also wished to see him. He did not know whom to answer, and for a few
seconds collected his thoughts. Then the Russian ambassador took him by
the shoulder, led him to the window, and began to talk to him.

Contrary to Bilibin's forecast the news he had brought was joyfully
received. A thanksgiving service was arranged, Kutuzov was awarded the
Grand Cross of Maria Theresa, and the whole army received rewards.
Bolkonski was invited everywhere, and had to spend the whole morning
calling on the principal Austrian dignitaries. Between four and five in
the afternoon, having made all his calls, he was returning to Bilibin's
house thinking out a letter to his father about the battle and his visit
to Brunn. At the door he found a vehicle half full of luggage. Franz,
Bilibin's man, was dragging a portmanteau with some difficulty out of
the front door.

Before returning to Bilibin's Prince Andrew had gone to a bookshop to
provide himself with some books for the campaign, and had spent some
time in the shop.

"What is it?" he asked.

"Oh, your excellency!" said Franz, with difficulty rolling the
portmanteau into the vehicle, "we are to move on still farther. The
scoundrel is again at our heels!"

"Eh? What?" asked Prince Andrew.

Bilibin came out to meet him. His usually calm face showed excitement.

"There now! Confess that this is delightful," said he. "This affair of
the Thabor Bridge, at Vienna.... They have crossed without striking a
blow!"

Prince Andrew could not understand.

"But where do you come from not to know what every coachman in the town
knows?"

"I come from the archduchess'. I heard nothing there."

"And you didn't see that everybody is packing up?"

"I did not... What is it all about?" inquired Prince Andrew impatiently.

"What's it all about? Why, the French have crossed the bridge that
Auersperg was defending, and the bridge was not blown up: so Murat is
now rushing along the road to Brunn and will be here in a day or two."

"What? Here? But why did they not blow up the bridge, if it was mined?"

"That is what I ask you. No one, not even Bonaparte, knows why."

Bolkonski shrugged his shoulders.

"But if the bridge is crossed it means that the army too is lost? It
will be cut off," said he.

"That's just it," answered Bilibin. "Listen! The French entered Vienna
as I told you. Very well. Next day, which was yesterday, those
gentlemen, messieurs les marechaux, * Murat, Lannes, and Belliard, mount
and ride to the bridge. (Observe that all three are Gascons.)
'Gentlemen,' says one of them, 'you know the Thabor Bridge is mined and
doubly mined and that there are menacing fortifications at its head and
an army of fifteen thousand men has been ordered to blow up the bridge
and not let us cross? But it will please our sovereign the Emperor
Napoleon if we take this bridge, so let us three go and take it!' 'Yes,
let's!' say the others. And off they go and take the bridge, cross it,
and now with their whole army are on this side of the Danube, marching
on us, you, and your lines of communication."


* The marshalls.

"Stop jesting," said Prince Andrew sadly and seriously. This news
grieved him and yet he was pleased.

As soon as he learned that the Russian army was in such a hopeless
situation it occurred to him that it was he who was destined to lead it
out of this position; that here was the Toulon that would lift him from
the ranks of obscure officers and offer him the first step to fame!
Listening to Bilibin he was already imagining how on reaching the army
he would give an opinion at the war council which would be the only one
that could save the army, and how he alone would be entrusted with the
executing of the plan.

"Stop this jesting," he said.

"I am not jesting," Bilibin went on. "Nothing is truer or sadder. These
gentlemen ride onto the bridge alone and wave white handkerchiefs; they
assure the officer on duty that they, the marshals, are on their way to
negotiate with Prince Auersperg. He lets them enter the tête-de-pont. *
They spin him a thousand gasconades, saying that the war is over, that
the Emperor Francis is arranging a meeting with Bonaparte, that they
desire to see Prince Auersperg, and so on. The officer sends for
Auersperg; these gentlemen embrace the officers, crack jokes, sit on the
cannon, and meanwhile a French battalion gets to the bridge unobserved,
flings the bags of incendiary material into the water, and approaches
the tête-de-pont. At length appears the lieutenant general, our dear
Prince Auersperg von Mautern himself. 'Dearest foe! Flower of the
Austrian army, hero of the Turkish wars Hostilities are ended, we can
shake one another's hand.... The Emperor Napoleon burns with impatience
to make Prince Auersperg's acquaintance.' In a word, those gentlemen,
Gascons indeed, so bewildered him with fine words, and he is so
flattered by his rapidly established intimacy with the French marshals,
and so dazzled by the sight of Murat's mantle and ostrich plumes, qu'il
n'y voit que du feu, et oublie celui qu'il devait faire faire sur
l'ennemi!" *(2) In spite of the animation of his speech, Bilibin did not
forget to pause after this mot to give time for its due appreciation.
"The French battalion rushes to the bridgehead, spikes the guns, and the
bridge is taken! But what is best of all," he went on, his excitement
subsiding under the delightful interest of his own story, "is that the
sergeant in charge of the cannon which was to give the signal to fire
the mines and blow up the bridge, this sergeant, seeing that the French
troops were running onto the bridge, was about to fire, but Lannes
stayed his hand. The sergeant, who was evidently wiser than his general,
goes up to Auersperg and says: 'Prince, you are being deceived, here are
the French!' Murat, seeing that all is lost if the sergeant is allowed
to speak, turns to Auersperg with feigned astonishment (he is a true
Gascon) and says: 'I don't recognize the world-famous Austrian
discipline, if you allow a subordinate to address you like that!' It was
a stroke of genius. Prince Auersperg feels his dignity at stake and
orders the sergeant to be arrested. Come, you must own that this affair
of the Thabor Bridge is delightful! It is not exactly stupidity, nor
rascality...."


* Bridgehead.

* (2) That their fire gets into his eyes and he forgets that he ought to
be firing at the enemy.

"It may be treachery," said Prince Andrew, vividly imagining the gray
overcoats, wounds, the smoke of gunpowder, the sounds of firing, and the
glory that awaited him.

"Not that either. That puts the court in too bad a light," replied
Bilibin. "It's not treachery nor rascality nor stupidity: it is just as
at Ulm... it is..."--he seemed to be trying to find the right
expression. "C'est... c'est du Mack. Nous sommes mackes (It is... it is
a bit of Mack. We are Macked)," he concluded, feeling that he had
produced a good epigram, a fresh one that would be repeated. His
hitherto puckered brow became smooth as a sign of pleasure, and with a
slight smile he began to examine his nails.

"Where are you off to?" he said suddenly to Prince Andrew who had risen
and was going toward his room.

"I am going away."

"Where to?"

"To the army."

"But you meant to stay another two days?"

"But now I am off at once."

And Prince Andrew after giving directions about his departure went to
his room.

"Do you know, mon cher," said Bilibin following him, "I have been
thinking about you. Why are you going?"

And in proof of the conclusiveness of his opinion all the wrinkles
vanished from his face.

Prince Andrew looked inquiringly at him and gave no reply.

"Why are you going? I know you think it your duty to gallop back to the
army now that it is in danger. I understand that. Mon cher, it is
heroism!"

"Not at all," said Prince Andrew.

"But as you are a philosopher, be a consistent one, look at the other
side of the question and you will see that your duty, on the contrary,
is to take care of yourself. Leave it to those who are no longer fit for
anything else.... You have not been ordered to return and have not been
dismissed from here; therefore, you can stay and go with us wherever our
ill luck takes us. They say we are going to Olmutz, and Olmutz is a very
decent town. You and I will travel comfortably in my caleche."

"Do stop joking, Bilibin," cried Bolkonski.

"I am speaking sincerely as a friend! Consider! Where and why are you
going, when you might remain here? You are faced by one of two things,"
and the skin over his left temple puckered, "either you will not reach
your regiment before peace is concluded, or you will share defeat and
disgrace with Kutuzov's whole army."

And Bilibin unwrinkled his temple, feeling that the dilemma was
insoluble.

"I cannot argue about it," replied Prince Andrew coldly, but he thought:
"I am going to save the army."

"My dear fellow, you are a hero!" said Bilibin.




CHAPTER XIII

That same night, having taken leave of the Minister of War, Bolkonski
set off to rejoin the army, not knowing where he would find it and
fearing to be captured by the French on the way to Krems.

In Brunn everybody attached to the court was packing up, and the heavy
baggage was already being dispatched to Olmutz. Near Hetzelsdorf Prince
Andrew struck the high road along which the Russian army was moving with
great haste and in the greatest disorder. The road was so obstructed
with carts that it was impossible to get by in a carriage. Prince Andrew
took a horse and a Cossack from a Cossack commander, and hungry and
weary, making his way past the baggage wagons, rode in search of the
commander-in-chief and of his own luggage. Very sinister reports of the
position of the army reached him as he went along, and the appearance of
the troops in their disorderly flight confirmed these rumors.

"Cette armee russe que l'or de l'Angleterre a transportee des extremites
de l'univers, nous allons lui faire eprouver le meme sort--(le sort de
l'armee d'Ulm)." * He remembered these words in Bonaparte's address to
his army at the beginning of the campaign, and they awoke in him
astonishment at the genius of his hero, a feeling of wounded pride, and
a hope of glory. "And should there be nothing left but to die?" he
thought. "Well, if need be, I shall do it no worse than others."


* "That Russian army which has been brought from the ends of the earth
by English gold, we shall cause to share the same fate--(the fate of the
army at Ulm)."

He looked with disdain at the endless confused mass of detachments,
carts, guns, artillery, and again baggage wagons and vehicles of all
kinds overtaking one another and blocking the muddy road, three and
sometimes four abreast. From all sides, behind and before, as far as ear
could reach, there were the rattle of wheels, the creaking of carts and
gun carriages, the tramp of horses, the crack of whips, shouts, the
urging of horses, and the swearing of soldiers, orderlies, and officers.
All along the sides of the road fallen horses were to be seen, some
flayed, some not, and broken-down carts beside which solitary soldiers
sat waiting for something, and again soldiers straggling from their
companies, crowds of whom set off to the neighboring villages, or
returned from them dragging sheep, fowls, hay, and bulging sacks. At
each ascent or descent of the road the crowds were yet denser and the
din of shouting more incessant. Soldiers floundering knee-deep in mud
pushed the guns and wagons themselves. Whips cracked, hoofs slipped,
traces broke, and lungs were strained with shouting. The officers
directing the march rode backward and forward between the carts. Their
voices were but feebly heard amid the uproar and one saw by their faces
that they despaired of the possibility of checking this disorder.

"Here is our dear Orthodox Russian army," thought Bolkonski, recalling
Bilibin's words.

Wishing to find out where the commander-in-chief was, he rode up to a
convoy. Directly opposite to him came a strange one-horse vehicle,
evidently rigged up by soldiers out of any available materials and
looking like something between a cart, a cabriolet, and a caleche. A
soldier was driving, and a woman enveloped in shawls sat behind the
apron under the leather hood of the vehicle. Prince Andrew rode up and
was just putting his question to a soldier when his attention was
diverted by the desperate shrieks of the woman in the vehicle. An
officer in charge of transport was beating the soldier who was driving
the woman's vehicle for trying to get ahead of others, and the strokes
of his whip fell on the apron of the equipage. The woman screamed
piercingly. Seeing Prince Andrew she leaned out from behind the apron
and, waving her thin arms from under the woolen shawl, cried:

"Mr. Aide-de-camp! Mr. Aide-de-camp!... For heaven's sake... Protect me!
What will become of us? I am the wife of the doctor of the Seventh
Chasseurs.... They won't let us pass, we are left behind and have lost
our people..."

"I'll flatten you into a pancake!" shouted the angry officer to the
soldier. "Turn back with your slut!"

"Mr. Aide-de-camp! Help me!... What does it all mean?" screamed the
doctor's wife.

"Kindly let this cart pass. Don't you see it's a woman?" said Prince
Andrew riding up to the officer.

The officer glanced at him, and without replying turned again to the
soldier. "I'll teach you to push on!... Back!"

"Let them pass, I tell you!" repeated Prince Andrew, compressing his
lips.

"And who are you?" cried the officer, turning on him with tipsy rage,
"who are you? Are you in command here? Eh? I am commander here, not you!
Go back or I'll flatten you into a pancake," repeated he. This
expression evidently pleased him.

"That was a nice snub for the little aide-de-camp," came a voice from
behind.

Prince Andrew saw that the officer was in that state of senseless, tipsy
rage when a man does not know what he is saying. He saw that his
championship of the doctor's wife in her queer trap might expose him to
what he dreaded more than anything in the world--to ridicule; but his
instinct urged him on. Before the officer finished his sentence Prince
Andrew, his face distorted with fury, rode up to him and raised his
riding whip.

"Kind...ly let--them--pass!"

The officer flourished his arm and hastily rode away.

"It's all the fault of these fellows on the staff that there's this
disorder," he muttered. "Do as you like."

Prince Andrew without lifting his eyes rode hastily away from the
doctor's wife, who was calling him her deliverer, and recalling with a
sense of disgust the minutest details of this humiliating scene he
galloped on to the village where he was told that the commander-in-chief
was.

On reaching the village he dismounted and went to the nearest house,
intending to rest if but for a moment, eat something, and try to sort
out the stinging and tormenting thoughts that confused his mind. "This
is a mob of scoundrels and not an army," he was thinking as he went up
to the window of the first house, when a familiar voice called him by
name.

He turned round. Nesvitski's handsome face looked out of the little
window. Nesvitski, moving his moist lips as he chewed something, and
flourishing his arm, called him to enter.

"Bolkonski! Bolkonski!... Don't you hear? Eh? Come quick..." he shouted.

Entering the house, Prince Andrew saw Nesvitski and another adjutant
having something to eat. They hastily turned round to him asking if he
had any news. On their familiar faces he read agitation and alarm. This
was particularly noticeable on Nesvitski's usually laughing countenance.

"Where is the commander-in-chief?" asked Bolkonski.

"Here, in that house," answered the adjutant.

"Well, is it true that it's peace and capitulation?" asked Nesvitski.

"I was going to ask you. I know nothing except that it was all I could
do to get here."

"And we, my dear boy! It's terrible! I was wrong to laugh at Mack, we're
getting it still worse," said Nesvitski. "But sit down and have
something to eat."

"You won't be able to find either your baggage or anything else now,
Prince. And God only knows where your man Peter is," said the other
adjutant.

"Where are headquarters?"

"We are to spend the night in Znaim."

"Well, I have got all I need into packs for two horses," said Nesvitski.
"They've made up splendid packs for me--fit to cross the Bohemian
mountains with. It's a bad lookout, old fellow! But what's the matter
with you? You must be ill to shiver like that," he added, noticing that
Prince Andrew winced as at an electric shock.

"It's nothing," replied Prince Andrew.

He had just remembered his recent encounter with the doctor's wife and
the convoy officer.

"What is the commander-in-chief doing here?" he asked.

"I can't make out at all," said Nesvitski.

"Well, all I can make out is that everything is abominable, abominable,
quite abominable!" said Prince Andrew, and he went off to the house
where the commander-in-chief was.

Passing by Kutuzov's carriage and the exhausted saddle horses of his
suite, with their Cossacks who were talking loudly together, Prince
Andrew entered the passage. Kutuzov himself, he was told, was in the
house with Prince Bagration and Weyrother. Weyrother was the Austrian
general who had succeeded Schmidt. In the passage little Kozlovski was
squatting on his heels in front of a clerk. The clerk, with cuffs turned
up, was hastily writing at a tub turned bottom upwards. Kozlovski's face
looked worn--he too had evidently not slept all night. He glanced at
Prince Andrew and did not even nod to him.

"Second line... have you written it?" he continued dictating to the
clerk. "The Kiev Grenadiers, Podolian..."

"One can't write so fast, your honor," said the clerk, glancing angrily
and disrespectfully at Kozlovski.

Through the door came the sounds of Kutuzov's voice, excited and
dissatisfied, interrupted by another, an unfamiliar voice. From the
sound of these voices, the inattentive way Kozlovski looked at him, the
disrespectful manner of the exhausted clerk, the fact that the clerk and
Kozlovski were squatting on the floor by a tub so near to the commander
in chief, and from the noisy laughter of the Cossacks holding the horses
near the window, Prince Andrew felt that something important and
disastrous was about to happen.

He turned to Kozlovski with urgent questions.

"Immediately, Prince," said Kozlovski. "Dispositions for Bagration."

"What about capitulation?"

"Nothing of the sort. Orders are issued for a battle."

Prince Andrew moved toward the door from whence voices were heard. Just
as he was going to open it the sounds ceased, the door opened, and
Kutuzov with his eagle nose and puffy face appeared in the doorway.
Prince Andrew stood right in front of Kutuzov but the expression of the
commander in chief's one sound eye showed him to be so preoccupied with
thoughts and anxieties as to be oblivious of his presence. He looked
straight at his adjutant's face without recognizing him.

"Well, have you finished?" said he to Kozlovski.

"One moment, your excellency."

Bagration, a gaunt middle-aged man of medium height with a firm,
impassive face of Oriental type, came out after the commander-in-chief.

"I have the honor to present myself," repeated Prince Andrew rather
loudly, handing Kutuzov an envelope.

"Ah, from Vienna? Very good. Later, later!"

Kutuzov went out into the porch with Bagration.

"Well, good-by, Prince," said he to Bagration. "My blessing, and may
Christ be with you in your great endeavor!"

His face suddenly softened and tears came into his eyes. With his left
hand he drew Bagration toward him, and with his right, on which he wore
a ring, he made the sign of the cross over him with a gesture evidently
habitual, offering his puffy cheek, but Bagration kissed him on the neck
instead.

"Christ be with you!" Kutuzov repeated and went toward his carriage.
"Get in with me," said he to Bolkonski.

"Your excellency, I should like to be of use here. Allow me to remain
with Prince Bagration's detachment."

"Get in," said Kutuzov, and noticing that Bolkonski still delayed, he
added: "I need good officers myself, need them myself!"

They got into the carriage and drove for a few minutes in silence.

"There is still much, much before us," he said, as if with an old man's
penetration he understood all that was passing in Bolkonski's mind. "If
a tenth part of his detachment returns I shall thank God," he added as
if speaking to himself.

Prince Andrew glanced at Kutuzov's face only a foot distant from him and
involuntarily noticed the carefully washed seams of the scar near his
temple, where an Ismail bullet had pierced his skull, and the empty eye
socket. "Yes, he has a right to speak so calmly of those men's death,"
thought Bolkonski.

"That is why I beg to be sent to that detachment," he said.

Kutuzov did not reply. He seemed to have forgotten what he had been
saying, and sat plunged in thought. Five minutes later, gently swaying
on the soft springs of the carriage, he turned to Prince Andrew. There
was not a trace of agitation on his face. With delicate irony he
questioned Prince Andrew about the details of his interview with the
Emperor, about the remarks he had heard at court concerning the Krems
affair, and about some ladies they both knew.




CHAPTER XIV

On November 1 Kutuzov had received, through a spy, news that the army he
commanded was in an almost hopeless position. The spy reported that the
French, after crossing the bridge at Vienna, were advancing in immense
force upon Kutuzov's line of communication with the troops that were
arriving from Russia. If Kutuzov decided to remain at Krems, Napoleon's
army of one hundred and fifty thousand men would cut him off completely
and surround his exhausted army of forty thousand, and he would find
himself in the position of Mack at Ulm. If Kutuzov decided to abandon
the road connecting him with the troops arriving from Russia, he would
have to march with no road into unknown parts of the Bohemian mountains,
defending himself against superior forces of the enemy and abandoning
all hope of a junction with Buxhowden. If Kutuzov decided to retreat
along the road from Krems to Olmutz, to unite with the troops arriving
from Russia, he risked being forestalled on that road by the French who
had crossed the Vienna bridge, and encumbered by his baggage and
transport, having to accept battle on the march against an enemy three
times as strong, who would hem him in from two sides.

Kutuzov chose this latter course.

The French, the spy reported, having crossed the Vienna bridge, were
advancing by forced marches toward Znaim, which lay sixty-six miles off
on the line of Kutuzov's retreat. If he reached Znaim before the French,
there would be great hope of saving the army; to let the French
forestall him at Znaim meant the exposure of his whole army to a
disgrace such as that of Ulm, or to utter destruction. But to forestall
the French with his whole army was impossible. The road for the French
from Vienna to Znaim was shorter and better than the road for the
Russians from Krems to Znaim.

The night he received the news, Kutuzov sent Bagration's vanguard, four
thousand strong, to the right across the hills from the Krems-Znaim to
the Vienna-Znaim road. Bagration was to make this march without resting,
and to halt facing Vienna with Znaim to his rear, and if he succeeded in
forestalling the French he was to delay them as long as possible.
Kutuzov himself with all his transport took the road to Znaim.

Marching thirty miles that stormy night across roadless hills, with his
hungry, ill-shod soldiers, and losing a third of his men as stragglers
by the way, Bagration came out on the Vienna-Znaim road at Hollabrunn a
few hours ahead of the French who were approaching Hollabrunn from
Vienna. Kutuzov with his transport had still to march for some days
before he could reach Znaim. Hence Bagration with his four thousand
hungry, exhausted men would have to detain for days the whole enemy army
that came upon him at Hollabrunn, which was clearly impossible. But a
freak of fate made the impossible possible. The success of the trick
that had placed the Vienna bridge in the hands of the French without a
fight led Murat to try to deceive Kutuzov in a similar way. Meeting
Bagration's weak detachment on the Znaim road he supposed it to be
Kutuzov's whole army. To be able to crush it absolutely he awaited the
arrival of the rest of the troops who were on their way from Vienna, and
with this object offered a three days' truce on condition that both
armies should remain in position without moving. Murat declared that
negotiations for peace were already proceeding, and that he therefore
offered this truce to avoid unnecessary bloodshed. Count Nostitz, the
Austrian general occupying the advanced posts, believed Murat's emissary
and retired, leaving Bagration's division exposed. Another emissary rode
to the Russian line to announce the peace negotiations and to offer the
Russian army the three days' truce. Bagration replied that he was not
authorized either to accept or refuse a truce and sent his adjutant to
Kutuzov to report the offer he had received.

A truce was Kutuzov's sole chance of gaining time, giving Bagration's
exhausted troops some rest, and letting the transport and heavy convoys
(whose movements were concealed from the French) advance if but one
stage nearer Znaim. The offer of a truce gave the only, and a quite
unexpected, chance of saving the army. On receiving the news he
immediately dispatched Adjutant General Wintzingerode, who was in
attendance on him, to the enemy camp. Wintzingerode was not merely to
agree to the truce but also to offer terms of capitulation, and
meanwhile Kutuzov sent his adjutants back to hasten to the utmost the
movements of the baggage trains of the entire army along the Krems-Znaim
road. Bagration's exhausted and hungry detachment, which alone covered
this movement of the transport and of the whole army, had to remain
stationary in face of an enemy eight times as strong as itself.

Kutuzov's expectations that the proposals of capitulation (which were in
no way binding) might give time for part of the transport to pass, and
also that Murat's mistake would very soon be discovered, proved correct.
As soon as Bonaparte (who was at Schonbrunn, sixteen miles from
Hollabrunn) received Murat's dispatch with the proposal of a truce and a
capitulation, he detected a ruse and wrote the following letter to
Murat:

Schonbrunn, 25th Brumaire, 1805,

at eight o'clock in the morning

To PRINCE MURAT,

I cannot find words to express to you my displeasure. You command only
my advance guard, and have no right to arrange an armistice without my
order. You are causing me to lose the fruits of a campaign. Break the
armistice immediately and march on the enemy. Inform him that the
general who signed that capitulation had no right to do so, and that no
one but the Emperor of Russia has that right.

If, however, the Emperor of Russia ratifies that convention, I will
ratify it; but it is only a trick. March on, destroy the Russian
army.... You are in a position to seize its baggage and artillery.

The Russian Emperor's aide-de-camp is an impostor. Officers are nothing
when they have no powers; this one had none.... The Austrians let
themselves be tricked at the crossing of the Vienna bridge, you are
letting yourself be tricked by an aide-de-camp of the Emperor.

NAPOLEON

Bonaparte's adjutant rode full gallop with this menacing letter to
Murat. Bonaparte himself, not trusting to his generals, moved with all
the Guards to the field of battle, afraid of letting a ready victim
escape, and Bagration's four thousand men merrily lighted campfires,
dried and warmed themselves, cooked their porridge for the first time
for three days, and not one of them knew or imagined what was in store
for him.




CHAPTER XV

Between three and four o'clock in the afternoon Prince Andrew, who had
persisted in his request to Kutuzov, arrived at Grunth and reported
himself to Bagration. Bonaparte's adjutant had not yet reached Murat's
detachment and the battle had not yet begun. In Bagration's detachment
no one knew anything of the general position of affairs. They talked of
peace but did not believe in its possibility; others talked of a battle
but also disbelieved in the nearness of an engagement. Bagration,
knowing Bolkonski to be a favorite and trusted adjutant, received him
with distinction and special marks of favor, explaining to him that
there would probably be an engagement that day or the next, and giving
him full liberty to remain with him during the battle or to join the
rearguard and have an eye on the order of retreat, "which is also very
important."

"However, there will hardly be an engagement today," said Bagration as
if to reassure Prince Andrew.

"If he is one of the ordinary little staff dandies sent to earn a medal
he can get his reward just as well in the rearguard, but if he wishes to
stay with me, let him... he'll be of use here if he's a brave officer,"
thought Bagration. Prince Andrew, without replying, asked the prince's
permission to ride round the position to see the disposition of the
forces, so as to know his bearings should he be sent to execute an
order. The officer on duty, a handsome, elegantly dressed man with a
diamond ring on his forefinger, who was fond of speaking French though
he spoke it badly, offered to conduct Prince Andrew.

On all sides they saw rain-soaked officers with dejected faces who
seemed to be seeking something, and soldiers dragging doors, benches,
and fencing from the village.

"There now, Prince! We can't stop those fellows," said the staff officer
pointing to the soldiers. "The officers don't keep them in hand. And
there," he pointed to a sutler's tent, "they crowd in and sit. This
morning I turned them all out and now look, it's full again. I must go
there, Prince, and scare them a bit. It won't take a moment."

"Yes, let's go in and I will get myself a roll and some cheese," said
Prince Andrew who had not yet had time to eat anything.

"Why didn't you mention it, Prince? I would have offered you something."

They dismounted and entered the tent. Several officers, with flushed and
weary faces, were sitting at the table eating and drinking.

"Now what does this mean, gentlemen?" said the staff officer, in the
reproachful tone of a man who has repeated the same thing more than
once. "You know it won't do to leave your posts like this. The prince
gave orders that no one should leave his post. Now you, Captain," and he
turned to a thin, dirty little artillery officer who without his boots
(he had given them to the canteen keeper to dry), in only his stockings,
rose when they entered, smiling not altogether comfortably.

"Well, aren't you ashamed of yourself, Captain Tushin?" he continued.
"One would think that as an artillery officer you would set a good
example, yet here you are without your boots! The alarm will be sounded
and you'll be in a pretty position without your boots!" (The staff
officer smiled.) "Kindly return to your posts, gentlemen, all of you,
all!" he added in a tone of command.

Prince Andrew smiled involuntarily as he looked at the artillery officer
Tushin, who silent and smiling, shifting from one stockinged foot to the
other, glanced inquiringly with his large, intelligent, kindly eyes from
Prince Andrew to the staff officer.

"The soldiers say it feels easier without boots," said Captain Tushin
smiling shyly in his uncomfortable position, evidently wishing to adopt
a jocular tone. But before he had finished he felt that his jest was
unacceptable and had not come off. He grew confused.

"Kindly return to your posts," said the staff officer trying to preserve
his gravity.

Prince Andrew glanced again at the artillery officer's small figure.
There was something peculiar about it, quite unsoldierly, rather comic,
but extremely attractive.

The staff officer and Prince Andrew mounted their horses and rode on.

Having ridden beyond the village, continually meeting and overtaking
soldiers and officers of various regiments, they saw on their left some
entrenchments being thrown up, the freshly dug clay of which showed up
red. Several battalions of soldiers, in their shirt sleeves despite the
cold wind, swarmed in these earthworks like a host of white ants;
spadefuls of red clay were continually being thrown up from behind the
bank by unseen hands. Prince Andrew and the officer rode up, looked at
the entrenchment, and went on again. Just behind it they came upon some
dozens of soldiers, continually replaced by others, who ran from the
entrenchment. They had to hold their noses and put their horses to a
trot to escape from the poisoned atmosphere of these latrines.

"Voila l'agrement des camps, monsieur le Prince," * said the staff
officer.


* "This is a pleasure one gets in camp, Prince."

They rode up the opposite hill. From there the French could already be
seen. Prince Andrew stopped and began examining the position.

"That's our battery," said the staff officer indicating the highest
point. "It's in charge of the queer fellow we saw without his boots. You
can see everything from there; let's go there, Prince."

"Thank you very much, I will go on alone," said Prince Andrew, wishing
to rid himself of this staff officer's company, "please don't trouble
yourself further."

The staff officer remained behind and Prince Andrew rode on alone.

The farther forward and nearer the enemy he went, the more orderly and
cheerful were the troops. The greatest disorder and depression had been
in the baggage train he had passed that morning on the Znaim road seven
miles away from the French. At Grunth also some apprehension and alarm
could be felt, but the nearer Prince Andrew came to the French lines the
more confident was the appearance of our troops. The soldiers in their
greatcoats were ranged in lines, the sergeants major and company
officers were counting the men, poking the last man in each section in
the ribs and telling him to hold his hand up. Soldiers scattered over
the whole place were dragging logs and brushwood and were building
shelters with merry chatter and laughter; around the fires sat others,
dressed and undressed, drying their shirts and leg bands or mending
boots or overcoats and crowding round the boilers and porridge cookers.
In one company dinner was ready, and the soldiers were gazing eagerly at
the steaming boiler, waiting till the sample, which a quartermaster
sergeant was carrying in a wooden bowl to an officer who sat on a log
before his shelter, had been tasted.

Another company, a lucky one for not all the companies had vodka,
crowded round a pockmarked, broad-shouldered sergeant major who, tilting
a keg, filled one after another the canteen lids held out to him. The
soldiers lifted the canteen lids to their lips with reverential faces,
emptied them, rolling the vodka in their mouths, and walked away from
the sergeant major with brightened expressions, licking their lips and
wiping them on the sleeves of their greatcoats. All their faces were as
serene as if all this were happening at home awaiting peaceful
encampment, and not within sight of the enemy before an action in which
at least half of them would be left on the field. After passing a
chasseur regiment and in the lines of the Kiev grenadiers--fine fellows
busy with similar peaceful affairs--near the shelter of the regimental
commander, higher than and different from the others, Prince Andrew came
out in front of a platoon of grenadiers before whom lay a naked man. Two
soldiers held him while two others were flourishing their switches and
striking him regularly on his bare back. The man shrieked unnaturally. A
stout major was pacing up and down the line, and regardless of the
screams kept repeating:

"It's a shame for a soldier to steal; a soldier must be honest,
honorable, and brave, but if he robs his fellows there is no honor in
him, he's a scoundrel. Go on! Go on!"

So the swishing sound of the strokes, and the desperate but unnatural
screams, continued.

"Go on, go on!" said the major.

A young officer with a bewildered and pained expression on his face
stepped away from the man and looked round inquiringly at the adjutant
as he rode by.

Prince Andrew, having reached the front line, rode along it. Our front
line and that of the enemy were far apart on the right and left flanks,
but in the center where the men with a flag of truce had passed that
morning, the lines were so near together that the men could see one
another's faces and speak to one another. Besides the soldiers who
formed the picket line on either side, there were many curious onlookers
who, jesting and laughing, stared at their strange foreign enemies.

Since early morning--despite an injunction not to approach the picket
line--the officers had been unable to keep sight-seers away. The
soldiers forming the picket line, like showmen exhibiting a curiosity,
no longer looked at the French but paid attention to the sight-seers and
grew weary waiting to be relieved. Prince Andrew halted to have a look
at the French.

"Look! Look there!" one soldier was saying to another, pointing to a
Russian musketeer who had gone up to the picket line with an officer and
was rapidly and excitedly talking to a French grenadier. "Hark to him
jabbering! Fine, isn't it? It's all the Frenchy can do to keep up with
him. There now, Sidorov!"

"Wait a bit and listen. It's fine!" answered Sidorov, who was considered
an adept at French.

The soldier to whom the laughers referred was Dolokhov. Prince Andrew
recognized him and stopped to listen to what he was saying. Dolokhov had
come from the left flank where their regiment was stationed, with his
captain.

"Now then, go on, go on!" incited the officer, bending forward and
trying not to lose a word of the speech which was incomprehensible to
him. "More, please: more! What's he saying?"

Dolokhov did not answer the captain; he had been drawn into a hot
dispute with the French grenadier. They were naturally talking about the
campaign. The Frenchman, confusing the Austrians with the Russians, was
trying to prove that the Russians had surrendered and had fled all the
way from Ulm, while Dolokhov maintained that the Russians had not
surrendered but had beaten the French.

"We have orders to drive you off here, and we shall drive you off," said
Dolokhov.

"Only take care you and your Cossacks are not all captured!" said the
French grenadier.

The French onlookers and listeners laughed.

"We'll make you dance as we did under Suvorov...," * said Dolokhov.


* "On vous fera danser."

"Qu' est-ce qu'il chante?" * asked a Frenchman.


* "What's he singing about?"

"It's ancient history," said another, guessing that it referred to a
former war. "The Emperor will teach your Suvara as he has taught the
others..."

"Bonaparte..." began Dolokhov, but the Frenchman interrupted him.

"Not Bonaparte. He is the Emperor! Sacre nom...!" cried he angrily.

"The devil skin your Emperor."

And Dolokhov swore at him in coarse soldier's Russian and shouldering
his musket walked away.

"Let us go, Ivan Lukich," he said to the captain.

"Ah, that's the way to talk French," said the picket soldiers. "Now,
Sidorov, you have a try!"

Sidorov, turning to the French, winked, and began to jabber meaningless
sounds very fast: "Kari, mala, tafa, safi, muter, Kaska," he said,
trying to give an expressive intonation to his voice.

"Ho! ho! ho! Ha! ha! ha! ha! Ouh! ouh!" came peals of such healthy and
good-humored laughter from the soldiers that it infected the French
involuntarily, so much so that the only thing left to do seemed to be to
unload the muskets, explode the ammunition, and all return home as
quickly as possible.

But the guns remained loaded, the loopholes in blockhouses and
entrenchments looked out just as menacingly, and the unlimbered cannon
confronted one another as before.




CHAPTER XVI

Having ridden round the whole line from right flank to left, Prince
Andrew made his way up to the battery from which the staff officer had
told him the whole field could be seen. Here he dismounted, and stopped
beside the farthest of the four unlimbered cannon. Before the guns an
artillery sentry was pacing up and down; he stood at attention when the
officer arrived, but at a sign resumed his measured, monotonous pacing.
Behind the guns were their limbers and still farther back picket ropes
and artillerymen's bonfires. To the left, not far from the farthest
cannon, was a small, newly constructed wattle shed from which came the
sound of officers' voices in eager conversation.

It was true that a view over nearly the whole Russian position and the
greater part of the enemy's opened out from this battery. Just facing
it, on the crest of the opposite hill, the village of Schon Grabern
could be seen, and in three places to left and right the French troops
amid the smoke of their campfires, the greater part of whom were
evidently in the village itself and behind the hill. To the left from
that village, amid the smoke, was something resembling a battery, but it
was impossible to see it clearly with the naked eye. Our right flank was
posted on a rather steep incline which dominated the French position.
Our infantry were stationed there, and at the farthest point the
dragoons. In the center, where Tushin's battery stood and from which
Prince Andrew was surveying the position, was the easiest and most
direct descent and ascent to the brook separating us from Schon Grabern.
On the left our troops were close to a copse, in which smoked the
bonfires of our infantry who were felling wood. The French line was
wider than ours, and it was plain that they could easily outflank us on
both sides. Behind our position was a steep and deep dip, making it
difficult for artillery and cavalry to retire. Prince Andrew took out
his notebook and, leaning on the cannon, sketched a plan of the
position. He made some notes on two points, intending to mention them to
Bagration. His idea was, first, to concentrate all the artillery in the
center, and secondly, to withdraw the cavalry to the other side of the
dip. Prince Andrew, being always near the commander in chief, closely
following the mass movements and general orders, and constantly studying
historical accounts of battles, involuntarily pictured to himself the
course of events in the forthcoming action in broad outline. He imagined
only important possibilities: "If the enemy attacks the right flank," he
said to himself, "the Kiev grenadiers and the Podolsk chasseurs must
hold their position till reserves from the center come up. In that case
the dragoons could successfully make a flank counterattack. If they
attack our center we, having the center battery on this high ground,
shall withdraw the left flank under its cover, and retreat to the dip by
echelons." So he reasoned.... All the time he had been beside the gun,
he had heard the voices of the officers distinctly, but as often happens
had not understood a word of what they were saying. Suddenly, however,
he was struck by a voice coming from the shed, and its tone was so
sincere that he could not but listen.

"No, friend," said a pleasant and, as it seemed to Prince Andrew, a
familiar voice, "what I say is that if it were possible to know what is
beyond death, none of us would be afraid of it. That's so, friend."

Another, a younger voice, interrupted him: "Afraid or not, you can't
escape it anyhow."

"All the same, one is afraid! Oh, you clever people," said a third manly
voice interrupting them both. "Of course you artillery men are very
wise, because you can take everything along with you--vodka and snacks."

And the owner of the manly voice, evidently an infantry officer,
laughed.

"Yes, one is afraid," continued the first speaker, he of the familiar
voice. "One is afraid of the unknown, that's what it is. Whatever we may
say about the soul going to the sky... we know there is no sky but only
an atmosphere."

The manly voice again interrupted the artillery officer.

"Well, stand us some of your herb vodka, Tushin," it said.

"Why," thought Prince Andrew, "that's the captain who stood up in the
sutler's hut without his boots." He recognized the agreeable,
philosophizing voice with pleasure.

"Some herb vodka? Certainly!" said Tushin. "But still, to conceive a
future life..."

He did not finish. Just then there was a whistle in the air; nearer and
nearer, faster and louder, louder and faster, a cannon ball, as if it
had not finished saying what was necessary, thudded into the ground near
the shed with super human force, throwing up a mass of earth. The ground
seemed to groan at the terrible impact.

And immediately Tushin, with a short pipe in the corner of his mouth and
his kind, intelligent face rather pale, rushed out of the shed followed
by the owner of the manly voice, a dashing infantry officer who hurried
off to his company, buttoning up his coat as he ran.




CHAPTER XVII

Mounting his horse again Prince Andrew lingered with the battery,
looking at the puff from the gun that had sent the ball. His eyes ran
rapidly over the wide space, but he only saw that the hitherto
motionless masses of the French now swayed and that there really was a
battery to their left. The smoke above it had not yet dispersed. Two
mounted Frenchmen, probably adjutants, were galloping up the hill. A
small but distinctly visible enemy column was moving down the hill,
probably to strengthen the front line. The smoke of the first shot had
not yet dispersed before another puff appeared, followed by a report.
The battle had begun! Prince Andrew turned his horse and galloped back
to Grunth to find Prince Bagration. He heard the cannonade behind him
growing louder and more frequent. Evidently our guns had begun to reply.
From the bottom of the slope, where the parleys had taken place, came
the report of musketry.

Lemarrois had just arrived at a gallop with Bonaparte's stern letter,
and Murat, humiliated and anxious to expiate his fault, had at once
moved his forces to attack the center and outflank both the Russian
wings, hoping before evening and before the arrival of the Emperor to
crush the contemptible detachment that stood before him.

"It has begun. Here it is!" thought Prince Andrew, feeling the blood
rush to his heart. "But where and how will my Toulon present itself?"

Passing between the companies that had been eating porridge and drinking
vodka a quarter of an hour before, he saw everywhere the same rapid
movement of soldiers forming ranks and getting their muskets ready, and
on all their faces he recognized the same eagerness that filled his
heart. "It has begun! Here it is, dreadful but enjoyable!" was what the
face of each soldier and each officer seemed to say.

Before he had reached the embankments that were being thrown up, he saw,
in the light of the dull autumn evening, mounted men coming toward him.
The foremost, wearing a Cossack cloak and lambskin cap and riding a
white horse, was Prince Bagration. Prince Andrew stopped, waiting for
him to come up; Prince Bagration reined in his horse and recognizing
Prince Andrew nodded to him. He still looked ahead while Prince Andrew
told him what he had seen.

The feeling, "It has begun! Here it is!" was seen even on Prince
Bagration's hard brown face with its half-closed, dull, sleepy eyes.
Prince Andrew gazed with anxious curiosity at that impassive face and
wished he could tell what, if anything, this man was thinking and
feeling at that moment. "Is there anything at all behind that impassive
face?" Prince Andrew asked himself as he looked. Prince Bagration bent
his head in sign of agreement with what Prince Andrew told him, and
said, "Very good!" in a tone that seemed to imply that everything that
took place and was reported to him was exactly what he had foreseen.
Prince Andrew, out of breath with his rapid ride, spoke quickly. Prince
Bagration, uttering his words with an Oriental accent, spoke
particularly slowly, as if to impress the fact that there was no need to
hurry. However, he put his horse to a trot in the direction of Tushin's
battery. Prince Andrew followed with the suite. Behind Prince Bagration
rode an officer of the suite, the prince's personal adjutant, Zherkov,
an orderly officer, the staff officer on duty, riding a fine bobtailed
horse, and a civilian--an accountant who had asked permission to be
present at the battle out of curiosity. The accountant, a stout, full-
faced man, looked around him with a naive smile of satisfaction and
presented a strange appearance among the hussars, Cossacks, and
adjutants, in his camlet coat, as he jolted on his horse with a convoy
officer's saddle.

"He wants to see a battle," said Zherkov to Bolkonski, pointing to the
accountant, "but he feels a pain in the pit of his stomach already."

"Oh, leave off!" said the accountant with a beaming but rather cunning
smile, as if flattered at being made the subject of Zherkov's joke, and
purposely trying to appear stupider than he really was.

"It is very strange, mon Monsieur Prince," said the staff officer. (He
remembered that in French there is some peculiar way of addressing a
prince, but could not get it quite right.)

By this time they were all approaching Tushin's battery, and a ball
struck the ground in front of them.

"What's that that has fallen?" asked the accountant with a naive smile.

"A French pancake," answered Zherkov.

"So that's what they hit with?" asked the accountant. "How awful!"

He seemed to swell with satisfaction. He had hardly finished speaking
when they again heard an unexpectedly violent whistling which suddenly
ended with a thud into something soft... f-f-flop! and a Cossack, riding
a little to their right and behind the accountant, crashed to earth with
his horse. Zherkov and the staff officer bent over their saddles and
turned their horses away. The accountant stopped, facing the Cossack,
and examined him with attentive curiosity. The Cossack was dead, but the
horse still struggled.

Prince Bagration screwed up his eyes, looked round, and, seeing the
cause of the confusion, turned away with indifference, as if to say, "Is
it worth while noticing trifles?" He reined in his horse with the care
of a skillful rider and, slightly bending over, disengaged his saber
which had caught in his cloak. It was an old-fashioned saber of a kind
no longer in general use. Prince Andrew remembered the story of Suvorov
giving his saber to Bagration in Italy, and the recollection was
particularly pleasant at that moment. They had reached the battery at
which Prince Andrew had been when he examined the battlefield.

"Whose company?" asked Prince Bagration of an artilleryman standing by
the ammunition wagon.

He asked, "Whose company?" but he really meant, "Are you frightened
here?" and the artilleryman understood him.

"Captain Tushin's, your excellency!" shouted the red-haired, freckled
gunner in a merry voice, standing to attention.

"Yes, yes," muttered Bagration as if considering something, and he rode
past the limbers to the farthest cannon.

As he approached, a ringing shot issued from it deafening him and his
suite, and in the smoke that suddenly surrounded the gun they could see
the gunners who had seized it straining to roll it quickly back to its
former position. A huge, broad-shouldered gunner, Number One, holding a
mop, his legs far apart, sprang to the wheel; while Number Two with a
trembling hand placed a charge in the cannon's mouth. The short, round-
shouldered Captain Tushin, stumbling over the tail of the gun carriage,
moved forward and, not noticing the general, looked out shading his eyes
with his small hand.

"Lift it two lines more and it will be just right," cried he in a feeble
voice to which he tried to impart a dashing note, ill-suited to his weak
figure. "Number Two!" he squeaked. "Fire, Medvedev!"

Bagration called to him, and Tushin, raising three fingers to his cap
with a bashful and awkward gesture not at all like a military salute but
like a priest's benediction, approached the general. Though Tushin's
guns had been intended to cannonade the valley, he was firing incendiary
balls at the village of Schon Grabern visible just opposite, in front of
which large masses of French were advancing.

No one had given Tushin orders where and at what to fire, but after
consulting his sergeant major, Zakharchenko, for whom he had great
respect, he had decided that it would be a good thing to set fire to the
village. "Very good!" said Bagration in reply to the officer's report,
and began deliberately to examine the whole battlefield extended before
him. The French had advanced nearest on our right. Below the height on
which the Kiev regiment was stationed, in the hollow where the rivulet
flowed, the soul-stirring rolling and crackling of musketry was heard,
and much farther to the right beyond the dragoons, the officer of the
suite pointed out to Bagration a French column that was outflanking us.
To the left the horizon bounded by the adjacent wood. Prince Bagration
ordered two battalions from the center to be sent to reinforce the right
flank. The officer of the suite ventured to remark to the prince that if
these battalions went away, the guns would remain without support.
Prince Bagration turned to the officer and with his dull eyes looked at
him in silence. It seemed to Prince Andrew that the officer's remark was
just and that really no answer could be made to it. But at that moment
an adjutant galloped up with a message from the commander of the
regiment in the hollow and news that immense masses of the French were
coming down upon them and that his regiment was in disorder and was
retreating upon the Kiev grenadiers. Prince Bagration bowed his head in
sign of assent and approval. He rode off at a walk to the right and sent
an adjutant to the dragoons with orders to attack the French. But this
adjutant returned half an hour later with the news that the commander of
the dragoons had already retreated beyond the dip in the ground, as a
heavy fire had been opened on him and he was losing men uselessly, and
so had hastened to throw some sharpshooters into the wood.

"Very good!" said Bagration.

As he was leaving the battery, firing was heard on the left also, and as
it was too far to the left flank for him to have time to go there
himself, Prince Bagration sent Zherkov to tell the general in command
(the one who had paraded his regiment before Kutuzov at Braunau) that he
must retreat as quickly as possible behind the hollow in the rear, as
the right flank would probably not be able to withstand the enemy's
attack very long. About Tushin and the battalion that had been in
support of his battery all was forgotten. Prince Andrew listened
attentively to Bagration's colloquies with the commanding officers and
the orders he gave them and, to his surprise, found that no orders were
really given, but that Prince Bagration tried to make it appear that
everything done by necessity, by accident, or by the will of subordinate
commanders was done, if not by his direct command, at least in accord
with his intentions. Prince Andrew noticed, however, that though what
happened was due to chance and was independent of the commander's will,
owing to the tact Bagration showed, his presence was very valuable.
Officers who approached him with disturbed countenances became calm;
soldiers and officers greeted him gaily, grew more cheerful in his
presence, and were evidently anxious to display their courage before
him.




CHAPTER XVIII

Prince Bagration, having reached the highest point of our right flank,
began riding downhill to where the roll of musketry was heard but where
on account of the smoke nothing could be seen. The nearer they got to
the hollow the less they could see but the more they felt the nearness
of the actual battlefield. They began to meet wounded men. One with a
bleeding head and no cap was being dragged along by two soldiers who
supported him under the arms. There was a gurgle in his throat and he
was spitting blood. A bullet had evidently hit him in the throat or
mouth. Another was walking sturdily by himself but without his musket,
groaning aloud and swinging his arm which had just been hurt, while
blood from it was streaming over his greatcoat as from a bottle. He had
that moment been wounded and his face showed fear rather than suffering.
Crossing a road they descended a steep incline and saw several men lying
on the ground; they also met a crowd of soldiers some of whom were
unwounded. The soldiers were ascending the hill breathing heavily, and
despite the general's presence were talking loudly and gesticulating. In
front of them rows of gray cloaks were already visible through the
smoke, and an officer catching sight of Bagration rushed shouting after
the crowd of retreating soldiers, ordering them back. Bagration rode up
to the ranks along which shots crackled now here and now there, drowning
the sound of voices and the shouts of command. The whole air reeked with
smoke. The excited faces of the soldiers were blackened with it. Some
were using their ramrods, others putting powder on the touchpans or
taking charges from their pouches, while others were firing, though who
they were firing at could not be seen for the smoke which there was no
wind to carry away. A pleasant humming and whistling of bullets were
often heard. "What is this?" thought Prince Andrew approaching the crowd
of soldiers. "It can't be an attack, for they are not moving; it can't
be a square--for they are not drawn up for that."

The commander of the regiment, a thin, feeble-looking old man with a
pleasant smile--his eyelids drooping more than half over his old eyes,
giving him a mild expression, rode up to Bagration and welcomed him as a
host welcomes an honored guest. He reported that his regiment had been
attacked by French cavalry and that, though the attack had been
repulsed, he had lost more than half his men. He said the attack had
been repulsed, employing this military term to describe what had
occurred to his regiment, but in reality he did not himself know what
had happened during that half-hour to the troops entrusted to him, and
could not say with certainty whether the attack had been repulsed or his
regiment had been broken up. All he knew was that at the commencement of
the action balls and shells began flying all over his regiment and
hitting men and that afterwards someone had shouted "Cavalry!" and our
men had begun firing. They were still firing, not at the cavalry which
had disappeared, but at French infantry who had come into the hollow and
were firing at our men. Prince Bagration bowed his head as a sign that
this was exactly what he had desired and expected. Turning to his
adjutant he ordered him to bring down the two battalions of the Sixth
Chasseurs whom they had just passed. Prince Andrew was struck by the
changed expression on Prince Bagration's face at this moment. It
expressed the concentrated and happy resolution you see on the face of a
man who on a hot day takes a final run before plunging into the water.
The dull, sleepy expression was no longer there, nor the affectation of
profound thought. The round, steady, hawk's eyes looked before him
eagerly and rather disdainfully, not resting on anything although his
movements were still slow and measured.

The commander of the regiment turned to Prince Bagration, entreating him
to go back as it was too dangerous to remain where they were. "Please,
your excellency, for God's sake!" he kept saying, glancing for support
at an officer of the suite who turned away from him. "There, you see!"
and he drew attention to the bullets whistling, singing, and hissing
continually around them. He spoke in the tone of entreaty and reproach
that a carpenter uses to a gentleman who has picked up an ax: "We are
used to it, but you, sir, will blister your hands." He spoke as if those
bullets could not kill him, and his half-closed eyes gave still more
persuasiveness to his words. The staff officer joined in the colonel's
appeals, but Bagration did not reply; he only gave an order to cease
firing and re-form, so as to give room for the two approaching
battalions. While he was speaking, the curtain of smoke that had
concealed the hollow, driven by a rising wind, began to move from right
to left as if drawn by an invisible hand, and the hill opposite, with
the French moving about on it, opened out before them. All eyes fastened
involuntarily on this French column advancing against them and winding
down over the uneven ground. One could already see the soldiers' shaggy
caps, distinguish the officers from the men, and see the standard
flapping against its staff.

"They march splendidly," remarked someone in Bagration's suite.

The head of the column had already descended into the hollow. The clash
would take place on this side of it...

The remains of our regiment which had been in action rapidly formed up
and moved to the right; from behind it, dispersing the laggards, came
two battalions of the Sixth Chasseurs in fine order. Before they had
reached Bagration, the weighty tread of the mass of men marching in step
could be heard. On their left flank, nearest to Bagration, marched a
company commander, a fine round-faced man, with a stupid and happy
expression--the same man who had rushed out of the wattle shed. At that
moment he was clearly thinking of nothing but how dashing a fellow he
would appear as he passed the commander.

With the self-satisfaction of a man on parade, he stepped lightly with
his muscular legs as if sailing along, stretching himself to his full
height without the smallest effort, his ease contrasting with the heavy
tread of the soldiers who were keeping step with him. He carried close
to his leg a narrow unsheathed sword (small, curved, and not like a real
weapon) and looked now at the superior officers and now back at the men
without losing step, his whole powerful body turning flexibly. It was as
if all the powers of his soul were concentrated on passing the commander
in the best possible manner, and feeling that he was doing it well he
was happy. "Left... left... left..." he seemed to repeat to himself at
each alternate step; and in time to this, with stern but varied faces,
the wall of soldiers burdened with knapsacks and muskets marched in
step, and each one of these hundreds of soldiers seemed to be repeating
to himself at each alternate step, "Left... left... left..." A fat major
skirted a bush, puffing and falling out of step; a soldier who had
fallen behind, his face showing alarm at his defection, ran at a trot,
panting to catch up with his company. A cannon ball, cleaving the air,
flew over the heads of Bagration and his suite, and fell into the column
to the measure of "Left... left!" "Close up!" came the company
commander's voice in jaunty tones. The soldiers passed in a semicircle
round something where the ball had fallen, and an old trooper on the
flank, a noncommissioned officer who had stopped beside the dead men,
ran to catch up his line and, falling into step with a hop, looked back
angrily, and through the ominous silence and the regular tramp of feet
beating the ground in unison, one seemed to hear left... left... left.

"Well done, lads!" said Prince Bagration.

"Glad to do our best, your ex'len-lency!" came a confused shout from the
ranks. A morose soldier marching on the left turned his eyes on
Bagration as he shouted, with an expression that seemed to say: "We know
that ourselves!" Another, without looking round, as though fearing to
relax, shouted with his mouth wide open and passed on.

The order was given to halt and down knapsacks.

Bagration rode round the ranks that had marched past him and dismounted.
He gave the reins to a Cossack, took off and handed over his felt coat,
stretched his legs, and set his cap straight. The head of the French
column, with its officers leading, appeared from below the hill.

"Forward, with God!" said Bagration, in a resolute, sonorous voice,
turning for a moment to the front line, and slightly swinging his arms,
he went forward uneasily over the rough field with the awkward gait of a
cavalryman. Prince Andrew felt that an invisible power was leading him
forward, and experienced great happiness.

The French were already near. Prince Andrew, walking beside Bagration,
could clearly distinguish their bandoliers, red epaulets, and even their
faces. (He distinctly saw an old French officer who, with gaitered legs
and turned-out toes, climbed the hill with difficulty.) Prince Bagration
gave no further orders and silently continued to walk on in front of the
ranks. Suddenly one shot after another rang out from the French, smoke
appeared all along their uneven ranks, and musket shots sounded. Several
of our men fell, among them the round-faced officer who had marched so
gaily and complacently. But at the moment the first report was heard,
Bagration looked round and shouted, "Hurrah!"

"Hurrah--ah!--ah!" rang a long-drawn shout from our ranks, and passing
Bagration and racing one another they rushed in an irregular but joyous
and eager crowd down the hill at their disordered foe.




CHAPTER XIX

The attack of the Sixth Chasseurs secured the retreat of our right
flank. In the center Tushin's forgotten battery, which had managed to
set fire to the Schon Grabern village, delayed the French advance. The
French were putting out the fire which the wind was spreading, and thus
gave us time to retreat. The retirement of the center to the other side
of the dip in the ground at the rear was hurried and noisy, but the
different companies did not get mixed. But our left--which consisted of
the Azov and Podolsk infantry and the Pavlograd hussars--was
simultaneously attacked and outflanked by superior French forces under
Lannes and was thrown into confusion. Bagration had sent Zherkov to the
general commanding that left flank with orders to retreat immediately.

Zherkov, not removing his hand from his cap, turned his horse about and
galloped off. But no sooner had he left Bagration than his courage
failed him. He was seized by panic and could not go where it was
dangerous.

Having reached the left flank, instead of going to the front where the
firing was, he began to look for the general and his staff where they
could not possibly be, and so did not deliver the order.

The command of the left flank belonged by seniority to the commander of
the regiment Kutuzov had reviewed at Braunau and in which Dolokhov was
serving as a private. But the command of the extreme left flank had been
assigned to the commander of the Pavlograd regiment in which Rostov was
serving, and a misunderstanding arose. The two commanders were much
exasperated with one another and, long after the action had begun on the
right flank and the French were already advancing, were engaged in
discussion with the sole object of offending one another. But the
regiments, both cavalry and infantry, were by no means ready for the
impending action. From privates to general they were not expecting a
battle and were engaged in peaceful occupations, the cavalry feeding the
horses and the infantry collecting wood.

"He higher iss dan I in rank," said the German colonel of the hussars,
flushing and addressing an adjutant who had ridden up, "so let him do
what he vill, but I cannot sacrifice my hussars... Bugler, sount ze
retreat!"

But haste was becoming imperative. Cannon and musketry, mingling
together, thundered on the right and in the center, while the capotes of
Lannes' sharpshooters were already seen crossing the milldam and forming
up within twice the range of a musket shot. The general in command of
the infantry went toward his horse with jerky steps, and having mounted
drew himself up very straight and tall and rode to the Pavlograd
commander. The commanders met with polite bows but with secret
malevolence in their hearts.

"Once again, Colonel," said the general, "I can't leave half my men in
the wood. I beg of you, I beg of you," he repeated, "to occupy the
position and prepare for an attack."

"I peg of you yourself not to mix in vot is not your business!" suddenly
replied the irate colonel. "If you vere in the cavalry..."

"I am not in the cavalry, Colonel, but I am a Russian general and if you
are not aware of the fact..."

"Quite avare, your excellency," suddenly shouted the colonel, touching
his horse and turning purple in the face. "Vill you be so goot to come
to ze front and see dat zis position iss no goot? I don't vish to
destroy my men for your pleasure!"

"You forget yourself, Colonel. I am not considering my own pleasure and
I won't allow it to be said!"

Taking the colonel's outburst as a challenge to his courage, the general
expanded his chest and rode, frowning, beside him to the front line, as
if their differences would be settled there amongst the bullets. They
reached the front, several bullets sped over them, and they halted in
silence. There was nothing fresh to be seen from the line, for from
where they had been before it had been evident that it was impossible
for cavalry to act among the bushes and broken ground, as well as that
the French were outflanking our left. The general and colonel looked
sternly and significantly at one another like two fighting cocks
preparing for battle, each vainly trying to detect signs of cowardice in
the other. Both passed the examination successfully. As there was
nothing to be said, and neither wished to give occasion for it to be
alleged that he had been the first to leave the range of fire, they
would have remained there for a long time testing each other's courage
had it not been that just then they heard the rattle of musketry and a
muffled shout almost behind them in the wood. The French had attacked
the men collecting wood in the copse. It was no longer possible for the
hussars to retreat with the infantry. They were cut off from the line of
retreat on the left by the French. However inconvenient the position, it
was now necessary to attack in order to cut a way through for
themselves.

The squadron in which Rostov was serving had scarcely time to mount
before it was halted facing the enemy. Again, as at the Enns bridge,
there was nothing between the squadron and the enemy, and again that
terrible dividing line of uncertainty and fear--resembling the line
separating the living from the dead--lay between them. All were
conscious of this unseen line, and the question whether they would cross
it or not, and how they would cross it, agitated them all.

The colonel rode to the front, angrily gave some reply to questions put
to him by the officers, and, like a man desperately insisting on having
his own way, gave an order. No one said anything definite, but the rumor
of an attack spread through the squadron. The command to form up rang
out and the sabers whizzed as they were drawn from their scabbards.
Still no one moved. The troops of the left flank, infantry and hussars
alike, felt that the commander did not himself know what to do, and this
irresolution communicated itself to the men.

"If only they would be quick!" thought Rostov, feeling that at last the
time had come to experience the joy of an attack of which he had so
often heard from his fellow hussars.

"Fo'ward, with God, lads!" rang out Denisov's voice. "At a twot
fo'ward!"

The horses' croups began to sway in the front line. Rook pulled at the
reins and started of his own accord.

Before him, on the right, Rostov saw the front lines of his hussars and
still farther ahead a dark line which he could not see distinctly but
took to be the enemy. Shots could be heard, but some way off.

"Faster!" came the word of command, and Rostov felt Rook's flanks
drooping as he broke into a gallop.

Rostov anticipated his horse's movements and became more and more
elated. He had noticed a solitary tree ahead of him. This tree had been
in the middle of the line that had seemed so terrible--and now he had
crossed that line and not only was there nothing terrible, but
everything was becoming more and more happy and animated. "Oh, how I
will slash at him!" thought Rostov, gripping the hilt of his saber.

"Hur-a-a-a-ah!" came a roar of voices. "Let anyone come my way now,"
thought Rostov driving his spurs into Rook and letting him go at a full
gallop so that he outstripped the others. Ahead, the enemy was already
visible. Suddenly something like a birch broom seemed to sweep over the
squadron. Rostov raised his saber, ready to strike, but at that instant
the trooper Nikitenko, who was galloping ahead, shot away from him, and
Rostov felt as in a dream that he continued to be carried forward with
unnatural speed but yet stayed on the same spot. From behind him
Bondarchuk, an hussar he knew, jolted against him and looked angrily at
him. Bondarchuk's horse swerved and galloped past.

"How is it I am not moving? I have fallen, I am killed!" Rostov asked
and answered at the same instant. He was alone in the middle of a field.
Instead of the moving horses and hussars' backs, he saw nothing before
him but the motionless earth and the stubble around him. There was warm
blood under his arm. "No, I am wounded and the horse is killed." Rook
tried to rise on his forelegs but fell back, pinning his rider's leg.
Blood was flowing from his head; he struggled but could not rise. Rostov
also tried to rise but fell back, his sabretache having become entangled
in the saddle. Where our men were, and where the French, he did not
know. There was no one near.

Having disentangled his leg, he rose. "Where, on which side, was now the
line that had so sharply divided the two armies?" he asked himself and
could not answer. "Can something bad have happened to me?" he wondered
as he got up: and at that moment he felt that something superfluous was
hanging on his benumbed left arm. The wrist felt as if it were not his.
He examined his hand carefully, vainly trying to find blood on it. "Ah,
here are people coming," he thought joyfully, seeing some men running
toward him. "They will help me!" In front came a man wearing a strange
shako and a blue cloak, swarthy, sunburned, and with a hooked nose. Then
came two more, and many more running behind. One of them said something
strange, not in Russian. In among the hindmost of these men wearing
similar shakos was a Russian hussar. He was being held by the arms and
his horse was being led behind him.

"It must be one of ours, a prisoner. Yes. Can it be that they will take
me too? Who are these men?" thought Rostov, scarcely believing his eyes.
"Can they be French?" He looked at the approaching Frenchmen, and though
but a moment before he had been galloping to get at them and hack them
to pieces, their proximity now seemed so awful that he could not believe
his eyes. "Who are they? Why are they running? Can they be coming at me?
And why? To kill me? Me whom everyone is so fond of?" He remembered his
mother's love for him, and his family's, and his friends', and the
enemy's intention to kill him seemed impossible. "But perhaps they may
do it!" For more than ten seconds he stood not moving from the spot or
realizing the situation. The foremost Frenchman, the one with the hooked
nose, was already so close that the expression of his face could be
seen. And the excited, alien face of that man, his bayonet hanging down,
holding his breath, and running so lightly, frightened Rostov. He seized
his pistol and, instead of firing it, flung it at the Frenchman and ran
with all his might toward the bushes. He did not now run with the
feeling of doubt and conflict with which he had trodden the Enns bridge,
but with the feeling of a hare fleeing from the hounds. One single
sentiment, that of fear for his young and happy life, possessed his
whole being. Rapidly leaping the furrows, he fled across the field with
the impetuosity he used to show at catchplay, now and then turning his
good-natured, pale, young face to look back. A shudder of terror went
through him: "No, better not look," he thought, but having reached the
bushes he glanced round once more. The French had fallen behind, and
just as he looked round the first man changed his run to a walk and,
turning, shouted something loudly to a comrade farther back. Rostov
paused. "No, there's some mistake," thought he. "They can't have wanted
to kill me." But at the same time, his left arm felt as heavy as if a
seventy-pound weight were tied to it. He could run no more. The
Frenchman also stopped and took aim. Rostov closed his eyes and stooped
down. One bullet and then another whistled past him. He mustered his
last remaining strength, took hold of his left hand with his right, and
reached the bushes. Behind these were some Russian sharpshooters.




CHAPTER XX

The infantry regiments that had been caught unawares in the outskirts of
the wood ran out of it, the different companies getting mixed, and
retreated as a disorderly crowd. One soldier, in his fear, uttered the
senseless cry, "Cut off!" that is so terrible in battle, and that word
infected the whole crowd with a feeling of panic.

"Surrounded! Cut off? We're lost!" shouted the fugitives.

The moment he heard the firing and the cry from behind, the general
realized that something dreadful had happened to his regiment, and the
thought that he, an exemplary officer of many years' service who had
never been to blame, might be held responsible at headquarters for
negligence or inefficiency so staggered him that, forgetting the
recalcitrant cavalry colonel, his own dignity as a general, and above
all quite forgetting the danger and all regard for self-preservation, he
clutched the crupper of his saddle and, spurring his horse, galloped to
the regiment under a hail of bullets which fell around, but fortunately
missed him. His one desire was to know what was happening and at any
cost correct, or remedy, the mistake if he had made one, so that he, an
exemplary officer of twenty-two years' service, who had never been
censured, should not be held to blame.

Having galloped safely through the French, he reached a field behind the
copse across which our men, regardless of orders, were running and
descending the valley. That moment of moral hesitation which decides the
fate of battles had arrived. Would this disorderly crowd of soldiers
attend to the voice of their commander, or would they, disregarding him,
continue their flight? Despite his desperate shouts that used to seem so
terrible to the soldiers, despite his furious purple countenance
distorted out of all likeness to his former self, and the flourishing of
his saber, the soldiers all continued to run, talking, firing into the
air, and disobeying orders. The moral hesitation which decided the fate
of battles was evidently culminating in a panic.

The general had a fit of coughing as a result of shouting and of the
powder smoke and stopped in despair. Everything seemed lost. But at that
moment the French who were attacking, suddenly and without any apparent
reason, ran back and disappeared from the outskirts, and Russian
sharpshooters showed themselves in the copse. It was Timokhin's company,
which alone had maintained its order in the wood and, having lain in
ambush in a ditch, now attacked the French unexpectedly. Timokhin, armed
only with a sword, had rushed at the enemy with such a desperate cry and
such mad, drunken determination that, taken by surprise, the French had
thrown down their muskets and run. Dolokhov, running beside Timokhin,
killed a Frenchman at close quarters and was the first to seize the
surrendering French officer by his collar. Our fugitives returned, the
battalions re-formed, and the French who had nearly cut our left flank
in half were for the moment repulsed. Our reserve units were able to
join up, and the fight was at an end. The regimental commander and Major
Ekonomov had stopped beside a bridge, letting the retreating companies
pass by them, when a soldier came up and took hold of the commander's
stirrup, almost leaning against him. The man was wearing a bluish coat
of broadcloth, he had no knapsack or cap, his head was bandaged, and
over his shoulder a French munition pouch was slung. He had an officer's
sword in his hand. The soldier was pale, his blue eyes looked impudently
into the commander's face, and his lips were smiling. Though the
commander was occupied in giving instructions to Major Ekonomov, he
could not help taking notice of the soldier.

"Your excellency, here are two trophies," said Dolokhov, pointing to the
French sword and pouch. "I have taken an officer prisoner. I stopped the
company." Dolokhov breathed heavily from weariness and spoke in abrupt
sentences. "The whole company can bear witness. I beg you will remember
this, your excellency!"

"All right, all right," replied the commander, and turned to Major
Ekonomov.

But Dolokhov did not go away; he untied the handkerchief around his
head, pulled it off, and showed the blood congealed on his hair.

"A bayonet wound. I remained at the front. Remember, your excellency!"

Tushin's battery had been forgotten and only at the very end of the
action did Prince Bagration, still hearing the cannonade in the center,
send his orderly staff officer, and later Prince Andrew also, to order
the battery to retire as quickly as possible. When the supports attached
to Tushin's battery had been moved away in the middle of the action by
someone's order, the battery had continued firing and was only not
captured by the French because the enemy could not surmise that anyone
could have the effrontery to continue firing from four quite undefended
guns. On the contrary, the energetic action of that battery led the
French to suppose that here--in the center--the main Russian forces were
concentrated. Twice they had attempted to attack this point, but on each
occasion had been driven back by grapeshot from the four isolated guns
on the hillock.

Soon after Prince Bagration had left him, Tushin had succeeded in
setting fire to Schon Grabern.

"Look at them scurrying! It's burning! Just see the smoke! Fine! Grand!
Look at the smoke, the smoke!" exclaimed the artillerymen, brightening
up.

All the guns, without waiting for orders, were being fired in the
direction of the conflagration. As if urging each other on, the soldiers
cried at each shot: "Fine! That's good! Look at it... Grand!" The fire,
fanned by the breeze, was rapidly spreading. The French columns that had
advanced beyond the village went back; but as though in revenge for this
failure, the enemy placed ten guns to the right of the village and began
firing them at Tushin's battery.

In their childlike glee, aroused by the fire and their luck in
successfully cannonading the French, our artillerymen only noticed this
battery when two balls, and then four more, fell among our guns, one
knocking over two horses and another tearing off a munition-wagon
driver's leg. Their spirits once roused were, however, not diminished,
but only changed character. The horses were replaced by others from a
reserve gun carriage, the wounded were carried away, and the four guns
were turned against the ten-gun battery. Tushin's companion officer had
been killed at the beginning of the engagement and within an hour
seventeen of the forty men of the guns' crews had been disabled, but the
artillerymen were still as merry and lively as ever. Twice they noticed
the French appearing below them, and then they fired grapeshot at them.

Little Tushin, moving feebly and awkwardly, kept telling his orderly to
"refill my pipe for that one!" and then, scattering sparks from it, ran
forward shading his eyes with his small hand to look at the French.

"Smack at 'em, lads!" he kept saying, seizing the guns by the wheels and
working the screws himself.

Amid the smoke, deafened by the incessant reports which always made him
jump, Tushin not taking his pipe from his mouth ran from gun to gun, now
aiming, now counting the charges, now giving orders about replacing dead
or wounded horses and harnessing fresh ones, and shouting in his feeble
voice, so high pitched and irresolute. His face grew more and more
animated. Only when a man was killed or wounded did he frown and turn
away from the sight, shouting angrily at the men who, as is always the
case, hesitated about lifting the injured or dead. The soldiers, for the
most part handsome fellows and, as is always the case in an artillery
company, a head and shoulders taller and twice as broad as their
officer--all looked at their commander like children in an embarrassing
situation, and the expression on his face was invariably reflected on
theirs.

Owing to the terrible uproar and the necessity for concentration and
activity, Tushin did not experience the slightest unpleasant sense of
fear, and the thought that he might be killed or badly wounded never
occurred to him. On the contrary, he became more and more elated. It
seemed to him that it was a very long time ago, almost a day, since he
had first seen the enemy and fired the first shot, and that the corner
of the field he stood on was well-known and familiar ground. Though he
thought of everything, considered everything, and did everything the
best of officers could do in his position, he was in a state akin to
feverish delirium or drunkenness.

From the deafening sounds of his own guns around him, the whistle and
thud of the enemy's cannon balls, from the flushed and perspiring faces
of the crew bustling round the guns, from the sight of the blood of men
and horses, from the little puffs of smoke on the enemy's side (always
followed by a ball flying past and striking the earth, a man, a gun, a
horse), from the sight of all these things a fantastic world of his own
had taken possession of his brain and at that moment afforded him
pleasure. The enemy's guns were in his fancy not guns but pipes from
which occasional puffs were blown by an invisible smoker.

"There... he's puffing again," muttered Tushin to himself, as a small
cloud rose from the hill and was borne in a streak to the left by the
wind.

"Now look out for the ball... we'll throw it back."

"What do you want, your honor?" asked an artilleryman, standing close
by, who heard him muttering.

"Nothing... only a shell..." he answered.

"Come along, our Matvevna!" he said to himself. "Matvevna" * was the
name his fancy gave to the farthest gun of the battery, which was large
and of an old pattern. The French swarming round their guns seemed to
him like ants. In that world, the handsome drunkard Number One of the
second gun's crew was "uncle"; Tushin looked at him more often than at
anyone else and took delight in his every movement. The sound of
musketry at the foot of the hill, now diminishing, now increasing,
seemed like someone's breathing. He listened intently to the ebb and
flow of these sounds.


* Daughter of Matthew.

"Ah! Breathing again, breathing!" he muttered to himself.

He imagined himself as an enormously tall, powerful man who was throwing
cannon balls at the French with both hands.

"Now then, Matvevna, dear old lady, don't let me down!" he was saying as
he moved from the gun, when a strange, unfamiliar voice called above his
head: "Captain Tushin! Captain!"

Tushin turned round in dismay. It was the staff officer who had turned
him out of the booth at Grunth. He was shouting in a gasping voice:

"Are you mad? You have twice been ordered to retreat, and you..."

"Why are they down on me?" thought Tushin, looking in alarm at his
superior.

"I... don't..." he muttered, holding up two fingers to his cap. "I..."

But the staff officer did not finish what he wanted to say. A cannon
ball, flying close to him, caused him to duck and bend over his horse.
He paused, and just as he was about to say something more, another ball
stopped him. He turned his horse and galloped off.

"Retire! All to retire!" he shouted from a distance.

The soldiers laughed. A moment later, an adjutant arrived with the same
order.

It was Prince Andrew. The first thing he saw on riding up to the space
where Tushin's guns were stationed was an unharnessed horse with a
broken leg, that lay screaming piteously beside the harnessed horses.
Blood was gushing from its leg as from a spring. Among the limbers lay
several dead men. One ball after another passed over as he approached
and he felt a nervous shudder run down his spine. But the mere thought
of being afraid roused him again. "I cannot be afraid," thought he, and
dismounted slowly among the guns. He delivered the order and did not
leave the battery. He decided to have the guns removed from their
positions and withdrawn in his presence. Together with Tushin, stepping
across the bodies and under a terrible fire from the French, he attended
to the removal of the guns.

"A staff officer was here a minute ago, but skipped off," said an
artilleryman to Prince Andrew. "Not like your honor!"

Prince Andrew said nothing to Tushin. They were both so busy as to seem
not to notice one another. When having limbered up the only two cannon
that remained uninjured out of the four, they began moving down the hill
(one shattered gun and one unicorn were left behind), Prince Andrew rode
up to Tushin.

"Well, till we meet again..." he said, holding out his hand to Tushin.

"Good-bye, my dear fellow," said Tushin. "Dear soul! Good-bye, my dear
fellow!" and for some unknown reason tears suddenly filled his eyes.




CHAPTER XXI

The wind had fallen and black clouds, merging with the powder smoke,
hung low over the field of battle on the horizon. It was growing dark
and the glow of two conflagrations was the more conspicuous. The
cannonade was dying down, but the rattle of musketry behind and on the
right sounded oftener and nearer. As soon as Tushin with his guns,
continually driving round or coming upon wounded men, was out of range
of fire and had descended into the dip, he was met by some of the staff,
among them the staff officer and Zherkov, who had been twice sent to
Tushin's battery but had never reached it. Interrupting one another,
they all gave, and transmitted, orders as to how to proceed,
reprimanding and reproaching him. Tushin gave no orders, and, silently--
fearing to speak because at every word he felt ready to weep without
knowing why--rode behind on his artillery nag. Though the orders were to
abandon the wounded, many of them dragged themselves after troops and
begged for seats on the gun carriages. The jaunty infantry officer who
just before the battle had rushed out of Tushin's wattle shed was laid,
with a bullet in his stomach, on "Matvevna's" carriage. At the foot of
the hill, a pale hussar cadet, supporting one hand with the other, came
up to Tushin and asked for a seat.

"Captain, for God's sake! I've hurt my arm," he said timidly. "For God's
sake... I can't walk. For God's sake!"

It was plain that this cadet had already repeatedly asked for a lift and
been refused. He asked in a hesitating, piteous voice.

"Tell them to give me a seat, for God's sake!"

"Give him a seat," said Tushin. "Lay a cloak for him to sit on, lad," he
said, addressing his favorite soldier. "And where is the wounded
officer?"

"He has been set down. He died," replied someone.

"Help him up. Sit down, dear fellow, sit down! Spread out the cloak,
Antonov."

The cadet was Rostov. With one hand he supported the other; he was pale
and his jaw trembled, shivering feverishly. He was placed on "Matvevna,"
the gun from which they had removed the dead officer. The cloak they
spread under him was wet with blood which stained his breeches and arm.

"What, are you wounded, my lad?" said Tushin, approaching the gun on
which Rostov sat.

"No, it's a sprain."

"Then what is this blood on the gun carriage?" inquired Tushin.

"It was the officer, your honor, stained it," answered the artilleryman,
wiping away the blood with his coat sleeve, as if apologizing for the
state of his gun.

It was all that they could do to get the guns up the rise aided by the
infantry, and having reached the village of Gruntersdorf they halted. It
had grown so dark that one could not distinguish the uniforms ten paces
off, and the firing had begun to subside. Suddenly, near by on the
right, shouting and firing were again heard. Flashes of shot gleamed in
the darkness. This was the last French attack and was met by soldiers
who had sheltered in the village houses. They all rushed out of the
village again, but Tushin's guns could not move, and the artillerymen,
Tushin, and the cadet exchanged silent glances as they awaited their
fate. The firing died down and soldiers, talking eagerly, streamed out
of a side street.

"Not hurt, Petrov?" asked one.

"We've given it 'em hot, mate! They won't make another push now," said
another.

"You couldn't see a thing. How they shot at their own fellows! Nothing
could be seen. Pitch-dark, brother! Isn't there something to drink?"

The French had been repulsed for the last time. And again and again in
the complete darkness Tushin's guns moved forward, surrounded by the
humming infantry as by a frame.

In the darkness, it seemed as though a gloomy unseen river was flowing
always in one direction, humming with whispers and talk and the sound of
hoofs and wheels. Amid the general rumble, the groans and voices of the
wounded were more distinctly heard than any other sound in the darkness
of the night. The gloom that enveloped the army was filled with their
groans, which seemed to melt into one with the darkness of the night.
After a while the moving mass became agitated, someone rode past on a
white horse followed by his suite, and said something in passing: "What
did he say? Where to, now? Halt, is it? Did he thank us?" came eager
questions from all sides. The whole moving mass began pressing closer
together and a report spread that they were ordered to halt: evidently
those in front had halted. All remained where they were in the middle of
the muddy road.

Fires were lighted and the talk became more audible. Captain Tushin,
having given orders to his company, sent a soldier to find a dressing
station or a doctor for the cadet, and sat down by a bonfire the
soldiers had kindled on the road. Rostov, too, dragged himself to the
fire. From pain, cold, and damp, a feverish shivering shook his whole
body. Drowsiness was irresistibly mastering him, but he kept awake by an
excruciating pain in his arm, for which he could find no satisfactory
position. He kept closing his eyes and then again looking at the fire,
which seemed to him dazzlingly red, and at the feeble, round-shouldered
figure of Tushin who was sitting cross-legged like a Turk beside him.
Tushin's large, kind, intelligent eyes were fixed with sympathy and
commiseration on Rostov, who saw that Tushin with his whole heart wished
to help him but could not.

From all sides were heard the footsteps and talk of the infantry, who
were walking, driving past, and settling down all around. The sound of
voices, the tramping feet, the horses' hoofs moving in mud, the
crackling of wood fires near and afar, merged into one tremulous rumble.

It was no longer, as before, a dark, unseen river flowing through the
gloom, but a dark sea swelling and gradually subsiding after a storm.
Rostov looked at and listened listlessly to what passed before and
around him. An infantryman came to the fire, squatted on his heels, held
his hands to the blaze, and turned away his face.

"You don't mind your honor?" he asked Tushin. "I've lost my company,
your honor. I don't know where... such bad luck!"

With the soldier, an infantry officer with a bandaged cheek came up to
the bonfire, and addressing Tushin asked him to have the guns moved a
trifle to let a wagon go past. After he had gone, two soldiers rushed to
the campfire. They were quarreling and fighting desperately, each trying
to snatch from the other a boot they were both holding on to.

"You picked it up?... I dare say! You're very smart!" one of them
shouted hoarsely.

Then a thin, pale soldier, his neck bandaged with a bloodstained leg
band, came up and in angry tones asked the artillerymen for water.

"Must one die like a dog?" said he.

Tushin told them to give the man some water. Then a cheerful soldier ran
up, begging a little fire for the infantry.

"A nice little hot torch for the infantry! Good luck to you, fellow
countrymen. Thanks for the fire--we'll return it with interest," said
he, carrying away into the darkness a glowing stick.

Next came four soldiers, carrying something heavy on a cloak, and passed
by the fire. One of them stumbled.

"Who the devil has put the logs on the road?" snarled he.

"He's dead--why carry him?" said another.

"Shut up!"

And they disappeared into the darkness with their load.

"Still aching?" Tushin asked Rostov in a whisper.

"Yes."

"Your honor, you're wanted by the general. He is in the hut here," said
a gunner, coming up to Tushin.

"Coming, friend."

Tushin rose and, buttoning his greatcoat and pulling it straight, walked
away from the fire.

Not far from the artillery campfire, in a hut that had been prepared for
him, Prince Bagration sat at dinner, talking with some commanding
officers who had gathered at his quarters. The little old man with the
half-closed eyes was there greedily gnawing a mutton bone, and the
general who had served blamelessly for twenty-two years, flushed by a
glass of vodka and the dinner; and the staff officer with the signet
ring, and Zherkov, uneasily glancing at them all, and Prince Andrew,
pale, with compressed lips and feverishly glittering eyes.

In a corner of the hut stood a standard captured from the French, and
the accountant with the naive face was feeling its texture, shaking his
head in perplexity--perhaps because the banner really interested him,
perhaps because it was hard for him, hungry as he was, to look on at a
dinner where there was no place for him. In the next hut there was a
French colonel who had been taken prisoner by our dragoons. Our officers
were flocking in to look at him. Prince Bagration was thanking the
individual commanders and inquiring into details of the action and our
losses. The general whose regiment had been inspected at Braunau was
informing the prince that as soon as the action began he had withdrawn
from the wood, mustered the men who were woodcutting, and, allowing the
French to pass him, had made a bayonet charge with two battalions and
had broken up the French troops.

"When I saw, your excellency, that their first battalion was
disorganized, I stopped in the road and thought: 'I'll let them come on
and will meet them with the fire of the whole battalion'--and that's
what I did."

The general had so wished to do this and was so sorry he had not managed
to do it that it seemed to him as if it had really happened. Perhaps it
might really have been so? Could one possibly make out amid all that
confusion what did or did not happen?

"By the way, your excellency, I should inform you," he continued--
remembering Dolokhov's conversation with Kutuzov and his last interview
with the gentleman-ranker--"that Private Dolokhov, who was reduced to
the ranks, took a French officer prisoner in my presence and
particularly distinguished himself."

"I saw the Pavlograd hussars attack there, your excellency," chimed in
Zherkov, looking uneasily around. He had not seen the hussars all that
day, but had heard about them from an infantry officer. "They broke up
two squares, your excellency."

Several of those present smiled at Zherkov's words, expecting one of his
usual jokes, but noticing that what he was saying redounded to the glory
of our arms and of the day's work, they assumed a serious expression,
though many of them knew that what he was saying was a lie devoid of any
foundation. Prince Bagration turned to the old colonel:

"Gentlemen, I thank you all; all arms have behaved heroically: infantry,
cavalry, and artillery. How was it that two guns were abandoned in the
center?" he inquired, searching with his eyes for someone. (Prince
Bagration did not ask about the guns on the left flank; he knew that all
the guns there had been abandoned at the very beginning of the action.)
"I think I sent you?" he added, turning to the staff officer on duty.

"One was damaged," answered the staff officer, "and the other I can't
understand. I was there all the time giving orders and had only just
left.... It is true that it was hot there," he added, modestly.

Someone mentioned that Captain Tushin was bivouacking close to the
village and had already been sent for.

"Oh, but you were there?" said Prince Bagration, addressing Prince
Andrew.

"Of course, we only just missed one another," said the staff officer,
with a smile to Bolkonski.

"I had not the pleasure of seeing you," said Prince Andrew, coldly and
abruptly.

All were silent. Tushin appeared at the threshold and made his way
timidly from behind the backs of the generals. As he stepped past the
generals in the crowded hut, feeling embarrassed as he always was by the
sight of his superiors, he did not notice the staff of the banner and
stumbled over it. Several of those present laughed.

"How was it a gun was abandoned?" asked Bagration, frowning, not so much
at the captain as at those who were laughing, among whom Zherkov laughed
loudest.

Only now, when he was confronted by the stern authorities, did his guilt
and the disgrace of having lost two guns and yet remaining alive present
themselves to Tushin in all their horror. He had been so excited that he
had not thought about it until that moment. The officers' laughter
confused him still more. He stood before Bagration with his lower jaw
trembling and was hardly able to mutter: "I don't know... your
excellency... I had no men... your excellency."

"You might have taken some from the covering troops."

Tushin did not say that there were no covering troops, though that was
perfectly true. He was afraid of getting some other officer into
trouble, and silently fixed his eyes on Bagration as a schoolboy who has
blundered looks at an examiner.

The silence lasted some time. Prince Bagration, apparently not wishing
to be severe, found nothing to say; the others did not venture to
intervene. Prince Andrew looked at Tushin from under his brows and his
fingers twitched nervously.

"Your excellency!" Prince Andrew broke the silence with his abrupt
voice, "you were pleased to send me to Captain Tushin's battery. I went
there and found two thirds of the men and horses knocked out, two guns
smashed, and no supports at all."

Prince Bagration and Tushin looked with equal intentness at Bolkonski,
who spoke with suppressed agitation.

"And, if your excellency will allow me to express my opinion," he
continued, "we owe today's success chiefly to the action of that battery
and the heroic endurance of Captain Tushin and his company," and without
awaiting a reply, Prince Andrew rose and left the table.

Prince Bagration looked at Tushin, evidently reluctant to show distrust
in Bolkonski's emphatic opinion yet not feeling able fully to credit it,
bent his head, and told Tushin that he could go. Prince Andrew went out
with him.

"Thank you; you saved me, my dear fellow!" said Tushin.

Prince Andrew gave him a look, but said nothing and went away. He felt
sad and depressed. It was all so strange, so unlike what he had hoped.

"Who are they? Why are they here? What do they want? And when will all
this end?" thought Rostov, looking at the changing shadows before him.
The pain in his arm became more and more intense. Irresistible
drowsiness overpowered him, red rings danced before his eyes, and the
impression of those voices and faces and a sense of loneliness merged
with the physical pain. It was they, these soldiers--wounded and
unwounded--it was they who were crushing, weighing down, and twisting
the sinews and scorching the flesh of his sprained arm and shoulder. To
rid himself of them he closed his eyes.

For a moment he dozed, but in that short interval innumerable things
appeared to him in a dream: his mother and her large white hand, Sonya's
thin little shoulders, Natasha's eyes and laughter, Denisov with his
voice and mustache, and Telyanin and all that affair with Telyanin and
Bogdanich. That affair was the same thing as this soldier with the harsh
voice, and it was that affair and this soldier that were so agonizingly,
incessantly pulling and pressing his arm and always dragging it in one
direction. He tried to get away from them, but they would not for an
instant let his shoulder move a hair's breadth. It would not ache--it
would be well--if only they did not pull it, but it was impossible to
get rid of them.

He opened his eyes and looked up. The black canopy of night hung less
than a yard above the glow of the charcoal. Flakes of falling snow were
fluttering in that light. Tushin had not returned, the doctor had not
come. He was alone now, except for a soldier who was sitting naked at
the other side of the fire, warming his thin yellow body.

"Nobody wants me!" thought Rostov. "There is no one to help me or pity
me. Yet I was once at home, strong, happy, and loved." He sighed and,
doing so, groaned involuntarily.

"Eh, is anything hurting you?" asked the soldier, shaking his shirt out
over the fire, and not waiting for an answer he gave a grunt and added:
"What a lot of men have been crippled today--frightful!"

Rostov did not listen to the soldier. He looked at the snowflakes
fluttering above the fire and remembered a Russian winter at his warm,
bright home, his fluffy fur coat, his quickly gliding sleigh, his
healthy body, and all the affection and care of his family. "And why did
I come here?" he wondered.

Next day the French army did not renew their attack, and the remnant of
Bagration's detachment was reunited to Kutuzov's army.

BOOK THREE: 1805




CHAPTER I

Prince Vasili was not a man who deliberately thought out his plans.
Still less did he think of injuring anyone for his own advantage. He was
merely a man of the world who had got on and to whom getting on had
become a habit. Schemes and devices for which he never rightly accounted
to himself, but which formed the whole interest of his life, were
constantly shaping themselves in his mind, arising from the
circumstances and persons he met. Of these plans he had not merely one
or two in his head but dozens, some only beginning to form themselves,
some approaching achievement, and some in course of disintegration. He
did not, for instance, say to himself: "This man now has influence, I
must gain his confidence and friendship and through him obtain a special
grant." Nor did he say to himself: "Pierre is a rich man, I must entice
him to marry my daughter and lend me the forty thousand rubles I need."
But when he came across a man of position his instinct immediately told
him that this man could be useful, and without any premeditation Prince
Vasili took the first opportunity to gain his confidence, flatter him,
become intimate with him, and finally make his request.

He had Pierre at hand in Moscow and procured for him an appointment as
Gentleman of the Bedchamber, which at that time conferred the status of
Councilor of State, and insisted on the young man accompanying him to
Petersburg and staying at his house. With apparent absent-mindedness,
yet with unhesitating assurance that he was doing the right thing,
Prince Vasili did everything to get Pierre to marry his daughter. Had he
thought out his plans beforehand he could not have been so natural and
shown such unaffected familiarity in intercourse with everybody both
above and below him in social standing. Something always drew him toward
those richer and more powerful than himself and he had rare skill in
seizing the most opportune moment for making use of people.

Pierre, on unexpectedly becoming Count Bezukhov and a rich man, felt
himself after his recent loneliness and freedom from cares so beset and
preoccupied that only in bed was he able to be by himself. He had to
sign papers, to present himself at government offices, the purpose of
which was not clear to him, to question his chief steward, to visit his
estate near Moscow, and to receive many people who formerly did not even
wish to know of his existence but would now have been offended and
grieved had he chosen not to see them. These different people--
businessmen, relations, and acquaintances alike--were all disposed to
treat the young heir in the most friendly and flattering manner: they
were all evidently firmly convinced of Pierre's noble qualities. He was
always hearing such words as: "With your remarkable kindness," or, "With
your excellent heart," "You are yourself so honorable Count," or, "Were
he as clever as you," and so on, till he began sincerely to believe in
his own exceptional kindness and extraordinary intelligence, the more so
as in the depth of his heart it had always seemed to him that he really
was very kind and intelligent. Even people who had formerly been
spiteful toward him and evidently unfriendly now became gentle and
affectionate. The angry eldest princess, with the long waist and hair
plastered down like a doll's, had come into Pierre's room after the
funeral. With drooping eyes and frequent blushes she told him she was
very sorry about their past misunderstandings and did not now feel she
had a right to ask him for anything, except only for permission, after
the blow she had received, to remain for a few weeks longer in the house
she so loved and where she had sacrificed so much. She could not refrain
from weeping at these words. Touched that this statuesque princess could
so change, Pierre took her hand and begged her forgiveness, without
knowing what for. From that day the eldest princess quite changed toward
Pierre and began knitting a striped scarf for him.

"Do this for my sake, mon cher; after all, she had to put up with a
great deal from the deceased," said Prince Vasili to him, handing him a
deed to sign for the princess' benefit.

Prince Vasili had come to the conclusion that it was necessary to throw
this bone--a bill for thirty thousand rubles--to the poor princess that
it might not occur to her to speak of his share in the affair of the
inlaid portfolio. Pierre signed the deed and after that the princess
grew still kinder. The younger sisters also became affectionate to him,
especially the youngest, the pretty one with the mole, who often made
him feel confused by her smiles and her own confusion when meeting him.

It seemed so natural to Pierre that everyone should like him, and it
would have seemed so unnatural had anyone disliked him, that he could
not but believe in the sincerity of those around him. Besides, he had no
time to ask himself whether these people were sincere or not. He was
always busy and always felt in a state of mild and cheerful
intoxication. He felt as though he were the center of some important and
general movement; that something was constantly expected of him, that if
he did not do it he would grieve and disappoint many people, but if he
did this and that, all would be well; and he did what was demanded of
him, but still that happy result always remained in the future.

More than anyone else, Prince Vasili took possession of Pierre's affairs
and of Pierre himself in those early days. From the death of Count
Bezukhov he did not let go his hold of the lad. He had the air of a man
oppressed by business, weary and suffering, who yet would not, for
pity's sake, leave this helpless youth who, after all, was the son of
his old friend and the possessor of such enormous wealth, to the caprice
of fate and the designs of rogues. During the few days he spent in
Moscow after the death of Count Bezukhov, he would call Pierre, or go to
him himself, and tell him what ought to be done in a tone of weariness
and assurance, as if he were adding every time: "You know I am
overwhelmed with business and it is purely out of charity that I trouble
myself about you, and you also know quite well that what I propose is
the only thing possible."

"Well, my dear fellow, tomorrow we are off at last," said Prince Vasili
one day, closing his eyes and fingering Pierre's elbow, speaking as if
he were saying something which had long since been agreed upon and could
not now be altered. "We start tomorrow and I'm giving you a place in my
carriage. I am very glad. All our important business here is now
settled, and I ought to have been off long ago. Here is something I have
received from the chancellor. I asked him for you, and you have been
entered in the diplomatic corps and made a Gentleman of the Bedchamber.
The diplomatic career now lies open before you."

Notwithstanding the tone of wearied assurance with which these words
were pronounced, Pierre, who had so long been considering his career,
wished to make some suggestion. But Prince Vasili interrupted him in the
special deep cooing tone, precluding the possibility of interrupting his
speech, which he used in extreme cases when special persuasion was
needed.

"Mais, mon cher, I did this for my own sake, to satisfy my conscience,
and there is nothing to thank me for. No one has ever complained yet of
being too much loved; and besides, you are free, you could throw it up
tomorrow. But you will see everything for yourself when you get to
Petersburg. It is high time for you to get away from these terrible
recollections." Prince Vasili sighed. "Yes, yes, my boy. And my valet
can go in your carriage. Ah! I was nearly forgetting," he added. "You
know, mon cher, your father and I had some accounts to settle, so I have
received what was due from the Ryazan estate and will keep it; you won't
require it. We'll go into the accounts later."

By "what was due from the Ryazan estate" Prince Vasili meant several
thousand rubles quitrent received from Pierre's peasants, which the
prince had retained for himself.

In Petersburg, as in Moscow, Pierre found the same atmosphere of
gentleness and affection. He could not refuse the post, or rather the
rank (for he did nothing), that Prince Vasili had procured for him, and
acquaintances, invitations, and social occupations were so numerous
that, even more than in Moscow, he felt a sense of bewilderment, bustle,
and continual expectation of some good, always in front of him but never
attained.

Of his former bachelor acquaintances many were no longer in Petersburg.
The Guards had gone to the front; Dolokhov had been reduced to the
ranks; Anatole was in the army somewhere in the provinces; Prince Andrew
was abroad; so Pierre had not the opportunity to spend his nights as he
used to like to spend them, or to open his mind by intimate talks with a
friend older than himself and whom he respected. His whole time was
taken up with dinners and balls and was spent chiefly at Prince Vasili's
house in the company of the stout princess, his wife, and his beautiful
daughter Helene.

Like the others, Anna Pavlovna Scherer showed Pierre the change of
attitude toward him that had taken place in society.

Formerly in Anna Pavlovna's presence, Pierre had always felt that what
he was saying was out of place, tactless and unsuitable, that remarks
which seemed to him clever while they formed in his mind became foolish
as soon as he uttered them, while on the contrary Hippolyte's stupidest
remarks came out clever and apt. Now everything Pierre said was
charmant. Even if Anna Pavlovna did not say so, he could see that she
wished to and only refrained out of regard for his modesty.

In the beginning of the winter of 1805-6 Pierre received one of Anna
Pavlovna's usual pink notes with an invitation to which was added: "You
will find the beautiful Helene here, whom it is always delightful to
see."

When he read that sentence, Pierre felt for the first time that some
link which other people recognized had grown up between himself and
Helene, and that thought both alarmed him, as if some obligation were
being imposed on him which he could not fulfill, and pleased him as an
entertaining supposition.

Anna Pavlovna's "At Home" was like the former one, only the novelty she
offered her guests this time was not Mortemart, but a diplomatist fresh
from Berlin with the very latest details of the Emperor Alexander's
visit to Potsdam, and of how the two august friends had pledged
themselves in an indissoluble alliance to uphold the cause of justice
against the enemy of the human race. Anna Pavlovna received Pierre with
a shade of melancholy, evidently relating to the young man's recent loss
by the death of Count Bezukhov (everyone constantly considered it a duty
to assure Pierre that he was greatly afflicted by the death of the
father he had hardly known), and her melancholy was just like the august
melancholy she showed at the mention of her most august Majesty the
Empress Marya Fedorovna. Pierre felt flattered by this. Anna Pavlovna
arranged the different groups in her drawing room with her habitual
skill. The large group, in which were Prince Vasili and the generals,
had the benefit of the diplomat. Another group was at the tea table.
Pierre wished to join the former, but Anna Pavlovna--who was in the
excited condition of a commander on a battlefield to whom thousands of
new and brilliant ideas occur which there is hardly time to put in
action--seeing Pierre, touched his sleeve with her finger, saying:

"Wait a bit, I have something in view for you this evening." (She
glanced at Helene and smiled at her.) "My dear Helene, be charitable to
my poor aunt who adores you. Go and keep her company for ten minutes.
And that it will not be too dull, here is the dear count who will not
refuse to accompany you."

The beauty went to the aunt, but Anna Pavlovna detained Pierre, looking
as if she had to give some final necessary instructions.

"Isn't she exquisite?" she said to Pierre, pointing to the stately
beauty as she glided away. "And how she carries herself! For so young a
girl, such tact, such masterly perfection of manner! It comes from her
heart. Happy the man who wins her! With her the least worldly of men
would occupy a most brilliant position in society. Don't you think so? I
only wanted to know your opinion," and Anna Pavlovna let Pierre go.

Pierre, in reply, sincerely agreed with her as to Helene's perfection of
manner. If he ever thought of Helene, it was just of her beauty and her
remarkable skill in appearing silently dignified in society.

The old aunt received the two young people in her corner, but seemed
desirous of hiding her adoration for Helene and inclined rather to show
her fear of Anna Pavlovna. She looked at her niece, as if inquiring what
she was to do with these people. On leaving them, Anna Pavlovna again
touched Pierre's sleeve, saying: "I hope you won't say that it is dull
in my house again," and she glanced at Helene.

Helene smiled, with a look implying that she did not admit the
possibility of anyone seeing her without being enchanted. The aunt
coughed, swallowed, and said in French that she was very pleased to see
Helene, then she turned to Pierre with the same words of welcome and the
same look. In the middle of a dull and halting conversation, Helene
turned to Pierre with the beautiful bright smile that she gave to
everyone. Pierre was so used to that smile, and it had so little meaning
for him, that he paid no attention to it. The aunt was just speaking of
a collection of snuffboxes that had belonged to Pierre's father, Count
Bezukhov, and showed them her own box. Princess Helene asked to see the
portrait of the aunt's husband on the box lid.

"That is probably the work of Vinesse," said Pierre, mentioning a
celebrated miniaturist, and he leaned over the table to take the
snuffbox while trying to hear what was being said at the other table.

He half rose, meaning to go round, but the aunt handed him the snuffbox,
passing it across Helene's back. Helene stooped forward to make room,
and looked round with a smile. She was, as always at evening parties,
wearing a dress such as was then fashionable, cut very low at front and
back. Her bust, which had always seemed like marble to Pierre, was so
close to him that his shortsighted eyes could not but perceive the
living charm of her neck and shoulders, so near to his lips that he need
only have bent his head a little to have touched them. He was conscious
of the warmth of her body, the scent of perfume, and the creaking of her
corset as she moved. He did not see her marble beauty forming a complete
whole with her dress, but all the charm of her body only covered by her
garments. And having once seen this he could not help being aware of it,
just as we cannot renew an illusion we have once seen through.

"So you have never noticed before how beautiful I am?" Helene seemed to
say. "You had not noticed that I am a woman? Yes, I am a woman who may
belong to anyone--to you too," said her glance. And at that moment
Pierre felt that Helene not only could, but must, be his wife, and that
it could not be otherwise.

He knew this at that moment as surely as if he had been standing at the
altar with her. How and when this would be he did not know, he did not
even know if it would be a good thing (he even felt, he knew not why,
that it would be a bad thing), but he knew it would happen.

Pierre dropped his eyes, lifted them again, and wished once more to see
her as a distant beauty far removed from him, as he had seen her every
day until then, but he could no longer do it. He could not, any more
than a man who has been looking at a tuft of steppe grass through the
mist and taking it for a tree can again take it for a tree after he has
once recognized it to be a tuft of grass. She was terribly close to him.
She already had power over him, and between them there was no longer any
barrier except the barrier of his own will.

"Well, I will leave you in your little corner," came Anna Pavlovna's
voice, "I see you are all right there."

And Pierre, anxiously trying to remember whether he had done anything
reprehensible, looked round with a blush. It seemed to him that everyone
knew what had happened to him as he knew it himself.

A little later when he went up to the large circle, Anna Pavlovna said
to him: "I hear you are refitting your Petersburg house?"

This was true. The architect had told him that it was necessary, and
Pierre, without knowing why, was having his enormous Petersburg house
done up.

"That's a good thing, but don't move from Prince Vasili's. It is good to
have a friend like the prince," she said, smiling at Prince Vasili. "I
know something about that. Don't I? And you are still so young. You need
advice. Don't be angry with me for exercising an old woman's privilege."

She paused, as women always do, expecting something after they have
mentioned their age. "If you marry it will be a different thing," she
continued, uniting them both in one glance. Pierre did not look at
Helene nor she at him. But she was just as terribly close to him. He
muttered something and colored.

When he got home he could not sleep for a long time for thinking of what
had happened. What had happened? Nothing. He had merely understood that
the woman he had known as a child, of whom when her beauty was mentioned
he had said absent-mindedly: "Yes, she's good looking," he had
understood that this woman might belong to him.

"But she's stupid. I have myself said she is stupid," he thought. "There
is something nasty, something wrong, in the feeling she excites in me. I
have been told that her brother Anatole was in love with her and she
with him, that there was quite a scandal and that that's why he was sent
away. Hippolyte is her brother... Prince Vasili is her father... It's
bad...." he reflected, but while he was thinking this (the reflection
was still incomplete), he caught himself smiling and was conscious that
another line of thought had sprung up, and while thinking of her
worthlessness he was also dreaming of how she would be his wife, how she
would love him become quite different, and how all he had thought and
heard of her might be false. And he again saw her not as the daughter of
Prince Vasili, but visualized her whole body only veiled by its gray
dress. "But no! Why did this thought never occur to me before?" and
again he told himself that it was impossible, that there would be
something unnatural, and as it seemed to him dishonorable, in this
marriage. He recalled her former words and looks and the words and looks
of those who had seen them together. He recalled Anna Pavlovna's words
and looks when she spoke to him about his house, recalled thousands of
such hints from Prince Vasili and others, and was seized by terror lest
he had already, in some way, bound himself to do something that was
evidently wrong and that he ought not to do. But at the very time he was
expressing this conviction to himself, in another part of his mind her
image rose in all its womanly beauty.




CHAPTER II

In November, 1805, Prince Vasili had to go on a tour of inspection in
four different provinces. He had arranged this for himself so as to
visit his neglected estates at the same time and pick up his son Anatole
where his regiment was stationed, and take him to visit Prince Nicholas
Bolkonski in order to arrange a match for him with the daughter of that
rich old man. But before leaving home and undertaking these new affairs,
Prince Vasili had to settle matters with Pierre, who, it is true, had
latterly spent whole days at home, that is, in Prince Vasili's house
where he was staying, and had been absurd, excited, and foolish in
Helene's presence (as a lover should be), but had not yet proposed to
her.

"This is all very fine, but things must be settled," said Prince Vasili
to himself, with a sorrowful sigh, one morning, feeling that Pierre who
was under such obligations to him ("But never mind that") was not
behaving very well in this matter. "Youth, frivolity... well, God be
with him," thought he, relishing his own goodness of heart, "but it must
be brought to a head. The day after tomorrow will be Lelya's name day. I
will invite two or three people, and if he does not understand what he
ought to do then it will be my affair--yes, my affair. I am her father."

Six weeks after Anna Pavlovna's "At Home" and after the sleepless night
when he had decided that to marry Helene would be a calamity and that he
ought to avoid her and go away, Pierre, despite that decision, had not
left Prince Vasili's and felt with terror that in people's eyes he was
every day more and more connected with her, that it was impossible for
him to return to his former conception of her, that he could not break
away from her, and that though it would be a terrible thing he would
have to unite his fate with hers. He might perhaps have been able to
free himself but that Prince Vasili (who had rarely before given
receptions) now hardly let a day go by without having an evening party
at which Pierre had to be present unless he wished to spoil the general
pleasure and disappoint everyone's expectation. Prince Vasili, in the
rare moments when he was at home, would take Pierre's hand in passing
and draw it downwards, or absent-mindedly hold out his wrinkled, clean-
shaven cheek for Pierre to kiss and would say: "Till tomorrow," or, "Be
in to dinner or I shall not see you," or, "I am staying in for your
sake," and so on. And though Prince Vasili, when he stayed in (as he
said) for Pierre's sake, hardly exchanged a couple of words with him,
Pierre felt unable to disappoint him. Every day he said to himself one
and the same thing: "It is time I understood her and made up my mind
what she really is. Was I mistaken before, or am I mistaken now? No, she
is not stupid, she is an excellent girl," he sometimes said to himself
"she never makes a mistake, never says anything stupid. She says little,
but what she does say is always clear and simple, so she is not stupid.
She never was abashed and is not abashed now, so she cannot be a bad
woman!" He had often begun to make reflections or think aloud in her
company, and she had always answered him either by a brief but
appropriate remark--showing that it did not interest her--or by a silent
look and smile which more palpably than anything else showed Pierre her
superiority. She was right in regarding all arguments as nonsense in
comparison with that smile.

She always addressed him with a radiantly confiding smile meant for him
alone, in which there was something more significant than in the general
smile that usually brightened her face. Pierre knew that everyone was
waiting for him to say a word and cross a certain line, and he knew that
sooner or later he would step across it, but an incomprehensible terror
seized him at the thought of that dreadful step. A thousand times during
that month and a half while he felt himself drawn nearer and nearer to
that dreadful abyss, Pierre said to himself: "What am I doing? I need
resolution. Can it be that I have none?"

He wished to take a decision, but felt with dismay that in this matter
he lacked that strength of will which he had known in himself and really
possessed. Pierre was one of those who are only strong when they feel
themselves quite innocent, and since that day when he was overpowered by
a feeling of desire while stooping over the snuffbox at Anna Pavlovna's,
an unacknowledged sense of the guilt of that desire paralyzed his will.

On Helene's name day, a small party of just their own people--as his
wife said--met for supper at Prince Vasili's. All these friends and
relations had been given to understand that the fate of the young girl
would be decided that evening. The visitors were seated at supper.
Princess Kuragina, a portly imposing woman who had once been handsome,
was sitting at the head of the table. On either side of her sat the more
important guests--an old general and his wife, and Anna Pavlovna
Scherer. At the other end sat the younger and less important guests, and
there too sat the members of the family, and Pierre and Helene, side by
side. Prince Vasili was not having any supper: he went round the table
in a merry mood, sitting down now by one, now by another, of the guests.
To each of them he made some careless and agreeable remark except to
Pierre and Helene, whose presence he seemed not to notice. He enlivened
the whole party. The wax candles burned brightly, the silver and crystal
gleamed, so did the ladies' toilets and the gold and silver of the men's
epaulets; servants in scarlet liveries moved round the table, the
clatter of plates, knives, and glasses mingled with the animated hum of
several conversations. At one end of the table, the old chamberlain was
heard assuring an old baroness that he loved her passionately, at which
she laughed; at the other could be heard the story of the misfortunes of
some Mary Viktorovna or other. At the center of the table, Prince Vasili
attracted everybody's attention. With a facetious smile on his face, he
was telling the ladies about last Wednesday's meeting of the Imperial
Council, at which Sergey Kuzmich Vyazmitinov, the new military governor
general of Petersburg, had received and read the then famous rescript of
the Emperor Alexander from the army to Sergey Kuzmich, in which the
Emperor said that he was receiving from all sides declarations of the
people's loyalty, that the declaration from Petersburg gave him
particular pleasure, and that he was proud to be at the head of such a
nation and would endeavor to be worthy of it. This rescript began with
the words: "Sergey Kuzmich, From all sides reports reach me," etc.

"Well, and so he never got farther than: 'Sergey Kuzmich'?" asked one of
the ladies.

"Exactly, not a hair's breadth farther," answered Prince Vasili,
laughing, "'Sergey Kuzmich... From all sides... From all sides... Sergey
Kuzmich...' Poor Vyazmitinov could not get any farther! He began the
rescript again and again, but as soon as he uttered 'Sergey' he sobbed,
'Kuz-mi-ch,' tears, and 'From all sides' was smothered in sobs and he
could get no farther. And again his handkerchief, and again: 'Sergey
Kuzmich, From all sides,'... and tears, till at last somebody else was
asked to read it."

"Kuzmich... From all sides... and then tears," someone repeated
laughing.

"Don't be unkind," cried Anna Pavlovna from her end of the table holding
up a threatening finger. "He is such a worthy and excellent man, our
dear Vyazmitinov...."

Everybody laughed a great deal. At the head of the table, where the
honored guests sat, everyone seemed to be in high spirits and under the
influence of a variety of exciting sensations. Only Pierre and Helene
sat silently side by side almost at the bottom of the table, a
suppressed smile brightening both their faces, a smile that had nothing
to do with Sergey Kuzmich--a smile of bashfulness at their own feelings.
But much as all the rest laughed, talked, and joked, much as they
enjoyed their Rhine wine, saute, and ices, and however they avoided
looking at the young couple, and heedless and unobservant as they seemed
of them, one could feel by the occasional glances they gave that the
story about Sergey Kuzmich, the laughter, and the food were all a
pretense, and that the whole attention of that company was directed to--
Pierre and Helene. Prince Vasili mimicked the sobbing of Sergey Kuzmich
and at the same time his eyes glanced toward his daughter, and while he
laughed the expression on his face clearly said: "Yes... it's getting
on, it will all be settled today." Anna Pavlovna threatened him on
behalf of "our dear Vyazmitinov," and in her eyes, which, for an
instant, glanced at Pierre, Prince Vasili read a congratulation on his
future son-in-law and on his daughter's happiness. The old princess
sighed sadly as she offered some wine to the old lady next to her and
glanced angrily at her daughter, and her sigh seemed to say: "Yes,
there's nothing left for you and me but to sip sweet wine, my dear, now
that the time has come for these young ones to be thus boldly,
provocatively happy." "And what nonsense all this is that I am saying!"
thought a diplomatist, glancing at the happy faces of the lovers.
"That's happiness!"

Into the insignificant, trifling, and artificial interests uniting that
society had entered the simple feeling of the attraction of a healthy
and handsome young man and woman for one another. And this human feeling
dominated everything else and soared above all their affected chatter.
Jests fell flat, news was not interesting, and the animation was
evidently forced. Not only the guests but even the footmen waiting at
table seemed to feel this, and they forgot their duties as they looked
at the beautiful Helene with her radiant face and at the red, broad, and
happy though uneasy face of Pierre. It seemed as if the very light of
the candles was focused on those two happy faces alone.

Pierre felt that he was the center of it all, and this both pleased and
embarrassed him. He was like a man entirely absorbed in some occupation.
He did not see, hear, or understand anything clearly. Only now and then
detached ideas and impressions from the world of reality shot
unexpectedly through his mind.

"So it is all finished!" he thought. "And how has it all happened? How
quickly! Now I know that not because of her alone, nor of myself alone,
but because of everyone, it must inevitably come about. They are all
expecting it, they are so sure that it will happen that I cannot, I
cannot, disappoint them. But how will it be? I do not know, but it will
certainly happen!" thought Pierre, glancing at those dazzling shoulders
close to his eyes.

Or he would suddenly feel ashamed of he knew not what. He felt it
awkward to attract everyone's attention and to be considered a lucky man
and, with his plain face, to be looked on as a sort of Paris possessed
of a Helen. "But no doubt it always is and must be so!" he consoled
himself. "And besides, what have I done to bring it about? How did it
begin? I traveled from Moscow with Prince Vasili. Then there was
nothing. So why should I not stay at his house? Then I played cards with
her and picked up her reticule and drove out with her. How did it begin,
when did it all come about?" And here he was sitting by her side as her
betrothed, seeing, hearing, feeling her nearness, her breathing, her
movements, her beauty. Then it would suddenly seem to him that it was
not she but he was so unusually beautiful, and that that was why they
all looked so at him, and flattered by this general admiration he would
expand his chest, raise his head, and rejoice at his good fortune.
Suddenly he heard a familiar voice repeating something to him a second
time. But Pierre was so absorbed that he did not understand what was
said.

"I am asking you when you last heard from Bolkonski," repeated Prince
Vasili a third time. "How absent-minded you are, my dear fellow."

Prince Vasili smiled, and Pierre noticed that everyone was smiling at
him and Helene. "Well, what of it, if you all know it?" thought Pierre.
"What of it? It's the truth!" and he himself smiled his gentle childlike
smile, and Helene smiled too.

"When did you get the letter? Was it from Olmutz?" repeated Prince
Vasili, who pretended to want to know this in order to settle a dispute.

"How can one talk or think of such trifles?" thought Pierre.

"Yes, from Olmutz," he answered, with a sigh.

After supper Pierre with his partner followed the others into the
drawing room. The guests began to disperse, some without taking leave of
Helene. Some, as if unwilling to distract her from an important
occupation, came up to her for a moment and made haste to go away,
refusing to let her see them off. The diplomatist preserved a mournful
silence as he left the drawing room. He pictured the vanity of his
diplomatic career in comparison with Pierre's happiness. The old general
grumbled at his wife when she asked how his leg was. "Oh, the old fool,"
he thought. "That Princess Helene will be beautiful still when she's
fifty."

"I think I may congratulate you," whispered Anna Pavlovna to the old
princess, kissing her soundly. "If I hadn't this headache I'd have
stayed longer."

The old princess did not reply, she was tormented by jealousy of her
daughter's happiness.

While the guests were taking their leave Pierre remained for a long time
alone with Helene in the little drawing room where they were sitting. He
had often before, during the last six weeks, remained alone with her,
but had never spoken to her of love. Now he felt that it was inevitable,
but he could not make up his mind to take the final step. He felt
ashamed; he felt that he was occupying someone else's place here beside
Helene. "This happiness is not for you," some inner voice whispered to
him. "This happiness is for those who have not in them what there is in
you."

But, as he had to say something, he began by asking her whether she was
satisfied with the party. She replied in her usual simple manner that
this name day of hers had been one of the pleasantest she had ever had.

Some of the nearest relatives had not yet left. They were sitting in the
large drawing room. Prince Vasili came up to Pierre with languid
footsteps. Pierre rose and said it was getting late. Prince Vasili gave
him a look of stern inquiry, as though what Pierre had just said was so
strange that one could not take it in. But then the expression of
severity changed, and he drew Pierre's hand downwards, made him sit
down, and smiled affectionately.

"Well, Lelya?" he asked, turning instantly to his daughter and
addressing her with the careless tone of habitual tenderness natural to
parents who have petted their children from babyhood, but which Prince
Vasili had only acquired by imitating other parents.

And he again turned to Pierre.

"Sergey Kuzmich--From all sides-" he said, unbuttoning the top button of
his waistcoat.

Pierre smiled, but his smile showed that he knew it was not the story
about Sergey Kuzmich that interested Prince Vasili just then, and Prince
Vasili saw that Pierre knew this. He suddenly muttered something and
went away. It seemed to Pierre that even the prince was disconcerted.
The sight of the discomposure of that old man of the world touched
Pierre: he looked at Helene and she too seemed disconcerted, and her
look seemed to say: "Well, it is your own fault."

"The step must be taken but I cannot, I cannot!" thought Pierre, and he
again began speaking about indifferent matters, about Sergey Kuzmich,
asking what the point of the story was as he had not heard it properly.
Helene answered with a smile that she too had missed it.

When Prince Vasili returned to the drawing room, the princess, his wife,
was talking in low tones to the elderly lady about Pierre.

"Of course, it is a very brilliant match, but happiness, my dear..."

"Marriages are made in heaven," replied the elderly lady.

Prince Vasili passed by, seeming not to hear the ladies, and sat down on
a sofa in a far corner of the room. He closed his eyes and seemed to be
dozing. His head sank forward and then he roused himself.

"Aline," he said to his wife, "go and see what they are about."

The princess went up to the door, passed by it with a dignified and
indifferent air, and glanced into the little drawing room. Pierre and
Helene still sat talking just as before.

"Still the same," she said to her husband.

Prince Vasili frowned, twisting his mouth, his cheeks quivered and his
face assumed the coarse, unpleasant expression peculiar to him. Shaking
himself, he rose, threw back his head, and with resolute steps went past
the ladies into the little drawing room. With quick steps he went
joyfully up to Pierre. His face was so unusually triumphant that Pierre
rose in alarm on seeing it.

"Thank God!" said Prince Vasili. "My wife has told me everything!" (He
put one arm around Pierre and the other around his daughter.)--"My dear
boy... Lelya... I am very pleased." (His voice trembled.) "I loved your
father... and she will make you a good wife... God bless you!..."

He embraced his daughter, and then again Pierre, and kissed him with his
malodorous mouth. Tears actually moistened his cheeks.

"Princess, come here!" he shouted.

The old princess came in and also wept. The elderly lady was using her
handkerchief too. Pierre was kissed, and he kissed the beautiful
Helene's hand several times. After a while they were left alone again.

"All this had to be and could not be otherwise," thought Pierre, "so it
is useless to ask whether it is good or bad. It is good because it's
definite and one is rid of the old tormenting doubt." Pierre held the
hand of his betrothed in silence, looking at her beautiful bosom as it
rose and fell.

"Helene!" he said aloud and paused.

"Something special is always said in such cases," he thought, but could
not remember what it was that people say. He looked at her face. She
drew nearer to him. Her face flushed.

"Oh, take those off... those..." she said, pointing to his spectacles.

Pierre took them off, and his eyes, besides the strange look eyes have
from which spectacles have just been removed, had also a frightened and
inquiring look. He was about to stoop over her hand and kiss it, but
with a rapid, almost brutal movement of her head, she intercepted his
lips and met them with her own. Her face struck Pierre, by its altered,
unpleasantly excited expression.

"It is too late now, it's done; besides I love her," thought Pierre.

"Je vous aime!" * he said, remembering what has to be said at such
moments: but his words sounded so weak that he felt ashamed of himself.


* "I love you."

Six weeks later he was married, and settled in Count Bezukhov's large,
newly furnished Petersburg house, the happy possessor, as people said,
of a wife who was a celebrated beauty and of millions of money.




CHAPTER III

Old Prince Nicholas Bolkonski received a letter from Prince Vasili in
November, 1805, announcing that he and his son would be paying him a
visit. "I am starting on a journey of inspection, and of course I shall
think nothing of an extra seventy miles to come and see you at the same
time, my honored benefactor," wrote Prince Vasili. "My son Anatole is
accompanying me on his way to the army, so I hope you will allow him
personally to express the deep respect that, emulating his father, he
feels for you."

"It seems that there will be no need to bring Mary out, suitors are
coming to us of their own accord," incautiously remarked the little
princess on hearing the news.

Prince Nicholas frowned, but said nothing.

A fortnight after the letter Prince Vasili's servants came one evening
in advance of him, and he and his son arrived next day.

Old Bolkonski had always had a poor opinion of Prince Vasili's
character, but more so recently, since in the new reigns of Paul and
Alexander Prince Vasili had risen to high position and honors. And now,
from the hints contained in his letter and given by the little princess,
he saw which way the wind was blowing, and his low opinion changed into
a feeling of contemptuous ill will. He snorted whenever he mentioned
him. On the day of Prince Vasili's arrival, Prince Bolkonski was
particularly discontented and out of temper. Whether he was in a bad
temper because Prince Vasili was coming, or whether his being in a bad
temper made him specially annoyed at Prince Vasili's visit, he was in a
bad temper, and in the morning Tikhon had already advised the architect
not to go to the prince with his report.

"Do you hear how he's walking?" said Tikhon, drawing the architect's
attention to the sound of the prince's footsteps. "Stepping flat on his
heels--we know what that means...."

However, at nine o'clock the prince, in his velvet coat with a sable
collar and cap, went out for his usual walk. It had snowed the day
before and the path to the hothouse, along which the prince was in the
habit of walking, had been swept: the marks of the broom were still
visible in the snow and a shovel had been left sticking in one of the
soft snowbanks that bordered both sides of the path. The prince went
through the conservatories, the serfs' quarters, and the outbuildings,
frowning and silent.

"Can a sleigh pass?" he asked his overseer, a venerable man, resembling
his master in manners and looks, who was accompanying him back to the
house.

"The snow is deep. I am having the avenue swept, your honor."

The prince bowed his head and went up to the porch. "God be thanked,"
thought the overseer, "the storm has blown over!"

"It would have been hard to drive up, your honor," he added. "I heard,
your honor, that a minister is coming to visit your honor."

The prince turned round to the overseer and fixed his eyes on him,
frowning.

"What? A minister? What minister? Who gave orders?" he said in his
shrill, harsh voice. "The road is not swept for the princess my
daughter, but for a minister! For me, there are no ministers!"

"Your honor, I thought..."

"You thought!" shouted the prince, his words coming more and more
rapidly and indistinctly. "You thought!... Rascals! Blackguards!... I'll
teach you to think!" and lifting his stick he swung it and would have
hit Alpatych, the overseer, had not the latter instinctively avoided the
blow. "Thought... Blackguards..." shouted the prince rapidly.

But although Alpatych, frightened at his own temerity in avoiding the
stroke, came up to the prince, bowing his bald head resignedly before
him, or perhaps for that very reason, the prince, though he continued to
shout: "Blackguards!... Throw the snow back on the road!" did not lift
his stick again but hurried into the house.

Before dinner, Princess Mary and Mademoiselle Bourienne, who knew that
the prince was in a bad humor, stood awaiting him; Mademoiselle
Bourienne with a radiant face that said: "I know nothing, I am the same
as usual," and Princess Mary pale, frightened, and with downcast eyes.
What she found hardest to bear was to know that on such occasions she
ought to behave like Mademoiselle Bourienne, but could not. She thought:
"If I seem not to notice he will think that I do not sympathize with
him; if I seem sad and out of spirits myself, he will say (as he has
done before) that I'm in the dumps."

The prince looked at his daughter's frightened face and snorted.

"Fool... or dummy!" he muttered.

"And the other one is not here. They've been telling tales," he thought-
-referring to the little princess who was not in the dining room.

"Where is the princess?" he asked. "Hiding?"

"She is not very well," answered Mademoiselle Bourienne with a bright
smile, "so she won't come down. It is natural in her state."

"Hm! Hm!" muttered the prince, sitting down.

His plate seemed to him not quite clean, and pointing to a spot he flung
it away. Tikhon caught it and handed it to a footman. The little
princess was not unwell, but had such an overpowering fear of the prince
that, hearing he was in a bad humor, she had decided not to appear.

"I am afraid for the baby," she said to Mademoiselle Bourienne: "Heaven
knows what a fright might do."

In general at Bald Hills the little princess lived in constant fear, and
with a sense of antipathy to the old prince which she did not realize
because the fear was so much the stronger feeling. The prince
reciprocated this antipathy, but it was overpowered by his contempt for
her. When the little princess had grown accustomed to life at Bald
Hills, she took a special fancy to Mademoiselle Bourienne, spent whole
days with her, asked her to sleep in her room, and often talked with her
about the old prince and criticized him.

"So we are to have visitors, mon prince?" remarked Mademoiselle
Bourienne, unfolding her white napkin with her rosy fingers. "His
Excellency Prince Vasili Kuragin and his son, I understand?" she said
inquiringly.

"Hm!--his excellency is a puppy.... I got him his appointment in the
service," said the prince disdainfully. "Why his son is coming I don't
understand. Perhaps Princess Elizabeth and Princess Mary know. I don't
want him." (He looked at his blushing daughter.) "Are you unwell today?
Eh? Afraid of the 'minister' as that idiot Alpatych called him this
morning?"

"No, mon pere."

Though Mademoiselle Bourienne had been so unsuccessful in her choice of
a subject, she did not stop talking, but chattered about the
conservatories and the beauty of a flower that had just opened, and
after the soup the prince became more genial.

After dinner, he went to see his daughter-in-law. The little princess
was sitting at a small table, chattering with Masha, her maid. She grew
pale on seeing her father-in-law.

She was much altered. She was now plain rather than pretty. Her cheeks
had sunk, her lip was drawn up, and her eyes drawn down.

"Yes, I feel a kind of oppression," she said in reply to the prince's
question as to how she felt.

"Do you want anything?"

"No, merci, mon pere."

"Well, all right, all right."

He left the room and went to the waiting room where Alpatych stood with
bowed head.

"Has the snow been shoveled back?"

"Yes, your excellency. Forgive me for heaven's sake... It was only my
stupidity."

"All right, all right," interrupted the prince, and laughing his
unnatural way, he stretched out his hand for Alpatych to kiss, and then
proceeded to his study.

Prince Vasili arrived that evening. He was met in the avenue by coachmen
and footmen, who, with loud shouts, dragged his sleighs up to one of the
lodges over the road purposely laden with snow.

Prince Vasili and Anatole had separate rooms assigned to them.

Anatole, having taken off his overcoat, sat with arms akimbo before a
table on a corner of which he smilingly and absent-mindedly fixed his
large and handsome eyes. He regarded his whole life as a continual round
of amusement which someone for some reason had to provide for him. And
he looked on this visit to a churlish old man and a rich and ugly
heiress in the same way. All this might, he thought, turn out very well
and amusingly. "And why not marry her if she really has so much money?
That never does any harm," thought Anatole.

He shaved and scented himself with the care and elegance which had
become habitual to him and, his handsome head held high, entered his
father's room with the good-humored and victorious air natural to him.
Prince Vasili's two valets were busy dressing him, and he looked round
with much animation and cheerfully nodded to his son as the latter
entered, as if to say: "Yes, that's how I want you to look."

"I say, Father, joking apart, is she very hideous?" Anatole asked, as if
continuing a conversation the subject of which had often been mentioned
during the journey.

"Enough! What nonsense! Above all, try to be respectful and cautious
with the old prince."

"If he starts a row I'll go away," said Prince Anatole. "I can't bear
those old men! Eh?"

"Remember, for you everything depends on this."

In the meantime, not only was it known in the maidservants' rooms that
the minister and his son had arrived, but the appearance of both had
been minutely described. Princess Mary was sitting alone in her room,
vainly trying to master her agitation.

"Why did they write, why did Lise tell me about it? It can never
happen!" she said, looking at herself in the glass. "How shall I enter
the drawing room? Even if I like him I can't now be myself with him."
The mere thought of her father's look filled her with terror. The little
princess and Mademoiselle Bourienne had already received from Masha, the
lady's maid, the necessary report of how handsome the minister's son
was, with his rosy cheeks and dark eyebrows, and with what difficulty
the father had dragged his legs upstairs while the son had followed him
like an eagle, three steps at a time. Having received this information,
the little princess and Mademoiselle Bourienne, whose chattering voices
had reached her from the corridor, went into Princess Mary's room.

"You know they've come, Marie?" said the little princess, waddling in,
and sinking heavily into an armchair.

She was no longer in the loose gown she generally wore in the morning,
but had on one of her best dresses. Her hair was carefully done and her
face was animated, which, however, did not conceal its sunken and faded
outlines. Dressed as she used to be in Petersburg society, it was still
more noticeable how much plainer she had become. Some unobtrusive touch
had been added to Mademoiselle Bourienne's toilet which rendered her
fresh and pretty face yet more attractive.

"What! Are you going to remain as you are, dear princess?" she began.
"They'll be announcing that the gentlemen are in the drawing room and we
shall have to go down, and you have not smartened yourself up at all!"

The little princess got up, rang for the maid, and hurriedly and merrily
began to devise and carry out a plan of how Princess Mary should be
dressed. Princess Mary's self-esteem was wounded by the fact that the
arrival of a suitor agitated her, and still more so by both her
companions' not having the least conception that it could be otherwise.
To tell them that she felt ashamed for herself and for them would be to
betray her agitation, while to decline their offers to dress her would
prolong their banter and insistence. She flushed, her beautiful eyes
grew dim, red blotches came on her face, and it took on the unattractive
martyrlike expression it so often wore, as she submitted herself to
Mademoiselle Bourienne and Lise. Both these women quite sincerely tried
to make her look pretty. She was so plain that neither of them could
think of her as a rival, so they began dressing her with perfect
sincerity, and with the naive and firm conviction women have that dress
can make a face pretty.

"No really, my dear, this dress is not pretty," said Lise, looking
sideways at Princess Mary from a little distance. "You have a maroon
dress, have it fetched. Really! You know the fate of your whole life may
be at stake. But this one is too light, it's not becoming!"

It was not the dress, but the face and whole figure of Princess Mary
that was not pretty, but neither Mademoiselle Bourienne nor the little
princess felt this; they still thought that if a blue ribbon were placed
in the hair, the hair combed up, and the blue scarf arranged lower on
the best maroon dress, and so on, all would be well. They forgot that
the frightened face and the figure could not be altered, and that
however they might change the setting and adornment of that face, it
would still remain piteous and plain. After two or three changes to
which Princess Mary meekly submitted, just as her hair had been arranged
on the top of her head (a style that quite altered and spoiled her
looks) and she had put on a maroon dress with a pale-blue scarf, the
little princess walked twice round her, now adjusting a fold of the
dress with her little hand, now arranging the scarf and looking at her
with her head bent first on one side and then on the other.

"No, it will not do," she said decidedly, clasping her hands. "No, Mary,
really this dress does not suit you. I prefer you in your little gray
everyday dress. Now please, do it for my sake. Katie," she said to the
maid, "bring the princess her gray dress, and you'll see, Mademoiselle
Bourienne, how I shall arrange it," she added, smiling with a foretaste
of artistic pleasure.

But when Katie brought the required dress, Princess Mary remained
sitting motionless before the glass, looking at her face, and saw in the
mirror her eyes full of tears and her mouth quivering, ready to burst
into sobs.

"Come, dear princess," said Mademoiselle Bourienne, "just one more
little effort."

The little princess, taking the dress from the maid, came up to Princess
Mary.

"Well, now we'll arrange something quite simple and becoming," she said.

The three voices, hers, Mademoiselle Bourienne's, and Katie's, who was
laughing at something, mingled in a merry sound, like the chirping of
birds.

"No, leave me alone," said Princess Mary.

Her voice sounded so serious and so sad that the chirping of the birds
was silenced at once. They looked at the beautiful, large, thoughtful
eyes full of tears and of thoughts, gazing shiningly and imploringly at
them, and understood that it was useless and even cruel to insist.

"At least, change your coiffure," said the little princess. "Didn't I
tell you," she went on, turning reproachfully to Mademoiselle Bourienne,
"Mary's is a face which such a coiffure does not suit in the least. Not
in the least! Please change it."

"Leave me alone, please leave me alone! It is all quite the same to me,"
answered a voice struggling with tears.

Mademoiselle Bourienne and the little princess had to own to themselves
that Princess Mary in this guise looked very plain, worse than usual,
but it was too late. She was looking at them with an expression they
both knew, an expression thoughtful and sad. This expression in Princess
Mary did not frighten them (she never inspired fear in anyone), but they
knew that when it appeared on her face, she became mute and was not to
be shaken in her determination.

"You will change it, won't you?" said Lise. And as Princess Mary gave no
answer, she left the room.

Princess Mary was left alone. She did not comply with Lise's request,
she not only left her hair as it was, but did not even look in her
glass. Letting her arms fall helplessly, she sat with downcast eyes and
pondered. A husband, a man, a strong dominant and strangely attractive
being rose in her imagination, and carried her into a totally different
happy world of his own. She fancied a child, her own--such as she had
seen the day before in the arms of her nurse's daughter--at her own
breast, the husband standing by and gazing tenderly at her and the
child. "But no, it is impossible, I am too ugly," she thought.

"Please come to tea. The prince will be out in a moment," came the
maid's voice at the door.

She roused herself, and felt appalled at what she had been thinking, and
before going down she went into the room where the icons hung and, her
eyes fixed on the dark face of a large icon of the Saviour lit by a
lamp, she stood before it with folded hands for a few moments. A painful
doubt filled her soul. Could the joy of love, of earthly love for a man,
be for her? In her thoughts of marriage Princess Mary dreamed of
happiness and of children, but her strongest, most deeply hidden longing
was for earthly love. The more she tried to hide this feeling from
others and even from herself, the stronger it grew. "O God," she said,
"how am I to stifle in my heart these temptations of the devil? How am I
to renounce forever these vile fancies, so as peacefully to fulfill Thy
will?" And scarcely had she put that question than God gave her the
answer in her own heart. "Desire nothing for thyself, seek nothing, be
not anxious or envious. Man's future and thy own fate must remain hidden
from thee, but live so that thou mayest be ready for anything. If it be
God's will to prove thee in the duties of marriage, be ready to fulfill
His will." With this consoling thought (but yet with a hope for the
fulfillment of her forbidden earthly longing) Princess Mary sighed, and
having crossed herself went down, thinking neither of her gown and
coiffure nor of how she would go in nor of what she would say. What
could all that matter in comparison with the will of God, without Whose
care not a hair of man's head can fall?




CHAPTER IV

When Princess Mary came down, Prince Vasili and his son were already in
the drawing room, talking to the little princess and Mademoiselle
Bourienne. When she entered with her heavy step, treading on her heels,
the gentlemen and Mademoiselle Bourienne rose and the little princess,
indicating her to the gentlemen, said: "Voila Marie!" Princess Mary saw
them all and saw them in detail. She saw Prince Vasili's face, serious
for an instant at the sight of her, but immediately smiling again, and
the little princess curiously noting the impression "Marie" produced on
the visitors. And she saw Mademoiselle Bourienne, with her ribbon and
pretty face, and her unusually animated look which was fixed on him, but
him she could not see, she only saw something large, brilliant, and
handsome moving toward her as she entered the room. Prince Vasili
approached first, and she kissed the bold forehead that bent over her
hand and answered his question by saying that, on the contrary, she
remembered him quite well. Then Anatole came up to her. She still could
not see him. She only felt a soft hand taking hers firmly, and she
touched with her lips a white forehead, over which was beautiful light-
brown hair smelling of pomade. When she looked up at him she was struck
by his beauty. Anatole stood with his right thumb under a button of his
uniform, his chest expanded and his back drawn in, slightly swinging one
foot, and, with his head a little bent, looked with beaming face at the
princess without speaking and evidently not thinking about her at all.
Anatole was not quick-witted, nor ready or eloquent in conversation, but
he had the faculty, so invaluable in society, of composure and
imperturbable self-possession. If a man lacking in self-confidence
remains dumb on a first introduction and betrays a consciousness of the
impropriety of such silence and an anxiety to find something to say, the
effect is bad. But Anatole was dumb, swung his foot, and smilingly
examined the princess' hair. It was evident that he could be silent in
this way for a very long time. "If anyone finds this silence
inconvenient, let him talk, but I don't want to," he seemed to say.
Besides this, in his behavior to women Anatole had a manner which
particularly inspires in them curiosity, awe, and even love--a
supercilious consciousness of his own superiority. It was as if he said
to them: "I know you, I know you, but why should I bother about you?
You'd be only too glad, of course." Perhaps he did not really think this
when he met women--even probably he did not, for in general he thought
very little--but his looks and manner gave that impression. The princess
felt this, and as if wishing to show him that she did not even dare
expect to interest him, she turned to his father. The conversation was
general and animated, thanks to Princess Lise's voice and little downy
lip that lifted over her white teeth. She met Prince Vasili with that
playful manner often employed by lively chatty people, and consisting in
the assumption that between the person they so address and themselves
there are some semi-private, long-established jokes and amusing
reminiscences, though no such reminiscences really exist--just as none
existed in this case. Prince Vasili readily adopted her tone and the
little princess also drew Anatole, whom she hardly knew, into these
amusing recollections of things that had never occurred. Mademoiselle
Bourienne also shared them and even Princess Mary felt herself
pleasantly made to share in these merry reminiscences.

"Here at least we shall have the benefit of your company all to
ourselves, dear prince," said the little princess (of course, in French)
to Prince Vasili. "It's not as at Annette's * receptions where you
always ran away; you remember cette chere Annette!"


* Anna Pavlovna.

"Ah, but you won't talk politics to me like Annette!"

"And our little tea table?"

"Oh, yes!"

"Why is it you were never at Annette's?" the little princess asked
Anatole. "Ah, I know, I know," she said with a sly glance, "your brother
Hippolyte told me about your goings on. Oh!" and she shook her finger at
him, "I have even heard of your doings in Paris!"

"And didn't Hippolyte tell you?" asked Prince Vasili, turning to his son
and seizing the little princess' arm as if she would have run away and
he had just managed to catch her, "didn't he tell you how he himself was
pining for the dear princess, and how she showed him the door? Oh, she
is a pearl among women, Princess," he added, turning to Princess Mary.

When Paris was mentioned, Mademoiselle Bourienne for her part seized the
opportunity of joining in the general current of recollections.

She took the liberty of inquiring whether it was long since Anatole had
left Paris and how he had liked that city. Anatole answered the
Frenchwoman very readily and, looking at her with a smile, talked to her
about her native land. When he saw the pretty little Bourienne, Anatole
came to the conclusion that he would not find Bald Hills dull either.
"Not at all bad!" he thought, examining her, "not at all bad, that
little companion! I hope she will bring her along with her when we're
married, la petite est gentille." *


* The little one is charming.

The old prince dressed leisurely in his study, frowning and considering
what he was to do. The coming of these visitors annoyed him. "What are
Prince Vasili and that son of his to me? Prince Vasili is a shallow
braggart and his son, no doubt, is a fine specimen," he grumbled to
himself. What angered him was that the coming of these visitors revived
in his mind an unsettled question he always tried to stifle, one about
which he always deceived himself. The question was whether he could ever
bring himself to part from his daughter and give her to a husband. The
prince never directly asked himself that question, knowing beforehand
that he would have to answer it justly, and justice clashed not only
with his feelings but with the very possibility of life. Life without
Princess Mary, little as he seemed to value her, was unthinkable to him.
"And why should she marry?" he thought. "To be unhappy for certain.
There's Lise, married to Andrew--a better husband one would think could
hardly be found nowadays--but is she contented with her lot? And who
would marry Marie for love? Plain and awkward! They'll take her for her
connections and wealth. Are there no women living unmarried, and even
the happier for it?" So thought Prince Bolkonski while dressing, and yet
the question he was always putting off demanded an immediate answer.
Prince Vasili had brought his son with the evident intention of
proposing, and today or tomorrow he would probably ask for an answer.
His birth and position in society were not bad. "Well, I've nothing
against it," the prince said to himself, "but he must be worthy of her.
And that is what we shall see."

"That is what we shall see! That is what we shall see!" he added aloud.

He entered the drawing room with his usual alert step, glancing rapidly
round the company. He noticed the change in the little princess' dress,
Mademoiselle Bourienne's ribbon, Princess Mary's unbecoming coiffure,
Mademoiselle Bourienne's and Anatole's smiles, and the loneliness of his
daughter amid the general conversation. "Got herself up like a fool!" he
thought, looking irritably at her. "She is shameless, and he ignores
her!"

He went straight up to Prince Vasili.

"Well! How d'ye do? How d'ye do? Glad to see you!"

"Friendship laughs at distance," began Prince Vasili in his usual rapid,
self-confident, familiar tone. "Here is my second son; please love and
befriend him."

Prince Bolkonski surveyed Anatole.

"Fine young fellow! Fine young fellow!" he said. "Well, come and kiss
me," and he offered his cheek.

Anatole kissed the old man, and looked at him with curiosity and perfect
composure, waiting for a display of the eccentricities his father had
told him to expect.

Prince Bolkonski sat down in his usual place in the corner of the sofa
and, drawing up an armchair for Prince Vasili, pointed to it and began
questioning him about political affairs and news. He seemed to listen
attentively to what Prince Vasili said, but kept glancing at Princess
Mary.

"And so they are writing from Potsdam already?" he said, repeating
Prince Vasili's last words. Then rising, he suddenly went up to his
daughter.

"Is it for visitors you've got yourself up like that, eh?" said he.
"Fine, very fine! You have done up your hair in this new way for the
visitors, and before the visitors I tell you that in future you are
never to dare to change your way of dress without my consent."

"It was my fault, mon pere," interceded the little princess, with a
blush.

"You must do as you please," said Prince Bolkonski, bowing to his
daughter-in-law, "but she need not make a fool of herself, she's plain
enough as it is."

And he sat down again, paying no more attention to his daughter, who was
reduced to tears.

"On the contrary, that coiffure suits the princess very well," said
Prince Vasili.

"Now you, young prince, what's your name?" said Prince Bolkonski,
turning to Anatole, "come here, let us talk and get acquainted."

"Now the fun begins," thought Anatole, sitting down with a smile beside
the old prince.

"Well, my dear boy, I hear you've been educated abroad, not taught to
read and write by the deacon, like your father and me. Now tell me, my
dear boy, are you serving in the Horse Guards?" asked the old man,
scrutinizing Anatole closely and intently.

"No, I have been transferred to the line," said Anatole, hardly able to
restrain his laughter.

"Ah! That's a good thing. So, my dear boy, you wish to serve the Tsar
and the country? It is wartime. Such a fine fellow must serve. Well, are
you off to the front?"

"No, Prince, our regiment has gone to the front, but I am attached...
what is it I am attached to, Papa?" said Anatole, turning to his father
with a laugh.

"A splendid soldier, splendid! 'What am I attached to!' Ha, ha, ha!"
laughed Prince Bolkonski, and Anatole laughed still louder. Suddenly
Prince Bolkonski frowned.

"You may go," he said to Anatole.

Anatole returned smiling to the ladies.

"And so you've had him educated abroad, Prince Vasili, haven't you?"
said the old prince to Prince Vasili.

"I have done my best for him, and I can assure you the education there
is much better than ours."

"Yes, everything is different nowadays, everything is changed. The lad's
a fine fellow, a fine fellow! Well, come with me now." He took Prince
Vasili's arm and led him to his study. As soon as they were alone
together, Prince Vasili announced his hopes and wishes to the old
prince.

"Well, do you think I shall prevent her, that I can't part from her?"
said the old prince angrily. "What an idea! I'm ready for it tomorrow!
Only let me tell you, I want to know my son-in-law better. You know my
principles--everything aboveboard? I will ask her tomorrow in your
presence; if she is willing, then he can stay on. He can stay and I'll
see." The old prince snorted. "Let her marry, it's all the same to me!"
he screamed in the same piercing tone as when parting from his son.

"I will tell you frankly," said Prince Vasili in the tone of a crafty
man convinced of the futility of being cunning with so keen-sighted a
companion. "You know, you see right through people. Anatole is no
genius, but he is an honest, goodhearted lad; an excellent son or
kinsman."

"All right, all right, we'll see!"

As always happens when women lead lonely lives for any length of time
without male society, on Anatole's appearance all the three women of
Prince Bolkonski's household felt that their life had not been real till
then. Their powers of reasoning, feeling, and observing immediately
increased tenfold, and their life, which seemed to have been passed in
darkness, was suddenly lit up by a new brightness, full of significance.

Princess Mary grew quite unconscious of her face and coiffure. The
handsome open face of the man who might perhaps be her husband absorbed
all her attention. He seemed to her kind, brave, determined, manly, and
magnanimous. She felt convinced of that. Thousands of dreams of a future
family life continually rose in her imagination. She drove them away and
tried to conceal them.

"But am I not too cold with him?" thought the princess. "I try to be
reserved because in the depth of my soul I feel too near to him already,
but then he cannot know what I think of him and may imagine that I do
not like him."

And Princess Mary tried, but could not manage, to be cordial to her new
guest. "Poor girl, she's devilish ugly!" thought Anatole.

Mademoiselle Bourienne, also roused to great excitement by Anatole's
arrival, thought in another way. Of course, she, a handsome young woman
without any definite position, without relations or even a country, did
not intend to devote her life to serving Prince Bolkonski, to reading
aloud to him and being friends with Princess Mary. Mademoiselle
Bourienne had long been waiting for a Russian prince who, able to
appreciate at a glance her superiority to the plain, badly dressed,
ungainly Russian princesses, would fall in love with her and carry her
off; and here at last was a Russian prince. Mademoiselle Bourienne knew
a story, heard from her aunt but finished in her own way, which she
liked to repeat to herself. It was the story of a girl who had been
seduced, and to whom her poor mother (sa pauvre mere) appeared, and
reproached her for yielding to a man without being married. Mademoiselle
Bourienne was often touched to tears as in imagination she told this
story to him, her seducer. And now he, a real Russian prince, had
appeared. He would carry her away and then sa pauvre mere would appear
and he would marry her. So her future shaped itself in Mademoiselle
Bourienne's head at the very time she was talking to Anatole about
Paris. It was not calculation that guided her (she did not even for a
moment consider what she should do), but all this had long been familiar
to her, and now that Anatole had appeared it just grouped itself around
him and she wished and tried to please him as much as possible.

The little princess, like an old war horse that hears the trumpet,
unconsciously and quite forgetting her condition, prepared for the
familiar gallop of coquetry, without any ulterior motive or any
struggle, but with naive and lighthearted gaiety.

Although in female society Anatole usually assumed the role of a man
tired of being run after by women, his vanity was flattered by the
spectacle of his power over these three women. Besides that, he was
beginning to feel for the pretty and provocative Mademoiselle Bourienne
that passionate animal feeling which was apt to master him with great
suddenness and prompt him to the coarsest and most reckless actions.

After tea, the company went into the sitting room and Princess Mary was
asked to play on the clavichord. Anatole, laughing and in high spirits,
came and leaned on his elbows, facing her and beside Mademoiselle
Bourienne. Princess Mary felt his look with a painfully joyous emotion.
Her favorite sonata bore her into a most intimately poetic world and the
look she felt upon her made that world still more poetic. But Anatole's
expression, though his eyes were fixed on her, referred not to her but
to the movements of Mademoiselle Bourienne's little foot, which he was
then touching with his own under the clavichord. Mademoiselle Bourienne
was also looking at Princess Mary, and in her lovely eyes there was a
look of fearful joy and hope that was also new to the princess.

"How she loves me!" thought Princess Mary. "How happy I am now, and how
happy I may be with such a friend and such a husband! Husband? Can it be
possible?" she thought, not daring to look at his face, but still
feeling his eyes gazing at her.

In the evening, after supper, when all were about to retire, Anatole
kissed Princess Mary's hand. She did not know how she found the courage,
but she looked straight into his handsome face as it came near to her
shortsighted eyes. Turning from Princess Mary he went up and kissed
Mademoiselle Bourienne's hand. (This was not etiquette, but then he did
everything so simply and with such assurance!) Mademoiselle Bourienne
flushed, and gave the princess a frightened look.

"What delicacy!" thought the princess. "Is it possible that Amelie"
(Mademoiselle Bourienne) "thinks I could be jealous of her, and not
value her pure affection and devotion to me?" She went up to her and
kissed her warmly. Anatole went up to kiss the little princess' hand.

"No! No! No! When your father writes to tell me that you are behaving
well I will give you my hand to kiss. Not till then!" she said. And
smilingly raising a finger at him, she left the room.




CHAPTER V

They all separated, but, except Anatole who fell asleep as soon as he
got into bed, all kept awake a long time that night.

"Is he really to be my husband, this stranger who is so kind--yes, kind,
that is the chief thing," thought Princess Mary; and fear, which she had
seldom experienced, came upon her. She feared to look round, it seemed
to her that someone was there standing behind the screen in the dark
corner. And this someone was he--the devil--and he was also this man
with the white forehead, black eyebrows, and red lips.

She rang for her maid and asked her to sleep in her room.

Mademoiselle Bourienne walked up and down the conservatory for a long
time that evening, vainly expecting someone, now smiling at someone, now
working herself up to tears with the imaginary words of her pauvre mere
rebuking her for her fall.

The little princess grumbled to her maid that her bed was badly made.
She could not lie either on her face or on her side. Every position was
awkward and uncomfortable, and her burden oppressed her now more than
ever because Anatole's presence had vividly recalled to her the time
when she was not like that and when everything was light and gay. She
sat in an armchair in her dressing jacket and nightcap and Katie, sleepy
and disheveled, beat and turned the heavy feather bed for the third
time, muttering to herself.

"I told you it was all lumps and holes!" the little princess repeated.
"I should be glad enough to fall asleep, so it's not my fault!" and her
voice quivered like that of a child about to cry.

The old prince did not sleep either. Tikhon, half asleep, heard him
pacing angrily about and snorting. The old prince felt as though he had
been insulted through his daughter. The insult was the more pointed
because it concerned not himself but another, his daughter, whom he
loved more than himself. He kept telling himself that he would consider
the whole matter and decide what was right and how he should act, but
instead of that he only excited himself more and more.

"The first man that turns up--she forgets her father and everything
else, runs upstairs and does up her hair and wags her tail and is unlike
herself! Glad to throw her father over! And she knew I should notice it.
Fr... fr... fr! And don't I see that that idiot had eyes only for
Bourienne--I shall have to get rid of her. And how is it she has not
pride enough to see it? If she has no pride for herself she might at
least have some for my sake! She must be shown that the blockhead thinks
nothing of her and looks only at Bourienne. No, she has no pride... but
I'll let her see...."

The old prince knew that if he told his daughter she was making a
mistake and that Anatole meant to flirt with Mademoiselle Bourienne,
Princess Mary's self-esteem would be wounded and his point (not to be
parted from her) would be gained, so pacifying himself with this
thought, he called Tikhon and began to undress.

"What devil brought them here?" thought he, while Tikhon was putting the
nightshirt over his dried-up old body and gray-haired chest. "I never
invited them. They came to disturb my life--and there is not much of it
left."

"Devil take 'em!" he muttered, while his head was still covered by the
shirt.

Tikhon knew his master's habit of sometimes thinking aloud, and
therefore met with unaltered looks the angrily inquisitive expression of
the face that emerged from the shirt.

"Gone to bed?" asked the prince.

Tikhon, like all good valets, instinctively knew the direction of his
master's thoughts. He guessed that the question referred to Prince
Vasili and his son.

"They have gone to bed and put out their lights, your excellency."

"No good... no good..." said the prince rapidly, and thrusting his feet
into his slippers and his arms into the sleeves of his dressing gown, he
went to the couch on which he slept.

Though no words had passed between Anatole and Mademoiselle Bourienne,
they quite understood one another as to the first part of their romance,
up to the appearance of the pauvre mere; they understood that they had
much to say to one another in private and so they had been seeking an
opportunity since morning to meet one another alone. When Princess Mary
went to her father's room at the usual hour, Mademoiselle Bourienne and
Anatole met in the conservatory.

Princess Mary went to the door of the study with special trepidation. It
seemed to her that not only did everybody know that her fate would be
decided that day, but that they also knew what she thought about it. She
read this in Tikhon's face and in that of Prince Vasili's valet, who
made her a low bow when she met him in the corridor carrying hot water.

The old prince was very affectionate and careful in his treatment of his
daughter that morning. Princess Mary well knew this painstaking
expression of her father's. His face wore that expression when his dry
hands clenched with vexation at her not understanding a sum in
arithmetic, when rising from his chair he would walk away from her,
repeating in a low voice the same words several times over.

He came to the point at once, treating her ceremoniously.

"I have had a proposition made me concerning you," he said with an
unnatural smile. "I expect you have guessed that Prince Vasili has not
come and brought his pupil with him" (for some reason Prince Bolkonski
referred to Anatole as a "pupil") "for the sake of my beautiful eyes.
Last night a proposition was made me on your account and, as you know my
principles, I refer it to you."

"How am I to understand you, mon pere?" said the princess, growing pale
and then blushing.

"How understand me!" cried her father angrily. "Prince Vasili finds you
to his taste as a daughter-in-law and makes a proposal to you on his
pupil's behalf. That's how it's to be understood! 'How understand
it'!... And I ask you!"

"I do not know what you think, Father," whispered the princess.

"I? I? What of me? Leave me out of the question. I'm not going to get
married. What about you? That's what I want to know."

The princess saw that her father regarded the matter with disapproval,
but at that moment the thought occurred to her that her fate would be
decided now or never. She lowered her eyes so as not to see the gaze
under which she felt that she could not think, but would only be able to
submit from habit, and she said: "I wish only to do your will, but if I
had to express my own desire..." She had no time to finish. The old
prince interrupted her.

"That's admirable!" he shouted. "He will take you with your dowry and
take Mademoiselle Bourienne into the bargain. She'll be the wife, while
you..."

The prince stopped. He saw the effect these words had produced on his
daughter. She lowered her head and was ready to burst into tears.

"Now then, now then, I'm only joking!" he said. "Remember this,
Princess, I hold to the principle that a maiden has a full right to
choose. I give you freedom. Only remember that your life's happiness
depends on your decision. Never mind me!"

"But I do not know, Father!"

"There's no need to talk! He receives his orders and will marry you or
anybody; but you are free to choose.... Go to your room, think it over,
and come back in an hour and tell me in his presence: yes or no. I know
you will pray over it. Well, pray if you like, but you had better think
it over. Go! Yes or no, yes or no, yes or no!" he still shouted when the
princess, as if lost in a fog, had already staggered out of the study.

Her fate was decided and happily decided. But what her father had said
about Mademoiselle Bourienne was dreadful. It was untrue to be sure, but
still it was terrible, and she could not help thinking of it. She was
going straight on through the conservatory, neither seeing nor hearing
anything, when suddenly the well-known whispering of Mademoiselle
Bourienne aroused her. She raised her eyes, and two steps away saw
Anatole embracing the Frenchwoman and whispering something to her. With
a horrified expression on his handsome face, Anatole looked at Princess
Mary, but did not at once take his arm from the waist of Mademoiselle
Bourienne who had not yet seen her.

"Who's that? Why? Wait a moment!" Anatole's face seemed to say. Princess
Mary looked at them in silence. She could not understand it. At last
Mademoiselle Bourienne gave a scream and ran away. Anatole bowed to
Princess Mary with a gay smile, as if inviting her to join in a laugh at
this strange incident, and then shrugging his shoulders went to the door
that led to his own apartments.

An hour later, Tikhon came to call Princess Mary to the old prince; he
added that Prince Vasili was also there. When Tikhon came to her
Princess Mary was sitting on the sofa in her room, holding the weeping
Mademoiselle Bourienne in her arms and gently stroking her hair. The
princess' beautiful eyes with all their former calm radiance were
looking with tender affection and pity at Mademoiselle Bourienne's
pretty face.

"No, Princess, I have lost your affection forever!" said Mademoiselle
Bourienne.

"Why? I love you more than ever," said Princess Mary, "and I will try to
do all I can for your happiness."

"But you despise me. You who are so pure can never understand being so
carried away by passion. Oh, only my poor mother..."

"I quite understand," answered Princess Mary, with a sad smile. "Calm
yourself, my dear. I will go to my father," she said, and went out.

Prince Vasili, with one leg thrown high over the other and a snuffbox in
his hand, was sitting there with a smile of deep emotion on his face, as
if stirred to his heart's core and himself regretting and laughing at
his own sensibility, when Princess Mary entered. He hurriedly took a
pinch of snuff.

"Ah, my dear, my dear!" he began, rising and taking her by both hands.
Then, sighing, he added: "My son's fate is in your hands. Decide, my
dear, good, gentle Marie, whom I have always loved as a daughter!"

He drew back and a real tear appeared in his eye.

"Fr... fr..." snorted Prince Bolkonski. "The prince is making a
proposition to you in his pupil's--I mean, his son's--name. Do you wish
or not to be Prince Anatole Kuragin's wife? Reply: yes or no," he
shouted, "and then I shall reserve the right to state my opinion also.
Yes, my opinion, and only my opinion," added Prince Bolkonski, turning
to Prince Vasili and answering his imploring look. "Yes, or no?"

"My desire is never to leave you, Father, never to separate my life from
yours. I don't wish to marry," she answered positively, glancing at
Prince Vasili and at her father with her beautiful eyes.

"Humbug! Nonsense! Humbug, humbug, humbug!" cried Prince Bolkonski,
frowning and taking his daughter's hand; he did not kiss her, but only
bending his forehead to hers just touched it, and pressed her hand so
that she winced and uttered a cry.

Prince Vasili rose.

"My dear, I must tell you that this is a moment I shall never, never
forget. But, my dear, will you not give us a little hope of touching
this heart, so kind and generous? Say 'perhaps'... The future is so
long. Say 'perhaps.'"

"Prince, what I have said is all there is in my heart. I thank you for
the honor, but I shall never be your son's wife."

"Well, so that's finished, my dear fellow! I am very glad to have seen
you. Very glad! Go back to your rooms, Princess. Go!" said the old
prince. "Very, very glad to have seen you," repeated he, embracing
Prince Vasili.

"My vocation is a different one," thought Princess Mary. "My vocation is
to be happy with another kind of happiness, the happiness of love and
self-sacrifice. And cost what it may, I will arrange poor Amelie's
happiness, she loves him so passionately, and so passionately repents. I
will do all I can to arrange the match between them. If he is not rich I
will give her the means; I will ask my father and Andrew. I shall be so
happy when she is his wife. She is so unfortunate, a stranger, alone,
helpless! And, oh God, how passionately she must love him if she could
so far forget herself! Perhaps I might have done the same!..." thought
Princess Mary.




CHAPTER VI

It was long since the Rostovs had news of Nicholas. Not till midwinter
was the count at last handed a letter addressed in his son's
handwriting. On receiving it, he ran on tiptoe to his study in alarm and
haste, trying to escape notice, closed the door, and began to read the
letter.

Anna Mikhaylovna, who always knew everything that passed in the house,
on hearing of the arrival of the letter went softly into the room and
found the count with it in his hand, sobbing and laughing at the same
time.

Anna Mikhaylovna, though her circumstances had improved, was still
living with the Rostovs.

"My dear friend?" said she, in a tone of pathetic inquiry, prepared to
sympathize in any way.

The count sobbed yet more.

"Nikolenka... a letter... wa... a... s... wounded... my darling boy...
the countess... promoted to be an officer... thank God... How tell the
little countess!"

Anna Mikhaylovna sat down beside him, with her own handkerchief wiped
the tears from his eyes and from the letter, then having dried her own
eyes she comforted the count, and decided that at dinner and till
teatime she would prepare the countess, and after tea, with God's help,
would inform her.

At dinner Anna Mikhaylovna talked the whole time about the war news and
about Nikolenka, twice asked when the last letter had been received from
him, though she knew that already, and remarked that they might very
likely be getting a letter from him that day. Each time that these hints
began to make the countess anxious and she glanced uneasily at the count
and at Anna Mikhaylovna, the latter very adroitly turned the
conversation to insignificant matters. Natasha, who, of the whole
family, was the most gifted with a capacity to feel any shades of
intonation, look, and expression, pricked up her ears from the beginning
of the meal and was certain that there was some secret between her
father and Anna Mikhaylovna, that it had something to do with her
brother, and that Anna Mikhaylovna was preparing them for it. Bold as
she was, Natasha, who knew how sensitive her mother was to anything
relating to Nikolenka, did not venture to ask any questions at dinner,
but she was too excited to eat anything and kept wriggling about on her
chair regardless of her governess' remarks. After dinner, she rushed
head long after Anna Mikhaylovna and, dashing at her, flung herself on
her neck as soon as she overtook her in the sitting room.

"Auntie, darling, do tell me what it is!"

"Nothing, my dear."

"No, dearest, sweet one, honey, I won't give up--I know you know
something."

Anna Mikhaylovna shook her head.

"You are a little slyboots," she said.

"A letter from Nikolenka! I'm sure of it!" exclaimed Natasha, reading
confirmation in Anna Mikhaylovna's face.

"But for God's sake, be careful, you know how it may affect your mamma."

"I will, I will, only tell me! You won't? Then I will go and tell at
once."

Anna Mikhaylovna, in a few words, told her the contents of the letter,
on condition that she should tell no one.

"No, on my true word of honor," said Natasha, crossing herself, "I won't
tell anyone!" and she ran off at once to Sonya.

"Nikolenka... wounded... a letter," she announced in gleeful triumph.

"Nicholas!" was all Sonya said, instantly turning white.

Natasha, seeing the impression the news of her brother's wound produced
on Sonya, felt for the first time the sorrowful side of the news.

She rushed to Sonya, hugged her, and began to cry.

"A little wound, but he has been made an officer; he is well now, he
wrote himself," said she through her tears.

"There now! It's true that all you women are crybabies," remarked Petya,
pacing the room with large, resolute strides. "Now I'm very glad, very
glad indeed, that my brother has distinguished himself so. You are all
blubberers and understand nothing."

Natasha smiled through her tears.

"You haven't read the letter?" asked Sonya.

"No, but she said that it was all over and that he's now an officer."

"Thank God!" said Sonya, crossing herself. "But perhaps she deceived
you. Let us go to Mamma."

Petya paced the room in silence for a time.

"If I'd been in Nikolenka's place I would have killed even more of those
Frenchmen," he said. "What nasty brutes they are! I'd have killed so
many that there'd have been a heap of them."

"Hold your tongue, Petya, what a goose you are!"

"I'm not a goose, but they are who cry about trifles," said Petya.

"Do you remember him?" Natasha suddenly asked, after a moment's silence.

Sonya smiled.

"Do I remember Nicholas?"

"No, Sonya, but do you remember so that you remember him perfectly,
remember everything?" said Natasha, with an expressive gesture,
evidently wishing to give her words a very definite meaning. "I remember
Nikolenka too, I remember him well," she said. "But I don't remember
Boris. I don't remember him a bit."

"What! You don't remember Boris?" asked Sonya in surprise.

"It's not that I don't remember--I know what he is like, but not as I
remember Nikolenka. Him--I just shut my eyes and remember, but Boris...
No!" (She shut her eyes.) "No! there's nothing at all."

"Oh, Natasha!" said Sonya, looking ecstatically and earnestly at her
friend as if she did not consider her worthy to hear what she meant to
say and as if she were saying it to someone else, with whom joking was
out of the question, "I am in love with your brother once for all and,
whatever may happen to him or to me, shall never cease to love him as
long as I live."

Natasha looked at Sonya with wondering and inquisitive eyes, and said
nothing. She felt that Sonya was speaking the truth, that there was such
love as Sonya was speaking of. But Natasha had not yet felt anything
like it. She believed it could be, but did not understand it.

"Shall you write to him?" she asked.

Sonya became thoughtful. The question of how to write to Nicholas, and
whether she ought to write, tormented her. Now that he was already an
officer and a wounded hero, would it be right to remind him of herself
and, as it might seem, of the obligations to her he had taken on
himself?

"I don't know. I think if he writes, I will write too," she said,
blushing.

"And you won't feel ashamed to write to him?"

Sonya smiled.

"No."

"And I should be ashamed to write to Boris. I'm not going to."

"Why should you be ashamed?"

"Well, I don't know. It's awkward and would make me ashamed."

"And I know why she'd be ashamed," said Petya, offended by Natasha's
previous remark. "It's because she was in love with that fat one in
spectacles" (that was how Petya described his namesake, the new Count
Bezukhov) "and now she's in love with that singer" (he meant Natasha's
Italian singing master), "that's why she's ashamed!"

"Petya, you're a stupid!" said Natasha.

"Not more stupid than you, madam," said the nine-year-old Petya, with
the air of an old brigadier.

The countess had been prepared by Anna Mikhaylovna's hints at dinner. On
retiring to her own room, she sat in an armchair, her eyes fixed on a
miniature portrait of her son on the lid of a snuffbox, while the tears
kept coming into her eyes. Anna Mikhaylovna, with the letter, came on
tiptoe to the countess' door and paused.

"Don't come in," she said to the old count who was following her. "Come
later." And she went in, closing the door behind her.

The count put his ear to the keyhole and listened.

At first he heard the sound of indifferent voices, then Anna
Mikhaylovna's voice alone in a long speech, then a cry, then silence,
then both voices together with glad intonations, and then footsteps.
Anna Mikhaylovna opened the door. Her face wore the proud expression of
a surgeon who has just performed a difficult operation and admits the
public to appreciate his skill.

"It is done!" she said to the count, pointing triumphantly to the
countess, who sat holding in one hand the snuffbox with its portrait and
in the other the letter, and pressing them alternately to her lips.

When she saw the count, she stretched out her arms to him, embraced his
bald head, over which she again looked at the letter and the portrait,
and in order to press them again to her lips, she slightly pushed away
the bald head. Vera, Natasha, Sonya, and Petya now entered the room, and
the reading of the letter began. After a brief description of the
campaign and the two battles in which he had taken part, and his
promotion, Nicholas said that he kissed his father's and mother's hands
asking for their blessing, and that he kissed Vera, Natasha, and Petya.
Besides that, he sent greetings to Monsieur Schelling, Madame Schoss,
and his old nurse, and asked them to kiss for him "dear Sonya, whom he
loved and thought of just the same as ever." When she heard this Sonya
blushed so that tears came into her eyes and, unable to bear the looks
turned upon her, ran away into the dancing hall, whirled round it at
full speed with her dress puffed out like a balloon, and, flushed and
smiling, plumped down on the floor. The countess was crying.

"Why are you crying, Mamma?" asked Vera. "From all he says one should be
glad and not cry."

This was quite true, but the count, the countess, and Natasha looked at
her reproachfully. "And who is it she takes after?" thought the
countess.

Nicholas' letter was read over hundreds of times, and those who were
considered worthy to hear it had to come to the countess, for she did
not let it out of her hands. The tutors came, and the nurses, and
Dmitri, and several acquaintances, and the countess reread the letter
each time with fresh pleasure and each time discovered in it fresh
proofs of Nikolenka's virtues. How strange, how extraordinary, how
joyful it seemed, that her son, the scarcely perceptible motion of whose
tiny limbs she had felt twenty years ago within her, that son about whom
she used to have quarrels with the too indulgent count, that son who had
first learned to say "pear" and then "granny," that this son should now
be away in a foreign land amid strange surroundings, a manly warrior
doing some kind of man's work of his own, without help or guidance. The
universal experience of ages, showing that children do grow
imperceptibly from the cradle to manhood, did not exist for the
countess. Her son's growth toward manhood, at each of its stages, had
seemed as extraordinary to her as if there had never existed the
millions of human beings who grew up in the same way. As twenty years
before, it seemed impossible that the little creature who lived
somewhere under her heart would ever cry, suck her breast, and begin to
speak, so now she could not believe that that little creature could be
this strong, brave man, this model son and officer that, judging by this
letter, he now was.

"What a style! How charmingly he describes!" said she, reading the
descriptive part of the letter. "And what a soul! Not a word about
himself.... Not a word! About some Denisov or other, though he himself,
I dare say, is braver than any of them. He says nothing about his
sufferings. What a heart! How like him it is! And how he has remembered
everybody! Not forgetting anyone. I always said when he was only so
high--I always said...."

For more than a week preparations were being made, rough drafts of
letters to Nicholas from all the household were written and copied out,
while under the supervision of the countess and the solicitude of the
count, money and all things necessary for the uniform and equipment of
the newly commissioned officer were collected. Anna Mikhaylovna,
practical woman that she was, had even managed by favor with army
authorities to secure advantageous means of communication for herself
and her son. She had opportunities of sending her letters to the Grand
Duke Constantine Pavlovich, who commanded the Guards. The Rostovs
supposed that The Russian Guards, Abroad, was quite a definite address,
and that if a letter reached the Grand Duke in command of the Guards
there was no reason why it should not reach the Pavlograd regiment,
which was presumably somewhere in the same neighborhood. And so it was
decided to send the letters and money by the Grand Duke's courier to
Boris and Boris was to forward them to Nicholas. The letters were from
the old count, the countess, Petya, Vera, Natasha, and Sonya, and
finally there were six thousand rubles for his outfit and various other
things the old count sent to his son.




CHAPTER VII

On the twelfth of November, Kutuzov's active army, in camp before
Olmutz, was preparing to be reviewed next day by the two Emperors--the
Russian and the Austrian. The Guards, just arrived from Russia, spent
the night ten miles from Olmutz and next morning were to come straight
to the review, reaching the field at Olmutz by ten o'clock.

That day Nicholas Rostov received a letter from Boris, telling him that
the Ismaylov regiment was quartered for the night ten miles from Olmutz
and that he wanted to see him as he had a letter and money for him.
Rostov was particularly in need of money now that the troops, after
their active service, were stationed near Olmutz and the camp swarmed
with well-provisioned sutlers and Austrian Jews offering all sorts of
tempting wares. The Pavlograds held feast after feast, celebrating
awards they had received for the campaign, and made expeditions to
Olmutz to visit a certain Caroline the Hungarian, who had recently
opened a restaurant there with girls as waitresses. Rostov, who had just
celebrated his promotion to a cornetcy and bought Denisov's horse,
Bedouin, was in debt all round, to his comrades and the sutlers. On
receiving Boris' letter he rode with a fellow officer to Olmutz, dined
there, drank a bottle of wine, and then set off alone to the Guards'
camp to find his old playmate. Rostov had not yet had time to get his
uniform. He had on a shabby cadet jacket, decorated with a soldier's
cross, equally shabby cadet's riding breeches lined with worn leather,
and an officer's saber with a sword knot. The Don horse he was riding
was one he had bought from a Cossack during the campaign, and he wore a
crumpled hussar cap stuck jauntily back on one side of his head. As he
rode up to the camp he thought how he would impress Boris and all his
comrades of the Guards by his appearance--that of a fighting hussar who
had been under fire.

The Guards had made their whole march as if on a pleasure trip, parading
their cleanliness and discipline. They had come by easy stages, their
knapsacks conveyed on carts, and the Austrian authorities had provided
excellent dinners for the officers at every halting place. The regiments
had entered and left the town with their bands playing, and by the Grand
Duke's orders the men had marched all the way in step (a practice on
which the Guards prided themselves), the officers on foot and at their
proper posts. Boris had been quartered, and had marched all the way,
with Berg who was already in command of a company. Berg, who had
obtained his captaincy during the campaign, had gained the confidence of
his superiors by his promptitude and accuracy and had arranged his money
matters very satisfactorily. Boris, during the campaign, had made the
acquaintance of many persons who might prove useful to him, and by a
letter of recommendation he had brought from Pierre had become
acquainted with Prince Andrew Bolkonski, through whom he hoped to obtain
a post on the commander-in-chief's staff. Berg and Boris, having rested
after yesterday's march, were sitting, clean and neatly dressed, at a
round table in the clean quarters allotted to them, playing chess. Berg
held a smoking pipe between his knees. Boris, in the accurate way
characteristic of him, was building a little pyramid of chessmen with
his delicate white fingers while awaiting Berg's move, and watched his
opponent's face, evidently thinking about the game as he always thought
only of whatever he was engaged on.

"Well, how are you going to get out of that?" he remarked.

"We'll try to," replied Berg, touching a pawn and then removing his
hand.

At that moment the door opened.

"Here he is at last!" shouted Rostov. "And Berg too! Oh, you
petisenfans, allay cushay dormir!" he exclaimed, imitating his Russian
nurse's French, at which he and Boris used to laugh long ago.

"Dear me, how you have changed!"

Boris rose to meet Rostov, but in doing so did not omit to steady and
replace some chessmen that were falling. He was about to embrace his
friend, but Nicholas avoided him. With that peculiar feeling of youth,
that dread of beaten tracks, and wish to express itself in a manner
different from that of its elders which is often insincere, Nicholas
wished to do something special on meeting his friend. He wanted to pinch
him, push him, do anything but kiss him--a thing everybody did. But
notwithstanding this, Boris embraced him in a quiet, friendly way and
kissed him three times.

They had not met for nearly half a year and, being at the age when young
men take their first steps on life's road, each saw immense changes in
the other, quite a new reflection of the society in which they had taken
those first steps. Both had changed greatly since they last met and both
were in a hurry to show the changes that had taken place in them.

"Oh, you damned dandies! Clean and fresh as if you'd been to a fete, not
like us sinners of the line," cried Rostov, with martial swagger and
with baritone notes in his voice, new to Boris, pointing to his own mud-
bespattered breeches. The German landlady, hearing Rostov's loud voice,
popped her head in at the door.

"Eh, is she pretty?" he asked with a wink.

"Why do you shout so? You'll frighten them!" said Boris. "I did not
expect you today," he added. "I only sent you the note yesterday by
Bolkonski--an adjutant of Kutuzov's, who's a friend of mine. I did not
think he would get it to you so quickly.... Well, how are you? Been
under fire already?" asked Boris.

Without answering, Rostov shook the soldier's Cross of St. George
fastened to the cording of his uniform and, indicating a bandaged arm,
glanced at Berg with a smile.

"As you see," he said.

"Indeed? Yes, yes!" said Boris, with a smile. "And we too have had a
splendid march. You know, of course, that His Imperial Highness rode
with our regiment all the time, so that we had every comfort and every
advantage. What receptions we had in Poland! What dinners and balls! I
can't tell you. And the Tsarevich was very gracious to all our
officers."

And the two friends told each other of their doings, the one of his
hussar revels and life in the fighting line, the other of the pleasures
and advantages of service under members of the Imperial family.

"Oh, you Guards!" said Rostov. "I say, send for some wine."

Boris made a grimace.

"If you really want it," said he.

He went to his bed, drew a purse from under the clean pillow, and sent
for wine.

"Yes, and I have some money and a letter to give you," he added.

Rostov took the letter and, throwing the money on the sofa, put both
arms on the table and began to read. After reading a few lines, he
glanced angrily at Berg, then, meeting his eyes, hid his face behind the
letter.

"Well, they've sent you a tidy sum," said Berg, eying the heavy purse
that sank into the sofa. "As for us, Count, we get along on our pay. I
can tell you for myself..."

"I say, Berg, my dear fellow," said Rostov, "when you get a letter from
home and meet one of your own people whom you want to talk everything
over with, and I happen to be there, I'll go at once, to be out of your
way! Do go somewhere, anywhere... to the devil!" he exclaimed, and
immediately seizing him by the shoulder and looking amiably into his
face, evidently wishing to soften the rudeness of his words, he added,
"Don't be hurt, my dear fellow; you know I speak from my heart as to an
old acquaintance."

"Oh, don't mention it, Count! I quite understand," said Berg, getting up
and speaking in a muffled and guttural voice.

"Go across to our hosts: they invited you," added Boris.

Berg put on the cleanest of coats, without a spot or speck of dust,
stood before a looking glass and brushed the hair on his temples
upwards, in the way affected by the Emperor Alexander, and, having
assured himself from the way Rostov looked at it that his coat had been
noticed, left the room with a pleasant smile.

"Oh dear, what a beast I am!" muttered Rostov, as he read the letter.

"Why?"

"Oh, what a pig I am, not to have written and to have given them such a
fright! Oh, what a pig I am!" he repeated, flushing suddenly. "Well,
have you sent Gabriel for some wine? All right let's have some!"

In the letter from his parents was enclosed a letter of recommendation
to Bagration which the old countess at Anna Mikhaylovna's advice had
obtained through an acquaintance and sent to her son, asking him to take
it to its destination and make use of it.

"What nonsense! Much I need it!" said Rostov, throwing the letter under
the table.

"Why have you thrown that away?" asked Boris.

"It is some letter of recommendation... what the devil do I want it
for!"

"Why 'What the devil'?" said Boris, picking it up and reading the
address. "This letter would be of great use to you."

"I want nothing, and I won't be anyone's adjutant."

"Why not?" inquired Boris.

"It's a lackey's job!"

"You are still the same dreamer, I see," remarked Boris, shaking his
head.

"And you're still the same diplomatist! But that's not the point...
Come, how are you?" asked Rostov.

"Well, as you see. So far everything's all right, but I confess I should
much like to be an adjutant and not remain at the front."

"Why?"

"Because when once a man starts on military service, he should try to
make as successful a career of it as possible."

"Oh, that's it!" said Rostov, evidently thinking of something else.

He looked intently and inquiringly into his friend's eyes, evidently
trying in vain to find the answer to some question.

Old Gabriel brought in the wine.

"Shouldn't we now send for Berg?" asked Boris. "He would drink with you.
I can't."

"Well, send for him... and how do you get on with that German?" asked
Rostov, with a contemptuous smile.

"He is a very, very nice, honest, and pleasant fellow," answered Boris.

Again Rostov looked intently into Boris' eyes and sighed. Berg returned,
and over the bottle of wine conversation between the three officers
became animated. The Guardsmen told Rostov of their march and how they
had been made much of in Russia, Poland, and abroad. They spoke of the
sayings and doings of their commander, the Grand Duke, and told stories
of his kindness and irascibility. Berg, as usual, kept silent when the
subject did not relate to himself, but in connection with the stories of
the Grand Duke's quick temper he related with gusto how in Galicia he
had managed to deal with the Grand Duke when the latter made a tour of
the regiments and was annoyed at the irregularity of a movement. With a
pleasant smile Berg related how the Grand Duke had ridden up to him in a
violent passion, shouting: "Arnauts!" ("Arnauts" was the Tsarevich's
favorite expression when he was in a rage) and called for the company
commander.

"Would you believe it, Count, I was not at all alarmed, because I knew I
was right. Without boasting, you know, I may say that I know the Army
Orders by heart and know the Regulations as well as I do the Lord's
Prayer. So, Count, there never is any negligence in my company, and so
my conscience was at ease. I came forward...." (Berg stood up and showed
how he presented himself, with his hand to his cap, and really it would
have been difficult for a face to express greater respect and self-
complacency than his did.) "Well, he stormed at me, as the saying is,
stormed and stormed and stormed! It was not a matter of life but rather
of death, as the saying is. 'Albanians!' and 'devils!' and 'To
Siberia!'" said Berg with a sagacious smile. "I knew I was in the right
so I kept silent; was not that best, Count?... 'Hey, are you dumb?' he
shouted. Still I remained silent. And what do you think, Count? The next
day it was not even mentioned in the Orders of the Day. That's what
keeping one's head means. That's the way, Count," said Berg, lighting
his pipe and emitting rings of smoke.

"Yes, that was fine," said Rostov, smiling.

But Boris noticed that he was preparing to make fun of Berg, and
skillfully changed the subject. He asked him to tell them how and where
he got his wound. This pleased Rostov and he began talking about it, and
as he went on became more and more animated. He told them of his Schon
Grabern affair, just as those who have taken part in a battle generally
do describe it, that is, as they would like it to have been, as they
have heard it described by others, and as sounds well, but not at all as
it really was. Rostov was a truthful young man and would on no account
have told a deliberate lie. He began his story meaning to tell
everything just as it happened, but imperceptibly, involuntarily, and
inevitably he lapsed into falsehood. If he had told the truth to his
hearers--who like himself had often heard stories of attacks and had
formed a definite idea of what an attack was and were expecting to hear
just such a story--they would either not have believed him or, still
worse, would have thought that Rostov was himself to blame since what
generally happens to the narrators of cavalry attacks had not happened
to him. He could not tell them simply that everyone went at a trot and
that he fell off his horse and sprained his arm and then ran as hard as
he could from a Frenchman into the wood. Besides, to tell everything as
it really happened, it would have been necessary to make an effort of
will to tell only what happened. It is very difficult to tell the truth,
and young people are rarely capable of it. His hearers expected a story
of how beside himself and all aflame with excitement, he had flown like
a storm at the square, cut his way in, slashed right and left, how his
saber had tasted flesh and he had fallen exhausted, and so on. And so he
told them all that.

In the middle of his story, just as he was saying: "You cannot imagine
what a strange frenzy one experiences during an attack," Prince Andrew,
whom Boris was expecting, entered the room. Prince Andrew, who liked to
help young men, was flattered by being asked for his assistance and
being well disposed toward Boris, who had managed to please him the day
before, he wished to do what the young man wanted. Having been sent with
papers from Kutuzov to the Tsarevich, he looked in on Boris, hoping to
find him alone. When he came in and saw an hussar of the line recounting
his military exploits (Prince Andrew could not endure that sort of man),
he gave Boris a pleasant smile, frowned as with half-closed eyes he
looked at Rostov, bowed slightly and wearily, and sat down languidly on
the sofa: he felt it unpleasant to have dropped in on bad company.
Rostov flushed up on noticing this, but he did not care, this was a mere
stranger. Glancing, however, at Boris, he saw that he too seemed ashamed
of the hussar of the line.

In spite of Prince Andrew's disagreeable, ironical tone, in spite of the
contempt with which Rostov, from his fighting army point of view,
regarded all these little adjutants on the staff of whom the newcomer
was evidently one, Rostov felt confused, blushed, and became silent.
Boris inquired what news there might be on the staff, and what, without
indiscretion, one might ask about our plans.

"We shall probably advance," replied Bolkonski, evidently reluctant to
say more in the presence of a stranger.

Berg took the opportunity to ask, with great politeness, whether, as was
rumored, the allowance of forage money to captains of companies would be
doubled. To this Prince Andrew answered with a smile that he could give
no opinion on such an important government order, and Berg laughed
gaily.

"As to your business," Prince Andrew continued, addressing Boris, "we
will talk of it later" (and he looked round at Rostov). "Come to me
after the review and we will do what is possible."

And, having glanced round the room, Prince Andrew turned to Rostov,
whose state of unconquerable childish embarrassment now changing to
anger he did not condescend to notice, and said: "I think you were
talking of the Schon Grabern affair? Were you there?"

"I was there," said Rostov angrily, as if intending to insult the aide-
de-camp.

Bolkonski noticed the hussar's state of mind, and it amused him. With a
slightly contemptuous smile, he said: "Yes, there are many stories now
told about that affair!"

"Yes, stories!" repeated Rostov loudly, looking with eyes suddenly grown
furious, now at Boris, now at Bolkonski. "Yes, many stories! But our
stories are the stories of men who have been under the enemy's fire! Our
stories have some weight, not like the stories of those fellows on the
staff who get rewards without doing anything!"

"Of whom you imagine me to be one?" said Prince Andrew, with a quiet and
particularly amiable smile.

A strange feeling of exasperation and yet of respect for this man's
self-possession mingled at that moment in Rostov's soul.

"I am not talking about you," he said, "I don't know you and, frankly, I
don't want to. I am speaking of the staff in general."

"And I will tell you this," Prince Andrew interrupted in a tone of quiet
authority, "you wish to insult me, and I am ready to agree with you that
it would be very easy to do so if you haven't sufficient self-respect,
but admit that the time and place are very badly chosen. In a day or two
we shall all have to take part in a greater and more serious duel, and
besides, Drubetskoy, who says he is an old friend of yours, is not at
all to blame that my face has the misfortune to displease you. However,"
he added rising, "you know my name and where to find me, but don't
forget that I do not regard either myself or you as having been at all
insulted, and as a man older than you, my advice is to let the matter
drop. Well then, on Friday after the review I shall expect you,
Drubetskoy. Au revoir!" exclaimed Prince Andrew, and with a bow to them
both he went out.

Only when Prince Andrew was gone did Rostov think of what he ought to
have said. And he was still more angry at having omitted to say it. He
ordered his horse at once and, coldly taking leave of Boris, rode home.
Should he go to headquarters next day and challenge that affected
adjutant, or really let the matter drop, was the question that worried
him all the way. He thought angrily of the pleasure he would have at
seeing the fright of that small and frail but proud man when covered by
his pistol, and then he felt with surprise that of all the men he knew
there was none he would so much like to have for a friend as that very
adjutant whom he so hated.




CHAPTER VIII

The day after Rostov had been to see Boris, a review was held of the
Austrian and Russian troops, both those freshly arrived from Russia and
those who had been campaigning under Kutuzov. The two Emperors, the
Russian with his heir the Tsarevich, and the Austrian with the Archduke,
inspected the allied army of eighty thousand men.

From early morning the smart clean troops were on the move, forming up
on the field before the fortress. Now thousands of feet and bayonets
moved and halted at the officers' command, turned with banners flying,
formed up at intervals, and wheeled round other similar masses of
infantry in different uniforms; now was heard the rhythmic beat of hoofs
and the jingling of showy cavalry in blue, red, and green braided
uniforms, with smartly dressed bandsmen in front mounted on black, roan,
or gray horses; then again, spreading out with the brazen clatter of the
polished shining cannon that quivered on the gun carriages and with the
smell of linstocks, came the artillery which crawled between the
infantry and cavalry and took up its appointed position. Not only the
generals in full parade uniforms, with their thin or thick waists drawn
in to the utmost, their red necks squeezed into their stiff collars, and
wearing scarves and all their decorations, not only the elegant, pomaded
officers, but every soldier with his freshly washed and shaven face and
his weapons clean and polished to the utmost, and every horse groomed
till its coat shone like satin and every hair of its wetted mane lay
smooth--felt that no small matter was happening, but an important and
solemn affair. Every general and every soldier was conscious of his own
insignificance, aware of being but a drop in that ocean of men, and yet
at the same time was conscious of his strength as a part of that
enormous whole.

From early morning strenuous activities and efforts had begun and by ten
o'clock all had been brought into due order. The ranks were drawn up on
the vast field. The whole army was extended in three lines: the cavalry
in front, behind it the artillery, and behind that again the infantry.

A space like a street was left between each two lines of troops. The
three parts of that army were sharply distinguished: Kutuzov's fighting
army (with the Pavlograds on the right flank of the front); those
recently arrived from Russia, both Guards and regiments of the line; and
the Austrian troops. But they all stood in the same lines, under one
command, and in a like order.

Like wind over leaves ran an excited whisper: "They're coming! They're
coming!" Alarmed voices were heard, and a stir of final preparation
swept over all the troops.

From the direction of Olmutz in front of them, a group was seen
approaching. And at that moment, though the day was still, a light gust
of wind blowing over the army slightly stirred the streamers on the
lances and the unfolded standards fluttered against their staffs. It
looked as if by that slight motion the army itself was expressing its
joy at the approach of the Emperors. One voice was heard shouting: "Eyes
front!" Then, like the crowing of cocks at sunrise, this was repeated by
others from various sides and all became silent.

In the deathlike stillness only the tramp of horses was heard. This was
the Emperors' suites. The Emperors rode up to the flank, and the
trumpets of the first cavalry regiment played the general march. It
seemed as though not the trumpeters were playing, but as if the army
itself, rejoicing at the Emperors' approach, had naturally burst into
music. Amid these sounds, only the youthful kindly voice of the Emperor
Alexander was clearly heard. He gave the words of greeting, and the
first regiment roared "Hurrah!" so deafeningly, continuously, and
joyfully that the men themselves were awed by their multitude and the
immensity of the power they constituted.

Rostov, standing in the front lines of Kutuzov's army which the Tsar
approached first, experienced the same feeling as every other man in
that army: a feeling of self-forgetfulness, a proud consciousness of
might, and a passionate attraction to him who was the cause of this
triumph.

He felt that at a single word from that man all this vast mass (and he
himself an insignificant atom in it) would go through fire and water,
commit crime, die, or perform deeds of highest heroism, and so he could
not but tremble and his heart stand still at the imminence of that word.

"Hurrah! Hurrah! Hurrah!" thundered from all sides, one regiment after
another greeting the Tsar with the strains of the march, and then
"Hurrah!"... Then the general march, and again "Hurrah! Hurrah!" growing
ever stronger and fuller and merging into a deafening roar.

Till the Tsar reached it, each regiment in its silence and immobility
seemed like a lifeless body, but as soon as he came up it became alive,
its thunder joining the roar of the whole line along which he had
already passed. Through the terrible and deafening roar of those voices,
amid the square masses of troops standing motionless as if turned to
stone, hundreds of riders composing the suites moved carelessly but
symmetrically and above all freely, and in front of them two men--the
Emperors. Upon them the undivided, tensely passionate attention of that
whole mass of men was concentrated.

The handsome young Emperor Alexander, in the uniform of the Horse
Guards, wearing a cocked hat with its peaks front and back, with his
pleasant face and resonant though not loud voice, attracted everyone's
attention.

Rostov was not far from the trumpeters, and with his keen sight had
recognized the Tsar and watched his approach. When he was within twenty
paces, and Nicholas could clearly distinguish every detail of his
handsome, happy young face, he experienced a feeling of tenderness and
ecstasy such as he had never before known. Every trait and every
movement of the Tsar's seemed to him enchanting.

Stopping in front of the Pavlograds, the Tsar said something in French
to the Austrian Emperor and smiled.

Seeing that smile, Rostov involuntarily smiled himself and felt a still
stronger flow of love for his sovereign. He longed to show that love in
some way and knowing that this was impossible was ready to cry. The Tsar
called the colonel of the regiment and said a few words to him.

"Oh God, what would happen to me if the Emperor spoke to me?" thought
Rostov. "I should die of happiness!"

The Tsar addressed the officers also: "I thank you all, gentlemen, I
thank you with my whole heart." To Rostov every word sounded like a
voice from heaven. How gladly would he have died at once for his Tsar!

"You have earned the St. George's standards and will be worthy of them."

"Oh, to die, to die for him," thought Rostov.

The Tsar said something more which Rostov did not hear, and the
soldiers, straining their lungs, shouted "Hurrah!"

Rostov too, bending over his saddle, shouted "Hurrah!" with all his
might, feeling that he would like to injure himself by that shout, if
only to express his rapture fully.

The Tsar stopped a few minutes in front of the hussars as if undecided.

"How can the Emperor be undecided?" thought Rostov, but then even this
indecision appeared to him majestic and enchanting, like everything else
the Tsar did.

That hesitation lasted only an instant. The Tsar's foot, in the narrow
pointed boot then fashionable, touched the groin of the bobtailed bay
mare he rode, his hand in a white glove gathered up the reins, and he
moved off accompanied by an irregularly swaying sea of aides-de-camp.
Farther and farther he rode away, stopping at other regiments, till at
last only his white plumes were visible to Rostov from amid the suites
that surrounded the Emperors.

Among the gentlemen of the suite, Rostov noticed Bolkonski, sitting his
horse indolently and carelessly. Rostov recalled their quarrel of
yesterday and the question presented itself whether he ought or ought
not to challenge Bolkonski. "Of course not!" he now thought. "Is it
worth thinking or speaking of it at such a moment? At a time of such
love, such rapture, and such self-sacrifice, what do any of our quarrels
and affronts matter? I love and forgive everybody now."

When the Emperor had passed nearly all the regiments, the troops began a
ceremonial march past him, and Rostov on Bedouin, recently purchased
from Denisov, rode past too, at the rear of his squadron--that is, alone
and in full view of the Emperor.

Before he reached him, Rostov, who was a splendid horseman, spurred
Bedouin twice and successfully put him to the showy trot in which the
animal went when excited. Bending his foaming muzzle to his chest, his
tail extended, Bedouin, as if also conscious of the Emperor's eye upon
him, passed splendidly, lifting his feet with a high and graceful
action, as if flying through the air without touching the ground.

Rostov himself, his legs well back and his stomach drawn in and feeling
himself one with his horse, rode past the Emperor with a frowning but
blissful face "like a vewy devil," as Denisov expressed it.

"Fine fellows, the Pavlograds!" remarked the Emperor.

"My God, how happy I should be if he ordered me to leap into the fire
this instant!" thought Rostov.

When the review was over, the newly arrived officers, and also
Kutuzov's, collected in groups and began to talk about the awards, about
the Austrians and their uniforms, about their lines, about Bonaparte,
and how badly the latter would fare now, especially if the Essen corps
arrived and Prussia took our side.

But the talk in every group was chiefly about the Emperor Alexander. His
every word and movement was described with ecstasy.

They all had but one wish: to advance as soon as possible against the
enemy under the Emperor's command. Commanded by the Emperor himself they
could not fail to vanquish anyone, be it whom it might: so thought
Rostov and most of the officers after the review.

All were then more confident of victory than the winning of two battles
would have made them.




CHAPTER IX

The day after the review, Boris, in his best uniform and with his
comrade Berg's best wishes for success, rode to Olmutz to see Bolkonski,
wishing to profit by his friendliness and obtain for himself the best
post he could--preferably that of adjutant to some important personage,
a position in the army which seemed to him most attractive. "It is all
very well for Rostov, whose father sends him ten thousand rubles at a
time, to talk about not wishing to cringe to anybody and not be anyone's
lackey, but I who have nothing but my brains have to make a career and
must not miss opportunities, but must avail myself of them!" he
reflected.

He did not find Prince Andrew in Olmutz that day, but the appearance of
the town where the headquarters and the diplomatic corps were stationed
and the two Emperors were living with their suites, households, and
courts only strengthened his desire to belong to that higher world.

He knew no one, and despite his smart Guardsman's uniform, all these
exalted personages passing in the streets in their elegant carriages
with their plumes, ribbons, and medals, both courtiers and military men,
seemed so immeasurably above him, an insignificant officer of the
Guards, that they not only did not wish to, but simply could not, be
aware of his existence. At the quarters of the commander-in-chief,
Kutuzov, where he inquired for Bolkonski, all the adjutants and even the
orderlies looked at him as if they wished to impress on him that a great
many officers like him were always coming there and that everybody was
heartily sick of them. In spite of this, or rather because of it, next
day, November 15, after dinner he again went to Olmutz and, entering the
house occupied by Kutuzov, asked for Bolkonski. Prince Andrew was in and
Boris was shown into a large hall probably formerly used for dancing,
but in which five beds now stood, and furniture of various kinds: a
table, chairs, and a clavichord. One adjutant, nearest the door, was
sitting at the table in a Persian dressing gown, writing. Another, the
red, stout Nesvitski, lay on a bed with his arms under his head,
laughing with an officer who had sat down beside him. A third was
playing a Viennese waltz on the clavichord, while a fourth, lying on the
clavichord, sang the tune. Bolkonski was not there. None of these
gentlemen changed his position on seeing Boris. The one who was writing
and whom Boris addressed turned round crossly and told him Bolkonski was
on duty and that he should go through the door on the left into the
reception room if he wished to see him. Boris thanked him and went to
the reception room, where he found some ten officers and generals.

When he entered, Prince Andrew, his eyes drooping contemptuously (with
that peculiar expression of polite weariness which plainly says, "If it
were not my duty I would not talk to you for a moment"), was listening
to an old Russian general with decorations, who stood very erect, almost
on tiptoe, with a soldier's obsequious expression on his purple face,
reporting something.

"Very well, then, be so good as to wait," said Prince Andrew to the
general, in Russian, speaking with the French intonation he affected
when he wished to speak contemptuously, and noticing Boris, Prince
Andrew, paying no more heed to the general who ran after him imploring
him to hear something more, nodded and turned to him with a cheerful
smile.

At that moment Boris clearly realized what he had before surmised, that
in the army, besides the subordination and discipline prescribed in the
military code, which he and the others knew in the regiment, there was
another, more important, subordination, which made this tight-laced,
purple-faced general wait respectfully while Captain Prince Andrew, for
his own pleasure, chose to chat with Lieutenant Drubetskoy. More than
ever was Boris resolved to serve in future not according to the written
code, but under this unwritten law. He felt now that merely by having
been recommended to Prince Andrew he had already risen above the general
who at the front had the power to annihilate him, a lieutenant of the
Guards. Prince Andrew came up to him and took his hand.

"I am very sorry you did not find me in yesterday. I was fussing about
with Germans all day. We went with Weyrother to survey the dispositions.
When Germans start being accurate, there's no end to it!"

Boris smiled, as if he understood what Prince Andrew was alluding to as
something generally known. But it was the first time he had heard
Weyrother's name, or even the term "dispositions."

"Well, my dear fellow, so you still want to be an adjutant? I have been
thinking about you."

"Yes, I was thinking"--for some reason Boris could not help blushing--
"of asking the commander-in-chief. He has had a letter from Prince
Kuragin about me. I only wanted to ask because I fear the Guards won't
be in action," he added as if in apology.

"All right, all right. We'll talk it over," replied Prince Andrew. "Only
let me report this gentleman's business, and I shall be at your
disposal."

While Prince Andrew went to report about the purple-faced general, that
gentleman--evidently not sharing Boris' conception of the advantages of
the unwritten code of subordination--looked so fixedly at the
presumptuous lieutenant who had prevented his finishing what he had to
say to the adjutant that Boris felt uncomfortable. He turned away and
waited impatiently for Prince Andrew's return from the commander-in-
chief's room.

"You see, my dear fellow, I have been thinking about you," said Prince
Andrew when they had gone into the large room where the clavichord was.
"It's no use your going to the commander-in-chief. He would say a lot of
pleasant things, ask you to dinner" ("That would not be bad as regards
the unwritten code," thought Boris), "but nothing more would come of it.
There will soon be a battalion of us aides-de-camp and adjutants! But
this is what we'll do: I have a good friend, an adjutant general and an
excellent fellow, Prince Dolgorukov; and though you may not know it, the
fact is that now Kutuzov with his staff and all of us count for nothing.
Everything is now centered round the Emperor. So we will go to
Dolgorukov; I have to go there anyhow and I have already spoken to him
about you. We shall see whether he cannot attach you to himself or find
a place for you somewhere nearer the sun."

Prince Andrew always became specially keen when he had to guide a young
man and help him to worldly success. Under cover of obtaining help of
this kind for another, which from pride he would never accept for
himself, he kept in touch with the circle which confers success and
which attracted him. He very readily took up Boris' cause and went with
him to Dolgorukov.

It was late in the evening when they entered the palace at Olmutz
occupied by the Emperors and their retinues.

That same day a council of war had been held in which all the members of
the Hofkriegsrath and both Emperors took part. At that council, contrary
to the views of the old generals Kutuzov and Prince Schwartzenberg, it
had been decided to advance immediately and give battle to Bonaparte.
The council of war was just over when Prince Andrew accompanied by Boris
arrived at the palace to find Dolgorukov. Everyone at headquarters was
still under the spell of the day's council, at which the party of the
young had triumphed. The voices of those who counseled delay and advised
waiting for something else before advancing had been so completely
silenced and their arguments confuted by such conclusive evidence of the
advantages of attacking that what had been discussed at the council--the
coming battle and the victory that would certainly result from it--no
longer seemed to be in the future but in the past. All the advantages
were on our side. Our enormous forces, undoubtedly superior to
Napoleon's, were concentrated in one place, the troops inspired by the
Emperors' presence were eager for action. The strategic position where
the operations would take place was familiar in all its details to the
Austrian General Weyrother: a lucky accident had ordained that the
Austrian army should maneuver the previous year on the very fields where
the French had now to be fought; the adjacent locality was known and
shown in every detail on the maps, and Bonaparte, evidently weakened,
was undertaking nothing.

Dolgorukov, one of the warmest advocates of an attack, had just returned
from the council, tired and exhausted but eager and proud of the victory
that had been gained. Prince Andrew introduced his protege, but Prince
Dolgorukov politely and firmly pressing his hand said nothing to Boris
and, evidently unable to suppress the thoughts which were uppermost in
his mind at that moment, addressed Prince Andrew in French.

"Ah, my dear fellow, what a battle we have gained! God grant that the
one that will result from it will be as victorious! However, dear
fellow," he said abruptly and eagerly, "I must confess to having been
unjust to the Austrians and especially to Weyrother. What exactitude,
what minuteness, what knowledge of the locality, what foresight for
every eventuality, every possibility even to the smallest detail! No, my
dear fellow, no conditions better than our present ones could have been
devised. This combination of Austrian precision with Russian valor--what
more could be wished for?"

"So the attack is definitely resolved on?" asked Bolkonski.

"And do you know, my dear fellow, it seems to me that Bonaparte has
decidedly lost bearings, you know that a letter was received from him
today for the Emperor." Dolgorukov smiled significantly.

"Is that so? And what did he say?" inquired Bolkonski.

"What can he say? Tra-di-ri-di-ra and so on... merely to gain time. I
tell you he is in our hands, that's certain! But what was most amusing,"
he continued, with a sudden, good-natured laugh, "was that we could not
think how to address the reply! If not as 'Consul' and of course not as
'Emperor,' it seemed to me it should be to 'General Bonaparte.'"

"But between not recognizing him as Emperor and calling him General
Bonaparte, there is a difference," remarked Bolkonski.

"That's just it," interrupted Dolgorukov quickly, laughing. "You know
Bilibin--he's a very clever fellow. He suggested addressing him as
'Usurper and Enemy of Mankind.'"

Dolgorukov laughed merrily.

"Only that?" said Bolkonski.

"All the same, it was Bilibin who found a suitable form for the address.
He is a wise and clever fellow."

"What was it?"

"To the Head of the French Government... Au chef du gouvernement
francais," said Dolgorukov, with grave satisfaction. "Good, wasn't it?"

"Yes, but he will dislike it extremely," said Bolkonski.

"Oh yes, very much! My brother knows him, he's dined with him--the
present Emperor--more than once in Paris, and tells me he never met a
more cunning or subtle diplomatist--you know, a combination of French
adroitness and Italian play-acting! Do you know the tale about him and
Count Markov? Count Markov was the only man who knew how to handle him.
You know the story of the handkerchief? It is delightful!"

And the talkative Dolgorukov, turning now to Boris, now to Prince
Andrew, told how Bonaparte wishing to test Markov, our ambassador,
purposely dropped a handkerchief in front of him and stood looking at
Markov, probably expecting Markov to pick it up for him, and how Markov
immediately dropped his own beside it and picked it up without touching
Bonaparte's.

"Delightful!" said Bolkonski. "But I have come to you, Prince, as a
petitioner on behalf of this young man. You see..." but before Prince
Andrew could finish, an aide-de-camp came in to summon Dolgorukov to the
Emperor.

"Oh, what a nuisance," said Dolgorukov, getting up hurriedly and
pressing the hands of Prince Andrew and Boris. "You know I should be
very glad to do all in my power both for you and for this dear young
man." Again he pressed the hand of the latter with an expression of
good-natured, sincere, and animated levity. "But you see... another
time!"

Boris was excited by the thought of being so close to the higher powers
as he felt himself to be at that moment. He was conscious that here he
was in contact with the springs that set in motion the enormous
movements of the mass of which in his regiment he felt himself a tiny,
obedient, and insignificant atom. They followed Prince Dolgorukov out
into the corridor and met--coming out of the door of the Emperor's room
by which Dolgorukov had entered--a short man in civilian clothes with a
clever face and sharply projecting jaw which, without spoiling his face,
gave him a peculiar vivacity and shiftiness of expression. This short
man nodded to Dolgorukov as to an intimate friend and stared at Prince
Andrew with cool intensity, walking straight toward him and evidently
expecting him to bow or to step out of his way. Prince Andrew did
neither: a look of animosity appeared on his face and the other turned
away and went down the side of the corridor.

"Who was that?" asked Boris.

"He is one of the most remarkable, but to me most unpleasant of men--the
Minister of Foreign Affairs, Prince Adam Czartoryski.... It is such men
as he who decide the fate of nations," added Bolkonski with a sigh he
could not suppress, as they passed out of the palace.

Next day, the army began its campaign, and up to the very battle of
Austerlitz, Boris was unable to see either Prince Andrew or Dolgorukov
again and remained for a while with the Ismaylov regiment.




CHAPTER X

At dawn on the sixteenth of November, Denisov's squadron, in which
Nicholas Rostov served and which was in Prince Bagration's detachment,
moved from the place where it had spent the night, advancing into action
as arranged, and after going behind other columns for about two thirds
of a mile was stopped on the highroad. Rostov saw the Cossacks and then
the first and second squadrons of hussars and infantry battalions and
artillery pass by and go forward and then Generals Bagration and
Dolgorukov ride past with their adjutants. All the fear before action
which he had experienced as previously, all the inner struggle to
conquer that fear, all his dreams of distinguishing himself as a true
hussar in this battle, had been wasted. Their squadron remained in
reserve and Nicholas Rostov spent that day in a dull and wretched mood.
At nine in the morning, he heard firing in front and shouts of hurrah,
and saw wounded being brought back (there were not many of them), and at
last he saw how a whole detachment of French cavalry was brought in,
convoyed by a sotnya of Cossacks. Evidently the affair was over and,
though not big, had been a successful engagement. The men and officers
returning spoke of a brilliant victory, of the occupation of the town of
Wischau and the capture of a whole French squadron. The day was bright
and sunny after a sharp night frost, and the cheerful glitter of that
autumn day was in keeping with the news of victory which was conveyed,
not only by the tales of those who had taken part in it, but also by the
joyful expression on the faces of soldiers, officers, generals, and
adjutants, as they passed Rostov going or coming. And Nicholas, who had
vainly suffered all the dread that precedes a battle and had spent that
happy day in inactivity, was all the more depressed.

"Come here, Wostov. Let's dwink to dwown our gwief!" shouted Denisov,
who had settled down by the roadside with a flask and some food.

The officers gathered round Denisov's canteen, eating and talking.

"There! They are bringing another!" cried one of the officers,
indicating a captive French dragoon who was being brought in on foot by
two Cossacks.

One of them was leading by the bridle a fine large French horse he had
taken from the prisoner.

"Sell us that horse!" Denisov called out to the Cossacks.

"If you like, your honor!"

The officers got up and stood round the Cossacks and their prisoner. The
French dragoon was a young Alsatian who spoke French with a German
accent. He was breathless with agitation, his face was red, and when he
heard some French spoken he at once began speaking to the officers,
addressing first one, then another. He said he would not have been
taken, it was not his fault but the corporal's who had sent him to seize
some horsecloths, though he had told him the Russians were there. And at
every word he added: "But don't hurt my little horse!" and stroked the
animal. It was plain that he did not quite grasp where he was. Now he
excused himself for having been taken prisoner and now, imagining
himself before his own officers, insisted on his soldierly discipline
and zeal in the service. He brought with him into our rearguard all the
freshness of atmosphere of the French army, which was so alien to us.

The Cossacks sold the horse for two gold pieces, and Rostov, being the
richest of the officers now that he had received his money, bought it.

"But don't hurt my little horse!" said the Alsatian good-naturedly to
Rostov when the animal was handed over to the hussar.

Rostov smilingly reassured the dragoon and gave him money.

"Alley! Alley!" said the Cossack, touching the prisoner's arm to make
him go on.

"The Emperor! The Emperor!" was suddenly heard among the hussars.

All began to run and bustle, and Rostov saw coming up the road behind
him several riders with white plumes in their hats. In a moment everyone
was in his place, waiting.

Rostov did not 
[truncated]


--- artifacts/textualize/textual/textual-llms.txt ---
# Textual

> Textual aims to enable developers to build user interfaces quickly and efficiently using Python. Its primary purpose is to provide a versatile framework that works in both terminal and web environments, allowing for the creation of interactive applications with minimal code. It also emphasizes ease of debugging, extensibility through custom commands, and deployment capabilities (e.g., serving apps via `textual serve`).

**Remember:**
- Cross-platform support for terminal and web
- Python-based API with asynchronous capabilities
- Decoupled component architecture
- Integrated testing framework
- Widget library with layout system
- CSS styling for UI design

## Docs
- [README](https://raw.githubusercontent.com/textualize/textual/main/docs/examples/styles/README.md): install & quickstart.
- [How To](https://raw.githubusercontent.com/textualize/textual/main/docs/how-to/index.md): worked example.
- [Tutorial](https://raw.githubusercontent.com/textualize/textual/main/docs/tutorial.md): worked example.
- [Border Sub Title Align All Example](https://raw.githubusercontent.com/textualize/textual/main/docs/snippets/border_sub_title_align_all_example.md): worked example.
- [Border Vs Outline Example](https://raw.githubusercontent.com/textualize/textual/main/docs/snippets/border_vs_outline_example.md): worked example.
- [Center Things](https://raw.githubusercontent.com/textualize/textual/main/docs/how-to/center-things.md): worked example.
- [Design A Layout](https://raw.githubusercontent.com/textualize/textual/main/docs/how-to/design-a-layout.md): worked example.
- [Package With Hatch](https://raw.githubusercontent.com/textualize/textual/main/docs/how-to/package-with-hatch.md): worked example.
- [Render And Compose](https://raw.githubusercontent.com/textualize/textual/main/docs/how-to/render-and-compose.md): worked example.
- [Style Inline Apps](https://raw.githubusercontent.com/textualize/textual/main/docs/how-to/style-inline-apps.md): worked example.

## Tutorials
- [README](https://raw.githubusercontent.com/textualize/textual/main/examples/README.md): install & quickstart.
- [Breakpoints](https://raw.githubusercontent.com/textualize/textual/main/examples/breakpoints.py): worked example.
- [Calculator](https://raw.githubusercontent.com/textualize/textual/main/examples/calculator.py): worked example.
- [Clock](https://raw.githubusercontent.com/textualize/textual/main/examples/clock.py): worked example.
- [Code Browser](https://raw.githubusercontent.com/textualize/textual/main/examples/code_browser.py): worked example.
- [Color Command](https://raw.githubusercontent.com/textualize/textual/main/examples/color_command.py): worked example.
- [Demo](https://raw.githubusercontent.com/textualize/textual/main/examples/demo.md): worked example.
- [Dictionary](https://raw.githubusercontent.com/textualize/textual/main/examples/dictionary.py): worked example.
- [Example](https://raw.githubusercontent.com/textualize/textual/main/examples/example.md): worked example.
- [Five By Five](https://raw.githubusercontent.com/textualize/textual/main/examples/five_by_five.md): worked example.

## API
- [README](https://raw.githubusercontent.com/textualize/textual/main/reference/README.md): install & quickstart.
- [Test Edit Via Api](https://raw.githubusercontent.com/textualize/textual/main/tests/text_area/test_edit_via_api.py): docs page.
- [Test Escaping](https://raw.githubusercontent.com/textualize/textual/main/tests/command_palette/test_escaping.py): docs page.
- [Test Option List Option Subclass](https://raw.githubusercontent.com/textualize/textual/main/tests/option_list/test_option_list_option_subclass.py): docs page.
- [Focus Component Class](https://raw.githubusercontent.com/textualize/textual/main/tests/snapshot_tests/snapshot_apps/focus_component_class.py): docs page.
- [Footer Classic Styling](https://raw.githubusercontent.com/textualize/textual/main/tests/snapshot_tests/snapshot_apps/footer_classic_styling.py): docs page.
- [Line Api Scrollbars](https://raw.githubusercontent.com/textualize/textual/main/tests/snapshot_tests/snapshot_apps/line_api_scrollbars.py): docs page.
- [Markdown Component Classes Reloading](https://raw.githubusercontent.com/textualize/textual/main/tests/snapshot_tests/snapshot_apps/markdown_component_classes_reloading.py): docs page.

## Concepts
- [Faq](https://raw.githubusercontent.com/textualize/textual/main/.faq/FAQ.md): core concept.
- [Suggest](https://raw.githubusercontent.com/textualize/textual/main/.faq/suggest.md): core concept.
- [Design](https://raw.githubusercontent.com/textualize/textual/main/src/textual/design.py): docs page.

## Optional
- [Changelog](https://raw.githubusercontent.com/textualize/textual/main/CHANGELOG.md): version history.
- [Contributing](https://raw.githubusercontent.com/textualize/textual/main/CONTRIBUTING.md): docs page.
- [Code Of Conduct](https://raw.githubusercontent.com/textualize/textual/main/CODE_OF_CONDUCT.md): docs page.
- [README](https://raw.githubusercontent.com/textualize/textual/main/README.md): docs page.

## .Github
- [Pull Request Template](https://raw.githubusercontent.com/textualize/textual/main/.github/PULL_REQUEST_TEMPLATE.md): docs page.
- [Bug Report](https://raw.githubusercontent.com/textualize/textual/main/.github/ISSUE_TEMPLATE/bug_report.md): docs page.

## Notes
- [README](https://raw.githubusercontent.com/textualize/textual/main/notes/README.md): install & quickstart.
- [Refresh](https://raw.githubusercontent.com/textualize/textual/main/notes/refresh.md): docs page.
- [Snapshot Testing](https://raw.githubusercontent.com/textualize/textual/main/notes/snapshot_testing.md): docs page.

## Questions
- [README](https://raw.githubusercontent.com/textualize/textual/main/questions/README.md): install & quickstart.
- [Align Center Middle.Question](https://raw.githubusercontent.com/textualize/textual/main/questions/align-center-middle.question.md): docs page.
- [Compose Result.Question](https://raw.githubusercontent.com/textualize/textual/main/questions/compose-result.question.md): docs page.
- [Copy Text.Question](https://raw.githubusercontent.com/textualize/textual/main/questions/copy-text.question.md): docs page.
- [Images.Question](https://raw.githubusercontent.com/textualize/textual/main/questions/images.question.md): docs page.
- [Pass Args To App.Question](https://raw.githubusercontent.com/textualize/textual/main/questions/pass-args-to-app.question.md): docs page.
- [Transparent Background.Question](https://raw.githubusercontent.com/textualize/textual/main/questions/transparent-background.question.md): docs page.
- [Why Do Some Keys Not Make It To My App.Question](https://raw.githubusercontent.com/textualize/textual/main/questions/why-do-some-keys-not-make-it-to-my-app.question.md): docs page.
- [Why Looks Bad On Macos.Question](https://raw.githubusercontent.com/textualize/textual/main/questions/why-looks-bad-on-macos.question.md): docs page.
- [Why No Ansi Themes.Question](https://raw.githubusercontent.com/textualize/textual/main/questions/why-no-ansi-themes.question.md): docs page.

## Src
- [Actions](https://raw.githubusercontent.com/textualize/textual/main/src/textual/actions.py): docs page.
- [Animator](https://raw.githubusercontent.com/textualize/textual/main/src/textual/_animator.py): docs page.
- [Ansi Sequences](https://raw.githubusercontent.com/textualize/textual/main/src/textual/_ansi_sequences.py): docs page.
- [Ansi Theme](https://raw.githubusercontent.com/textualize/textual/main/src/textual/_ansi_theme.py): docs page.
- [App](https://raw.githubusercontent.com/textualize/textual/main/src/textual/app.py): docs page.
- [Arrange](https://raw.githubusercontent.com/textualize/textual/main/src/textual/_arrange.py): docs page.
- [Await Complete](https://raw.githubusercontent.com/textualize/textual/main/src/textual/await_complete.py): docs page.
- [Await Remove](https://raw.githubusercontent.com/textualize/textual/main/src/textual/await_remove.py): docs page.
- [Binary Encode](https://raw.githubusercontent.com/textualize/textual/main/src/textual/_binary_encode.py): docs page.
- [Binding](https://raw.githubusercontent.com/textualize/textual/main/src/textual/binding.py): docs page.

## Tests
- [Deadlock](https://raw.githubusercontent.com/textualize/textual/main/tests/deadlock.py): docs page.
- [Init](https://raw.githubusercontent.com/textualize/textual/main/tests/__init__.py): docs page.
- [Test Actions](https://raw.githubusercontent.com/textualize/textual/main/tests/test_actions.py): docs page.
- [Test Animation](https://raw.githubusercontent.com/textualize/textual/main/tests/test_animation.py): docs page.
- [Test Animator](https://raw.githubusercontent.com/textualize/textual/main/tests/test_animator.py): docs page.
- [Test App](https://raw.githubusercontent.com/textualize/textual/main/tests/test_app.py): docs page.
- [Test App Focus Blur](https://raw.githubusercontent.com/textualize/textual/main/tests/test_app_focus_blur.py): docs page.
- [Test Arrange](https://raw.githubusercontent.com/textualize/textual/main/tests/test_arrange.py): docs page.
- [Test Auto Refresh](https://raw.githubusercontent.com/textualize/textual/main/tests/test_auto_refresh.py): docs page.
- [Test Await Remove](https://raw.githubusercontent.com/textualize/textual/main/tests/test_await_remove.py): docs page.

## Tools
- [Gen Easings Tests](https://raw.githubusercontent.com/textualize/textual/main/tools/gen_easings_tests.ts): docs page.
- [Widget Documentation](https://raw.githubusercontent.com/textualize/textual/main/tools/widget_documentation.py): docs page.


--- artifacts/textualize/textual/textual-llms-full.txt ---
# llms-full (private-aware)
> Built by GitHub fetches (raw or authenticated). Large files may be truncated.

--- docs/examples/styles/README.md ---
These are the examples from the documentation, used to generate screenshots.

You can run them with the textual CLI.

For example:

```
textual run text_style.py
```


--- docs/how-to/index.md ---
# How To

Welcome to the How To section.

Here you will find How To articles which cover various topics at a higher level than the Guide or Reference.
We will be adding more articles in the future.
If there is anything you would like to see covered, [open an issue](https://github.com/Textualize/textual/issues) in the Textual repository!


--- docs/tutorial.md ---
---
hide:
  - navigation
---

# Tutorial

Welcome to the Textual Tutorial!

By the end of this page you should have a solid understanding of app development with Textual.

!!! quote

    If you want people to build things, make it fun.

    &mdash; **Will McGugan** (creator of Rich and Textual)

## Video series

This tutorial has an accompanying [video series](https://www.youtube.com/playlist?list=PLHhDR_Q5Me1MxO4LmfzMNNQyKfwa275Qe) which covers the same content.

## Stopwatch Application

We're going to build a stopwatch application. This application should show a list of stopwatches with buttons to start, stop, and reset the stopwatches. We also want the user to be able to add and remove stopwatches as required.

This will be a simple yet **fully featured** app &mdash; you could distribute this app if you wanted to!

Here's what the finished app will look like:


```{.textual path="docs/examples/tutorial/stopwatch.py" title="stopwatch.py" press="tab,enter,tab,enter,tab,enter,tab,enter"}
```

!!! info

    Did you notice the `^p palette` at the bottom right hand corner?
    This is the [Command Palette](./guide/command_palette.md).
    You can think of it as a dedicated command prompt for your app.

### Get the code

If you want to try the finished Stopwatch app and follow along with the code, first make sure you have [Textual installed](getting_started.md) then check out the [Textual](https://github.com/Textualize/textual) repository:

=== "HTTPS"

    ```bash
    git clone https://github.com/Textualize/textual.git
    ```

=== "SSH"

    ```bash
    git clone git@github.com:Textualize/textual.git
    ```

=== "GitHub CLI"

    ```bash
    gh repo clone Textualize/textual
    ```


With the repository cloned, navigate to `docs/examples/tutorial` and run `stopwatch.py`.

```bash
cd textual/docs/examples/tutorial
python stopwatch.py
```

## Type hints (in brief)

!!! tip inline end

    Type hints are entirely optional in Textual. We've included them in the example code but it's up to you whether you add them to your own projects.

We're a big fan of Python type hints at Textualize. If you haven't encountered type hinting, it's a way to express the types of your data, parameters, and return values. Type hinting allows tools like [mypy](https://mypy.readthedocs.io/en/stable/) to catch bugs before your code runs.

The following function contains type hints:

```python
def repeat(text: str, count: int) -> str:
    """Repeat a string a given number of times."""
    return text * count
```

Parameter types follow a colon. So `text: str` indicates that `text` requires a string and `count: int` means that `count` requires an integer.

Return types follow `->`. So `-> str:` indicates this method returns a string.


## The App class

The first step in building a Textual app is to import and extend the `App` class. Here's a basic app class we will use as a starting point for the stopwatch app.

```python title="stopwatch01.py"
--8<-- "docs/examples/tutorial/stopwatch01.py"
```

If you run this code, you should see something like the following:


```{.textual path="docs/examples/tutorial/stopwatch01.py" title="stopwatch01.py"}
```

Hit the ++d++ key to toggle between light and dark themes.

```{.textual path="docs/examples/tutorial/stopwatch01.py" press="d" title="stopwatch01.py"}
```

Hit ++ctrl+q++ to exit the app and return to the command prompt.

### A closer look at the App class

Let's examine `stopwatch01.py` in more detail.

```python title="stopwatch01.py" hl_lines="1 2"
--8<-- "docs/examples/tutorial/stopwatch01.py"
```

The first line imports `App` class, which is the base class for all Textual apps.
The second line imports two builtin widgets: [`Footer`](widgets/footer.md) which shows a bar at the bottom of the screen with bound keys, and [`Header`](widgets/header.md) which shows a title at the top of the screen.
Widgets are re-usable components responsible for managing a part of the screen.
We will cover how to build widgets in this tutorial.

The following lines define the app itself:

```python title="stopwatch01.py" hl_lines="5-19"
--8<-- "docs/examples/tutorial/stopwatch01.py"
```

The App class is where most of the logic of Textual apps is written. It is responsible for loading configuration, setting up widgets, handling keys, and more.

Here's what the above app defines:

- `BINDINGS` is a list of tuples that maps (or *binds*) keys to actions in your app. The first value in the tuple is the key; the second value is the name of the action; the final value is a short description. We have a single binding which maps the ++d++ key on to the "toggle_dark" action. See [key bindings](./guide/input.md#bindings) in the guide for details.

-  `compose()` is where we construct a user interface with widgets. The `compose()` method may return a list of widgets, but it is generally easier to _yield_ them (making this method a generator). In the example code we yield an instance of each of the widget classes we imported, i.e. `Header()` and `Footer()`.

- `action_toggle_dark()` defines an _action_ method. Actions are methods beginning with `action_` followed by the name of the action. The `BINDINGS` list above tells Textual to run this action when the user hits the ++d++ key. See [actions](./guide/actions.md) in the guide for details.

```python title="stopwatch01.py" hl_lines="22-24"
--8<-- "docs/examples/tutorial/stopwatch01.py"
```

The final three lines create an instance of the app and calls the [run()][textual.app.App.run] method which puts your terminal into *application mode* and runs the app until you exit with ++ctrl+q++. This happens within a `__name__ == "__main__"` block so we could run the app with `python stopwatch01.py` or import it as part of a larger project.

## Designing a UI with widgets

Textual has a large number of [builtin widgets](./widget_gallery.md).
For our app we will need new widgets, which we can create by extending and combining the builtin widgets.

Before we dive into building widgets, let's first sketch a design for the app &mdash; so we know what we're aiming for.


<div class="excalidraw">
--8<-- "docs/images/stopwatch.excalidraw.svg"
</div>

### Custom widgets

We need a `Stopwatch` widget composed of the following _child_ widgets:

- A "Start" button
- A "Stop" button
- A "Reset" button
- A time display

Let's add those to the app.
Just a skeleton for now, we will add the rest of the features as we go.

```python title="stopwatch02.py" hl_lines="2-3 6-7 10-18 30"
--8<-- "docs/examples/tutorial/stopwatch02.py"
```

We've imported two new widgets in this code: [`Button`](widgets/button.md) for the buttons and [`Digits`](widgets/digits.md) for the time display.
Additionally, we've imported [`HorizontalGroup`][textual.containers.HorizontalGroup] and [`VerticalScroll`][textual.containers.VerticalScroll] from `textual.containers` (as the name of the module suggests, *containers* are widgets which contain other widgets).
We will use these container widgets to define the general layout of our interface.

The `TimeDisplay` is currently very simple, all it does is extend `Digits` without adding any new features. We will flesh this out later.

The `Stopwatch` widget class extends the `HorizontalGroup` container class, which will arrange its children into a horizontal row. The Stopwatch's `compose()` adds those children, which correspond to the components from the sketch above.

!!! tip "Coordinating widgets"

    If you are building custom widgets of your own, be sure to see guide on [coordinating widgets](./guide/widgets.md#coordinating-widgets).

#### The buttons

The Button constructor takes a label to be displayed in the button (`"Start"`, `"Stop"`, or `"Reset"`). Additionally, some of the buttons set the following parameters:

- `id` is an identifier we can use to tell the buttons apart in code and apply styles. More on that later.
- `variant` is a string which selects a default style. The "success" variant makes the button green, and the "error" variant makes it red.

### Composing the widgets

The new line in `StopwatchApp.compose()` yields a single `VerticalScroll` which will scroll if the contents don't quite fit. This widget also takes care of key bindings required for scrolling, like ++up++, ++down++, ++page-down++, ++page-up++, ++home++, ++end++, etc.

When widgets contain other widgets (like `VerticalScroll`) they will typically accept their child widgets as positional arguments.
So the line `yield VerticalScroll(Stopwatch(), Stopwatch(), Stopwatch())` creates a `VerticalScroll` containing three `Stopwatch` widgets.


### The unstyled app

Let's see what happens when we run `stopwatch02.py`.

```{.textual path="docs/examples/tutorial/stopwatch02.py" title="stopwatch02.py"}
```

The elements of the stopwatch application are there, but it doesn't look much like the sketch. This is because we have yet to apply any _styles_ to our new widgets.

## Writing Textual CSS

Every widget has a `styles` object with a number of attributes that impact how the widget will appear. Here's how you might set white text and a blue background for a widget:

```python
self.styles.background = "blue"
self.styles.color = "white"
```

While it's possible to set all styles for an app this way, it is rarely necessary. Textual has support for CSS (Cascading Style Sheets), a technology used by web browsers. CSS files are data files loaded by your app which contain information about styles to apply to your widgets.

!!! info

    The dialect of CSS used in Textual is greatly simplified over web based CSS and easier to learn.


CSS makes it easy to iterate on the design of your app and enables [live-editing](./guide/devtools.md#live-editing) &mdash; you can edit CSS and see the changes without restarting the app!


Let's add a CSS file to our application.

```python title="stopwatch03.py" hl_lines="24"
--8<-- "docs/examples/tutorial/stopwatch03.py"
```

Adding the `CSS_PATH` class variable tells Textual to load the following file when the app starts:

```css title="stopwatch03.tcss"
--8<-- "docs/examples/tutorial/stopwatch03.tcss"
```

If we run the app now, it will look *very* different.

```{.textual path="docs/examples/tutorial/stopwatch03.py" title="stopwatch03.py"}
```

This app looks much more like our sketch. Let's look at how Textual uses `stopwatch03.tcss` to apply styles.

### CSS basics

CSS files contain a number of _declaration blocks_. Here's the first such block from `stopwatch03.tcss` again:

```css
Stopwatch {
    background: $boost;
    height: 5;
    margin: 1;
    min-width: 50;
    padding: 1;
}
```

The first line tells Textual that the styles should apply to the `Stopwatch` widget. The lines between the curly brackets contain the styles themselves.

Here's how this CSS code changes how the `Stopwatch` widget is displayed.

<div class="excalidraw">
--8<-- "docs/images/stopwatch_widgets.excalidraw.svg"
</div>

- `background: $boost` sets the background color to `$boost`. The `$` prefix picks a pre-defined color from the builtin theme. There are other ways to specify colors such as `"blue"` or `rgb(20,46,210)`.
- `height: 5` sets the height of our widget to 5 lines of text.
- `margin: 1` sets a margin of 1 cell around the `Stopwatch` widget to create a little space between widgets in the list.
- `min-width: 50` sets the minimum width of our widget to 50 cells.
- `padding: 1` sets a padding of 1 cell around the child widgets.


Here's the rest of `stopwatch03.tcss` which contains further declaration blocks:

```css
TimeDisplay {   
    text-align: center;
    color: $foreground-muted;
    height: 3;
}

Button {
    width: 16;
}

#start {
    dock: left;
}

#stop {
    dock: left;
    display: none;
}

#reset {
    dock: right;
}
```

The `TimeDisplay` block aligns text to the center (`text-align:`), sets its color (`color:`), and sets its height (`height:`) to 3 lines.

The `Button` block sets the width (`width:`) of buttons to 16 cells (character widths).

The last 3 blocks have a slightly different format. When the declaration begins with a `#` then the styles will be applied to widgets with a matching "id" attribute. We've set an ID on the `Button` widgets we yielded in `compose`. For instance the first button has `id="start"` which matches `#start` in the CSS.

The buttons have a `dock` style which aligns the widget to a given edge.
The start and stop buttons are docked to the left edge, while the reset button is docked to the right edge.

You may have noticed that the stop button (`#stop` in the CSS) has `display: none;`. This tells Textual to not show the button. We do this because we don't want to display the stop button when the timer is *not* running. Similarly, we don't want to show the start button when the timer is running. We will cover how to manage such dynamic user interfaces in the next section.

### Dynamic CSS

We want our `Stopwatch` widget to have two states: a default state with a Start and Reset button; and a _started_ state with a Stop button. When a stopwatch is started it should also have a green background to indicate it is currently active.

<div class="excalidraw">
--8<-- "docs/images/css_stopwatch.excalidraw.svg"
</div>


We can accomplish this with a CSS _class_. Not to be confused with a Python class, a CSS class is like a tag you can add to a widget to modify its styles. A widget may have any number of CSS classes, which may be added and removed to change its appearance.

Here's the new CSS:

```css title="stopwatch04.tcss" hl_lines="32-52"
--8<-- "docs/examples/tutorial/stopwatch04.tcss"
```

These new rules are prefixed with `.started`. The `.` indicates that `.started` refers to a CSS class called "started". The new styles will be applied only to widgets that have this CSS class.

Some of the new styles have more than one selector separated by a space. The space indicates that the rule should match the second selector if it is a child of the first. Let's look at one of these styles:

```css
.started #start {
    display: none
}
```

The `.started` selector matches any widget with a `"started"` CSS class.
While `#start` matches a widget with an ID of `"start"`.
Combining the two selectors with a space (`.started #start`) creates a new selector that will match the start button *only* if it is also inside a container with a CSS class of "started".

As before, the `display: none` rule will cause any matching widgets to be hidden from view. 

If we were to write this in English, it would be something like: "Hide the start button if the widget is already started".

### Manipulating classes

Modifying a widget's CSS classes is a convenient way to update visuals without introducing a lot of messy display related code.

You can add and remove CSS classes with the [add_class()][textual.dom.DOMNode.add_class] and [remove_class()][textual.dom.DOMNode.remove_class] methods.
We will use these methods to connect the started state to the Start / Stop buttons.

The following code will start or stop the stopwatches in response to clicking a button:

```python title="stopwatch04.py" hl_lines="13-18"
--8<-- "docs/examples/tutorial/stopwatch04.py"
```

The `on_button_pressed` method is an *event handler*. Event handlers are methods called by Textual in response to an *event* such as a key press, mouse click, etc.
Event handlers begin with `on_` followed by the name of the event they will handle.
Hence `on_button_pressed` will handle the button pressed event.

See the guide on [message handlers](./guide/events.md#message-handlers) for the details on how to write event handlers.

If you run `stopwatch04.py` now you will be able to toggle between the two states by clicking the first button:

```{.textual path="docs/examples/tutorial/stopwatch04.py" title="stopwatch04.py" press="tab,tab,tab,enter"}
```

When the button event handler adds or removes the `"started"` CSS class, Textual re-applies the CSS and updates the visuals.


## Reactive attributes

A recurring theme in Textual is that you rarely need to explicitly update a widget's visuals.
It is possible: you can call [refresh()][textual.widget.Widget.refresh] to display new data.
However, Textual prefers to do this automatically via _reactive_ attributes.

Reactive attributes work like any other attribute, such as those you might set in an `__init__` method, but allow Textual to detect when you assign to them, in addition to some other [*superpowers*](./guide/reactivity.md).

To add a reactive attribute, import [reactive][textual.reactive.reactive] and create an instance in your class scope.

Let's add reactives to our stopwatch to calculate and display the elapsed time.

```python title="stopwatch05.py" hl_lines="1 5 12-27 45"
--8<-- "docs/examples/tutorial/stopwatch05.py"
```

We have added two reactive attributes to the `TimeDisplay` widget: `start_time` will contain the time the stopwatch was started (in seconds), and `time` will contain the time to be displayed in the `Stopwatch` widget.

Both attributes will be available on `self` as if you had assigned them in `__init__`.
If you write to either of these attributes the widget will update automatically.

!!! info

    The `monotonic` function in this example is imported from the standard library `time` module.
    It is similar to `time.time` but won't go backwards if the system clock is changed.

The first argument to `reactive` may be a default value for the attribute or a callable that returns a default value.
We set the default for `start_time` to the `monotonic` function which will be called to initialize the attribute with the current time when the `TimeDisplay` is added to the app.
The `time` attribute has a simple float as the default, so `self.time` will be initialized to `0`.


The `on_mount` method is an event handler called when the widget is first added to the application (or _mounted_ in Textual terminology). In this method we call [set_interval()][textual.message_pump.MessagePump.set_interval] to create a timer which calls `self.update_time` sixty times a second. This `update_time` method calculates the time elapsed since the widget started and assigns it to `self.time` &mdash; which brings us to one of Reactive's super-powers.

If you implement a method that begins with `watch_` followed by the name of a reactive attribute, then the method will be called when the attribute is modified.
Such methods are known as *watch methods*.

Because `watch_time` watches the `time` attribute, when we update `self.time` 60 times a second we also implicitly call `watch_time` which converts the elapsed time to a string and updates the widget with a call to `self.update`.
Because this happens automatically, we don't need to pass in an initial argument to `TimeDisplay`.

The end result is that the `Stopwatch` widgets show the time elapsed since the widget was created:

```{.textual path="docs/examples/tutorial/stopwatch05.py" title="stopwatch05.py"}
```

We've seen how we can update widgets with a timer, but we still need to wire up the buttons so we can operate stopwatches independently.

### Wiring buttons

We need to be able to start, stop, and reset each stopwatch independently. We can do this by adding a few more methods to the `TimeDisplay` class.


```python title="stopwatch06.py" hl_lines="14 18 22 30-44 50-61"
--8<-- "docs/examples/tutorial/stopwatch06.py"
```

Here's a summary of the changes made to `TimeDisplay`.

- We've added a `total` reactive attribute to store the total time elapsed between clicking the start and stop buttons.
- The call to `set_interval` has grown a `pause=True` argument which starts the timer in pause mode (when a timer is paused it won't run until [resume()][textual.timer.Timer.resume] is called). This is because we don't want the time to update until the user hits the start button.
- The `update_time` method now adds `total` to the current time to account for the time between any previous clicks of the start and stop buttons.
- We've stored the result of `set_interval` which returns a [Timer](textual.timer.Timer) object. We will use this to _resume_ the timer when we start the Stopwatch.
- We've added `start()`, `stop()`, and `reset()` methods.

In addition, the `on_button_pressed` method on `Stopwatch` has grown some code to manage the time display when the user clicks a button. Let's look at that in detail:

```python
    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Event handler called when a button is pressed."""
        button_id = event.button.id
        time_display = self.query_one(TimeDisplay)
        if button_id == "start":
            time_display.start()
            self.add_class("started")
        elif button_id == "stop":
            time_display.stop()
            self.remove_class("started")
        elif button_id == "reset":
            time_display.reset()
```

This code supplies missing features and makes our app useful. We've made the following changes.

- The first line retrieves `id` attribute of the button that was pressed. We can use this to decide what to do in response.
- The second line calls [`query_one`][textual.dom.DOMNode.query_one] to get a reference to the `TimeDisplay` widget.
- We call the method on `TimeDisplay` that matches the pressed button.
- We add the `"started"` class when the Stopwatch is started (`self.add_class("started")`), and remove it (`self.remove_class("started")`) when it is stopped. This will update the Stopwatch visuals via CSS.

If you run `stopwatch06.py` you will be able to use the stopwatches independently.

```{.textual path="docs/examples/tutorial/stopwatch06.py" title="stopwatch06.py" press="tab,enter,tab,enter,tab"}
```

The only remaining feature of the Stopwatch app left to implement is the ability to add and remove stopwatches.

## Dynamic widgets

The Stopwatch app creates widgets when it starts via the `compose` method. We will also need to create new widgets while the app is running, and remove widgets we no longer need. We can do this by calling [mount()][textual.widget.Widget.mount] to add a widget, and [remove()][textual.widget.Widget.remove] to remove a widget.

Let's use these methods to implement adding and removing stopwatches to our app.

```python title="stopwatch.py" hl_lines="78-79 86 88-92 94-98"
--8<-- "docs/examples/tutorial/stopwatch.py"
```

Here's a summary of the changes:

- The `VerticalScroll` object in `StopWatchApp` grew a `"timers"` ID.
- Added `action_add_stopwatch` to add a new stopwatch.
- Added `action_remove_stopwatch` to remove a stopwatch.
- Added keybindings for the actions.

The `action_add_stopwatch` method creates and mounts a new stopwatch. Note the call to [query_one()][textual.dom.DOMNode.query_one] with a CSS selector of `"#timers"` which gets the timer's container via its ID.
Once mounted, the new Stopwatch will appear in the terminal. That last line in `action_add_stopwatch` calls [scroll_visible()][textual.widget.Widget.scroll_visible] which will scroll the container to make the new `Stopwatch` visible (if required).

The `action_remove_stopwatch` function calls [query()][textual.dom.DOMNode.query] with a CSS selector of `"Stopwatch"` which gets all the `Stopwatch` widgets.
If there are stopwatches then the action calls [last()][textual.css.query.DOMQuery.last] to get the last stopwatch, and [remove()][textual.css.query.DOMQuery.remove] to remove it.

If you run `stopwatch.py` now you can add a new stopwatch with the ++a++ key and remove a stopwatch with ++r++.

```{.textual path="docs/examples/tutorial/stopwatch.py" title="stopwatch.py" press="d,a,a,a,a,a,a,a,tab,enter,tab"}
```

## What next?

Congratulations on building your first Textual application! This tutorial has covered a lot of ground. If you are the type that prefers to learn a framework by coding, feel free. You could tweak `stopwatch.py` or look through the examples.

Read the guide for the full details on how to build sophisticated TUI applications with Textual.


--- docs/snippets/border_sub_title_align_all_example.md ---
This example shows all border title and subtitle alignments, together with some examples of how (sub)titles can have custom markup.
Open the code tabs to see the details of the code examples.

=== "Output"

    ```{.textual path="docs/examples/styles/border_sub_title_align_all.py"}
    ```

=== "border_sub_title_align_all.py"

    ```py hl_lines="6 20 26 32 41 42 44 47 53 59 65"
    --8<-- "docs/examples/styles/border_sub_title_align_all.py"
    ```

    1. Border (sub)titles can contain nested markup.
    2. Long (sub)titles get truncated and occupy as much space as possible.
    3. (Sub)titles can be stylised with Rich markup.
    4. An empty (sub)title isn't displayed.
    5. The markup can even contain Rich links.
    6. If the widget does not have a border, the title and subtitle are not shown.
    7. When the side borders are not set, the (sub)title will align with the edge of the widget.
    8. The title and subtitle are aligned on the left and very long, so they get truncated and we can still see the rightmost character of the border edge.
    9. The title and subtitle are centered and very long, so they get truncated and are centered with one character of padding on each side.
    10. The title and subtitle are aligned on the right and very long, so they get truncated and we can still see the leftmost character of the border edge.
    11. An auxiliary function to create labels with border title and subtitle.

=== "border_sub_title_align_all.tcss"

    ```css hl_lines="12 16 30 34 41 46"
    --8<-- "docs/examples/styles/border_sub_title_align_all.tcss"
    ```

    1. The default alignment for the title is `left` and the default alignment for the subtitle is `right`.
    2. Specifying an alignment when the (sub)title is too long has no effect. (Although, it will have an effect if the (sub)title is shortened or if the widget is widened.)
    3. Setting the alignment does not affect empty (sub)titles.
    4. If the border is not set, or set to `none`/`hidden`, the (sub)title is not shown.
    5. If the (sub)title alignment is on a side which does not have a border edge, the (sub)title will be flush to that side.
    6. Naturally, (sub)title positioning is affected by padding.


--- docs/snippets/border_vs_outline_example.md ---
The next example makes the difference between [`border`](../styles/border.md) and [`outline`](../styles/outline.md) clearer by having three labels side-by-side.
They contain the same text, have the same width and height, and are styled exactly the same up to their [`border`](../styles/border.md) and [`outline`](../styles/outline.md) styles.

This example also shows that a widget cannot contain both a `border` and an `outline`:

=== "Output"

    ```{.textual path="docs/examples/styles/outline_vs_border.py"}
    ```

=== "outline_vs_border.py"

    ```python
    --8<-- "docs/examples/styles/outline_vs_border.py"
    ```

=== "outline_vs_border.tcss"

    ```css hl_lines="5-7 9-11"
    --8<-- "docs/examples/styles/outline_vs_border.tcss"
    ```


--- docs/how-to/center-things.md ---
# Center things

If you have ever needed to center something in a web page, you will be glad to know it is **much** easier in Textual.

This article discusses a few different ways in which things can be centered, and the differences between them.

## Aligning widgets

The [align](../styles/align.md) rule will center a widget relative to one or both edges.
This rule is applied to a *container*, and will impact how the container's children are arranged.
Let's see this in practice with a trivial app containing a [Static](../widgets/static.md) widget:

```python
--8<-- "docs/examples/how-to/center01.py"
```

Here's the output:

```{.textual path="docs/examples/how-to/center01.py"}
```

The container of the widget is the screen, which has the `align: center middle;` rule applied. The
`center` part tells Textual to align in the horizontal direction, and `middle` tells Textual to align in the vertical direction.

The output *may* surprise you.
The text appears to be aligned in the middle (i.e. vertical edge), but *left* aligned on the horizontal.
This isn't a bug &mdash; I promise.
Let's make a small change to reveal what is happening here.
In the next example, we will add a background and a border to our text:

!!! tip

    Adding a border is a very good way of visualizing layout issues, if something isn't behaving as you would expect.

```python hl_lines="13-16 20"
--8<-- "docs/examples/how-to/center02.py"
```

The static widget will now have a blue background and white border:

```{.textual path="docs/examples/how-to/center02.py"}
```

Note the static widget is as wide as the screen.
Since the widget is as wide as its container, there is no room for it to move in the horizontal direction.

!!! info

    The `align` rule applies to *widgets*, not the text.

In order to see the `center` alignment, we will have to make the widget smaller than the width of the screen.
Let's set the width of the Static widget to `auto`, which will make the widget just wide enough to fit the content:

```python hl_lines="16"
--8<-- "docs/examples/how-to/center03.py"
```

If you run this now, you should see the widget is aligned on both axis:

```{.textual path="docs/examples/how-to/center03.py"}
```

## Aligning text

In addition to aligning widgets, you may also want to align *text*.
In order to demonstrate the difference, lets update the example with some longer text.
We will also set the width of the widget to something smaller, to force the text to wrap.

```python hl_lines="4 18 23"
--8<-- "docs/examples/how-to/center04.py"
```

Here's what it looks like with longer text:

```{.textual path="docs/examples/how-to/center04.py"}
```

Note how the widget is centered, but the text within it is flushed to the left edge.
Left aligned text is the default, but you can also center the text with the [text-align](../styles/text_align.md) rule.
Let's center align the longer text by setting this rule:

```python hl_lines="19"
--8<-- "docs/examples/how-to/center05.py"
```

If you run this, you will see that each line of text is individually centered:

```{.textual path="docs/examples/how-to/center05.py"}
```

You can also use `text-align` to right align text or justify the text (align to both edges).

## Aligning content

There is one last rule that can help us center things.
The [content-align](../styles/content_align.md) rule aligns content *within* a widget.
It treats the text as a rectangular region and positions it relative to the space inside a widget's border.

In order to see why we might need this rule, we need to make the Static widget larger than required to fit the text.
Let's set the height of the Static widget to 9 to give the content room to move:

```python hl_lines="19"
--8<-- "docs/examples/how-to/center06.py"
```

Here's what it looks like with the larger widget:

```{.textual path="docs/examples/how-to/center06.py"}
```

Textual aligns a widget's content to the top border by default, which is why the space is below the text.
We can tell Textual to align the content to the center by setting `content-align: center middle`;

!!! note

    Strictly speaking, we only need to align the content vertically here (there is no room to move the content left or right)
    So we could have done `content-align-vertical: middle;`

```python hl_lines="21"
--8<-- "docs/examples/how-to/center07.py"
```

If you run this now, the content will be centered within the widget:

```{.textual path="docs/examples/how-to/center07.py"}
```

## Aligning multiple widgets

It's just as easy to align multiple widgets as it is a single widget.
Applying `align: center middle;` to the parent widget (screen or other container) will align all its children.

Let's create an example with two widgets.
The following code adds two widgets with auto dimensions:

```python
--8<-- "docs/examples/how-to/center08.py"
```

This produces the following output:

```{.textual path="docs/examples/how-to/center08.py"}
```

We can center both those widgets by applying the `align` rule as before:

```python hl_lines="9-11"
--8<-- "docs/examples/how-to/center09.py"
```

Here's the output:

```{.textual path="docs/examples/how-to/center09.py"}
```

Note how the widgets are aligned as if they are a single group.
In other words, their position relative to each other didn't change, just their position relative to the screen.

If you do want to center each widget independently, you can place each widget inside its own container, and set `align` for those containers.
Textual has a builtin [`Center`][textual.containers.Center] container for just this purpose.

Let's wrap our two widgets in a `Center` container:

```python hl_lines="2 22 24"
--8<-- "docs/examples/how-to/center10.py"
```

If you run this, you will see that the widgets are centered relative to each other, not just the screen:

```{.textual path="docs/examples/how-to/center10.py"}
```

## Summary

Keep the following in mind when you want to center content in Textual:

- In order to center a widget, it needs to be smaller than its container.
- The `align` rule is applied to the *parent* of the widget you want to center (i.e. the widget's container).
- The `text-align` rule aligns text on a line by line basis.
- The `content-align` rule aligns content *within* a widget.
- Use the [`Center`][textual.containers.Center] container if you want to align multiple widgets relative to each other.
- Add a border if the alignment isn't working as you would expect.

---

If you need further help, we are here to [help](../help.md).


--- docs/how-to/design-a-layout.md ---
# Design a Layout

This article discusses an approach you can take when designing the layout for your applications.

Textual's layout system is flexible enough to accommodate just about any application design you could conceive of, but it may be hard to know where to start. We will go through a few tips which will help you get over the initial hurdle of designing an application layout.


## Tip 1. Make a sketch

The initial design of your application is best done with a sketch.
You could use a drawing package such as [Excalidraw](https://excalidraw.com/) for your sketch, but pen and paper is equally as good.

Start by drawing a rectangle to represent a blank terminal, then draw a rectangle for each element in your application. Annotate each of the rectangles with the content they will contain, and note whether they will scroll (and in what direction).

For the purposes of this article we are going to design a layout for a Twitter or Mastodon client, which will have a header / footer and a number of columns.

!!! note

    The approach we are discussing here is applicable even if the app you want to build looks nothing like our sketch!

Here's our sketch:

<div class="excalidraw">
--8<-- "docs/images/how-to/layout.excalidraw.svg"
</div>

It's rough, but it's all we need.

## Tip 2. Work outside in

Like a sculpture with a block of marble, it is best to work from the outside towards the center.
If your design has fixed elements (like a header, footer, or sidebar), start with those first.

In our sketch we have a header and footer.
Since these are the outermost widgets, we will begin by adding them.

!!! tip

    Textual has builtin [Header](../widgets/header.md) and [Footer](../widgets/footer.md) widgets which you could use in a real application.

The following example defines an [app](../guide/app.md), a [screen](../guide/screens.md), and our header and footer widgets.
Since we're starting from scratch and don't have any functionality for our widgets, we are going to use the [Placeholder][textual.widgets.Placeholder] widget to help us visualize our design.

In a real app, we would replace these placeholders with more useful content.

=== "layout01.py"

    ```python
    --8<-- "docs/examples/how-to/layout01.py"
    ```

    1. The Header widget extends Placeholder.
    2. The footer widget extends Placeholder.
    3. Creates the header widget (the id will be displayed within the placeholder widget).
    4. Creates the footer widget.

=== "Output"

    ```{.textual path="docs/examples/how-to/layout01.py"}
    ```

## Tip 3. Apply docks

This app works, but the header and footer don't behave as expected.
We want both of these widgets to be fixed to an edge of the screen and limited in height.
In Textual this is known as *docking* which you can apply with the [dock](../styles/dock.md) rule.

We will dock the header and footer to the top and bottom edges of the screen respectively, by adding a little [CSS](../guide/CSS.md) to the widget classes:

=== "layout02.py"

    ```python hl_lines="7-12 16-21"
    --8<-- "docs/examples/how-to/layout02.py"
    ```

=== "Output"

    ```{.textual path="docs/examples/how-to/layout02.py"}
    ```

The `DEFAULT_CSS` class variable is used to set CSS directly in Python code.
We could define these in an external CSS file, but writing the CSS inline like this can be convenient if it isn't too complex.

When you dock a widget, it reduces the available area for other widgets.
This means that Textual will automatically compensate for the 6 additional lines reserved for the header and footer.

## Tip 4. Use FR Units for flexible things

After we've added the header and footer, we want the remaining space to be used for the main interface, which will contain the columns in the sketch.
This area is flexible (will change according to the size of the terminal), so how do we ensure that it takes up precisely the space needed?

The simplest way is to use [fr](../css_types/scalar.md#fraction) units.
By setting both the width and height to `1fr`, we are telling Textual to divide the space equally amongst the remaining widgets.
There is only a single widget, so that widget will fill all of the remaining space.

Let's make that change.

=== "layout03.py"

    ```python hl_lines="24-31 38"
    --8<-- "docs/examples/how-to/layout03.py"
    ```

    1. Here's where we set the width and height to `1fr`. We also add a border just to illustrate the dimensions better.

=== "Output"

    ```{.textual path="docs/examples/how-to/layout03.py"}
    ```

As you can see, the central Columns area will resize with the terminal window.

## Tip 5. Use containers

Before we add content to the Columns area, we have an opportunity to simplify.
Rather than extend `Placeholder` for our `ColumnsContainer` widget, we can use one of the builtin *containers*.
A container is simply a widget designed to *contain* other widgets.
Containers are styled with `fr` units to fill the remaining space so we won't need to add any more CSS.

Let's replace the `ColumnsContainer` class in the previous example with a `HorizontalScroll` container, which also adds an automatic horizontal scrollbar.

=== "layout04.py"

    ```python hl_lines="2 29"
    --8<-- "docs/examples/how-to/layout04.py"
    ```

    1. The builtin container widget.


=== "Output"

    ```{.textual path="docs/examples/how-to/layout04.py"}
    ```

The container will appear as blank space until we add some widgets to it.

Let's add the columns to the `HorizontalScroll`.
A column is itself a container which will have a vertical scrollbar, so we will define our `Column` by subclassing `VerticalScroll`.
In a real app, these columns will likely be added dynamically from some kind of configuration, but let's add 4 to visualize the layout.

We will also define a `Tweet` placeholder and add a few to each column.

=== "layout05.py"

    ```python hl_lines="2 25-26 29-32 39-43"
    --8<-- "docs/examples/how-to/layout05.py"
    ```

=== "Output"

    ```{.textual path="docs/examples/how-to/layout05.py"}
    ```

Note from the output that each `Column` takes a quarter of the screen width.
This happens because `Column` extends a container which has a width of `1fr`.

It makes more sense for a column in a Twitter / Mastodon client to use a fixed width.
Let's set the width of the columns to 32.

We also want to reduce the height of each "tweet".
In the real app, you might set the height to "auto" so it fits the content, but lets set it to 5 lines for now.

Here's the final example and a reminder of the sketch.

=== "layout06.py"

    ```python hl_lines="25-32 36-46"
    --8<-- "docs/examples/how-to/layout06.py"
    ```

=== "Output"

    ```{.textual path="docs/examples/how-to/layout06.py" columns="100" lines="32"}
    ```

=== "Sketch"

    <div class="excalidraw">
    --8<-- "docs/images/how-to/layout.excalidraw.svg"
    </div>


A layout like this is a great starting point.
In a real app, you would start replacing each of the placeholders with [builtin](../widget_gallery.md) or [custom](../guide/widgets.md) widgets.


## Summary

Layout is the first thing you will tackle when building a Textual app.
The following tips will help you get started.

1. Make a sketch (pen and paper is fine).
2. Work outside in. Start with the entire space of the terminal, add the outermost content first.
3. Dock fixed widgets. If the content doesn't move or scroll, you probably want to *dock* it.
4. Make use of `fr` for flexible space within layouts.
5. Use containers to contain other widgets, particularly if they scroll!

---

If you need further help, we are here to [help](../help.md).


--- docs/how-to/package-with-hatch.md ---
# Package a Textual app with Hatch

Python apps may be distributed via [PyPI](https://pypi.org/) so they can be installed via `pip`.
This is known as *packaging*.
The packaging process for Textual apps is much the same as any Python library, with the additional requirement that we can launch our app from the command line.

!!! tip

    An alternative to packaging your app is to turn it into a web application with [textual-serve](https://github.com/Textualize/textual-serve).

In this How To we will cover how to use [Hatch](https://github.com/pypa/hatch) to package an example application.

Hatch is a *build tool* (a command line app to assist with packaging).
You could use any build tool to package a Textual app (such as [Poetry](https://python-poetry.org/) for example), but Hatch is a good choice given its large feature set and ease of use.


!!! info inline end "Calculator example"

    ```{.textual path="examples/calculator.py" columns=100 lines=41 press="3,.,1,4,5,9,2,wait:400"}
    ```

    This example is [`calculator.py`](https://github.com/Textualize/textual/blob/main/examples/calculator.py) taken from the examples directory in the Textual repository.


## Foreword

Packaging with Python can be a little intimidating if you haven't tackled it before.
But it's not all that complicated. 
When you have been through it once or twice, you should find it fairly straightforward.

## Example repository

See the [textual-calculator-hatch](https://github.com/Textualize/textual-calculator-hatch) repository for the project created in this How To.

## The example app

To demonstrate packaging we are going to take the calculator example from the examples directory, and publish it to PyPI.
The end goal is to allow a user to install it with pip:


```bash
pip install textual-calculator
```

Then launch the app from the command line:

```bash
calculator
```

## Installing Hatch

There are a few ways to install Hatch.
See the [official docs on installation](https://hatch.pypa.io/latest/install/) for the best method for your operating system.

Once installed, you should have the `hatch` command available on the command line.
Run the following to check Hatch was installed correctly:

```bash
hatch
```

## Hatch new

Hatch can create an initial directory structure and files with the `new` *subcommand*.
Enter `hatch new` followed by the name of your project.
For the calculator example, the name will be "textual calculator":

```batch
hatch new "textual calculator"
```

This will create the following directory structure:

```
textual-calculator
├── LICENSE.txt
├── README.md
├── pyproject.toml
├── src
│   └── textual_calculator
│       ├── __about__.py
│       └── __init__.py
└── tests
    └── __init__.py
```

This follows a well established convention when packaging Python code, and will create the following files:

- `LICENSE.txt` contains the license you want to distribute your code under.
- `README.md` is a markdown file containing information about your project, which will be displayed in PyPI and Github (if you use it). You can edit this with information about your app and how to use it.
- `pyproject.toml` is a [TOML](https://en.wikipedia.org/wiki/TOML) file which contains *metadata* (additional information) about your project and how to package it. This is a Python standard. This file may be edited manually or by a build tool (such as Hatch).
- `src/textual_calculator/__about__.py` contains the version number of your app. You should update this when you release new versions.
- `src/textual_calculator/__init__.py`  and `tests/__init__py` indicate the directory they are within contains Python code (these files are often empty).
 
In the top level is a directory called `src`.
This should contain a directory named after your project, and will be the name your code can be imported from.
In our example, this directory is `textual_calculator` so we can do `import textual_calculator` in Python code.

Additionally, there is a `tests` directory where you can add any [test](../guide/testing.md) code.

### More on naming

Note how Hatch replaced the space in the project name with a hyphen (i.e. `textual-calculator`), but the directory in `src` with an underscore (i.e. `textual_calculator`). This is because the directory in `src` contains the Python module, and a hyphen is not legal in a Python import. The top-level directory doesn't have this restriction and uses a hyphen, which is more typical for a directory name.

Bear this in mind if your project name contains spaces.


### Got existing code?

The `hatch new` command assumes you are starting from scratch.
If you have existing code you would like to package, navigate to your directory and run the following command (replace `<YOUR ROJECT NAME>` with the name of your project):

```
hatch new --init <YOUR PROJECT NAME>
```

This will generate a `pyproject.toml` in the current directory.

!!! note
    
    It will simplify things if your code follows the directory structure convention above. This may require that you move your files -- you only need to do this once!

## Adding code

Your code should reside inside `src/<PROJECT NAME>`.
For the calculator example we will copy `calculator.py` and `calculator.tcss` into the `src/textual_calculator` directory, so our directory will look like the following:

```
textual-calculator
├── LICENSE.txt
├── README.md
├── pyproject.toml
├── src
│   └── textual_calculator
│       ├── __about__.py
│       ├── __init__.py
│       ├── calculator.py
│       └── calculator.tcss
└── tests
    └── __init__.py
```

## Adding dependencies

Your Textual app will likely depend on other Python libraries (at the very least Textual itself).
We need to list these in `pyproject.toml` to ensure that these *dependencies* are installed alongside your app.

In `pyproject.toml` there should be a section beginning with `[project]`, which will look something like the following:

```toml
[project]
name = "textual-calculator"
dynamic = ["version"]
description = 'A example app'
readme = "README.md"
requires-python = ">=3.8"
license = "MIT"
keywords = []
authors = [
  { name = "Will McGugan", email = "redacted@textualize.io" },
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Programming Language :: Python",
  "Programming Language :: Python :: 3.8",
  "Programming Language :: Python :: 3.9",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: Implementation :: CPython",
  "Programming Language :: Python :: Implementation :: PyPy",
]
dependencies = []
```

We are interested in the `dependencies` value, which should list the app's dependencies.
If you want a particular version of a project you can add `==` followed by the version.

For the calculator, the only dependency is Textual.
We can add Textual by modifying the following line:

```toml
dependencies = ["textual==0.47.1"]
```

At the time of writing, the latest Textual is `0.47.1`.
The entry in `dependencies` will ensure we get that particular version, even when newer versions are released.

See the Hatch docs for more information on specifying [dependencies](https://hatch.pypa.io/latest/config/dependency/).

## Environments

A common problem when working with Python code is managing multiple projects with different dependencies.
For instance, if we had another app that used version `0.40.0` of Textual, it *may* break if we installed version `0.47.1`.

The standard way of solving this is with *virtual environments* (or *venvs*), which allow each project to have its own set of dependencies.
Hatch can create virtual environments for us, and makes working with them very easy.

To create a new virtual environment, navigate to the directory with the `pyproject.toml` file and run the following command (this is only require once, as the virtual environment will persist):

```bash
hatch env create
```

Then run the following command to activate the virtual environment:

```bash
hatch shell
```

If you run `python` now, it will have our app and its dependencies available for import:

```
$ python
Python 3.11.1 (main, Jan  1 2023, 10:28:48) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> from textual_calculator import calculator
```

### Running the app

You can launch the calculator from the command line with the following command:

```bash
python -m textual_calculator.calculator
```

The `-m` switch tells Python to import the module and run it.

Although you can run your app this way (and it is fine for development), it's not ideal for sharing.
It would be preferable to have a dedicated command to launch the app, so the user can easily run it from the command line.
To do that, we will need to add an *entry point* to pyproject.toml

## Entry points

An entry point is a function in your project that can be run from the command line.
For our calculator example, we first need to create a function that will run the app.
Add the following file to the `src/textual_calculator` folder, and name it `entry_points.py`:

```python
from textual_calculator.calculator import CalculatorApp


def calculator():
    app = CalculatorApp()
    app.run()
```

!!! tip

    If you already have a function that runs your app, you may not need an `entry_points.py` file.

Then edit `pyproject.toml` to add the following section:

```toml
[project.scripts]
calculator = "textual_calculator.entry_points:calculator"
```

Each entry in the `[project.scripts]` section (there can be more than one) maps a command on to an import and function name.
In the second line above, before the `=` character, `calculator` is the name of the command.
The string after the `=` character contains the name of the import (`textual_calculator.entry_points`), followed by a colon (`:`), and then the name of the function (also called `calculator`).

Specifying an entry point like this is equivalent to doing the following from the Python REPL:

```
>>> import textual_calculator.entry_points
>>> textual_calculator.entry_points.calculator()
```

To add the `calculator` command once you have edited `pyproject.toml`, run the following from the command line:

```bash
pip install -e .
```

!!! info

    You will have no doubt used `pip` before, but perhaps not with `-e .`.
    The addition of `-e` installs the project in *editable* mode which means pip won't copy the `.py` files code anywhere, the dot (`.`) indicates were installing the project in the current directory. 

Now you can launch the calculator from the command line as follows:

```bash
calculator
```

## Building 

Building produces archive files that contain your code.
When you install a package via pip or other tool, it will download one of these archives.

To build your project with Hatch, change to the directory containing your `pyproject.toml` and run the `hatch build` subcommand:

```
cd textual-calculator
hatch build
```

After a moment, you should find that Hatch has created a `dist` (distribution) folder, which contains the project archive files.
You don't typically need to use these files directly, but feel free to have a look at the directory contents.

!!! note "Packaging TCSS and other files"

    Hatch will typically include all the files needed by your project, i.e. the `.py` files.
    It will also include any Textual CSS (`.tcss`) files in the project directory.
    Not all build tools will include files other than `.py`; if you are using another build tool, you may have to consult the documentation for how to add the Textual CSS files.


## Publishing

After your project has been successfully built you are ready to publish it to PyPI.

If you don't have a PyPI account, you can [create one now](https://pypi.org/account/register/).
Be sure to follow the instructions to validate your email and set up 2FA (Two Factor Authentication).

Once you have an account, login to PyPI and go to the Account Settings tab.
Scroll down and click the "Add API token" button.
In the "Create API Token" form, create a token with name "Uploads" and select the "Entire project" scope, then click the "Create token" button.

Copy this API token (long string of random looking characters) somewhere safe.
This API token is how PyPI authenticates uploads are for your account, so you should never share your API token or upload it to the internet.

Run the following command (replacing `<YOUR API TOKEN>` with the text generated in the previous step):

```bash
hatch publish -u __token__ -a <YOUR API TOKEN>
```

Hatch will upload the distribution files, and you should see a PyPI URL in the terminal.

### Managing API Tokens

Creating an API token with the "all projects" permission is required for the first upload.
You may want to generate a new API token with permissions to upload a single project when you upload a new version of your app (and delete the old one).
This way if your token is leaked, it will only impact the one project.

### Publishing new versions

If you have made changes to your app, and you want to publish the updates, you will need to update the `version` value in the `__about__.py` file, then repeat the build and publish steps.

!!! tip "Managing version numbers"

    See [Semver](https://semver.org/) for a popular versioning system (used by Textual itself).

## Installing the calculator

From the user's point of view, they only need run the following command to install the calculator:

```bash
pip install textual_calculator
```

They will then be able to launch the calculator with the following command:

```bash
calculator
```

### Pipx

A downside of installing apps this way is that unless the user has created a [virtual environment](https://docs.python.org/3/library/venv.html), they may find it breaks other packages with conflicting dependencies.

A good solution to this issue is [pipx](https://github.com/pypa/pipx) which automatically creates virtual environments that won't conflict with any other Python commands.
Once PipX is installed, you can advise users to install your app with the following command:

```bash
pipx install textual_calculator
```

This will install the calculator and the `textual` dependency as before, but without the potential of dependency conflicts.

## Summary

1. Use a build system, such as [Hatch](https://hatch.pypa.io/latest/).
2. Initialize your project with `hatch new` (or equivalent).
3. Write a function to run your app, if there isn't one already.
4. Add your dependencies and entry points to `pyproject.toml`.
5. Build your app with `hatch build`.
6. Publish your app with `hatch publish`.

---

If you have any problems packaging Textual apps, we are here to [help](../help.md)!


--- docs/how-to/render-and-compose.md ---
# Render and compose

A common question that comes up on the [Textual Discord server](https://discord.gg/Enf6Z3qhVr) is what is the difference between [`render`][textual.widget.Widget.render] and [`compose`][textual.widget.Widget.compose] methods on a widget?
In this article we will clarify the differences, and use both these methods to build something fun.

<div class="video-wrapper">
<iframe width="1280" height="922" src="https://www.youtube.com/embed/dYU7jHyabX8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>

## Which method to use?

Render and compose are easy to confuse because they both ultimately define what a widget will look like, but they have quite different uses. 

The `render` method on a widget returns a [Rich](https://rich.readthedocs.io/en/latest/) renderable, which is anything you could print with Rich.
The simplest renderable is just text; so `render()` methods often return a string, but could equally return a [`Text`](https://rich.readthedocs.io/en/latest/text.html) instance, a [`Table`](https://rich.readthedocs.io/en/latest/tables.html), or anything else from Rich (or third party library).
Whatever is returned from `render()` will be combined with any styles from CSS and displayed within the widget's borders.

The `compose` method is used to build [*compound* widgets](../guide/widgets.md#compound-widgets) (widgets composed of other widgets).

A general rule of thumb, is that if you implement a `compose` method, there is no need for a `render` method because it is the widgets yielded from `compose` which define how the custom widget will look.
However, you *can* mix these two methods.
If you implement both, the `render` method will set the custom widget's *background* and `compose` will add widgets on top of that background.

## Combining render and compose

Let's look at an example that combines both these methods.
We will create a custom widget with a [linear gradient][textual.renderables.gradient.LinearGradient] as a background.
The background will be animated (I did promise *fun*)!

=== "render_compose.py"

    ```python
    --8<-- "docs/examples/how-to/render_compose.py"
    ```

    1. Refresh the widget 30 times a second.
    2. Compose our compound widget, which contains a single Static.
    3. Render a linear gradient in the background.

=== "Output"

    ```{.textual path="docs/examples/how-to/render_compose.py" columns="100" lines="40"}
    ```

The `Splash` custom widget has a `compose` method which adds a simple `Static` widget to display a message.
Additionally there is a `render` method which returns a renderable to fill the background with a gradient.

!!! tip

    As fun as this is, spinning animated gradients may be too distracting for most apps!

## Summary

Keep the following in mind when building [custom widgets](../guide/widgets.md).

1. Use `render` to return simple text, or a Rich renderable.
2. Use `compose` to create a widget out of other widgets.
3. If you define both, then `render` will be used as a *background*.


---

We are here to [help](../help.md)!


--- docs/how-to/style-inline-apps.md ---
# Style Inline Apps

Version 0.55.0 of Textual added support for running apps *inline* (below the prompt).
Running an inline app is as simple as adding `inline=True` to [`run()`][textual.app.App.run].

<iframe width="100%" style="aspect-ratio:757/804;" src="https://www.youtube.com/embed/dxAf3vDr4aQ" title="Textual Inline mode" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Your apps will typically run inline without modification, but you may want to make some tweaks for inline mode, which you can do with a little CSS.
This How-To will explain how.

Let's look at an inline app.
The following app displays the the current time (and keeps it up to date).

```python hl_lines="31"
--8<-- "docs/examples/how-to/inline01.py"
```

1. The `inline=True` runs the app inline.

With Textual's default settings, this clock will be displayed in 5 lines; 3 for the digits and 2 for a top and bottom border.

!!! note

    Textual also adds a blank line above inline apps for padding.
    To remove this default padding, you can set `INLINE_PADDING = 0` on your app class.

You can change the height or the border with CSS and the `:inline` pseudo-selector, which only matches rules in inline mode.
Let's update this app to remove the default border, and increase the height:

```python hl_lines="11-17"
--8<-- "docs/examples/how-to/inline02.py"
```

The highlighted CSS targets online inline mode.
By setting the `height` rule on Screen we can define how many lines the app should consume when it runs.
Setting `border: none` removes the default border when running in inline mode.

We've also added a rule to change the color of the clock when running inline.

## Summary

Most apps will not require modification to run inline, but if you want to tweak the height and border you can write CSS that targets inline mode with the `:inline` pseudo-selector.


--- examples/README.md ---
# Textual Examples

This directory contains example Textual applications.

To run them, navigate to the examples directory and enter `python` followed by the name of the Python file.

```
cd textual/examples
python pride.py
```


--- examples/breakpoints.py ---
from textual.app import App, ComposeResult
from textual.containers import Grid
from textual.widgets import Footer, Markdown, Placeholder

HELP = """\
## Breakpoints

A demonstration of how to make an app respond to the dimensions of the terminal.

Try resizing the terminal, then have a look at the source to see how it works!
"""


class BreakpointApp(App):

    # A breakpoint consists of a width and a class name to set
    HORIZONTAL_BREAKPOINTS = [
        (0, "-narrow"),
        (40, "-normal"),
        (80, "-wide"),
        (120, "-very-wide"),
    ]

    CSS = """
    Screen {        
        Placeholder { padding: 2; }
        Grid { grid-rows: auto; height: auto; }
        # Change the styles according to the breakpoint classes
        &.-narrow {
            Grid { grid-size: 1; }
        }
        &.-normal {
            Grid { grid-size: 2; }
        }
        &.-wide {
            Grid { grid-size: 4; }
        }
        &.-very-wide {
            Grid { grid-size: 6; }
        }
    }
    """

    def compose(self) -> ComposeResult:
        yield Markdown(HELP)
        with Grid():
            for n in range(16):
                yield Placeholder(f"Placeholder {n+1}")
        yield Footer()


if __name__ == "__main__":
    BreakpointApp().run()


--- examples/calculator.py ---
"""
An implementation of a classic calculator, with a layout inspired by macOS calculator.

Works like a real calculator. Click the buttons or press the equivalent keys.
"""

from decimal import Decimal

from textual import events, on
from textual.app import App, ComposeResult
from textual.containers import Container
from textual.css.query import NoMatches
from textual.reactive import var
from textual.widgets import Button, Digits


class CalculatorApp(App):
    """A working 'desktop' calculator."""

    CSS_PATH = "calculator.tcss"

    numbers = var("0")
    show_ac = var(True)
    left = var(Decimal("0"))
    right = var(Decimal("0"))
    value = var("")
    operator = var("plus")

    # Maps button IDs on to the corresponding key name
    NAME_MAP = {
        "asterisk": "multiply",
        "slash": "divide",
        "underscore": "plus-minus",
        "full_stop": "point",
        "plus_minus_sign": "plus-minus",
        "percent_sign": "percent",
        "equals_sign": "equals",
        "minus": "minus",
        "plus": "plus",
    }

    def watch_numbers(self, value: str) -> None:
        """Called when numbers is updated."""
        self.query_one("#numbers", Digits).update(value)

    def compute_show_ac(self) -> bool:
        """Compute switch to show AC or C button"""
        return self.value in ("", "0") and self.numbers == "0"

    def watch_show_ac(self, show_ac: bool) -> None:
        """Called when show_ac changes."""
        self.query_one("#c").display = not show_ac
        self.query_one("#ac").display = show_ac

    def compose(self) -> ComposeResult:
        """Add our buttons."""
        with Container(id="calculator"):
            yield Digits(id="numbers")
            yield Button("AC", id="ac", variant="primary")
            yield Button("C", id="c", variant="primary")
            yield Button("+/-", id="plus-minus", variant="primary")
            yield Button("%", id="percent", variant="primary")
            yield Button("÷", id="divide", variant="warning")
            yield Button("7", id="number-7", classes="number")
            yield Button("8", id="number-8", classes="number")
            yield Button("9", id="number-9", classes="number")
            yield Button("×", id="multiply", variant="warning")
            yield Button("4", id="number-4", classes="number")
            yield Button("5", id="number-5", classes="number")
            yield Button("6", id="number-6", classes="number")
            yield Button("-", id="minus", variant="warning")
            yield Button("1", id="number-1", classes="number")
            yield Button("2", id="number-2", classes="number")
            yield Button("3", id="number-3", classes="number")
            yield Button("+", id="plus", variant="warning")
            yield Button("0", id="number-0", classes="number")
            yield Button(".", id="point")
            yield Button("=", id="equals", variant="warning")

    def on_key(self, event: events.Key) -> None:
        """Called when the user presses a key."""

        def press(button_id: str) -> None:
            """Press a button, should it exist."""
            try:
                self.query_one(f"#{button_id}", Button).press()
            except NoMatches:
                pass

        key = event.key
        if key.isdecimal():
            press(f"number-{key}")
        elif key == "c":
            press("c")
            press("ac")
        else:
            button_id = self.NAME_MAP.get(key)
            if button_id is not None:
                press(self.NAME_MAP.get(key, key))

    @on(Button.Pressed, ".number")
    def number_pressed(self, event: Button.Pressed) -> None:
        """Pressed a number."""
        assert event.button.id is not None
        number = event.button.id.partition("-")[-1]
        self.numbers = self.value = self.value.lstrip("0") + number

    @on(Button.Pressed, "#plus-minus")
    def plus_minus_pressed(self) -> None:
        """Pressed + / -"""
        self.numbers = self.value = str(Decimal(self.value or "0") * -1)

    @on(Button.Pressed, "#percent")
    def percent_pressed(self) -> None:
        """Pressed %"""
        self.numbers = self.value = str(Decimal(self.value or "0") / Decimal(100))

    @on(Button.Pressed, "#point")
    def pressed_point(self) -> None:
        """Pressed ."""
        if "." not in self.value:
            self.numbers = self.value = (self.value or "0") + "."

    @on(Button.Pressed, "#ac")
    def pressed_ac(self) -> None:
        """Pressed AC"""
        self.value = ""
        self.left = self.right = Decimal(0)
        self.operator = "plus"
        self.numbers = "0"

    @on(Button.Pressed, "#c")
    def pressed_c(self) -> None:
        """Pressed C"""
        self.value = ""
        self.numbers = "0"

    def _do_math(self) -> None:
        """Does the math: LEFT OPERATOR RIGHT"""
        try:
            if self.operator == "plus":
                self.left += self.right
            elif self.operator == "minus":
                self.left -= self.right
            elif self.operator == "divide":
                self.left /= self.right
            elif self.operator == "multiply":
                self.left *= self.right
            self.numbers = str(self.left)
            self.value = ""
        except Exception:
            self.numbers = "Error"

    @on(Button.Pressed, "#plus,#minus,#divide,#multiply")
    def pressed_op(self, event: Button.Pressed) -> None:
        """Pressed one of the arithmetic operations."""
        self.right = Decimal(self.value or "0")
        self._do_math()
        assert event.button.id is not None
        self.operator = event.button.id

    @on(Button.Pressed, "#equals")
    def pressed_equals(self) -> None:
        """Pressed ="""
        if self.value:
            self.right = Decimal(self.value)
        self._do_math()


if __name__ == "__main__":
    CalculatorApp().run(inline=True)


--- examples/clock.py ---
"""
An App to show the current time.
"""

from datetime import datetime

from textual.app import App, ComposeResult
from textual.widgets import Digits


class ClockApp(App):
    CSS = """
    Screen { align: center middle; }
    Digits { width: auto; }
    """

    def compose(self) -> ComposeResult:
        yield Digits("")

    def on_ready(self) -> None:
        self.update_clock()
        self.set_interval(1, self.update_clock)

    def update_clock(self) -> None:
        clock = datetime.now().time()
        self.query_one(Digits).update(f"{clock:%T}")


if __name__ == "__main__":
    app = ClockApp()
    app.run()


--- examples/code_browser.py ---
"""
Code browser example.

Run with:

    python code_browser.py PATH
"""

from __future__ import annotations

import sys
from pathlib import Path

from rich.traceback import Traceback

from textual.app import App, ComposeResult
from textual.containers import Container, VerticalScroll
from textual.highlight import highlight
from textual.reactive import reactive, var
from textual.widgets import DirectoryTree, Footer, Header, Static


class CodeBrowser(App):
    """Textual code browser app."""

    CSS_PATH = "code_browser.tcss"
    BINDINGS = [
        ("f", "toggle_files", "Toggle Files"),
        ("q", "quit", "Quit"),
    ]

    show_tree = var(True)
    path: reactive[str | None] = reactive(None)

    def watch_show_tree(self, show_tree: bool) -> None:
        """Called when show_tree is modified."""
        self.set_class(show_tree, "-show-tree")

    def compose(self) -> ComposeResult:
        """Compose our UI."""
        path = "./" if len(sys.argv) < 2 else sys.argv[1]
        yield Header()
        with Container():
            yield DirectoryTree(path, id="tree-view")
            with VerticalScroll(id="code-view"):
                yield Static(id="code", expand=True)
        yield Footer()

    def on_mount(self) -> None:
        self.query_one(DirectoryTree).focus()

        def theme_change(_signal) -> None:
            """Force the syntax to use a different theme."""
            self.watch_path(self.path)

        self.theme_changed_signal.subscribe(self, theme_change)

    def on_directory_tree_file_selected(
        self, event: DirectoryTree.FileSelected
    ) -> None:
        """Called when the user click a file in the directory tree."""
        event.stop()
        self.path = str(event.path)

    def watch_path(self, path: str | None) -> None:
        """Called when path changes."""
        code_view = self.query_one("#code", Static)
        if path is None:
            code_view.update("")
            return
        try:
            code = Path(path).read_text(encoding="utf-8")
            syntax = highlight(code, path=path)
        except Exception:
            code_view.update(Traceback(theme="github-dark", width=None))
            self.sub_title = "ERROR"
        else:
            code_view.update(syntax)
            self.query_one("#code-view").scroll_home(animate=False)
            self.sub_title = path

    def action_toggle_files(self) -> None:
        """Called in response to key binding."""
        self.show_tree = not self.show_tree


if __name__ == "__main__":
    CodeBrowser().run()


--- examples/color_command.py ---
from dataclasses import dataclass
from functools import partial

from textual import on
from textual._color_constants import COLOR_NAME_TO_RGB
from textual.app import App, ComposeResult
from textual.command import Hit, Hits, Provider
from textual.message import Message
from textual.widgets import Header, Static


@dataclass
class SwitchColor(Message, bubble=False):
    """Message to tell the app to switch color."""

    color: str


class ColorCommands(Provider):
    """A command provider to select colors."""

    async def search(self, query: str) -> Hits:
        """Called for each key."""
        matcher = self.matcher(query)
        for color in COLOR_NAME_TO_RGB.keys():
            score = matcher.match(color)
            if score > 0:
                yield Hit(
                    score,
                    matcher.highlight(color),
                    partial(self.app.post_message, SwitchColor(color)),
                )


class ColorBlock(Static):
    """Simple block of color."""

    DEFAULT_CSS = """
    ColorBlock{
        padding: 3 6;
        margin: 1 2;
        color: auto;
    }
    """


class ColorApp(App):
    """Experiment with the command palette."""

    COMMANDS = App.COMMANDS | {ColorCommands}
    TITLE = "Press ctrl + p and type a color"

    def compose(self) -> ComposeResult:
        yield Header()

    @on(SwitchColor)
    def switch_color(self, event: SwitchColor) -> None:
        """Adds a color block on demand."""
        color_block = ColorBlock(event.color)
        color_block.styles.background = event.color
        self.mount(color_block)
        self.screen.scroll_end()


if __name__ == "__main__":
    app = ColorApp()
    app.run()


--- examples/demo.md ---
# Textual Markdown Browser

Welcome fellow adventurer! If you ran `markdown.py` from the terminal you are viewing `demo.md` with Textual's built in Markdown widget.

The widget supports much of the Markdown spec. There is also an optional Table of Contents sidebar which you will see to your left.

## Do You Want to Know More?

See [example.md](./example.md) for more examples of what this can do.


--- examples/dictionary.py ---
from __future__ import annotations

try:
    import httpx
except ImportError:
    raise ImportError("Please install httpx with 'pip install httpx' ")


from textual import getters, work
from textual.app import App, ComposeResult
from textual.containers import VerticalScroll
from textual.widgets import Input, Markdown


class DictionaryApp(App):
    """Searches a dictionary API as-you-type."""

    CSS_PATH = "dictionary.tcss"

    results = getters.query_one("#results", Markdown)
    input = getters.query_one(Input)

    def compose(self) -> ComposeResult:
        yield Input(placeholder="Search for a word", id="dictionary-search")
        with VerticalScroll(id="results-container"):
            yield Markdown(id="results")

    async def on_input_changed(self, message: Input.Changed) -> None:
        """A coroutine to handle a text changed message."""
        if message.value:
            self.lookup_word(message.value)
        else:
            # Clear the results
            await self.results.update("")

    @work(exclusive=True)
    async def lookup_word(self, word: str) -> None:
        """Looks up a word."""
        url = f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}"

        async with httpx.AsyncClient() as client:
            response = await client.get(url)
            try:
                results = response.json()
            except Exception:
                self.results.update(response.text)
                return

        if word == self.input.value:
            markdown = self.make_word_markdown(results)
            self.results.update(markdown)

    def make_word_markdown(self, results: object) -> str:
        """Convert the results into markdown."""
        lines = []
        if isinstance(results, dict):
            lines.append(f"# {results['title']}")
            lines.append(results["message"])
        elif isinstance(results, list):
            for result in results:
                lines.append(f"# {result['word']}")
                lines.append("")
                for meaning in result.get("meanings", []):
                    lines.append(f"_{meaning['partOfSpeech']}_")
                    lines.append("")
                    for definition in meaning.get("definitions", []):
                        lines.append(f" - {definition['definition']}")
                    lines.append("---")

        return "\n".join(lines)


if __name__ == "__main__":
    app = DictionaryApp()
    app.run()


--- examples/example.md ---
# Textual Markdown Browser - Demo

This Markdown file contains some examples of Markdown widgets.

## Headers

Headers levels 1 through 6 are supported.

### This is H3

This is H3 Content

#### This is H4

Header level 4 content. Drilling down into finer headings.

##### This is H5

Header level 5 content.

###### This is H6

Header level 6 content.

## Typography

The usual Markdown typography is supported. The exact output depends on your terminal, although most are fairly consistent.

### Emphasis

Emphasis is rendered with `*asterisks*`, and looks *like this*;

### Strong

Use two asterisks to indicate strong which renders in bold, e.g. `**strong**` render **strong**.

### Strikethrough

Two tildes indicates strikethrough, e.g. `~~cross out~~` render ~~cross out~~.

### Inline code ###

Inline code is indicated by backticks. e.g. `import this`.

## Horizontal rule

Draw a horizontal rule with three dashes (`---`).

---

Good for natural breaks in the content, that don't require another header.

## Lists

1. Lists can be ordered
2. Lists can be unordered
   - I must not fear.
     - Fear is the mind-killer.
       - Fear is the little-death that brings total obliteration.
         - I will face my fear.
           - I will permit it to pass over me and through me.
     - And when it has gone past, I will turn the inner eye to see its path.
   - Where the fear has gone there will be nothing. Only I will remain.

### Longer list

1. **Duke Leto I Atreides**, head of House Atreides
2. **Lady Jessica**, Bene Gesserit and concubine of Leto, and mother of Paul and Alia
3. **Paul Atreides**, son of Leto and Jessica
4. **Alia Atreides**, daughter of Leto and Jessica
5. **Gurney Halleck**, troubadour warrior of House Atreides
6. **Thufir Hawat**, Mentat and Master of Assassins of House Atreides
7. **Duncan Idaho**, swordmaster of House Atreides
8. **Dr. Wellington Yueh**, Suk doctor of House Atreides
9. **Leto**, first son of Paul and Chani who dies as a toddler
10. **Esmar Tuek**, a smuggler on Arrakis
11. **Staban Tuek**, son of Esmar

## Fences

Fenced code blocks are introduced with three back-ticks and the optional parser. Here we are rendering the code in a sub-widget with syntax highlighting and indent guides.

In the future I think we could add controls to export the code, copy to the clipboard. Heck, even run it and show the output?

```python
@lru_cache(maxsize=1024)
def split(self, cut_x: int, cut_y: int) -> tuple[Region, Region, Region, Region]:
    """Split a region into 4 from given x and y offsets (cuts).

    ```
                cut_x ↓
            ┌────────┐ ┌───┐
            │        │ │   │
            │    0   │ │ 1 │
            │        │ │   │
    cut_y → └────────┘ └───┘
            ┌────────┐ ┌───┐
            │    2   │ │ 3 │
            └────────┘ └───┘
    ```

    Args:
        cut_x (int): Offset from self.x where the cut should be made. If negative, the cut
            is taken from the right edge.
        cut_y (int): Offset from self.y where the cut should be made. If negative, the cut
            is taken from the lower edge.

    Returns:
        tuple[Region, Region, Region, Region]: Four new regions which add up to the original (self).
    """

    x, y, width, height = self
    if cut_x < 0:
        cut_x = width + cut_x
    if cut_y < 0:
        cut_y = height + cut_y

    _Region = Region
    return (
        _Region(x, y, cut_x, cut_y),
        _Region(x + cut_x, y, width - cut_x, cut_y),
        _Region(x, y + cut_y, cut_x, height - cut_y),
        _Region(x + cut_x, y + cut_y, width - cut_x, height - cut_y),
    )
```

## Quote

Quotes are introduced with a chevron, and render like this:

> I must not fear.
> Fear is the mind-killer.
> Fear is the little-death that brings total obliteration.
> I will face my fear.
> I will permit it to pass over me and through me.
> And when it has gone past, I will turn the inner eye to see its path.
> Where the fear has gone there will be nothing. Only I will remain."

Quotes nest nicely. Here's what quotes within quotes look like:

> I must not fear.
> > Fear is the mind-killer.
> > Fear is the little-death that brings total obliteration.
> > I will face my fear.
> > > I will permit it to pass over me and through me.
> > > And when it has gone past, I will turn the inner eye to see its path.
> > > Where the fear has gone there will be nothing. Only I will remain.

## Tables

Tables are supported, and render as a Rich table.

I would like to add controls to these widgets to export the table as CSV, which I think would be a nice feature. In the future we might also have sortable columns by clicking on the headers.


| Name            | Type   | Default | Description                        |
| --------------- | ------ | ------- | ---------------------------------- |
| `show_header`   | `bool` | `True`  | Show the table header              |
| `fixed_rows`    | `int`  | `0`     | Number of fixed rows               |
| `fixed_columns` | `int`  | `0`     | Number of fixed columns            |
| `zebra_stripes` | `bool` | `False` | Display alternating colors on rows |
| `header_height` | `int`  | `1`     | Height of header row               |
| `show_cursor`   | `bool` | `True`  | Show a cell cursor                 |


--- examples/five_by_five.md ---
# 5x5

## Introduction

An annoying puzzle for the terminal, built with [Textual](https://www.textualize.io/).

## Objective

The object of the game is to fill all of the squares. When you click on a
square, it, and the squares above, below and to the sides will be toggled.

It is possible to solve the puzzle in as few as 14 moves.

Good luck!

[//]: # (README.md ends here)


--- reference/README.md ---
Contains private docs, mainly for the developers reference


--- tests/text_area/test_edit_via_api.py ---
"""Tests editing the document using the API (replace etc.)

The tests in this module directly call the edit APIs on the TextArea rather
than going via bindings.

Note that more extensive testing for editing is done at the Document level.
"""

import pytest

from textual.app import App, ComposeResult
from textual.widgets import TextArea
from textual.widgets.text_area import EditResult, Selection

TEXT = """\
I must not fear.
Fear is the mind-killer.
Fear is the little-death that brings total obliteration.
I will face my fear.
"""

SIMPLE_TEXT = """\
ABCDE
FGHIJ
KLMNO
PQRST
UVWXY
Z
"""


class TextAreaApp(App):
    def compose(self) -> ComposeResult:
        text_area = TextArea()
        text_area.load_text(TEXT)
        yield text_area


async def test_insert_text_start_maintain_selection_offset():
    """Ensure that we can maintain the offset between the location
    an insert happens and the location of the selection."""
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.move_cursor((0, 5))
        text_area.insert("Hello", location=(0, 0))
        assert text_area.text == "Hello" + TEXT
        assert text_area.selection == Selection.cursor((0, 10))


async def test_insert_text_start():
    """The document is correctly updated on inserting at the start.
    If we don't maintain the selection offset, the cursor jumps
    to the end of the edit and the selection is empty."""
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.move_cursor((0, 5))
        text_area.insert("Hello", location=(0, 0), maintain_selection_offset=False)
        assert text_area.text == "Hello" + TEXT
        assert text_area.selection == Selection.cursor((0, 5))


async def test_insert_empty_string():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.load_text("0123456789")

        text_area.insert("", location=(0, 3))

        assert text_area.text == "0123456789"


async def test_replace_empty_string():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.load_text("0123456789")

        text_area.replace("", start=(0, 3), end=(0, 7))

        assert text_area.text == "012789"


@pytest.mark.parametrize(
    "cursor_location,insert_location,cursor_destination",
    [
        ((0, 3), (0, 2), (0, 4)),  # API insert just before cursor
        ((0, 3), (0, 3), (0, 4)),  # API insert at cursor location
        ((0, 3), (0, 4), (0, 3)),  # API insert just after cursor
        ((0, 3), (0, 5), (0, 3)),  # API insert just after cursor
    ],
)
async def test_insert_character_near_cursor_maintain_selection_offset(
    cursor_location,
    insert_location,
    cursor_destination,
):
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.load_text("012345")
        text_area.move_cursor(cursor_location)
        text_area.insert("X", location=insert_location)
        assert text_area.selection == Selection.cursor(cursor_destination)


@pytest.mark.parametrize(
    "cursor_location,insert_location,cursor_destination",
    [
        ((1, 0), (0, 0), (2, 0)),  # API insert before cursor row
        ((0, 0), (0, 0), (1, 0)),  # API insert right at cursor row
        ((0, 0), (1, 0), (0, 0)),  # API insert after cursor row
    ],
)
async def test_insert_newline_around_cursor_maintain_selection_offset(
    cursor_location,
    insert_location,
    cursor_destination
):
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.move_cursor(cursor_location)
        text_area.insert("X\n", location=insert_location)
        assert text_area.selection == Selection.cursor(cursor_destination)


async def test_insert_newlines_start():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.insert("\n\n\n")
        assert text_area.text == "\n\n\n" + TEXT
        assert text_area.selection == Selection.cursor((3, 0))


async def test_insert_newlines_end():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.insert("\n\n\n", location=(4, 0))
        assert text_area.text == TEXT + "\n\n\n"


async def test_insert_windows_newlines():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        # Although we're inserting windows newlines, the configured newline on
        # the Document inside the TextArea will be "\n", so when we check TextArea.text
        # we expect to see "\n".
        text_area.insert("\r\n\r\n\r\n")
        assert text_area.text == "\n\n\n" + TEXT


async def test_insert_old_mac_newlines():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.insert("\r\r\r")
        assert text_area.text == "\n\n\n" + TEXT


async def test_insert_text_non_cursor_location():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.insert("Hello", location=(4, 0))
        assert text_area.text == TEXT + "Hello"
        assert text_area.selection == Selection.cursor((0, 0))


async def test_insert_text_non_cursor_location_dont_maintain_offset():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.selection = Selection((2, 3), (3, 5))

        result = text_area.insert(
            "Hello",
            location=(4, 0),
            maintain_selection_offset=False,
        )

        assert result == EditResult(
            end_location=(4, 5),
            replaced_text="",
        )
        assert text_area.text == TEXT + "Hello"

        # Since maintain_selection_offset is False, the selection
        # is reset to a cursor and goes to the end of the insert.
        assert text_area.selection == Selection.cursor((4, 5))


async def test_insert_multiline_text():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.move_cursor((2, 5))
        text_area.insert("Hello,\nworld!", maintain_selection_offset=False)
        expected_content = """\
I must not fear.
Fear is the mind-killer.
Fear Hello,
world!is the little-death that brings total obliteration.
I will face my fear.
"""
        assert text_area.cursor_location == (3, 6)  # Cursor moved to end of insert
        assert text_area.text == expected_content


async def test_insert_multiline_text_maintain_offset():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.move_cursor((2, 5))
        result = text_area.insert("Hello,\nworld!")

        assert result == EditResult(
            end_location=(3, 6),
            replaced_text="",
        )

        # The insert happens at the cursor (default location)
        # Offset is maintained - we inserted 1 line so cursor shifts
        # down 1 line, and along by the length of the last insert line.
        assert text_area.cursor_location == (3, 6)
        expected_content = """\
I must not fear.
Fear is the mind-killer.
Fear Hello,
world!is the little-death that brings total obliteration.
I will face my fear.
"""
        assert text_area.text == expected_content


async def test_replace_multiline_text():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        # replace "Fear is the mind-killer\nFear is the little death...\n"
        # with "Hello,\nworld!\n"
        result = text_area.replace("Hello,\nworld!\n", start=(1, 0), end=(3, 0))
        expected_replaced_text = """\
Fear is the mind-killer.
Fear is the little-death that brings total obliteration.
"""
        assert result == EditResult(
            end_location=(3, 0),
            replaced_text=expected_replaced_text,
        )

        expected_content = """\
I must not fear.
Hello,
world!
I will face my fear.
"""
        assert text_area.selection == Selection.cursor((0, 0))  # cursor didnt move
        assert text_area.text == expected_content


async def test_replace_multiline_text_maintain_selection():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)

        # To begin with, the user selects the word "face"
        text_area.selection = Selection((3, 7), (3, 11))
        assert text_area.selected_text == "face"

        # Text is inserted via the API in a way that shifts
        # the start and end locations of the word "face" in
        # both the horizontal and vertical directions.
        text_area.replace(
            "Hello,\nworld!\n123\n456",
            start=(1, 0),
            end=(3, 0),
        )
        expected_content = """\
I must not fear.
Hello,
world!
123
456I will face my fear.
"""
        # Despite this insert, the selection locations are updated
        # and the word face is still highlighted. This ensures that
        # if text is insert programmatically, a user that is typing
        # won't lose their place - the cursor will maintain the same
        # relative position in the document as before.
        assert text_area.selected_text == "face"
        assert text_area.selection == Selection((4, 10), (4, 14))
        assert text_area.text == expected_content


async def test_delete_within_line():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.selection = Selection((0, 11), (0, 15))
        assert text_area.selected_text == "fear"

        # Delete some text before the selection location.
        result = text_area.delete((0, 6), (0, 10))

        # Even though the word has 'shifted' left, it's still selected.
        assert text_area.selection == Selection((0, 7), (0, 11))
        assert text_area.selected_text == "fear"

        # We've recorded exactly what text was replaced in the EditResult
        assert result == EditResult(
            end_location=(0, 6),
            replaced_text=" not",
        )

        expected_text = """\
I must fear.
Fear is the mind-killer.
Fear is the little-death that brings total obliteration.
I will face my fear.
"""
        assert text_area.text == expected_text


async def test_delete_within_line_dont_maintain_offset():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.delete((0, 6), (0, 10), maintain_selection_offset=False)
    expected_text = """\
I must fear.
Fear is the mind-killer.
Fear is the little-death that brings total obliteration.
I will face my fear.
"""
    assert text_area.selection == Selection.cursor((0, 6))  # cursor moved
    assert text_area.text == expected_text


async def test_delete_multiple_lines_selection_above():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)

        # User has selected text on the first line...
        text_area.selection = Selection((0, 2), (0, 6))
        assert text_area.selected_text == "must"

        # Some lines below are deleted...
        result = text_area.delete((1, 0), (3, 0))

        # The selection is not affected at all.
        assert text_area.selection == Selection((0, 2), (0, 6))

        # We've recorded the text that was deleted in the ReplaceResult.
        # Lines of index 1 and 2 were deleted. Since the end
        # location of the selection is (3, 0), the newline
        # marker is included in the deletion.
        expected_replaced_text = """\
Fear is the mind-killer.
Fear is the little-death that brings total obliteration.
"""
        assert result == EditResult(
            end_location=(1, 0),
            replaced_text=expected_replaced_text,
        )
        assert (
            text_area.text
            == """\
I must not fear.
I will face my fear.
"""
        )


async def test_delete_empty_document():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.load_text("")
        result = text_area.delete((0, 0), (1, 0))
        assert result.replaced_text == ""
        assert text_area.text == ""


async def test_clear():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.clear()


async def test_clear_empty_document():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.load_text("")
        text_area.clear()


@pytest.mark.parametrize(
    "select_from,select_to",
    [
        [(0, 3), (2, 1)],
        [(2, 1), (0, 3)],  # Ensuring independence from selection direction.
    ],
)
async def test_insert_text_multiline_selection_top(select_from, select_to):
    """
    An example to attempt to explain what we're testing here...

    X = edit range, * = character in TextArea, S = selection

    *********XX
    XXXXX***SSS
    SSSSSSSSSSS
    SSSS*******

    If an edit happens at XXXX, we need to ensure that the SSS on the
    same line is adjusted appropriately so that it's still highlighting
    the same characters as before.
    """
    app = TextAreaApp()
    async with app.run_test():
        # ABCDE
        # FGHIJ
        # KLMNO
        # PQRST
        # UVWXY
        # Z
        text_area = app.query_one(TextArea)
        text_area.load_text(SIMPLE_TEXT)
        text_area.selection = Selection(select_from, select_to)

        # Check what text is selected.
        expected_selected_text = "DE\nFGHIJ\nK"
        assert text_area.selected_text == expected_selected_text

        result = text_area.replace(
            "Hello",
            start=(0, 0),
            end=(0, 2),
        )

        assert result == EditResult(end_location=(0, 5), replaced_text="AB")

        # The edit range has grown from width 2 to width 5, so the
        # top line of the selection was adjusted (column+=3) such that the
        # same characters are highlighted:
        # ... the selection is not changed after programmatic insert
        # ... the same text is selected as before.
        assert text_area.selected_text == expected_selected_text

        # The resulting text in the TextArea is correct.
        assert text_area.text == "HelloCDE\nFGHIJ\nKLMNO\nPQRST\nUVWXY\nZ\n"


@pytest.mark.parametrize(
    "select_from,select_to",
    [
        [(0, 3), (2, 5)],
        [(2, 5), (0, 3)],  # Ensuring independence from selection direction.
    ],
)
async def test_insert_text_multiline_selection_bottom(select_from, select_to):
    """
    The edited text is within the selected text on the bottom line
    of the selection. The bottom of the selection should be adjusted
    such that any text that was previously selected is still selected.
    """
    app = TextAreaApp()
    async with app.run_test():
        # ABCDE
        # FGHIJ
        # KLMNO
        # PQRST
        # UVWXY
        # Z

        text_area = app.query_one(TextArea)
        text_area.load_text(SIMPLE_TEXT)
        text_area.selection = Selection(select_from, select_to)

        # Check what text is selected.
        assert text_area.selected_text == "DE\nFGHIJ\nKLMNO"

        result = text_area.replace(
            "*",
            start=(2, 0),
            end=(2, 3),
        )
        assert result == EditResult(end_location=(2, 1), replaced_text="KLM")

        # The 'NO' from the selection is still available on the
        # bottom selection line, however the 'KLM' is replaced
        # with '*'. Since 'NO' is still available, it's maintained
        # within the selection.
        assert text_area.selected_text == "DE\nFGHIJ\n*NO"

        # The resulting text in the TextArea is correct.
        # 'KLM' replaced with '*'
        assert text_area.text == "ABCDE\nFGHIJ\n*NO\nPQRST\nUVWXY\nZ\n"


async def test_delete_fully_within_selection():
    """User-facing selection should be best-effort adjusted when a programmatic
    replacement is made to the document."""
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.load_text("0123456789")
        text_area.selection = Selection((0, 2), (0, 7))
        assert text_area.selected_text == "23456"

        result = text_area.delete((0, 4), (0, 6))
        assert result == EditResult(
            replaced_text="45",
            end_location=(0, 4),
        )
        # We deleted 45, but the other characters are still available
        assert text_area.selected_text == "236"
        assert text_area.text == "01236789"


async def test_replace_fully_within_selection():
    """Adjust the selection when a replacement happens inside it."""
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.load_text("0123456789")
        text_area.selection = Selection((0, 2), (0, 7))
        assert text_area.selected_text == "23456"

        result = text_area.replace("XX", start=(0, 2), end=(0, 5))
        assert result == EditResult(
            replaced_text="234",
            end_location=(0, 4),
        )
        assert text_area.selected_text == "XX56"


async def test_text_setter():
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        new_text = "hello\nworld\n"
        text_area.text = new_text
        assert text_area.text == new_text


async def test_edits_on_read_only_mode():
    """API edits should still be permitted on read-only mode."""
    app = TextAreaApp()
    async with app.run_test():
        text_area = app.query_one(TextArea)
        text_area.text = "0123456789"
        text_area.read_only = True

        text_area.replace("X", (0, 1), (0, 5))
        assert text_area.text == "0X56789"

        text_area.insert("X")
        assert text_area.text == "X0X56789"

        text_area.delete((0, 0), (0, 2))
        assert text_area.text == "X56789"


--- tests/command_palette/test_escaping.py ---
from textual.app import App
from textual.command import CommandPalette, Hit, Hits, Provider


class SimpleSource(Provider):
    async def search(self, query: str) -> Hits:
        def goes_nowhere_does_nothing() -> None:
            pass

        yield Hit(1, query, goes_nowhere_does_nothing, query)


class CommandPaletteApp(App[None]):
    COMMANDS = {SimpleSource}

    def on_mount(self) -> None:
        self.action_command_palette()


async def test_escape_closes_when_no_list_visible() -> None:
    """Pressing escape when no list is visible should close the command palette."""
    async with CommandPaletteApp().run_test() as pilot:
        assert CommandPalette.is_open(pilot.app)
        await pilot.press("escape")
        assert not CommandPalette.is_open(pilot.app)


--- tests/option_list/test_option_list_option_subclass.py ---
"""Unit tests aimed at ensuring the option list option class can be subclassed."""

from __future__ import annotations

from textual.app import App, ComposeResult
from textual.widgets import OptionList
from textual.widgets.option_list import Option


class OptionWithExtras(Option):
    """An example subclass of a option."""

    def __init__(self, test: int) -> None:
        super().__init__(str(test), str(test), False)
        self.test = test


class OptionListApp(App[None]):
    """Test option list application."""

    def compose(self) -> ComposeResult:
        yield OptionList(*[OptionWithExtras(n) for n in range(100)])


async def test_option_list_with_subclassed_options() -> None:
    """It should be possible to build an option list with subclassed options."""
    async with OptionListApp().run_test() as pilot:
        option_list = pilot.app.query_one(OptionList)
        assert option_list.option_count == 100
        for n in range(option_list.option_count):
            for option in (
                option_list.get_option(str(n)),
                option_list.get_option_at_index(n),
            ):
                assert isinstance(option, OptionWithExtras)
                assert option.prompt == str(n)
                assert option.id == str(n)
                assert option.test == n


--- tests/snapshot_tests/snapshot_apps/focus_component_class.py ---
from rich.text import Text

from textual.app import App, ComposeResult, RenderResult
from textual.containers import VerticalScroll
from textual.widgets import Header, Footer
from textual.widget import Widget


class Tester(Widget, can_focus=True):
    COMPONENT_CLASSES = {"tester--text"}

    DEFAULT_CSS = """
    Tester {
        height: auto;
    }
    
    Tester:focus > .tester--text {
        background: red;
    }
    """

    def __init__(self, n: int) -> None:
        self.n = n
        super().__init__()

    def render(self) -> RenderResult:
        return Text(
            f"test widget {self.n}", style=self.get_component_rich_style("tester--text")
        )


class StyleBugApp(App[None]):
    def compose(self) -> ComposeResult:
        yield Header()
        with VerticalScroll():
            for n in range(40):
                yield Tester(n)
        yield Footer()


if __name__ == "__main__":
    StyleBugApp().run()


--- tests/snapshot_tests/snapshot_apps/footer_classic_styling.py ---
from textual.app import App, ComposeResult
from textual.widgets import Footer


class ClassicFooterStylingApp(App):
    """
    This app attempts to replicate the styling of the classic footer in
    Textual before v0.63.0, in particular the distinct background color
    for the binding keys and spacing between the key and binding description.

    Regression test for https://github.com/Textualize/textual/issues/4557
    """

    CSS = """
    Footer {
        background: $secondary;

        FooterKey {
            background: $secondary;
            color: $text;

            .footer-key--key {
                background: $secondary-darken-2;
                color: $text;
            }

            .footer-key--description {
                padding: 0 1;
            }
        }
    }
    """

    BINDINGS = [
        ("ctrl+t", "app.toggle_dark", "Toggle Dark mode"),
        ("ctrl+q", "quit", "Quit"),
    ]

    def compose(self) -> ComposeResult:
        yield Footer()


if __name__ == "__main__":
    app = ClassicFooterStylingApp()
    app.run()


--- tests/snapshot_tests/snapshot_apps/line_api_scrollbars.py ---
from rich.text import Text

from textual.app import App, ComposeResult
from textual.containers import VerticalScroll
from textual.widget import Widget
from textual.widgets import RichLog


class MyWidget(Widget):
    def render(self):
        return Text(
            "\n".join(f"{n} 0123456789" for n in range(20)),
            no_wrap=True,
            overflow="hidden",
            justify="left",
        )


class ScrollViewApp(App):
    CSS = """
    Screen {
        align: center middle;
    }

    RichLog {
        width:13;
        height:10;
    }

    VerticalScroll {
        width:13;
        height: 10;
        overflow: scroll;
        overflow-x: auto;
    }

    MyWidget {
        width:13;
        height:auto;
    }
    """

    def compose(self) -> ComposeResult:
        yield RichLog()
        yield VerticalScroll(MyWidget())

    def on_ready(self) -> None:
        self.query_one(RichLog).write("\n".join(f"{n} 0123456789" for n in range(20)))
        self.query_one(VerticalScroll).scroll_end(animate=False)


if __name__ == "__main__":
    app = ScrollViewApp()
    app.run()


--- tests/snapshot_tests/snapshot_apps/markdown_component_classes_reloading.py ---
from pathlib import Path

from textual.app import App, ComposeResult
from textual.widgets import Markdown

CSS_PATH = (Path(__file__) / "../markdown_component_classes_reloading.tcss").resolve()

CSS_PATH.write_text(
    """\
.code_inline,
.em,
.strong,
.s,
.markdown-table--header,
.markdown-table--lines,
{
    color: yellow;
}
"""
)

MD = """
# This is a **header**

| col1 | col2 |
| :- | :- |
| value 1 | value 2 |

Here's some code: `from itertools import product`.
**Bold text**
_Emphasized text_
~~strikethrough~~

```py
print("Hello, world!")
```

**That** was _some_ code.
"""


class MyApp(App[None]):
    CSS_PATH = CSS_PATH

    def compose(self) -> ComposeResult:
        yield Markdown(MD)


if __name__ == "__main__":
    MyApp().run()


--- .faq/FAQ.md ---
---
hide:
  - navigation
---

<!-- Auto-generated by FAQtory -->
<!-- Do not edit by hand! -->

# Frequently Asked Questions


Welcome to the Textual FAQ.
Here we try and answer any question that comes up frequently.
If you can't find what you are looking for here, see our other [help](./help.md) channels.

{%- for question in questions %}

<a name="{{ question.slug }}"></a>
## {{ question.title }}

{{ question.body }}

---

{%- endfor %}

Generated by [FAQtory](https://github.com/willmcgugan/faqtory)


--- .faq/suggest.md ---
{%- if questions -%}
{% if questions|length == 1 %}
We found the following entry in the [FAQ]({{ faq_url }}) which you may find helpful:
{%- else %}
We found the following entries in the [FAQ]({{ faq_url }}) which you may find helpful:
{%- endif %}

{% for question in questions %}
- [{{ question.title }}]({{ faq_url }}#{{ question.slug }})
{%- endfor %}

Feel free to close this issue if you found an answer in the FAQ. Otherwise, please give us a little time to review.

{%- else -%}
Thank you for your issue. Give us a little time to review it.

PS. You might want to check the [FAQ]({{ faq_url }}) if you haven't done so already.
{%- endif %}

This project is developed and maintained by Will McGugan. Consider [sponsoring Will's work on this project](https://github.com/sponsors/willmcgugan) (and others).

This is an automated reply, generated by [FAQtory](https://github.com/willmcgugan/faqtory)


--- src/textual/design.py ---
from __future__ import annotations

from typing import Iterable

import rich.repr
from rich.console import group
from rich.padding import Padding
from rich.table import Table
from rich.text import Text

from textual.color import WHITE, Color

NUMBER_OF_SHADES = 3

# Where no content exists
DEFAULT_DARK_BACKGROUND = "#121212"
# What text usually goes on top off
DEFAULT_DARK_SURFACE = "#1e1e1e"

DEFAULT_LIGHT_SURFACE = "#f5f5f5"
DEFAULT_LIGHT_BACKGROUND = "#efefef"


@rich.repr.auto
class ColorSystem:
    """Defines a standard set of colors and variations for building a UI.

    Primary is the main theme color
    Secondary is a second theme color
    """

    COLOR_NAMES = [
        "primary",
        "secondary",
        "background",
        "primary-background",
        "secondary-background",
        "surface",
        "panel",
        "boost",
        "warning",
        "error",
        "success",
        "accent",
    ]

    def __init__(
        self,
        primary: str,
        secondary: str | None = None,
        warning: str | None = None,
        error: str | None = None,
        success: str | None = None,
        accent: str | None = None,
        foreground: str | None = None,
        background: str | None = None,
        surface: str | None = None,
        panel: str | None = None,
        boost: str | None = None,
        dark: bool = False,
        luminosity_spread: float = 0.15,
        text_alpha: float = 0.95,
        variables: dict[str, str] | None = None,
    ):
        def parse(color: str | None) -> Color | None:
            if color is None:
                return None
            return Color.parse(color)

        self.primary = Color.parse(primary)
        self.secondary = parse(secondary)
        self.warning = parse(warning)
        self.error = parse(error)
        self.success = parse(success)
        self.accent = parse(accent)
        self.foreground = parse(foreground)
        self.background = parse(background)
        self.surface = parse(surface)
        self.panel = parse(panel)
        self.boost = parse(boost)
        self.dark = dark
        self.luminosity_spread = luminosity_spread
        self.text_alpha = text_alpha
        self.variables = variables or {}
        """Overrides for specific variables."""

    @property
    def shades(self) -> Iterable[str]:
        """The names of the colors and derived shades."""
        for color in self.COLOR_NAMES:
            for shade_number in range(-NUMBER_OF_SHADES, NUMBER_OF_SHADES + 1):
                if shade_number < 0:
                    yield f"{color}-darken-{abs(shade_number)}"
                elif shade_number > 0:
                    yield f"{color}-lighten-{shade_number}"
                else:
                    yield color

    def get_or_default(self, name: str, default: str) -> str:
        """Get the value of a color variable, or the default value if not set."""
        return self.variables.get(name, default)

    def generate(self) -> dict[str, str]:
        """Generate a mapping of color name on to a CSS color.

        Returns:
            A mapping of color name on to a CSS-style encoded color
        """

        primary = self.primary
        secondary = self.secondary or primary
        warning = self.warning or primary
        error = self.error or secondary
        success = self.success or secondary
        accent = self.accent or primary

        dark = self.dark
        luminosity_spread = self.luminosity_spread

        colors: dict[str, str] = {}

        if dark:
            background = self.background or Color.parse(DEFAULT_DARK_BACKGROUND)
            surface = self.surface or Color.parse(DEFAULT_DARK_SURFACE)
        else:
            background = self.background or Color.parse(DEFAULT_LIGHT_BACKGROUND)
            surface = self.surface or Color.parse(DEFAULT_LIGHT_SURFACE)

        foreground = self.foreground or (background.inverse)
        contrast_text = background.get_contrast_text(1.0)
        boost = self.boost or contrast_text.with_alpha(0.04)

        # Colored text
        colors["text-primary"] = contrast_text.tint(primary.with_alpha(0.66)).hex
        colors["text-secondary"] = contrast_text.tint(secondary.with_alpha(0.66)).hex
        colors["text-warning"] = contrast_text.tint(warning.with_alpha(0.66)).hex
        colors["text-error"] = contrast_text.tint(error.with_alpha(0.66)).hex
        colors["text-success"] = contrast_text.tint(success.with_alpha(0.66)).hex
        colors["text-accent"] = contrast_text.tint(accent.with_alpha(0.66)).hex

        if self.panel is None:
            panel = surface.blend(primary, 0.1, alpha=1)
            if dark:
                panel += boost
        else:
            panel = self.panel

        def luminosity_range(spread: float) -> Iterable[tuple[str, float]]:
            """Get the range of shades from darken2 to lighten2.

            Returns:
                Iterable of tuples (<SHADE SUFFIX, LUMINOSITY DELTA>)
            """
            luminosity_step = spread / 2
            for n in range(-NUMBER_OF_SHADES, +NUMBER_OF_SHADES + 1):
                if n < 0:
                    label = "-darken"
                elif n > 0:
                    label = "-lighten"
                else:
                    label = ""
                yield (f"{label}{'-' + str(abs(n)) if n else ''}"), n * luminosity_step

        # Color names and color
        COLORS: list[tuple[str, Color]] = [
            ("primary", primary),
            ("secondary", secondary),
            ("primary-background", primary),
            ("secondary-background", secondary),
            ("background", background),
            ("foreground", foreground),
            ("panel", panel),
            ("boost", boost),
            ("surface", surface),
            ("warning", warning),
            ("error", error),
            ("success", success),
            ("accent", accent),
        ]

        # Colors names that have a dark variant
        DARK_SHADES = {"primary-background", "secondary-background"}

        get = self.get_or_default

        for name, color in COLORS:
            is_dark_shade = dark and name in DARK_SHADES
            spread = luminosity_spread
            for shade_name, luminosity_delta in luminosity_range(spread):
                key = f"{name}{shade_name}"
                if color.ansi is not None:
                    colors[key] = color.hex
                elif is_dark_shade:
                    dark_background = background.blend(color, 0.15, alpha=1.0)
                    if key not in self.variables:
                        shade_color = dark_background.blend(
                            WHITE, spread + luminosity_delta, alpha=1.0
                        ).clamped
                        colors[key] = shade_color.hex
                    else:
                        colors[key] = self.variables[key]
                else:
                    colors[key] = get(key, color.lighten(luminosity_delta).hex)

        if foreground.ansi is None:
            colors["text"] = get("text", "auto 87%")
            colors["text-muted"] = get("text-muted", "auto 60%")
            colors["text-disabled"] = get("text-disabled", "auto 38%")
        else:
            colors["text"] = "ansi_default"
            colors["text-muted"] = "ansi_default"
            colors["text-disabled"] = "ansi_default"

        # Muted variants of base colors
        colors["primary-muted"] = get(
            "primary-muted", primary.blend(background, 0.7).hex
        )
        colors["secondary-muted"] = get(
            "secondary-muted", secondary.blend(background, 0.7).hex
        )
        colors["accent-muted"] = get("accent-muted", accent.blend(background, 0.7).hex)
        colors["warning-muted"] = get(
            "warning-muted", warning.blend(background, 0.7).hex
        )
        colors["error-muted"] = get("error-muted", error.blend(background, 0.7).hex)
        colors["success-muted"] = get(
            "success-muted", success.blend(background, 0.7).hex
        )

        # Foreground colors
        colors["foreground-muted"] = get(
            "foreground-muted", foreground.with_alpha(0.6).hex
        )
        colors["foreground-disabled"] = get(
            "foreground-disabled", foreground.with_alpha(0.38).hex
        )

        # The cursor color for widgets such as OptionList, DataTable, etc.
        colors["block-cursor-foreground"] = get(
            "block-cursor-foreground", colors["text"]
        )
        colors["block-cursor-background"] = get("block-cursor-background", primary.hex)
        colors["block-cursor-text-style"] = get("block-cursor-text-style", "bold")
        colors["block-cursor-blurred-foreground"] = get(
            "block-cursor-blurred-foreground", foreground.hex
        )
        colors["block-cursor-blurred-background"] = get(
            "block-cursor-blurred-background", primary.with_alpha(0.3).hex
        )
        colors["block-cursor-blurred-text-style"] = get(
            "block-cursor-blurred-text-style", "none"
        )
        colors["block-hover-background"] = get(
            "block-hover-background", boost.with_alpha(0.1).hex
        )

        # The border color for focused widgets which have a border.
        colors["border"] = get("border", primary.hex)
        colors["border-blurred"] = get("border-blurred", surface.darken(0.025).hex)

        # The surface color for builtin focused widgets
        colors["surface-active"] = get(
            "surface-active", surface.lighten(self.luminosity_spread / 2.5).hex
        )

        # The scrollbar colors
        colors["scrollbar"] = get(
            "scrollbar",
            (Color.parse(colors["background-darken-1"]) + primary.with_alpha(0.4)).hex,
        )
        colors["scrollbar-hover"] = get(
            "scrollbar-hover",
            (Color.parse(colors["background-darken-1"]) + primary.with_alpha(0.5)).hex,
        )
        # colors["scrollbar-active"] = get("scrollbar-active", colors["panel-lighten-2"])
        colors["scrollbar-active"] = get("scrollbar-active", primary.hex)
        colors["scrollbar-background"] = get(
            "scrollbar-background", colors["background-darken-1"]
        )
        colors["scrollbar-corner-color"] = get(
            "scrollbar-corner-color", colors["scrollbar-background"]
        )
        colors["scrollbar-background-hover"] = get(
            "scrollbar-background-hover", colors["scrollbar-background"]
        )
        colors["scrollbar-background-active"] = get(
            "scrollbar-background-active", colors["scrollbar-background"]
        )

        # Links
        colors["link-background"] = get("link-background", "initial")
        colors["link-background-hover"] = get("link-background-hover", primary.hex)
        colors["link-color"] = get("link-color", colors["text"])
        colors["link-style"] = get("link-style", "underline")
        colors["link-color-hover"] = get("link-color-hover", colors["text"])
        colors["link-style-hover"] = get("link-style-hover", "bold not underline")

        colors["footer-foreground"] = get("footer-foreground", foreground.hex)
        colors["footer-background"] = get("footer-background", panel.hex)

        colors["footer-key-foreground"] = get("footer-key-foreground", accent.hex)
        colors["footer-key-background"] = get("footer-key-background", "transparent")

        colors["footer-description-foreground"] = get(
            "footer-description-foreground", foreground.hex
        )
        colors["footer-description-background"] = get(
            "footer-description-background", "transparent"
        )

        colors["footer-item-background"] = get("footer-item-background", "transparent")

        colors["input-cursor-background"] = get(
            "input-cursor-background", foreground.hex
        )
        colors["input-cursor-foreground"] = get(
            "input-cursor-foreground", background.hex
        )
        colors["input-cursor-text-style"] = get("input-cursor-text-style", "none")
        colors["input-selection-background"] = get(
            "input-selection-background",
            Color.parse(colors["primary-lighten-1"]).with_alpha(0.4).hex,
        )

        # Markdown header styles
        colors["markdown-h1-color"] = get("markdown-h1-color", primary.hex)
        colors["markdown-h1-background"] = get("markdown-h1-background", "transparent")
        colors["markdown-h1-text-style"] = get("markdown-h1-text-style", "bold")

        colors["markdown-h2-color"] = get("markdown-h2-color", primary.hex)
        colors["markdown-h2-background"] = get("markdown-h2-background", "transparent")
        colors["markdown-h2-text-style"] = get("markdown-h2-text-style", "underline")

        colors["markdown-h3-color"] = get("markdown-h3-color", primary.hex)
        colors["markdown-h3-background"] = get("markdown-h3-background", "transparent")
        colors["markdown-h3-text-style"] = get("markdown-h3-text-style", "bold")

        colors["markdown-h4-color"] = get("markdown-h4-color", foreground.hex)
        colors["markdown-h4-background"] = get("markdown-h4-background", "transparent")
        colors["markdown-h4-text-style"] = get(
            "markdown-h4-text-style", "bold underline"
        )

        colors["markdown-h5-color"] = get("markdown-h5-color", foreground.hex)
        colors["markdown-h5-background"] = get("markdown-h5-background", "transparent")
        colors["markdown-h5-text-style"] = get("markdown-h5-text-style", "bold")

        colors["markdown-h6-color"] = get(
            "markdown-h6-color", colors["foreground-muted"]
        )
        colors["markdown-h6-background"] = get("markdown-h6-background", "transparent")
        colors["markdown-h6-text-style"] = get("markdown-h6-text-style", "bold")

        colors["button-foreground"] = get("button-foreground", foreground.hex)
        colors["button-color-foreground"] = get(
            "button-color-foreground", colors["text"]
        )
        colors["button-focus-text-style"] = get("button-focus-text-style", "b reverse")

        return colors


def show_design(light: ColorSystem, dark: ColorSystem) -> Table:
    """Generate a renderable to show color systems.

    Args:
        light: Light ColorSystem.
        dark: Dark ColorSystem

    Returns:
        Table showing all colors.
    """

    @group()
    def make_shades(system: ColorSystem):
        colors = system.generate()
        for name in system.shades:
            background = Color.parse(colors[name]).with_alpha(1.0)
            foreground = background + background.get_contrast_text(0.9)

            text = Text(f"${name}")

            yield Padding(text, 1, style=f"{foreground.hex6} on {background.hex6}")

    table = Table(box=None, expand=True)
    table.add_column("Light", justify="center")
    table.add_column("Dark", justify="center")
    table.add_row(make_shades(light), make_shades(dark))
    return table


--- CHANGELOG.md ---
# Change Log

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](http://keepachangelog.com/)
and this project adheres to [Semantic Versioning](http://semver.org/).

## [Unreleased]

### Fixed

- Fixed type hint aliasing for App under TYPE_CHECKING https://github.com/Textualize/textual/pull/6152

## [6.3.0] - 2025-10-11

### Added

- Added scrollbar-visibility rule https://github.com/Textualize/textual/pull/6156

### Fixed

- Fixed highlight not auto-detecting lexer https://github.com/Textualize/textual/pull/6167

### Changed

- Dropped support for Python3.8 https://github.com/Textualize/textual/pull/6121/
- Added support for Python3.14 https://github.com/Textualize/textual/pull/6121/

## [6.2.1] - 2025-10-01

- Fix inability to copy text outside of an input/textarea when it was focused https://github.com/Textualize/textual/pull/6148
- Fix issue when copying text after a double click https://github.com/Textualize/textual/pull/6148

## [6.2.0] - 2025-09-30

### Changed

- Eager tasks are now enabled On Python3.12 and above https://github.com/Textualize/textual/pull/6102
- `Widget._arrange` is now public (as `Widget.arrange`) https://github.com/Textualize/textual/pull/6108
- Reduced number of layout operations required to update the screen https://github.com/Textualize/textual/pull/6108
- The :hover pseudo-class no applies to the first widget under the mouse with a hover style set https://github.com/Textualize/textual/pull/6132
- The footer key hover background is more visible https://github.com/Textualize/textual/pull/6132
- Made `App.delay_update` public https://github.com/Textualize/textual/pull/6137
- Pilot.click will return True if the initial mouse down is on the specified target https://github.com/Textualize/textual/pull/6139

### Added

- Added `DOMNode.displayed_and_visible_children` https://github.com/Textualize/textual/pull/6102
- Added `Widget.process_layout` https://github.com/Textualize/textual/pull/6105
- Added `App.viewport_size` https://github.com/Textualize/textual/pull/6105
- Added `Screen.size` https://github.com/Textualize/textual/pull/6105
- Added `compact` to Binding.Group https://github.com/Textualize/textual/pull/6132
- Added `Screen.get_hover_widgets_at` https://github.com/Textualize/textual/pull/6132
- Added `Content.wrap` https://github.com/Textualize/textual/pull/6138
- Added support to allow support for manual keys in add_columns as well. https://github.com/Textualize/textual/pull/5923

### Fixed

- Fixed issue where Segments with a style of `None` aren't rendered https://github.com/Textualize/textual/pull/6109
- Fixed visual glitches and crash when changing `DataTable.header_height` https://github.com/Textualize/textual/pull/6128
- Fixed TextArea.placeholder not handling multi-lines https://github.com/Textualize/textual/pull/6138
- Fixed issue with RichLog when App.theme is set early https://github.com/Textualize/textual/pull/6141
- Fixed children of collapsible not being focusable after collapsible is expanded https://github.com/Textualize/textual/pull/6143

## [6.1.0] - 2025-08-01

### Added

- Added `Button.flat` boolean to enable flat button style https://github.com/Textualize/textual/pull/6094
- Added `namespaces` parameter to `run_action` https://github.com/Textualize/textual/pull/6094
- Added "block" border style https://github.com/Textualize/textual/pull/6094

## [6.0.0] - 2025-08-31

### Fixed

- Fix type hint for SelectType: only hashable types are allowed. https://github.com/Textualize/textual/pull/6034
- Fixed `Content.expand_tabs` https://github.com/Textualize/textual/pull/6038
- Fixed return value for `Pilot.double_click` and `Pilot.triple_click` https://github.com/Textualize/textual/pull/6035
- Fixed sizing issue with `Pretty` widget https://github.com/Textualize/textual/pull/6040 https://github.com/Textualize/textual/pull/6041
- Fixed garbled inline app output when `inline_no_clear=True` https://github.com/Textualize/textual/pull/6080

### Added

- Added `BAR_RENDERABLE` to `ProgressBar` widget https://github.com/Textualize/textual/pull/5963
- Added `OptionList.set_options` https://github.com/Textualize/textual/pull/6048
- Added `TextArea.suggestion` https://github.com/Textualize/textual/pull/6048
- Added `TextArea.placeholder` https://github.com/Textualize/textual/pull/6048
- Added `Header.format_title` and `App.format_title` for easier customization of title in the Header https://github.com/Textualize/textual/pull/6051
- Added `Widget.get_line_filters` and `App.get_line_filters` https://github.com/Textualize/textual/pull/6057
- Added `Binding.Group` https://github.com/Textualize/textual/pull/6070
- Added `DOMNode.displayed_children` https://github.com/Textualize/textual/pull/6070
- Added `TextArea.hide_suggestion_on_blur` boolean https://github.com/Textualize/textual/pull/6070
- Added `OptionList.highlighted_option` property https://github.com/Textualize/textual/pull/6090
- Added `TextArea.update_suggestion` method https://github.com/Textualize/textual/pull/6090
- Added `textual.getters.app` https://github.com/Textualize/textual/pull/6089

### Changed

- Breaking change: The `renderable` property on the `Static` widget has been changed to `content`. https://github.com/Textualize/textual/pull/6041
- Breaking change: `HeaderTitle` widget is now a static, with no `text` and `sub_text` reactives https://github.com/Textualize/textual/pull/6051
- Breaking change: Renamed `Label` constructor argument `renderable` to `content` for consistency https://github.com/Textualize/textual/pull/6045
- Breaking change: Optimization to line API to avoid applying background styles to widget content. In practice this means that you can no longer rely on blank Segments automatically getting the background color.

## [5.3.0] - 2025-08-07

### Added

- Added `Content.simplify` https://github.com/Textualize/textual/pull/6023
- Added `textual.reactive.Initialize` https://github.com/Textualize/textual/pull/6023

### Fixed

- Fixed issue with IDs in markdown https://github.com/Textualize/textual/pull/6019 https://github.com/Textualize/textual/pull/6023

## [5.2.0] - 2025-08-01

### Added

- Added a 'stream' layout, which is a lot like vertical but with fewer supported rules (which is why it is faster), will remain undocumented for now. https://github.com/Textualize/textual/pull/6013

## [5.1.1] - 2025-07-21

### Fixed

- Fixed overly large distribution, no code changes https://github.com/Textualize/textual/pull/6010

## [5.1.0] - 2025-07-31

### Added

- Added `empty` pseudo class, which applies when a widget has no displayed children https://github.com/Textualize/textual/pull/5999
- Added `Screen.action_focus` https://github.com/Textualize/textual/pull/5999
- Added support for left and right mouse scroll for terminals and input devices which support it https://github.com/Textualize/textual/pull/5995

### Changed

- `last-child`, `last-of-type`, `first-child`, and `first-of-type` apply to displayed children only https://github.com/Textualize/textual/pull/5999
- `textual.compose` is now public https://github.com/Textualize/textual/pull/5999

## [5.0.1] - 2025-07-25


### Fixed

- Fixed appending to Markdown widgets that were constructed with an existing document https://github.com/Textualize/textual/pull/5990

## [5.0.0] - 2025-07-25

### Added

- Added get_minimal_width to Visual protocol https://github.com/Textualize/textual/pull/5962
- Added `expand` and `shrink` attributes to GridLayout https://github.com/Textualize/textual/pull/5962
- Added `Markdown.get_stream` https://github.com/Textualize/textual/pull/5966
- Added `textual.highlight` module for syntax highlighting https://github.com/Textualize/textual/pull/5966
- Added `MessagePump.wait_for_refresh` method https://github.com/Textualize/textual/pull/5966
- Added `Widget.container_scroll_offset` https://github.com/Textualize/textual/commit/e84600cfb31630f8b5493bf1043a4a1e7c212f7c
- Added `Markdown.source` attribute to MarkdownBlocks https://github.com/Textualize/textual/commit/e84600cfb31630f8b5493bf1043a4a1e7c212f7c
- Added extension mechanism to Markdown https://github.com/Textualize/textual/commit/e84600cfb31630f8b5493bf1043a4a1e7c212f7c
- Added `index` to `ListView.Selected` event https://github.com/Textualize/textual/pull/5973
- Added `layout` switch to Static.update https://github.com/Textualize/textual/pull/5973

### Fixed

- Fixed `TextArea` issue with the `css` theme, where the background color was stuck from the previous theme https://github.com/Textualize/textual/issues/5964
- Fixed `TextArea` runtime crash caused by tree-sitter breaking change https://github.com/Textualize/textual/issues/5976

### Changed

- Improved rendering of Markdown tables (replace Rich table with grid) which allows text selection https://github.com/Textualize/textual/pull/5962
- Change look of command palette, to drop accented borders https://github.com/Textualize/textual/pull/5966
- Some style tweaks to Markdown https://github.com/Textualize/textual/commit/e84600cfb31630f8b5493bf1043a4a1e7c212f7c
- Content markup can now accept component classes when preceded by a dot, e.g. "Hello [.my_custo_style]World[/]!" https://github.com/Textualize/textual/pull/5981
- Breaking change: `Visual.render_strips` has a new signature. If you aren't explicitly building Visuals then this won't effect you. https://github.com/Textualize/textual/pull/5981
- Breaking change: The component classes on Markdown have been moved to MarkdownBlock. This won't affect you unless you have customize the Markdown CSS https://github.com/Textualize/textual/pull/5981
- The textual-speedups library will now be imported automatically if it is installed. Set `TEXTUAL_SPEEDUPS=0` to disable.
- Breaking change: Updated tree-sitter dependency for `syntax` extras now requires Python 3.10+ https://github.com/Textualize/textual/pull/5977
- Some `TextArea` syntax highlighting changes due to tree-sitter updates https://github.com/Textualize/textual/pull/5977

### Removed

- Breaking change: Removed `Markdown.code_dark_theme`, `Markdown.code_light_theme`, `Markdown.code_indent_guides` which are no longer needed with the new code fence. https://github.com/Textualize/textual/pull/5967
- Removed focus style from Markdown, as it can be a little expensive https://github.com/Textualize/textual/commit/e84600cfb31630f8b5493bf1043a4a1e7c212f7c

## [4.0.0] - 2025-07-12

### Fixed

- Fixed `query_one` and `query_exactly_one` not raising documented `WrongType` exception. https://github.com/Textualize/textual/pull/5945
- Fixed logging to a file on Windows https://github.com/Textualize/textual/issues/5941
- Fixed eight bit colors crashing when applying dim style https://github.com/Textualize/textual/pull/5957

### Changed

- Breaking change: `Widget.anchor` now has different semantics. It should be applied to a container and anchors to the bottom of the scroll position. https://github.com/Textualize/textual/pull/5950

### Added

- Added `Markdown.append` https://github.com/Textualize/textual/pull/5950
- Added `Widget.release_anchor` https://github.com/Textualize/textual/pull/5950
- Added `compact` parameter to `MaskedInput` https://github.com/Textualize/textual/pull/5952

## [3.7.1] - 2025-07-09

### Fixed

- Fixed broken text selection with soft_wrap=False https://github.com/Textualize/textual/pull/5940

## [3.7.0] - 2025-07-07

### Added

- Added textual.getters https://github.com/Textualize/textual/pull/5930
- Added a `show_cursor` boolean to TextArea https://github.com/Textualize/textual/pull/5934

### Changed

- Potential breaking change: Changed default `query_one` and `query_exactly_one` search to breadth first https://github.com/Textualize/textual/pull/5930
- Cursor is now visible by default when in read only mode (restoring pre 3.6.0 behavior) https://github.com/Textualize/textual/pull/5934

### Fixed

- Fixed issue with Keylines not scrolling https://github.com/Textualize/textual/pull/5936

## [3.6.0] - 2025-07-06

### Fixed

- Fixed issue with the "transparent" CSS value not being transparent when set using python https://github.com/Textualize/textual/pull/5890
- Fixed issue with pushing screens when Input has mouse captured https://github.com/Textualize/textual/pull/5900
- Implemented workaround for Ghostty bug which produces negative mouse coordinates https://github.com/Textualize/textual/pull/5926

### Changed

- Widget.release_mouse will now only release the mouse, if it was captured by self https://github.com/Textualize/textual/pull/5900
- Some optimizations to TextArea, which may be noticeable during scrolling (note: may break snapshots with a TextArea) https://github.com/Textualize/textual/pull/5925
- Selecting in the TextArea now hides the cursor until you release the mouse https://github.com/Textualize/textual/pull/5925
- Read only TextAreas will no longer display a cursor https://github.com/Textualize/textual/pull/5925

### Added

- Added `TextArea.highlight_cursor_line` toggle https://github.com/Textualize/textual/pull/5924

## [3.5.0] - 2025-06-20

### Changed

- Optimized startup https://github.com/Textualize/textual/pull/5869
- New blank visual which makes background faster to render (note this will break snapshots tests this version) https://github.com/Textualize/textual/pull/5869
- Exposed `code_indent_guides` boolean on Markdown widget https://github.com/Textualize/textual/pull/5874
- Changed code fence background to use CSS background rather than its code theme https://github.com/Textualize/textual/pull/5874

## [3.4.0] - 2025-06-14

### Fixed

- Fixed issues with initial flicker in `TextArea` rendering https://github.com/Textualize/textual/issues/5841vcomm
- Fixed issue with workers that have large parameter lists breaking dev tools https://github.com/Textualize/textual/pull/5850
- Fixed post_message failing on 3.8 https://github.com/Textualize/textual/pull/5848
- Fixed log not working from threads https://github.com/Textualize/textual/pull/5863

### Added

- Added experimental opt-in support for https://github.com/willmcgugan/textual-speedups

### Changed

- Content markup is now more lenient; if a 'tag' doesn't contain a valid style it will be included verbatim. https://github.com/Textualize/textual/pull/5851

## [3.3.0] - 2025-06-01

### Fixed

- Fixed `VERTICAL_BREAKPOINTS` doesn't work https://github.com/Textualize/textual/pull/5785
- Fixed `Button` allowing text selection https://github.com/Textualize/textual/pull/5770
- Fixed running `App.run` after `asyncio.run` https://github.com/Textualize/textual/pull/5799
- Fixed triggering a deprecation warning in py >= 3.10 https://github.com/Textualize/textual/pull/5799
- Fixed `Input` invalid cursor position after updating the value https://github.com/Textualize/textual/issues/5811
- Fixed `DEFAULT_CLASSES` when applied to App https://github.com/Textualize/textual/pull/5827
- Fixed order of implicit content tag closing https://github.com/Textualize/textual/pull/5823

### Added

- Exposed `CollapsibleTitle` https://github.com/Textualize/textual/pull/5810
- Added `Color.hsv` property and `Color.from_hsv` class method https://github.com/Textualize/textual/pull/5803
- Added `cursor_at_start` and `cursor_at_end` properties to the `Input` widget https://github.com/Textualize/textual/pull/5830

### Changed

- Added a few features to `python -m textual.markup` playground https://github.com/Textualize/textual/pull/5823

## [3.2.0] - 2025-05-02

### Fixed

- Fixed `OptionList` causing excessive redrawing https://github.com/Textualize/textual/pull/5766
- Log messages could be written to stdout when there was no app, which could happen when using run_async or threads. Now they will be suppressed, unless the env var `TEXTUAL_DEBUG` is set https://github.com/Textualize/textual/pull/5782

### Added

- Added `:first-child` and `:last-child` pseudo classes https://github.com/Textualize/textual/pull/5776
- Added `toggle_class` parameter to reactives https://github.com/Textualize/textual/pull/5778
- Added `compact` parameter and reactive to `Button`, `Input`, `ToggleButton`, `RadioSet`, `OptionList`, `TextArea` https://github.com/Textualize/textual/pull/5778
- Added `HORIZONTAL_BREAKPOINTS` and `VERTICAL_BREAKPOINTS` to `App` and `Screen` https://github.com/Textualize/textual/pull/5779

### Changed

- `RadioSet` now has a default width of `1fr` https://github.com/Textualize/textual/pull/5778

## [3.1.1] - 2025-04-22

### Fixed

- Fixed issue with tint filter https://github.com/Textualize/textual/pull/5757
- Fixed a crash when setting keymap before app mount https://github.com/Textualize/textual/issues/5742

## [3.1.0] - 2025-04-12

### Fixed

- Fixed markup escaping edge cases https://github.com/Textualize/textual/pull/5697
- Fixed incorrect auto height in Collapsible https://github.com/Textualize/textual/pull/5703
- Fixed issue with keymaps and single-letter keys https://github.com/Textualize/textual/pull/5726
- Fixed `OptionList` size after removing or clearing options https://github.com/Textualize/textual/issues/5728
- Fixed footer / key panel not updating when keymaps are applied https://github.com/Textualize/textual/pull/5724
- Fixed alignment not being applied when there are min and max limits on dimensions https://github.com/Textualize/textual/pull/5732
- Fixed issues with OptionList scrollbar not updating https://github.com/Textualize/textual/pull/5736
- Fixed allow_focus method not overriding `can_focus()` https://github.com/Textualize/textual/pull/5737
- Fixed overlap of Input / TextArea selection with arbitrary text selection https://github.com/Textualize/textual/pull/5739

### Changed

- Collapsible title now accepts str, Text, or Content https://github.com/Textualize/textual/pull/5697
- Rich Text objects will be converted to Content in OptionList and other widgets https://github.com/Textualize/textual/pull/5712
- Textual will always convert dim attributes to RGB by default https://github.com/Textualize/textual/pull/5715
- Notifications will now use content markup (previously they used Console markup) https://github.com/Textualize/textual/pull/5719

### Added

- Added `TEXTUAL_DIM_FACTOR` env var to set the opacity of the 'dim' ANSI attribute https://github.com/Textualize/textual/pull/5715
- `notify()` now accepts a `markup` parameter to disable rendering the message as markup https://github.com/Textualize/textual/pull/5719
- Added `Screen.text_selection_started_signal` https://github.com/Textualize/textual/pull/5739
- Added `App.clear_selection()` helper method to clear arbitrary text selection of active screen https://github.com/Textualize/textual/pull/5739

## [3.0.1] - 2025-04-01

### Fixed

- Fixed issue with modal dialog not refreshing

## [3.0.0] - 2025-03-27

### Changed

- Breaking change: `App.query` and friends will now always query the default (first) screen, not necessarily the active screen.
- Content now has a default argument of an empty string, so `Content()` is equivalent to `Content("")`
- Assigned names to Textual-specific threads: `textual-input`, `textual-output`. These should become visible in monitoring tools (ps, top, htop) as of Python 3.14. https://github.com/Textualize/textual/pull/5654
- Tabs now accept Content or Textual markup https://github.com/Textualize/textual/pull/5657
- Buttons will now use Textual markup rather than console markup
- tree-sitter languages are now loaded lazily, improving cold-start time https://github.com/Textualize/textual/pull/563

### Fixed

- Static and Label now accept Content objects, satisfying type checkers https://github.com/Textualize/textual/pull/5618
- Fixed click selection not being disabled when allow_select was set to false https://github.com/Textualize/textual/issues/5627
- Fixed crash on clicking line API border https://github.com/Textualize/textual/pull/5641
- Fixed Select.selection now correctly returns None if Select.BLANK is selected instead of an AssertionError
- Fixed additional spaces after text-wrapping https://github.com/Textualize/textual/pull/5657
- Added missing `scroll_end` parameter to the `Log.write_line` method https://github.com/Textualize/textual/pull/5672
- Restored support for blink https://github.com/Textualize/textual/pull/5675
- Fixed scrolling breaking on DataTable with `overflow: hidden` https://github.com/Textualize/textual/pull/5681

### Added

- Added Widget.preflight_checks to perform some debug checks after a widget is instantiated, to catch common errors. https://github.com/Textualize/textual/pull/5588
- Added text-padding style https://github.com/Textualize/textual/pull/5657
- Added `Content.first_line` property https://github.com/Textualize/textual/pull/5657
- Added `Content.from_text` constructor https://github.com/Textualize/textual/pull/5657
- Added `Content.empty` constructor https://github.com/Textualize/textual/pull/5657
- Added `Content.pad` method https://github.com/Textualize/textual/pull/5657
- Added `Style.has_transparent_foreground` property https://github.com/Textualize/textual/pull/5657


## [2.1.2] - 2025-02-26

### Fixed

- Fixed command palette fuzzy search bailing too early https://github.com/Textualize/textual/pull/5579

## [2.1.1] - 2025-02-22

### Fixed

- Fixed `Link` binding to open the link https://github.com/Textualize/textual/issues/5564
- Fixed IndexError in OptionList https://github.com/Textualize/textual/pull/5574
- Fixed issue with clear_panes breaking tabbed content https://github.com/Textualize/textual/pull/5573

### Changed

- The user can now interrupt a scroll to end by grabbing the scrollbar or scrolling in any other way. Press ++end++ or scroll to the end to restore default behavior. This is more intuitive that it may sound.

## [2.1.0] - 2025-02-19

### Fixed

- Fixed smooth scrolling broken on iTerm over SSH https://github.com/Textualize/textual/pull/5551
- Fixed height of auto container which contains auto height children https://github.com/Textualize/textual/pull/5552
- Fixed `Content.from_markup` not stripping control codes https://github.com/Textualize/textual/pull/5557
- Fixed `delta_x` and `delta_y` in mouse events when smooth scrolling is enabled https://github.com/Textualize/textual/pull/5556
- Fixed flipped title colors in panel border https://github.com/Textualize/textual/issues/5548
- Fixed detection of smooth scrolling https://github.com/Textualize/textual/pull/5558

### Added

- Added `pointer_x`, `pointer_y`, `pointer_screen_x`, and `pointer_screen_y` attributes to mouse events https://github.com/Textualize/textual/pull/5556

### Changed

- Animating the scrollbar while dragging is disabled if smooth scrolling is available https://github.com/Textualize/textual/pull/5558
- Renamed `TerminalSupportsInBandWindowResize` to `InBandWindowResize` https://github.com/Textualize/textual/pull/5558

## [2.0.4] - 2025-02-17

### Fixed

- Fixed smooth scrolling breaking mouse support in VSCode (and probably others) https://github.com/Textualize/textual/pull/5549

## [2.0.3] - 2025-02-16

### Fixed

- Fixed traceback from OptionList in Command Palette https://github.com/Textualize/textual/pull/5544

## [2.0.2] - 2025-02-16

### Fixed

- Fixed OptionList.add_options exhausting iterator https://github.com/Textualize/textual/pull/5540
- Fixed screen not refreshing after pop https://github.com/Textualize/textual/pull/5543

## [2.0.1] - 2025-02-16

### Fixed

- Fixed escape tags in Textual markup https://github.com/Textualize/textual/pull/5536

## [2.0.0] - 2025-02-16

### Added

- Added `Select.type_to_search` which allows you to type to move the cursor to a matching option https://github.com/Textualize/textual/pull/5403
- Added `from_app_focus` to `Focus` event to indicate if a widget is being focused because the app itself has regained focus or not https://github.com/Textualize/textual/pull/5379
- Added `Blurred` message to `Input` widget (matching `Submitted` and `Changed`) to make it easier to synchronize with `validate_on` parameter when set to 'blur'.
- Added `Offset.transpose` https://github.com/Textualize/textual/pull/5409
- Added `screen--selection` component class to define style for selection https://github.com/Textualize/textual/pull/5409
- Added `Widget.select_container` property https://github.com/Textualize/textual/pull/5409
- Added `Widget.select_all` https://github.com/Textualize/textual/pull/5409
- Added `Region.bottom_right_inclusive` https://github.com/Textualize/textual/pull/5409
- Added double click to select, triple click to select all in container https://github.com/Textualize/textual/pull/5409
- Added arbitrary text selection https://github.com/Textualize/textual/pull/5409
- Added Widget.ALLOW_SELECT classvar for a per-widget switch to disable text selection https://github.com/Textualize/textual/pull/5409
- Added Widget.allow_select method for programmatic control of text selection https://github.com/Textualize/textual/pull/5409
- Added App.ALLOW_SELECT for a global switch to disable text selection https://github.com/Textualize/textual/pull/5409
- Added `DOMNode.query_ancestor` https://github.com/Textualize/textual/pull/5409
- Added selection to Log widget https://github.com/Textualize/textual/pull/5467
- Added `text-wrap` and `text-overflow` CSS values https://github.com/Textualize/textual/pull/5485
- Added Textual markup to replace Rich markup https://github.com/Textualize/textual/pull/5485
- Added `Content.from_markup` https://github.com/Textualize/textual/pull/5485

### Fixed

- Fixed `Pilot.click` not working with `times` parameter https://github.com/Textualize/textual/pull/5398
- Fixed select refocusing itself too late https://github.com/Textualize/textual/pull/5420
- Fixed layout of the keys in the help panel when a key has a tooltip but no description https://github.com/Textualize/textual/issues/5436
- The content of an `Input` will now only be automatically selected when the widget is focused by the user, not when the app itself has regained focus (similar to web browsers). https://github.com/Textualize/textual/pull/5379
- Updated `TextArea` and `Input` behavior when there is a selection and the user presses left or right https://github.com/Textualize/textual/pull/5400
- Footer can now be scrolled horizontally without holding `shift` https://github.com/Textualize/textual/pull/5404
- Modified _on_blur method in `Input` to post a `Blurred` message
- Fixed Log widget not refreshing on resize https://github.com/Textualize/textual/pull/5460
- Fixed special case with calculating the height of a container where all children have dynamic heights https://github.com/Textualize/textual/pull/5463
- Fixed scrollbars ignoring background opacity https://github.com/Textualize/textual/issues/5458
- Fixed `Header` icon showing command palette tooltip when disabled https://github.com/Textualize/textual/pull/5427

### Changed

- Breaking change: OptionList no longer supports `Separator`, a separator may be specified with `None`
- Implemented smooth (pixel perfect) scrolling on supported terminals. Set `TEXTUAL_SMOOTH_SCROLL=0` to disable.

### Removed

- Breaking change: Removed `wrap` argument from OptionList (use CSS `text-wrap: nowrap; text-overflow: ellipsis`)
- Breaking change: Removed `tooltip` argument from OptionList. Use `tooltip` attribute or `with_tooltip(...)` method.

## [1.0.0] - 2024-12-12

### Added

- Added `App.clipboard` https://github.com/Textualize/textual/pull/5352
- Added standard cut/copy/paste (ctrl+x, ctrl+c, ctrl+v) bindings to Input / TextArea https://github.com/Textualize/textual/pull/5352 & https://github.com/Textualize/textual/pull/5374
- Added `system` boolean to Binding, which hides the binding from the help panel https://github.com/Textualize/textual/pull/5352
- Added support for double/triple/etc clicks via `chain` attribute on `Click` events https://github.com/Textualize/textual/pull/5369
- Added `times` parameter to `Pilot.click` method, for simulating rapid clicks https://github.com/Textualize/textual/pull/5369
- Text can now be select using mouse or keyboard in the Input widget https://github.com/Textualize/textual/pull/5340

### Changed

- Breaking change: Change default quit key to `ctrl+q` https://github.com/Textualize/textual/pull/5352
- The command palette will now select the top item automatically https://github.com/Textualize/textual/pull/5361
- `ctrl+shift+k` now deletes the current line in `TextArea`, and `ctrl+x` will cut
the selection if there is one, otherwise it will cut the current line https://github.com/Textualize/textual/pull/5374
- Implemented a better matching algorithm for the command palette https://github.com/Textualize/textual/pull/5365

### Fixed

- Fixed issue with alignment in auto containers https://github.com/Textualize/textual/pull/5360

## [0.89.1] - 2024-12-05

### Fixed

- Fixed alignment of docked widgets https://github.com/Textualize/textual/pull/5347

## [0.89.0] - 2024-12-05

### Added

- Added "tab" border style https://github.com/Textualize/textual/pull/5335
- Added support for XML syntax highlighting https://github.com/Textualize/textual/pull/5320
- Added `TextArea.update_highlight_query` https://github.com/Textualize/textual/pull/5320
- `Input` widget now supports text selection via mouse and keyboard https://github.com/Textualize/textual/pull/5340
- Added new keybinds (hold shift) for text selection in `Input` https://github.com/Textualize/textual/pull/5340
- Added `Input.selection` reactive attribute for reading and updating the current selection https://github.com/Textualize/textual/pull/5340
- Added `Input.select_on_focus` (default `True`) to enable/disable selecting all text in an `Input` on focus https://github.com/Textualize/textual/pull/5340
- Added methods `Input.replace`, `Input.insert`, `Input.delete`, `Input.delete_selection` for editing text https://github.com/Textualize/textual/pull/5340
- Added `Input.selected_text` property for getting the currently selected text https://github.com/Textualize/textual/pull/5340
- `Input` can now be scrolled independently of cursor position (hold shift and scroll with the mouse wheel in supported environments) https://github.com/Textualize/textual/pull/5340

### Changed

- Breaking change: Removed `Input` reactive attributes `view_position`, `cursor_position` (now exists as a property which proxies to the `Input.selection` reactive attribute), https://github.com/Textualize/textual/pull/5340
- `Input.restrict` now checked on all edit operations (rather than just on `insert`) https://github.com/Textualize/textual/pull/5340

### Fixed

- Fixed Select not scrolling highlight in to view when clicked https://github.com/Textualize/textual/issues/5255
- Upgraded tree-sitter to 0.23+ (`syntax` extras) https://github.com/Textualize/textual/pull/5320
- Some syntax highlighting changes due to tree-sitter updates https://github.com/Textualize/textual/pull/5320
- Breaking change: `Document.query_syntax_tree` signature changed https://github.com/Textualize/textual/pull/5320
- Breaking change: `TextArea.register_language` signature changed https://github.com/Textualize/textual/pull/5320
- Breaking change: `SyntaxAwareDocument.language_name` property removed https://github.com/Textualize/textual/pull/5320
- Breaking change: Kotlin syntax highlighting removed from `TextArea` https://github.com/Textualize/textual/pull/5320
- Fixed selection list wrapping https://github.com/Textualize/textual/pull/5331
- Fixed CSS encoding issue on Windows https://github.com/Textualize/textual/pull/5324

## [0.88.1] - 2024-11-30

### Fixed

- Fixed excessive rendering of the OptionList https://github.com/Textualize/textual/pull/5311
- Fixed rendering glitches in Select https://github.com/Textualize/textual/pull/5311

## [0.88.0] - 2024-11-29

### Fixed

- Fixed infinite loop in `Widget.anchor` https://github.com/Textualize/textual/pull/5290
- Restores the ability to supply console markup to command list https://github.com/Textualize/textual/pull/5294
- Fixed delayed App Resize event https://github.com/Textualize/textual/pull/5296
- Fixed `ListView` not updating its index or highlighting after removing items https://github.com/Textualize/textual/issues/5114
- Fixed ListView focus styling rule being too broad https://github.com/Textualize/textual/pull/5304
- Fixed issue with auto-generated tab IDs https://github.com/Textualize/textual/pull/5298

### Changed

- `ListView.pop` now returns `AwaitComplete` rather than `AwaitRemove` https://github.com/Textualize/textual/pull/5135
- `ListView.remove_items` now returns `AwaitComplete` rather than `AwaitRemove` https://github.com/Textualize/textual/pull/5135


## [0.87.1] - 2024-11-24

### Fixed

- Fixed offset not being applied to grid layout https://github.com/Textualize/textual/pull/5281
- Fixed Select overlay set to auto width https://github.com/Textualize/textual/pull/5282

## [0.87.0] - 2024-11-24

### Added

- Added Styles.has_any_rules https://github.com/Textualize/textual/pull/5264
- Added `position` CSS rule. https://github.com/Textualize/textual/pull/5278
- Added `Widget.set_scroll` https://github.com/Textualize/textual/pull/5278
- Added `Select.selection` https://github.com/Textualize/textual/pull/5278

### Fixed

- Fixed offset applied to docked widgets https://github.com/Textualize/textual/pull/5264
- Fixed loading widgets responding to input https://github.com/Textualize/textual/pull/5267

## [0.86.3] - 2024-11-19

### Changed

- Updated the tutorial (text and code) https://github.com/Textualize/textual/pull/5257

### Fixed

- Fixed a glitch with the scrollbar that occurs when you hold `a` to add stopwatches in the tutorial app https://github.com/Textualize/textual/pull/5257


## [0.86.2] - 2024-11-18

### Fixed

- Fixed visibility glitch for widgets with an offset https://github.com/Textualize/textual/pull/5253
- Fixed theme variables being unavailable in code until refresh_css was called https://github.com/Textualize/textual/pull/5254


## [0.86.1] - 2024-11-16

### Fixed

- Tweaks to demo

## [0.86.0] - 2024-11-16

### Fixed

- Fixed duplicated key displays in the help panel https://github.com/Textualize/textual/issues/5037
- Fixed `TextArea` mouse selection with tab characters https://github.com/Textualize/textual/issues/5212
- Fixed `Tabs` not updating the highlighting after removing a tab https://github.com/Textualize/textual/issues/5218

### Added

- Added `App.theme` reactive attribute https://github.com/Textualize/textual/pull/5087
- Added various starter themes https://github.com/Textualize/textual/pull/5087
- Added "Change theme" command to command palette https://github.com/Textualize/textual/pull/5087
- Added `variant` parameter to `Label` widget for quick access to common styles https://github.com/Textualize/textual/pull/5087
- Added `App.get_theme` which returns a theme by name https://github.com/Textualize/textual/pull/5087
- Added `App.register_theme` and `App.unregister_theme` https://github.com/Textualize/textual/pull/5087
- Added `App.theme_changed_signal` https://github.com/Textualize/textual/pull/5087
- Added `App.available_themes` property which returns a mapping of theme names to `Theme` instances https://github.com/Textualize/textual/pull/5087
- Added `App.current_theme` property which returns the currently active theme object https://github.com/Textualize/textual/pull/5087
- Added `App.get_theme_variable_defaults` which returns a mapping of theme variables to their default values https://github.com/Textualize/textual/pull/5087
- Added `App.search` which allows bringing up a fuzzy search list of commands on-demand https://github.com/Textualize/textual/pull/5087
- Added `App.search_themes` which allows bringing up a fuzzy search list of themes on-demand https://github.com/Textualize/textual/pull/5087
- Added `textual.theme.ThemeProvider`, a command palette provider which returns all registered themes https://github.com/Textualize/textual/pull/5087
- Added several new built-in CSS variables https://github.com/Textualize/textual/pull/5087
- Added support for in-band terminal resize protocol https://github.com/Textualize/textual/pull/5217
- Added TEXTUAL_THEME environment var, which should be a comma separated list of desired themes https://github.com/Textualize/textual/pull/5238
- Added `Widget.is_scrolling` https://github.com/Textualize/textual/pull/5238
- Added `Tree.add_json` https://github.com/Textualize/textual/pull/5238

### Changed

- `Driver.process_event` is now `Driver.process_message` https://github.com/Textualize/textual/pull/5217
- `Driver.send_event` is now `Driver.send_message` https://github.com/Textualize/textual/pull/5217
- Added `can_focus` and `can_focus_children` parameters to scrollable container types. https://github.com/Textualize/textual/pull/5226
- Added `textual.lazy.Reveal` https://github.com/Textualize/textual/pull/5226
- Added `Screen.action_blur` https://github.com/Textualize/textual/pull/5226
- `Click` events can now be used with the on decorator to match the originally clicked widget https://github.com/Textualize/textual/pull/5238
- Breaking change: Removed `App.dark` reactive attribute https://github.com/Textualize/textual/pull/5087
- Breaking change: To improve consistency, several changes have been made to default widget CSS and the CSS variables which ship with Textual. On upgrading, your app will likely look different. All of these changes can be overidden with your own CSS. https://github.com/Textualize/textual/pull/5087

### Removed

- Removed `App.HOVER_EFFECTS_SCROLL_PAUSE` https://github.com/Textualize/textual/pull/5238

## [0.85.2] - 2024-11-02

- Fixed broken focus-within https://github.com/Textualize/textual/pull/5190

## [0.85.1] - 2024-10-26

### Fixed

- Fixed encoding issue when saving files such as screenshots on Windows https://github.com/Textualize/textual/pull/5182

## [0.85.0] - 2024-10-25

### Changed

- Grid will now size children to the maximum height of a row https://github.com/Textualize/textual/pull/5113
- Markdown links will be opened with `App.open_url` automatically https://github.com/Textualize/textual/pull/5113
- The universal selector (`*`) will now not match widgets with the class `-textual-system` (scrollbars, notifications etc) https://github.com/Textualize/textual/pull/5113
- Renamed `Screen.can_view` and `Widget.can_view` to `Screen.can_view_entire` and `Widget.can_view_entire` https://github.com/Textualize/textual/pull/5174

### Added

- Added Link widget https://github.com/Textualize/textual/pull/5113
- Added `open_links` to `Markdown` and `MarkdownViewer` widgets https://github.com/Textualize/textual/pull/5113
- Added `App.DEFAULT_MODE` https://github.com/Textualize/textual/pull/5113
- Added `Containers.HorizontalGroup` and `Containers.VerticalGroup` https://github.com/Textualize/textual/pull/5113
- Added `$`, `£`, `€`, `(`, `)` symbols to Digits https://github.com/Textualize/textual/pull/5113
- Added `Button.action` parameter to invoke action when clicked https://github.com/Textualize/textual/pull/5113
- Added `immediate` parameter to scroll methods https://github.com/Textualize/textual/pull/5164
- Added `textual._loop.loop_from_index` https://github.com/Textualize/textual/pull/5164
- Added `min_color` and `max_color` to Sparklines constructor, which take precedence over CSS https://github.com/Textualize/textual/pull/5174
- Added new demo `python -m textual`, not *quite* finished but better than the old one https://github.com/Textualize/textual/pull/5174
- Added `Screen.can_view_partial` and `Widget.can_view_partial` https://github.com/Textualize/textual/pull/5174
- Added `App.is_web` property to indicate if the app is running via a web browser https://github.com/Textualize/textual/pull/5128
- `Enter` and `Leave` events can now be used with the `on` decorator https://github.com/Textualize/textual/pull/5159

### Fixed

- Fixed glitchy ListView https://github.com/Textualize/textual/issues/5163

## [0.84.0] - 2024-10-22

### Fixed

- Fixed `RadioSet` not being scrollable https://github.com/Textualize/textual/issues/5100
- Fixed infinite loop in TextArea https://github.com/Textualize/textual/pull/5154

### Added

- Added `background-tint` CSS rule https://github.com/Textualize/textual/pull/5117
- Added `:first-of-type`, `:last-of-type`, `:odd`, and `:even` pseudo classes https://github.com/Textualize/textual/pull/5139

## [0.83.0] - 2024-10-10

### Added

- Added support for A-F to Digits widget https://github.com/Textualize/textual/pull/5094
- Added `Region.constrain` https://github.com/Textualize/textual/pull/5097

### Changed

- `Screen.ALLOW_IN_MAXIMIZED_VIEW` will now default to `App.ALLOW_IN_MAXIMIZED_VIEW` https://github.com/Textualize/textual/pull/5088
- Widgets matching `.-textual-system` will now be included in the maximize view by default https://github.com/Textualize/textual/pull/5088
- Digits are now thin by default, style with text-style: bold to get bold digits https://github.com/Textualize/textual/pull/5094
- Made `Widget.absolute_offset` public https://github.com/Textualize/textual/pull/5097
- Tooltips are now displayed directly below the mouse cursor https://github.com/Textualize/textual/pull/5097
- `Region.inflect` will now assume that margins overlap https://github.com/Textualize/textual/pull/5097
- `Pilot.click` and friends will now accept a widget, in addition to a selector https://github.com/Textualize/textual/pull/5095

## [0.82.0] - 2024-10-03

### Fixed

- Fixed issue with screen not updating when auto_refresh was enabled https://github.com/Textualize/textual/pull/5063
- Fixed issues regarding loading indicator https://github.com/Textualize/textual/pull/5079
- Fixed issues with inspecting the lazy loaded widgets module https://github.com/Textualize/textual/pull/5080

### Added

- Added `DOMNode.is_on_screen` property https://github.com/Textualize/textual/pull/5063
- Added support for keymaps (user configurable key bindings) https://github.com/Textualize/textual/pull/5038
- Added descriptions to bindings for all internal widgets, and updated casing to be consistent https://github.com/Textualize/textual/pull/5062

### Changed

- Breaking change: `Widget.set_loading` no longer return an awaitable https://github.com/Textualize/textual/pull/5079

## [0.81.0] - 2024-09-25

### Added

- Added `x_axis` and `y_axis` parameters to `Widget.scroll_to_region` https://github.com/Textualize/textual/pull/5047
- Added `Tree.move_cursor_to_line` https://github.com/Textualize/textual/pull/5052
- Added `Screen.pop_until_active` https://github.com/Textualize/textual/pull/5069

### Changed

- Tree will no longer scroll the X axis when moving the cursor https://github.com/Textualize/textual/pull/5047
- DirectoryTree will no longer select the first node https://github.com/Textualize/textual/pull/5052

### Fixed

- Fixed widgets occasionally not getting Resize events https://github.com/Textualize/textual/pull/5048
- Fixed tree regression https://github.com/Textualize/textual/pull/5052
- Fixed glitch with single line inline widget https://github.com/Textualize/textual/pull/5054

## [0.80.1] - 2024-09-24

### Fixed

- Fixed crash when exiting the app prematurely https://github.com/Textualize/textual/pull/5039
- Fixed exception constructing TextArea outside of App https://github.com/Textualize/textual/pull/5045

## [0.80.0] - 2024-09-23

### Added

- Added `MaskedInput` widget https://github.com/Textualize/textual/pull/4783
- Input validation for floats and integers accept embedded underscores, e.g., "1_234_567" is valid. https://github.com/Textualize/textual/pull/4784
- Support for `"none"` value added to `dock`, `hatch` and `split` styles https://github.com/Textualize/textual/pull/4982
- Support for `"none"` added to box and border style properties (e.g `widget.style.border = "none"`) https://github.com/Textualize/textual/pull/4982
- Docstrings added to most style properties https://github.com/Textualize/textual/pull/4982
- Added `ansi_color` switch to App to permit ANSI (themed) colors https://github.com/Textualize/textual/pull/5000
- Added `:ansi` pseudo class https://github.com/Textualize/textual/pull/5000
- Added `-ansi-scrollbar` style to widgets https://github.com/Textualize/textual/pull/5000
- Added `App.INLINE_PADDING` to define the number of spaces above inline apps https://github.com/Textualize/textual/pull/5000
- Added `nocolor` psuedoclass when NO_COLOR env var is set- `BINDING_GROUP_TITLE` now defaults to `None` https://github.com/Textualize/textual/pull/5023
- Added `TreeNode.siblings`, `TreeNode.next_sibling`, `TreeNode.previous_sibling`, `TreeNode.is_collapsed` https://github.com/Textualize/textual/pull/5023
- Added additional bindings to Tree widget https://github.com/Textualize/textual/pull/5023
- Added `Tree.center_scroll` https://github.com/Textualize/textual/pull/5023
- Added `Tree.unselect` https://github.com/Textualize/textual/pull/5023


### Changed

- Input validation for integers no longer accepts scientific notation like '1.5e2'; must be castable to int. https://github.com/Textualize/textual/pull/4784
- Default `scrollbar-size-vertical` changed to `2` in inline styles to match Widget default CSS (unlikely to affect users) https://github.com/Textualize/textual/pull/4982
- Removed border-right from `Toast` https://github.com/Textualize/textual/pull/4984
- Some fixes in `RichLog` result in slightly different semantics, see docstrings for details https://github.com/Textualize/textual/pull/4978
- Changed how scrollbars are rendered (will have no visual effect, but will break snapshot tests) https://github.com/Textualize/textual/pull/5000
- Added `enabled` switch to filters (mostly used internally) https://github.com/Textualize/textual/pull/5000
- `BINDING_GROUP_TITLE` now defaults to `None` https://github.com/Textualize/textual/pull/5023
- Breaking change: Changed how scrollbars are rendered so they work in ansi mode (will break snapshots) https://github.com/Textualize/textual/pull/5023

### Fixed

- Input validation of floats no longer accepts NaN (not a number). https://github.com/Textualize/textual/pull/4784
- Fixed issues with screenshots by simplifying segments only for snapshot tests https://github.com/Textualize/textual/issues/4929
- Fixed `RichLog.write` not respecting `width` parameter https://github.com/Textualize/textual/pull/4978
- Fixed `RichLog` writing at wrong width when `write` occurs before width is known (e.g. in `compose` or `on_mount`) https://github.com/Textualize/textual/pull/4978
- Fixed `RichLog.write` incorrectly shrinking width to `RichLog.min_width` when `shrink=True` (now shrinks to fit content area instead) https://github.com/Textualize/textual/pull/4978
- Fixed flicker when setting `dark` reactive on startup https://github.com/Textualize/textual/pull/4989
- Fixed command palette not sorting search results by their match score https://github.com/Textualize/textual/pull/4994
- Fixed `DataTable` cached height issue on re-populating the table when using auto-height rows https://github.com/Textualize/textual/pull/4992
- Fixed inline app output being cleared when `inline_no_clear=True` https://github.com/Textualize/textual/issues/5019

## [0.79.1] - 2024-08-31

### Fixed

- Fixed broken updates when non active screen changes https://github.com/Textualize/textual/pull/4957

## [0.79.0] - 2024-08-30

### Added

- Added `DOMNode.check_consume_key` https://github.com/Textualize/textual/pull/4940
- Added `App.ESCAPE_TO_MINIMIZE`, `App.screen_to_minimize`, and `Screen.ESCAPE_TO_MINIMIZE` https://github.com/Textualize/textual/pull/4951
- Added `DOMNode.query_exactly_one` https://github.com/Textualize/textual/pull/4950
- Added `SelectorSet.is_simple` https://github.com/Textualize/textual/pull/4950

### Changed

- KeyPanel will show multiple keys if bound to the same action https://github.com/Textualize/textual/pull/4940
- Breaking change: `DOMNode.query_one` will not `raise TooManyMatches` https://github.com/Textualize/textual/pull/4950

## [0.78.0] - 2024-08-27

### Added

- Added Maximize and Minimize system commands. https://github.com/Textualize/textual/pull/4931
- Added `Screen.maximize`, `Screen.minimize`, `Screen.action_maximize`, `Screen.action_minimize`, `Widget.is_maximized`, `Widget.allow_maximize`. https://github.com/Textualize/textual/pull/4931
- Added `Widget.ALLOW_MAXIMIZE`, `Screen.ALLOW_IN_MAXIMIZED_VIEW` classvars https://github.com/Textualize/textual/pull/4931

## [0.77.0] - 2024-08-22

### Added

- Added `tooltip` to Binding https://github.com/Textualize/textual/pull/4859
- Added a link to the command palette to the Footer (set `show_command_palette=False` to disable) https://github.com/Textualize/textual/pull/4867
- Added `TOOLTIP_DELAY` to App to customize time until a tooltip is displayed
- Added "Show keys" option to system commands to show a summary of key bindings. https://github.com/Textualize/textual/pull/4876
- Added "split" CSS style, currently undocumented, and may change. https://github.com/Textualize/textual/pull/4876
- Added `Region.get_spacing_between` https://github.com/Textualize/textual/pull/4876
- Added `App.COMMAND_PALETTE_KEY` to change default command palette key binding https://github.com/Textualize/textual/pull/4867
- Added `App.get_key_display` https://github.com/Textualize/textual/pull/4890
- Added `DOMNode.BINDING_GROUP` https://github.com/Textualize/textual/pull/4906
- Added `DOMNode.HELP` classvar which contains Markdown help to be shown in the help panel https://github.com/Textualize/textual/pull/4915
- Added `App.get_system_commands` https://github.com/Textualize/textual/pull/4920
- Added "Save Screenshot" system command https://github.com/Textualize/textual/pull/4922

### Changed

- Removed caps_lock and num_lock modifiers https://github.com/Textualize/textual/pull/4861
- Keys such as escape and space are now displayed in lower case in footer https://github.com/Textualize/textual/pull/4876
- Changed default command palette binding to `ctrl+p` https://github.com/Textualize/textual/pull/4867
- Removed `ctrl_to_caret` and `upper_case_keys` from Footer. These can be implemented in `App.get_key_display`.
- Renamed `SystemCommands` to `SystemCommandsProvider` https://github.com/Textualize/textual/pull/4920
- Breaking change: Removed `ClassicFooter` widget (please use new `Footer` widget) https://github.com/Textualize/textual/pull/4921
- Breaking change: `App.get_key_display` now requires `textual.binding.Binding` instead of `str`.
- Disallowed `Screen` instances in `App.SCREENS` and `App.MODES`

### Fixed

- Fix crash when `validate_on` value isn't a set https://github.com/Textualize/textual/pull/4868
- Fix `Input.cursor_blink` having no effect on the blink cycle after mounting https://github.com/Textualize/textual/pull/4869
- Fixed scrolling by page not taking scrollbar in to account https://github.com/Textualize/textual/pull/4916
- Fixed `App.MODES` being the same for all instances -- per-instance modes now exist internally

## [0.76.0]

### Changed

- Input cursor will no longer jump to the end on focus https://github.com/Textualize/textual/pull/4773
- Removed `Size.cip_size`, which was a clone of `crop_size`
- Widgets with auto dimensions will now grow if there is a scrollbar https://github.com/Textualize/textual/pull/4844
- Don't do automatic refresh when widget is not visible https://github.com/Textualize/textual/pull/4847
- Renamed `DOMNode._automatic_refresh` to `DOMNode.automatic_refresh` to allow for customization https://github.com/Textualize/textual/pull/4847

### Fixed

- Input cursor blink effect will now restart correctly when any action is performed on the input https://github.com/Textualize/textual/pull/4773
- Fixed bindings on same key not updating description https://github.com/Textualize/textual/pull/4850

### Added

- Textual will use the `ESCDELAY` env var when detecting escape keys https://github.com/Textualize/textual/pull/4848

## [0.75.1] - 2024-08-02

### Fixed

- Fixed issue with Enter events causing unresponsive UI https://github.com/Textualize/textual/pull/4833


## [0.75.0] - 2024-08-01

### Added

- Added `App.open_url` to open URLs in the web browser. When running via the WebDriver, the URL will be opened in the browser that is controlling the app https://github.com/Textualize/textual/pull/4819
- Added `Widget.is_mouse_over` https://github.com/Textualize/textual/pull/4818
- Added `node` attribute to `events.Enter` and `events.Leave` https://github.com/Textualize/textual/pull/4818

### Changed

- `events.Enter` and `events.Leave` events now bubble. https://github.com/Textualize/textual/pull/4818
- Renamed `Widget.mouse_over` to `Widget.mouse_hover` https://github.com/Textualize/textual/pull/4818

### Fixed

- Fixed issue with `mutate_reactive` and data binding https://github.com/Textualize/textual/pull/4828

## [0.74.0] - 2024-07-25

### Fixed

- Fixed issues in Kitty terminal after exiting app https://github.com/Textualize/textual/issues/4779
- Fixed exception when removing Selects https://github.com/Textualize/textual/pull/4786
- Fixed issue with non-clickable Footer keys https://github.com/Textualize/textual/pull/4798
- Fixed issue with recompose not working from Mount handler https://github.com/Textualize/textual/pull/4802

### Changed

- Calling `Screen.dismiss` with no arguments will invoke the screen callback with `None` (previously the callback wasn't invoke at all). https://github.com/Textualize/textual/pull/4795

## [0.73.0] - 2024-07-18

### Added

- Added `TextArea.line_number_start` reactive attribute https://github.com/Textualize/textual/pull/4471
- Added `TextArea.matching_bracket_location` property https://github.com/Textualize/textual/pull/4764
- Added `DOMNode.mutate_reactive` https://github.com/Textualize/textual/pull/4731
- Added "quality" parameter to `textual.color.Gradient` https://github.com/Textualize/textual/pull/4739
- Added `textual.color.Gradient.get_rich_color` https://github.com/Textualize/textual/pull/4739
- `Widget.remove_children` now accepts an iterable if widgets in addition to a selector https://github.com/Textualize/textual/issues/4735
- Raise `ValueError` with improved error message when number of cells inserted using `DataTable.add_row` doesn't match the number of columns in the table https://github.com/Textualize/textual/pull/4742
- Add `Tree.move_cursor` to programmatically move the cursor without selecting the node https://github.com/Textualize/textual/pull/4753
- Added `Footer` component style handling of padding for the key/description https://github.com/Textualize/textual/pull/4651
- `StringKey` is now exported from `data_table` https://github.com/Textualize/textual/pull/4760
- `TreeNode.add` and `TreeNode.add_leaf` now accepts `before` and `after` arguments to position a new node https://github.com/Textualize/textual/pull/4772
- Added a `gradient` parameter to the `ProgressBar` widget https://github.com/Textualize/textual/pull/4774

### Fixed

- Fixed issue with `Tabs` where disabled tabs could still be activated by clicking the underline https://github.com/Textualize/textual/issues/4701
- Fixed scroll_visible with margin https://github.com/Textualize/textual/pull/4719
- Fixed programmatically disabling button stuck in hover state https://github.com/Textualize/textual/pull/4724
- Fixed `DataTable` poor performance on startup and focus change when rows contain multi-line content https://github.com/Textualize/textual/pull/4748
- Fixed `Tree` and `DirectoryTree` horizontal scrolling off-by-2 https://github.com/Textualize/textual/pull/4744
- Fixed text-opacity in component styles https://github.com/Textualize/textual/pull/4747
- Ensure `Tree.select_node` sends `NodeSelected` message https://github.com/Textualize/textual/pull/4753
- Fixed message handlers not working when message types are assigned as the value of class vars https://github.com/Textualize/textual/pull/3940
- Fixed `CommandPalette` not focusing the input when opened when `App.AUTO_FOCUS` doesn't match the input https://github.com/Textualize/textual/pull/4763
- `SelectionList.SelectionToggled` will now be sent for each option when a bulk toggle is performed (e.g. `toggle_all`). Previously no messages were sent at all. https://github.com/Textualize/textual/pull/4759
- Fixed focus styles not being updated on blur https://github.com/Textualize/textual/pull/4771

### Changed

- "Discover" hits in the command palette are no longer sorted alphabetically https://github.com/Textualize/textual/pull/4720
- `TreeNodeSelected` messages are now posted before `TreeNodeExpanded` messages
when an expandable node is selected https://github.com/Textualize/textual/pull/4753
- `Markdown.LinkClicked.href` is now automatically unquoted https://github.com/Textualize/textual/pull/4749
- The mouse cursor hover effect of `Tree` and `DirectoryTree` will no longer linger after the mouse leaves the widget https://github.com/Textualize/textual/pull/4766


## [0.72.0] - 2024-07-09

### Changed

- More predictable DOM removals. https://github.com/Textualize/textual/pull/4708

### Fixed

- Fixed clicking separator in OptionList moving cursor https://github.com/Textualize/textual/issues/4710
- Fixed scrolling issue in OptionList https://github.com/Textualize/textual/pull/4709

## [0.71.0] - 2024-06-29

### Changed

- Snapshot tests will normalize SVG output so that changes with no visual impact don't break snapshots, but this release will break most of them.
- Breaking change: `App.push_screen` now returns an Awaitable rather than a screen. https://github.com/Textualize/textual/pull/4672
- Breaking change: `Screen.dismiss` now returns an Awaitable rather than a bool. https://github.com/Textualize/textual/pull/4672

### Fixed

- Fixed grid + keyline when the grid has auto dimensions https://github.com/Textualize/textual/pull/4680
- Fixed mouse code leakage https://github.com/Textualize/textual/pull/4681
- Fixed link inside markdown table not posting a `Markdown.LinkClicked` message https://github.com/Textualize/textual/issues/4683
- Fixed issue with mouse movements on non-active screen https://github.com/Textualize/textual/pull/4688

## [0.70.0] - 2024-06-19

### Fixed

- Fixed erroneous mouse 'ButtonDown' reporting for mouse movement when any-event mode is enabled in xterm. https://github.com/Textualize/textual/pull/3647

## [0.69.0] - 2024-06-16

### Added

- Added `App.simulate_key` https://github.com/Textualize/textual/pull/4657

### Fixed

- Fixed issue with pop_screen launched from an action https://github.com/Textualize/textual/pull/4657

### Changed

- `App.check_bindings` is now private
- `App.action_check_bindings` is now `App.action_simulate_key`

## [0.68.0] - 2024-06-14

### Added

- Added `ContentSwitcher.add_content`

### Fixed

- Improved handling of non-tty input https://github.com/Textualize/textual/pull/4647

## [0.67.1] - 2024-06-12

### Changed

- Reverts Vim keys in DataTable, provides alternatives https://github.com/Textualize/textual/pull/4638

## [0.67.0] - 2024-06-11

### Added

- Added support for Kitty's key protocol https://github.com/Textualize/textual/pull/4631
- `ctrl+pageup`/`ctrl+pagedown` will scroll page left/right in DataTable https://github.com/Textualize/textual/pull/4633
- `g`/`G` will scroll to the top/bottom of the DataTable https://github.com/Textualize/textual/pull/4633
- Added simple `hjkl` key bindings to move the cursor in DataTable https://github.com/Textualize/textual/pull/4633

### Changed

- `home` and `end` now works horizontally instead of vertically in DataTable https://github.com/Textualize/textual/pull/4633
- `Tree` and `DirectoryTree` nodes now have a bigger click target, spanning the full line https://github.com/Textualize/textual/pull/4636

### Fixed

- Fixed pageup/pagedown behavior in DataTable https://github.com/Textualize/textual/pull/4633
- Added `App.CLOSE_TIMEOUT` https://github.com/Textualize/textual/pull/4635
- Fixed deadlock on shutdown https://github.com/Textualize/textual/pull/4635

## [0.66.0] - 2024-06-08

### Changed

- `get_content_height` will now return 0 if the renderable is Falsey https://github.com/Textualize/textual/pull/4617
- Buttons may not be pressed within their "active_effect_duration" to prevent inadvertent activations https://github.com/Textualize/textual/pull/4621
- `Screen.dismiss` is now a noop if the screen isn't active. Previously it would raise a `ScreenStackError`, now it returns `False`. https://github.com/Textualize/textual/pull/4621
- Increased window for escape processing to 100ms https://github.com/Textualize/textual/pull/4625
- Tooltips are now hidden when any key is pressed https://github.com/Textualize/textual/pull/4625

### Added

- Added `Screen.is_active`
- Added `icon` reactive to Header widget https://github.com/Textualize/textual/pull/4627
- Added `time_format` reactive to Header widget https://github.com/Textualize/textual/pull/4627
- Added `tooltip` parameter to input widgets https://github.com/Textualize/textual/pull/4625

## [0.65.2] - 2024-06-06

### Fixed

- Fixed issue with notifications and screen switches https://github.com/Textualize/textual/pull/4615

### Added

- Added textual.rlock.RLock https://github.com/Textualize/textual/pull/4615

## [0.65.1] - 2024-06-05

### Fixed

- Fixed hot reloading with hatch rule https://github.com/Textualize/textual/pull/4606
- Fixed hatch style parsing https://github.com/Textualize/textual/pull/4606

## [0.65.0] - 2024-06-05

### Added

- Added Command Palette Opened, Closed, and OptionHighlighted events https://github.com/Textualize/textual/pull/4600
- Added hatch style https://github.com/Textualize/textual/pull/4603

### Fixed

- Fixed DataTable cursor flicker on scroll https://github.com/Textualize/textual/pull/4598

### Changes

- TabbedContent will automatically make tabs active when a widget in a pane is focused https://github.com/Textualize/textual/issues/4593

## [0.64.0] - 2024-06-03

### Fixed

- Fix traceback on exit https://github.com/Textualize/textual/pull/4575
- Fixed `Markdown.goto_anchor` no longer scrolling the heading into view https://github.com/Textualize/textual/pull/4583
- Fixed Footer flicker on initial focus https://github.com/Textualize/textual/issues/4573

## [0.63.6] - 2024-05-29

### Fixed

- Fixed issue with bindings not refreshing https://github.com/Textualize/textual/pull/4571

## [0.63.5] - 2024-05-28

### Fixed

- Fixed data table disappearing from tabs https://github.com/Textualize/textual/pull/4567

### Added

- Added `Styles.is_auto_width` and `Style.is_auto_height`

## [0.63.4] - 2024-05-26

### Added

- Added `immediate` switch to `Signal.publish`

### Fixed

- Fixed freeze in recompose from bindings https://github.com/Textualize/textual/pull/4558

## [0.63.3] - 2024-05-24

### Fixed

- Fixed `Footer` grid size https://github.com/Textualize/textual/pull/4545
- Fixed bindings not updated on auto focus https://github.com/Textualize/textual/pull/4551

### Changed

- Attempting to mount on a non-mounted widget now raises a MountError https://github.com/Textualize/textual/pull/4547

## [0.63.2] - 2024-05-23

### Fixed

- Fixed issue with namespaces in links https://github.com/Textualize/textual/pull/4546

## [0.63.1] - 2024-05-22

### Fixed

- Fixed display of multiple bindings https://github.com/Textualize/textual/pull/4543

## [0.63.0] - 2024-05-22

### Fixed

- Fixed actions in links https://github.com/Textualize/textual/pull/4540

### Changed

- Breaking change: New Footer (likely a drop in replacement, unless you have customized styles) https://github.com/Textualize/textual/pull/4537
- Stylistic changes to Markdown (simpler headers, less margin, etc) https://github.com/Textualize/textual/pull/4541

## [0.62.0] - 2024-05-20

### Added

- Added `start` and `end` properties to Markdown Navigator
- Added `Widget.anchor`, `Widget.clear_anchor`, and `Widget.is_anchored` https://github.com/Textualize/textual/pull/4530

## [0.61.1] - 2024-05-19

### Fixed

- Fixed auto grid columns ignoring gutter https://github.com/Textualize/textual/issues/4522

## [0.61.0] - 2024-05-18

### Added

- Added `App.get_default_screen` https://github.com/Textualize/textual/pull/4520
- Added dynamic binding via `DOMNode.check_action` https://github.com/Textualize/textual/pull/4516
- Added `"focused"` action namespace so you can bind a key to an action on the focused widget https://github.com/Textualize/textual/pull/4516
- Added "focused" to allowed action namespaces https://github.com/Textualize/textual/pull/4516

### Changed

- Breaking change: Actions (as used in bindings) will no longer check the app if they are unhandled. This was undocumented anyway, and not that useful. https://github.com/Textualize/textual/pull/4516
- Breaking change: Renamed `App.namespace_bindings` to `active_bindings`


## [0.60.1] - 2024-05-15

### Fixed

- Dependency issue

## [0.60.0] - 2024-05-14

### Fixed

- Fixed auto width not working for option lists https://github.com/Textualize/textual/pull/4507

### Added

- Added `DOMNode.query_children` https://github.com/Textualize/textual/pull/4508

## [0.59.0] - 2024-05-11

### Fixed

- Fixed `SelectionList` issues after removing an option https://github.com/Textualize/textual/pull/4464
- Fixed `ListView` bugs with the initial index https://github.com/Textualize/textual/pull/4452
- Fixed `Select` not closing https://github.com/Textualize/textual/pull/4499
- Fixed setting `loading=False` removing all child loading indicators https://github.com/Textualize/textual/pull/4499

### Changed

- When displaying a message using `App.exit()`, the console no longer highlights things such as numbers.

### Added

- Added `message_signal` to MessagePump, to listen to events sent to another widget. https://github.com/Textualize/textual/pull/4487
- Added `Widget.suppress_click` https://github.com/Textualize/textual/pull/4499

## [0.58.1] - 2024-05-01

### Fixed

- Fixed issue with Markdown mounting content lazily https://github.com/Textualize/textual/pull/4466
- Fixed intermittent issue with scrolling to focus https://github.com/Textualize/textual/commit/567caf8acb196260adf6a0a6250e3ff5093056d0
- Fixed issue with scrolling to center https://github.com/Textualize/textual/pull/4469


## [0.58.0] - 2024-04-25

### Fixed

- Fixed `TextArea` to end mouse selection only if currently selecting https://github.com/Textualize/textual/pull/4436
- Fixed issue with scroll_to_widget https://github.com/Textualize/textual/pull/4446
- Fixed issue with margins https://github.com/Textualize/textual/pull/4441

### Changed

- Added argument to signal callbacks https://github.com/Textualize/textual/pull/4438

## [0.57.1] - 2024-04-20

### Fixed

- Fixed an off-by-one error in the line number of the `Document.end` property https://github.com/Textualize/textual/issues/4426
- Fixed setting scrollbar colors not updating the scrollbar https://github.com/Textualize/textual/pull/4433
- Fixed flushing in inline mode https://github.com/Textualize/textual/pull/4435

### Added

- Added `Offset.clamp` and `Size.clamp_offset` https://github.com/Textualize/textual/pull/4435


## [0.57.0] - 2024-04-19

### Fixed

- Fixed `Integer` validator missing failure description when not a number https://github.com/Textualize/textual/issues/4413
- Fixed a crash in `DataTable` if you clicked a link in the border https://github.com/Textualize/textual/issues/4410
- Fixed issue with cursor position https://github.com/Textualize/textual/pull/4429

### Added

- Added `App.copy_to_clipboard` https://github.com/Textualize/textual/pull/4416

## [0.56.4] - 2024-04-09

### Fixed

- Disabled terminal synchronization in inline mode as it breaks on some terminals

## [0.56.3] - 2024-04-08

### Fixed

- Fixed inline mode not updating https://github.com/Textualize/textual/issues/4403

## [0.56.2] - 2024-04-07

### Fixed

- Fixed inline mode not clearing with multiple screen

## [0.56.1] - 2024-04-07

### Fixed

- Fixed flicker when non-current screen updates https://github.com/Textualize/textual/pull/4401

### Changed

- Removed additional line at the end of an inline app https://github.com/Textualize/textual/pull/4401

## [0.56.0] - 2024-04-06

### Added

- Added `Size.with_width` and `Size.with_height` https://github.com/Textualize/textual/pull/4393

### Fixed

- Fixed issue with inline mode and multiple screens https://github.com/Textualize/textual/pull/4393
- Fixed issue with priority bindings https://github.com/Textualize/textual/pull/4395

### Changed

- self.prevent can be used in a widget constructor to prevent messages on mount https://github.com/Textualize/textual/pull/4392


## [0.55.1] - 2024-04-2

### Fixed

- Fixed mouse escape sequences being generated with `mouse=False`

## [0.55.0] - 2024-04-1

### Fixed

- Fix priority bindings not appearing in footer when key clashes with focused widget https://github.com/Textualize/textual/pull/4342
- Reverted auto-width change https://github.com/Textualize/textual/pull/4369

### Changed

- Exceptions inside `Widget.compose` or workers weren't bubbling up in tests https://github.com/Textualize/textual/issues/4282
- Fixed `DataTable` scrolling issues by changing `max-height` back to 100% https://github.com/Textualize/textual/issues/4286
- Fixed `Button` not rendering correctly with console markup https://github.com/Textualize/textual/issues/4328

### Added

- Added `Document.start` and `end` location properties for convenience https://github.com/Textualize/textual/pull/4267
- Added support for JavaScript, Golang, Rust, Bash, Java and Kotlin to `TextArea` https://github.com/Textualize/textual/pull/4350
- Added `inline` parameter to `run` and `run_async` to run app inline (under the prompt). https://github.com/Textualize/textual/pull/4343
- Added `mouse` parameter to disable mouse support https://github.com/Textualize/textual/pull/4343

## [0.54.0] - 2024-03-26

### Fixed

- Fixed a crash in `TextArea` when undoing an edit to a selection the selection was made backwards https://github.com/Textualize/textual/issues/4301
- Fixed issue with flickering scrollbars https://github.com/Textualize/textual/pull/4315
- Fixed issue where narrow TextArea would repeatedly wrap due to scrollbar appearing/disappearing https://github.com/Textualize/textual/pull/4334
- Fix progress bar ETA not updating when setting `total` reactive https://github.com/Textualize/textual/pull/4316

### Changed

- ProgressBar won't show ETA until there is at least one second of samples https://github.com/Textualize/textual/pull/4316
- `Input` waits until an edit has been made, after entry to the widget, before offering a suggestion https://github.com/Textualize/textual/pull/4335

## [0.53.1] - 2024-03-18

### Fixed

- Fixed issue with data binding https://github.com/Textualize/textual/pull/4308

## [0.53.0] - 2024-03-18

### Added

- Mapping of ANSI colors to hex codes configurable via `App.ansi_theme_dark` and `App.ansi_theme_light` https://github.com/Textualize/textual/pull/4192
- `Pilot.resize_terminal` to resize the terminal in testing https://github.com/Textualize/textual/issues/4212
- Added `sort_children` method https://github.com/Textualize/textual/pull/4244
- Support for pseudo-classes in nested TCSS https://github.com/Textualize/textual/issues/4039

### Fixed

- Fixed `TextArea.code_editor` missing recently added attributes https://github.com/Textualize/textual/pull/4172
- Fixed `Sparkline` not working with data in a `deque` https://github.com/Textualize/textual/issues/3899
- Tooltips are now cleared when the related widget is no longer under them https://github.com/Textualize/textual/issues/3045
- Simplified tree-sitter highlight queries for HTML, which also seems to fix segfault issue https://github.com/Textualize/textual/pull/4195
- Fixed `DirectoryTree.path` no longer reacting to new values https://github.com/Textualize/textual/issues/4208
- Fixed content size cache with Pretty widget https://github.com/Textualize/textual/pull/4211
- Fixed `grid-gutter` interaction with Pretty widget https://github.com/Textualize/textual/pull/4219
- Fixed `TextArea` styling issue on alternate screens https://github.com/Textualize/textual/pull/4220
- Fixed writing to invisible `RichLog` https://github.com/Textualize/textual/pull/4223
- Fixed `RichLog.min_width` not being used https://github.com/Textualize/textual/pull/4223
- Rename `CollapsibleTitle.action_toggle` to `action_toggle_collapsible` to fix clash with `DOMNode.action_toggle` https://github.com/Textualize/textual/pull/4221
- Markdown component classes weren't refreshed when watching for CSS https://github.com/Textualize/textual/issues/3464
- Rename `Switch.action_toggle` to `action_toggle_switch` to fix clash with `DOMNode.action_toggle` https://github.com/Textualize/textual/issues/4262
- Fixed `OptionList.OptionHighlighted` leaking out of `Select` https://github.com/Textualize/textual/issues/4224
- Fixed `Tab` enable/disable messages leaking into `TabbedContent` https://github.com/Textualize/textual/issues/4233
- Fixed a style leak from `TabbedContent` https://github.com/Textualize/textual/issues/4232
- Fixed active hidden scrollbars not releasing the mouse https://github.com/Textualize/textual/issues/4274
- Fixed the mouse not being released when hiding a `TextArea` while mouse selection is happening https://github.com/Textualize/textual/issues/4292
- Fix mouse scrolling not working when mouse cursor is over a disabled child widget https://github.com/Textualize/textual/issues/4242

### Changed

- Clicking a non focusable widget focus ancestors https://github.com/Textualize/textual/pull/4236
- BREAKING: widget class names must start with a capital letter or an underscore `_` https://github.com/Textualize/textual/pull/4252
- BREAKING: for many widgets, messages are now sent when programmatic changes that mirror user input are made https://github.com/Textualize/textual/pull/4256
  - Changed `Collapsible`
  - Changed `Markdown`
  - Changed `Select`
  - Changed `SelectionList`
  - Changed `TabbedContent`
  - Changed `Tabs`
  - Changed `TextArea`
  - Changed `Tree`
- Improved ETA calculation for ProgressBar https://github.com/Textualize/textual/pull/4271
- BREAKING: `AppFocus` and `AppBlur` are now posted when the terminal window gains or loses focus, if the terminal supports this https://github.com/Textualize/textual/pull/4265
  - When the terminal window loses focus, the currently-focused widget will also lose focus.
  - When the terminal window regains focus, the previously-focused widget will regain focus.
- TextArea binding for <kbd>ctrl</kbd>+<kbd>k</kbd> will now delete the line if the line is empty https://github.com/Textualize/textual/issues/4277
- The active tab (in `Tabs`) / tab pane (in `TabbedContent`) can now be unset https://github.com/Textualize/textual/issues/4241

## [0.52.1] - 2024-02-20

### Fixed

- Fixed the check for animation level in `LoadingIndicator` https://github.com/Textualize/textual/issues/4188

## [0.52.0] - 2024-02-19

### Changed

- Textual now writes to stderr rather than stdout https://github.com/Textualize/textual/pull/4177

### Added

- Added an `asyncio` lock attribute `Widget.lock` to be used to synchronize widget state https://github.com/Textualize/textual/issues/4134
- Added support for environment variable `TEXTUAL_ANIMATIONS` to control what animations Textual displays https://github.com/Textualize/textual/pull/4062
- Add attribute `App.animation_level` to control whether animations on that app run or not https://github.com/Textualize/textual/pull/4062
- Added support for a `TEXTUAL_SCREENSHOT_LOCATION` environment variable to specify the location of an automated screenshot https://github.com/Textualize/textual/pull/4181/
- Added support for a `TEXTUAL_SCREENSHOT_FILENAME` environment variable to specify the filename of an automated screenshot https://github.com/Textualize/textual/pull/4181/
- Added an `asyncio` lock attribute `Widget.lock` to be used to synchronize widget state https://github.com/Textualize/textual/issues/4134
- `Widget.remove_children` now accepts a CSS selector to specify which children to remove https://github.com/Textualize/textual/pull/4183
- `Widget.batch` combines widget locking and app update batching https://github.com/Textualize/textual/pull/4183

## [0.51.0] - 2024-02-15

### Added

- TextArea now has `read_only` mode https://github.com/Textualize/textual/pull/4151
- Add some syntax highlighting to TextArea default theme https://github.com/Textualize/textual/pull/4149
- Add undo and redo to TextArea https://github.com/Textualize/textual/pull/4124
- Added support for command palette command discoverability https://github.com/Textualize/textual/pull/4154

### Fixed

- Fixed out-of-view `Tab` not being scrolled into view when `Tabs.active` is assigned https://github.com/Textualize/textual/issues/4150
- Fixed `TabbedContent.TabActivate` not being posted when `TabbedContent.active` is assigned https://github.com/Textualize/textual/issues/4150

### Changed

- Breaking change: Renamed `TextArea.tab_behaviour` to `TextArea.tab_behavior` https://github.com/Textualize/textual/pull/4124
- `TextArea.theme` now defaults to `"css"` instead of None, and is no longer optional https://github.com/Textualize/textual/pull/4157

### Fixed

- Improve support for selector lists in nested TCSS https://github.com/Textualize/textual/issues/3969
- Improve support for rule declarations after nested TCSS rule sets https://github.com/Textualize/textual/issues/3999

## [0.50.1] - 2024-02-09

### Fixed

- Fixed tint applied to ANSI colors https://github.com/Textualize/textual/pull/4142

## [0.50.0] - 2024-02-08

### Fixed

- Fixed issue with ANSI colors not being converted to truecolor https://github.com/Textualize/textual/pull/4138
- Fixed duplicate watch methods being attached to DOM nodes https://github.com/Textualize/textual/pull/4030
- Fixed using `watch` to create additional watchers would trigger other watch methods https://github.com/Textualize/textual/issues/3878

### Added

- Added support for configuring dark and light themes for code in `Markdown` https://github.com/Textualize/textual/issues/3997

## [0.49.0] - 2024-02-07

### Fixed

- Fixed scrolling in long `OptionList` by adding max height of 100% https://github.com/Textualize/textual/issues/4021
- Fixed `DirectoryTree.clear_node` not clearing the node specified https://github.com/Textualize/textual/issues/4122

### Changed

- `DirectoryTree.reload` and `DirectoryTree.reload_node` now preserve state when reloading https://github.com/Textualize/textual/issues/4056
- Fixed a crash in the TextArea when performing a backward replace https://github.com/Textualize/textual/pull/4126
- Fixed selection not updating correctly when pasting while there's a non-zero selection https://github.com/Textualize/textual/pull/4126
- Breaking change: `TextArea` will not use `Escape` to shift focus if the `tab_behaviour` is the default https://github.com/Textualize/textual/issues/4110
- `TextArea` cursor will now be invisible before first focus https://github.com/Textualize/textual/pull/4128
- Fix toggling `TextArea.cursor_blink` reactive when widget does not have focus https://github.com/Textualize/textual/pull/4128

### Added

- Added DOMQuery.set https://github.com/Textualize/textual/pull/4075
- Added DOMNode.set_reactive https://github.com/Textualize/textual/pull/4075
- Added DOMNode.data_bind https://github.com/Textualize/textual/pull/4075
- Added DOMNode.action_toggle https://github.com/Textualize/textual/pull/4075
- Added Worker.cancelled_event https://github.com/Textualize/textual/pull/4075
- `Tree` (and `DirectoryTree`) grew an attribute `lock` that can be used for synchronization across coroutines https://github.com/Textualize/textual/issues/4056


## [0.48.2] - 2024-02-02

### Fixed

- Fixed a hang in the Linux driver when connected to a pipe https://github.com/Textualize/textual/issues/4104
- Fixed broken `OptionList` `Option.id` mappings https://github.com/Textualize/textual/issues/4101

### Changed

- Breaking change: keyboard navigation in `RadioSet`, `ListView`, `OptionList`, and `SelectionList`, no longer allows highlighting disabled items https://github.com/Textualize/textual/issues/3881

## [0.48.1] - 2024-02-01

### Fixed

- `TextArea` uses CSS theme by default instead of `monokai` https://github.com/Textualize/textual/pull/4091

## [0.48.0] - 2024-02-01

### Changed

- Breaking change: Significant changes to `TextArea.__init__` default values/behaviour https://github.com/Textualize/textual/pull/3933
  - `soft_wrap=True` - soft wrapping is now enabled by default.
  - `show_line_numbers=False` - line numbers are now disabled by default.
  - `tab_behaviour="focus"` - pressing the tab key now switches focus instead of indenting by default.
- Breaking change: `TextArea` default theme changed to CSS, and default styling changed https://github.com/Textualize/textual/pull/4074
- Breaking change: `DOMNode.has_pseudo_class` now accepts a single name only https://github.com/Textualize/textual/pull/3970
- Made `textual.cache` (formerly `textual._cache`) public https://github.com/Textualize/textual/pull/3976
- `Tab.label` can now be used to change the label of a tab https://github.com/Textualize/textual/pull/3979
- Changed the default notification timeout from 3 to 5 seconds https://github.com/Textualize/textual/pull/4059
- Prior scroll animations are now cancelled on new scrolls https://github.com/Textualize/textual/pull/4081

### Added

- Added `DOMNode.has_pseudo_classes` https://github.com/Textualize/textual/pull/3970
- Added `Widget.allow_focus` and `Widget.allow_focus_children` https://github.com/Textualize/textual/pull/3989
- Added `TextArea.soft_wrap` reactive attribute added https://github.com/Textualize/textual/pull/3933
- Added `TextArea.tab_behaviour` reactive attribute added https://github.com/Textualize/textual/pull/3933
- Added `TextArea.code_editor` classmethod/alternative constructor https://github.com/Textualize/textual/pull/3933
- Added `TextArea.wrapped_document` attribute which can convert between wrapped visual coordinates and locations https://github.com/Textualize/textual/pull/3933
- Added `show_line_numbers` to `TextArea.__init__` https://github.com/Textualize/textual/pull/3933
- Added component classes allowing `TextArea` to be styled using CSS https://github.com/Textualize/textual/pull/4074
- Added `Query.blur` and `Query.focus` https://github.com/Textualize/textual/pull/4012
- Added `MessagePump.message_queue_size` https://github.com/Textualize/textual/pull/4012
- Added `TabbedContent.active_pane` https://github.com/Textualize/textual/pull/4012
- Added `App.suspend` https://github.com/Textualize/textual/pull/4064
- Added `App.action_suspend_process` https://github.com/Textualize/textual/pull/4064


### Fixed

- Parameter `animate` from `DataTable.move_cursor` was being ignored https://github.com/Textualize/textual/issues/3840
- Fixed a crash if `DirectoryTree.show_root` was set before the DOM was fully available https://github.com/Textualize/textual/issues/2363
- Live reloading of TCSS wouldn't apply CSS changes to screens under the top screen of the stack https://github.com/Textualize/textual/issues/3931
- `SelectionList` option IDs are usable as soon as the widget is instantiated https://github.com/Textualize/textual/issues/3903
- Fix issue with `Strip.crop` when crop window start aligned with strip end https://github.com/Textualize/textual/pull/3998
- Fixed Strip.crop_extend https://github.com/Textualize/textual/pull/4011
- Fix for percentage dimensions https://github.com/Textualize/textual/pull/4037
- Fixed a crash if the `TextArea` language was set but tree-sitter language binaries were not installed https://github.com/Textualize/textual/issues/4045
- Ensuring `TextArea.SelectionChanged` message only sends when the updated selection is different https://github.com/Textualize/textual/pull/3933
- Fixed declaration after nested rule set causing a parse error https://github.com/Textualize/textual/pull/4012
- ID and class validation was too lenient https://github.com/Textualize/textual/issues/3954
- Fixed CSS watcher crash if file becomes unreadable (even temporarily) https://github.com/Textualize/textual/pull/4079
- Fixed display of keys when used in conjunction with other keys https://github.com/Textualize/textual/pull/3050
- Fixed double detection of <kbd>Escape</kbd> on Windows https://github.com/Textualize/textual/issues/4038


## [0.47.1] - 2024-01-05

### Fixed

- Fixed nested specificity https://github.com/Textualize/textual/pull/3963

## [0.47.0] - 2024-01-04

### Fixed

- `Widget.move_child` would break if `before`/`after` is set to the index of the widget in `child` https://github.com/Textualize/textual/issues/1743
- Fixed auto width text not processing markup https://github.com/Textualize/textual/issues/3918
- Fixed `Tree.clear` not retaining the root's expanded state https://github.com/Textualize/textual/issues/3557

### Changed

- Breaking change: `Widget.move_child` parameters `before` and `after` are now keyword-only https://github.com/Textualize/textual/pull/3896
- Style tweak to toasts https://github.com/Textualize/textual/pull/3955

### Added

- Added textual.lazy https://github.com/Textualize/textual/pull/3936
- Added App.push_screen_wait https://github.com/Textualize/textual/pull/3955
- Added nesting of CSS https://github.com/Textualize/textual/pull/3946

## [0.46.0] - 2023-12-17

### Fixed

- Disabled radio buttons could be selected with the keyboard https://github.com/Textualize/textual/issues/3839
- Fixed zero width scrollbars causing content to disappear https://github.com/Textualize/textual/issues/3886

### Changed

- The tabs within a `TabbedContent` now prefix their IDs to stop any clash with their associated `TabPane` https://github.com/Textualize/textual/pull/3815
- Breaking change: `tab` is no longer a `@on` decorator selector for `TabbedContent.TabActivated` -- use `pane` instead https://github.com/Textualize/textual/pull/3815

### Added

- Added `Collapsible.title` reactive attribute https://github.com/Textualize/textual/pull/3830
- Added a `pane` attribute to `TabbedContent.TabActivated` https://github.com/Textualize/textual/pull/3815
- Added caching of rules attributes and `cache` parameter to Stylesheet.apply https://github.com/Textualize/textual/pull/3880

## [0.45.1] - 2023-12-12

### Fixed

- Fixed issues where styles wouldn't update if changed in mount. https://github.com/Textualize/textual/pull/3860

## [0.45.0] - 2023-12-12

### Fixed

- Fixed `DataTable.update_cell` not raising an error with an invalid column key https://github.com/Textualize/textual/issues/3335
- Fixed `Input` showing suggestions when not focused https://github.com/Textualize/textual/pull/3808
- Fixed loading indicator not covering scrollbars https://github.com/Textualize/textual/pull/3816

### Removed

- Removed renderables/align.py which was no longer used.

### Changed

- Dropped ALLOW_CHILDREN flag introduced in 0.43.0 https://github.com/Textualize/textual/pull/3814
- Widgets with an auto height in an auto height container will now expand if they have no siblings https://github.com/Textualize/textual/pull/3814
- Breaking change: Removed `limit_rules` from Stylesheet.apply https://github.com/Textualize/textual/pull/3844

### Added

- Added `get_loading_widget` to Widget and App customize the loading widget. https://github.com/Textualize/textual/pull/3816
- Added messages `Collapsible.Expanded` and `Collapsible.Collapsed` that inherit from `Collapsible.Toggled`. https://github.com/Textualize/textual/issues/3824

## [0.44.1] - 2023-12-4

### Fixed

- Fixed slow scrolling when there are many widgets https://github.com/Textualize/textual/pull/3801

## [0.44.0] - 2023-12-1

### Changed

- Breaking change: Dropped 3.7 support https://github.com/Textualize/textual/pull/3766
- Breaking changes https://github.com/Textualize/textual/issues/1530
 - `link-hover-background` renamed to `link-background-hover`
 - `link-hover-color` renamed to `link-color-hover`
 - `link-hover-style` renamed to `link-style-hover`
- `Tree` now forces a scroll when `scroll_to_node` is called https://github.com/Textualize/textual/pull/3786
- Brought rxvt's use of shift-numpad keys in line with most other terminals https://github.com/Textualize/textual/pull/3769

### Added

- Added support for Ctrl+Fn and Ctrl+Shift+Fn keys in urxvt https://github.com/Textualize/textual/pull/3737
- Friendly error messages when trying to mount non-widgets https://github.com/Textualize/textual/pull/3780
- Added `Select.from_values` class method that can be used to initialize a Select control with an iterator of values https://github.com/Textualize/textual/pull/3743

### Fixed

- Fixed NoWidget when mouse goes outside window https://github.com/Textualize/textual/pull/3790
- Removed spurious print statements from press_keys https://github.com/Textualize/textual/issues/3785

## [0.43.2] - 2023-11-29

### Fixed

- Fixed NoWidget error https://github.com/Textualize/textual/pull/3779

## [0.43.1] - 2023-11-29

### Fixed

- Fixed clicking on scrollbar moves TextArea cursor https://github.com/Textualize/textual/issues/3763

## [0.43.0] - 2023-11-28

### Fixed

- Fixed mouse targeting issue in `TextArea` when tabs were not fully expanded https://github.com/Textualize/textual/pull/3725
- Fixed `Select` not updating after changing the `prompt` reactive https://github.com/Textualize/textual/issues/2983
- Fixed flicker when updating Markdown https://github.com/Textualize/textual/pull/3757

### Added

- Added experimental Canvas class https://github.com/Textualize/textual/pull/3669/
- Added `keyline` rule https://github.com/Textualize/textual/pull/3669/
- Widgets can now have an ALLOW_CHILDREN (bool) classvar to disallow adding children to a widget https://github.com/Textualize/textual/pull/3758
- Added the ability to set the `label` property of a `Checkbox` https://github.com/Textualize/textual/pull/3765
- Added the ability to set the `label` property of a `RadioButton` https://github.com/Textualize/textual/pull/3765
- Added support for various modified edit and navigation keys in urxvt https://github.com/Textualize/textual/pull/3739
- Added app focus/blur for textual-web https://github.com/Textualize/textual/pull/3767

### Changed

- Method `MarkdownTableOfContents.set_table_of_contents` renamed to `MarkdownTableOfContents.rebuild_table_of_contents` https://github.com/Textualize/textual/pull/3730
- Exception `Tree.UnknownNodeID` moved out of `Tree`, import from `textual.widgets.tree` https://github.com/Textualize/textual/pull/3730
- Exception `TreeNode.RemoveRootError` moved out of `TreeNode`, import from `textual.widgets.tree` https://github.com/Textualize/textual/pull/3730
- Optimized startup time https://github.com/Textualize/textual/pull/3753
- App.COMMANDS or Screen.COMMANDS can now accept a callable which returns a command palette provider https://github.com/Textualize/textual/pull/3756

## [0.42.0] - 2023-11-22

### Fixed

- Duplicate CSS errors when parsing CSS from a screen https://github.com/Textualize/textual/issues/3581
- Added missing `blur` pseudo class https://github.com/Textualize/textual/issues/3439
- Fixed visual glitched characters on Windows due to Python limitation https://github.com/Textualize/textual/issues/2548
- Fixed `ScrollableContainer` to receive focus https://github.com/Textualize/textual/pull/3632
- Fixed app-level queries causing a crash when the command palette is active https://github.com/Textualize/textual/issues/3633
- Fixed outline not rendering correctly in some scenarios (e.g. on Button widgets) https://github.com/Textualize/textual/issues/3628
- Fixed live-reloading of screen CSS https://github.com/Textualize/textual/issues/3454
- `Select.value` could be in an invalid state https://github.com/Textualize/textual/issues/3612
- Off-by-one in CSS error reporting https://github.com/Textualize/textual/issues/3625
- Loading indicators and app notifications overlapped in the wrong order https://github.com/Textualize/textual/issues/3677
- Widgets being loaded are disabled and have their scrolling explicitly disabled too https://github.com/Textualize/textual/issues/3677
- Method render on a widget could be called before mounting said widget https://github.com/Textualize/textual/issues/2914

### Added

- Exceptions to `textual.widgets.select` https://github.com/Textualize/textual/pull/3614
  - `InvalidSelectValueError` for when setting a `Select` to an invalid value
  - `EmptySelectError` when creating/setting a `Select` to have no options when `allow_blank` is `False`
- `Select` methods https://github.com/Textualize/textual/pull/3614
  - `clear`
  - `is_blank`
- Constant `Select.BLANK` to flag an empty selection https://github.com/Textualize/textual/pull/3614
- Added `restrict`, `type`, `max_length`, and `valid_empty` to Input https://github.com/Textualize/textual/pull/3657
- Added `Pilot.mouse_down` to simulate `MouseDown` events https://github.com/Textualize/textual/pull/3495
- Added `Pilot.mouse_up` to simulate `MouseUp` events https://github.com/Textualize/textual/pull/3495
- Added `Widget.is_mounted` property https://github.com/Textualize/textual/pull/3709
- Added `TreeNode.refresh` https://github.com/Textualize/textual/pull/3639

### Changed

- CSS error reporting will no longer provide links to the files in question https://github.com/Textualize/textual/pull/3582
- inline CSS error reporting will report widget/class variable where the CSS was read from https://github.com/Textualize/textual/pull/3582
- Breaking change: `Tree.refresh_line` has now become an internal https://github.com/Textualize/textual/pull/3639
- Breaking change: Setting `Select.value` to `None` no longer clears the selection (See `Select.BLANK` and `Select.clear`) https://github.com/Textualize/textual/pull/3614
- Breaking change: `Button` no longer inherits from `Static`, now it inherits directly from `Widget` https://github.com/Textualize/textual/issues/3603
- Rich markup in markdown headings is now escaped when building the TOC https://github.com/Textualize/textual/issues/3689
- Mechanics behind mouse clicks. See [this](https://github.com/Textualize/textual/pull/3495#issue-1934915047) for more details. https://github.com/Textualize/textual/pull/3495
- Breaking change: max/min-width/height now includes padding and border. https://github.com/Textualize/textual/pull/3712

## [0.41.0] - 2023-10-31

### Fixed

- Fixed `Input.cursor_blink` reactive not changing blink state after `Input` was mounted https://github.com/Textualize/textual/pull/3498
- Fixed `Tabs.active` attribute value not being re-assigned after removing a tab or clearing https://github.com/Textualize/textual/pull/3498
- Fixed `DirectoryTree` race-condition crash when changing path https://github.com/Textualize/textual/pull/3498
- Fixed issue with `LRUCache.discard` https://github.com/Textualize/textual/issues/3537
- Fixed `DataTable` not scrolling to rows that were just added https://github.com/Textualize/textual/pull/3552
- Fixed cache bug with `DataTable.update_cell` https://github.com/Textualize/textual/pull/3551
- Fixed CSS errors being repeated https://github.com/Textualize/textual/pull/3566
- Fix issue with chunky highlights on buttons https://github.com/Textualize/textual/pull/3571
- Fixed `OptionList` event leakage from `CommandPalette` to `App`.
- Fixed crash in `LoadingIndicator` https://github.com/Textualize/textual/pull/3498
- Fixed crash when `Tabs` appeared as a descendant of `TabbedContent` in the DOM https://github.com/Textualize/textual/pull/3602
- Fixed the command palette cancelling other workers https://github.com/Textualize/textual/issues/3615

### Added

- Add Document `get_index_from_location` / `get_location_from_index` https://github.com/Textualize/textual/pull/3410
- Add setter for `TextArea.text` https://github.com/Textualize/textual/discussions/3525
- Added `key` argument to the `DataTable.sort()` method, allowing the table to be sorted using a custom function (or other callable) https://github.com/Textualize/textual/pull/3090
- Added `initial` to all css rules, which restores default (i.e. value from DEFAULT_CSS) https://github.com/Textualize/textual/pull/3566
- Added HorizontalPad to pad.py https://github.com/Textualize/textual/pull/3571
- Added `AwaitComplete` class, to be used for optionally awaitable return values https://github.com/Textualize/textual/pull/3498

### Changed

- Breaking change: `Button.ACTIVE_EFFECT_DURATION` classvar converted to `Button.active_effect_duration` attribute https://github.com/Textualize/textual/pull/3498
- Breaking change: `Input.blink_timer` made private (renamed to `Input._blink_timer`) https://github.com/Textualize/textual/pull/3498
- Breaking change: `Input.cursor_blink` reactive updated to not run on mount (now `init=False`) https://github.com/Textualize/textual/pull/3498
- Breaking change: `AwaitTabbedContent` class removed https://github.com/Textualize/textual/pull/3498
- Breaking change: `Tabs.remove_tab` now returns an `AwaitComplete` instead of an `AwaitRemove` https://github.com/Textualize/textual/pull/3498
- Breaking change: `Tabs.clear` now returns an `AwaitComplete` instead of an `AwaitRemove` https://github.com/Textualize/textual/pull/3498
- `TabbedContent.add_pane` now returns an `AwaitComplete` instead of an `AwaitTabbedContent` https://github.com/Textualize/textual/pull/3498
- `TabbedContent.remove_pane` now returns an `AwaitComplete` instead of an `AwaitTabbedContent` https://github.com/Textualize/textual/pull/3498
- `TabbedContent.clear_pane` now returns an `AwaitComplete` instead of an `AwaitTabbedContent` https://github.com/Textualize/textual/pull/3498
- `Tabs.add_tab` now returns an `AwaitComplete` instead of an `AwaitMount` https://github.com/Textualize/textual/pull/3498
- `DirectoryTree.reload` now returns an `AwaitComplete`, which may be awaited to ensure the node has finished being processed by the internal queue https://github.com/Textualize/textual/pull/3498
- `Tabs.remove_tab` now returns an `AwaitComplete`, which may be awaited to ensure the tab is unmounted and internal state is updated https://github.com/Textualize/textual/pull/3498
- `App.switch_mode` now returns an `AwaitMount`, which may be awaited to ensure the screen is mounted https://github.com/Textualize/textual/pull/3498
- Buttons will now display multiple lines, and have auto height https://github.com/Textualize/textual/pull/3539
- DataTable now has a max-height of 100vh rather than 100%, which doesn't work with auto
- Breaking change: empty rules now result in an error https://github.com/Textualize/textual/pull/3566
- Improved startup time by caching CSS parsing https://github.com/Textualize/textual/pull/3575
- Workers are now created/run in a thread-safe way https://github.com/Textualize/textual/pull/3586

## [0.40.0] - 2023-10-11

### Added

- Added `loading` reactive property to widgets https://github.com/Textualize/textual/pull/3509

## [0.39.0] - 2023-10-10

### Fixed

- `Pilot.click`/`Pilot.hover` can't use `Screen` as a selector https://github.com/Textualize/textual/issues/3395
- App exception when a `Tree` is initialized/mounted with `disabled=True` https://github.com/Textualize/textual/issues/3407
- Fixed `print` locations not being correctly reported in `textual console` https://github.com/Textualize/textual/issues/3237
- Fix location of IME and emoji popups https://github.com/Textualize/textual/pull/3408
- Fixed application freeze when pasting an emoji into an application on Windows https://github.com/Textualize/textual/issues/3178
- Fixed duplicate option ID handling in the `OptionList` https://github.com/Textualize/textual/issues/3455
- Fix crash when removing and updating DataTable cell at same time https://github.com/Textualize/textual/pull/3487
- Fixed fractional styles to allow integer values https://github.com/Textualize/textual/issues/3414
- Stop eating stdout/stderr in headless mode - print works again in tests https://github.com/Textualize/textual/pull/3486

### Added

- `OutOfBounds` exception to be raised by `Pilot` https://github.com/Textualize/textual/pull/3360
- `TextArea.cursor_screen_offset` property for getting the screen-relative position of the cursor https://github.com/Textualize/textual/pull/3408
- `Input.cursor_screen_offset` property for getting the screen-relative position of the cursor https://github.com/Textualize/textual/pull/3408
- Reactive `cell_padding` (and respective parameter) to define horizontal cell padding in data table columns https://github.com/Textualize/textual/issues/3435
- Added `Input.clear` method https://github.com/Textualize/textual/pull/3430
- Added `TextArea.SelectionChanged` and `TextArea.Changed` messages https://github.com/Textualize/textual/pull/3442
- Added `wait_for_dismiss` parameter to `App.push_screen` https://github.com/Textualize/textual/pull/3477
- Allow scrollbar-size to be set to 0 to achieve scrollable containers with no visible scrollbars https://github.com/Textualize/textual/pull/3488

### Changed

- Breaking change: tree-sitter and tree-sitter-languages dependencies moved to `syntax` extra https://github.com/Textualize/textual/pull/3398
- `Pilot.click`/`Pilot.hover` now raises `OutOfBounds` when clicking outside visible screen https://github.com/Textualize/textual/pull/3360
- `Pilot.click`/`Pilot.hover` now return a Boolean indicating whether the click/hover landed on the widget that matches the selector https://github.com/Textualize/textual/pull/3360
- Added a delay to when the `No Matches` message appears in the command palette, thus removing a flicker https://github.com/Textualize/textual/pull/3399
- Timer callbacks are now typed more loosely https://github.com/Textualize/textual/issues/3434

## [0.38.1] - 2023-09-21

### Fixed

- Hotfix - added missing highlight files in build distribution https://github.com/Textualize/textual/pull/3370

## [0.38.0] - 2023-09-21

### Added

- Added a TextArea https://github.com/Textualize/textual/pull/2931
- Added :dark and :light pseudo classes

### Fixed

- Fixed `DataTable` not updating component styles on hot-reloading https://github.com/Textualize/textual/issues/3312

### Changed

- Breaking change: CSS in DEFAULT_CSS is now automatically scoped to the widget (set SCOPED_CSS=False) to disable
- Breaking change: Changed `Markdown.goto_anchor` to return a boolean (if the anchor was found) instead of `None` https://github.com/Textualize/textual/pull/3334

## [0.37.1] - 2023-09-16

### Fixed

- Fixed the command palette crashing with a `TimeoutError` in any Python before 3.11 https://github.com/Textualize/textual/issues/3320
- Fixed `Input` event leakage from `CommandPalette` to `App`.

## [0.37.0] - 2023-09-15

### Added

- Added the command palette https://github.com/Textualize/textual/pull/3058
- `Input` is now validated when focus moves out of it https://github.com/Textualize/textual/pull/3193
- Attribute `Input.validate_on` (and `__init__` parameter of the same name) to customise when validation occurs https://github.com/Textualize/textual/pull/3193
- Screen-specific (sub-)title attributes https://github.com/Textualize/textual/pull/3199:
  - `Screen.TITLE`
  - `Screen.SUB_TITLE`
  - `Screen.title`
  - `Screen.sub_title`
- Properties `Header.screen_title` and `Header.screen_sub_title` https://github.com/Textualize/textual/pull/3199
- Added `DirectoryTree.DirectorySelected` message https://github.com/Textualize/textual/issues/3200
- Added `widgets.Collapsible` contributed by Sunyoung Yoo https://github.com/Textualize/textual/pull/2989

### Fixed

- Fixed a crash when removing an option from an `OptionList` while the mouse is hovering over the last option https://github.com/Textualize/textual/issues/3270
- Fixed a crash in `MarkdownViewer` when clicking on a link that contains an anchor https://github.com/Textualize/textual/issues/3094
- Fixed wrong message pump in pop_screen https://github.com/Textualize/textual/pull/3315

### Changed

- Widget.notify and App.notify are now thread-safe https://github.com/Textualize/textual/pull/3275
- Breaking change: Widget.notify and App.notify now return None https://github.com/Textualize/textual/pull/3275
- App.unnotify is now private (renamed to App._unnotify) https://github.com/Textualize/textual/pull/3275
- `Markdown.load` will now attempt to scroll to a related heading if an anchor is provided https://github.com/Textualize/textual/pull/3244
- `ProgressBar` explicitly supports being set back to its indeterminate state https://github.com/Textualize/textual/pull/3286

## [0.36.0] - 2023-09-05

### Added

- TCSS styles `layer` and `layers` can be strings https://github.com/Textualize/textual/pull/3169
- `App.return_code` for the app return code https://github.com/Textualize/textual/pull/3202
- Added `animate` switch to `Tree.scroll_to_line` and `Tree.scroll_to_node` https://github.com/Textualize/textual/pull/3210
- Added `Rule` widget https://github.com/Textualize/textual/pull/3209
- Added App.current_mode to get the current mode https://github.com/Textualize/textual/pull/3233

### Changed

- Reactive callbacks are now scheduled on the message pump of the reactable that is watching instead of the owner of reactive attribute https://github.com/Textualize/textual/pull/3065
- Callbacks scheduled with `call_next` will now have the same prevented messages as when the callback was scheduled https://github.com/Textualize/textual/pull/3065
- Added `cursor_type` to the `DataTable` constructor.
- Fixed `push_screen` not updating Screen.CSS styles https://github.com/Textualize/textual/issues/3217
- `DataTable.add_row` accepts `height=None` to automatically compute optimal height for a row https://github.com/Textualize/textual/pull/3213

### Fixed

- Fixed flicker when calling pop_screen multiple times https://github.com/Textualize/textual/issues/3126
- Fixed setting styles.layout not updating https://github.com/Textualize/textual/issues/3047
- Fixed flicker when scrolling tree up or down a line https://github.com/Textualize/textual/issues/3206

## [0.35.1]

### Fixed

- Fixed flash of 80x24 interface in textual-web

## [0.35.0]

### Added

- Ability to enable/disable tabs via the reactive `disabled` in tab panes https://github.com/Textualize/textual/pull/3152
- Textual-web driver support for Windows

### Fixed

- Could not hide/show/disable/enable tabs in nested `TabbedContent` https://github.com/Textualize/textual/pull/3150

## [0.34.0] - 2023-08-22

### Added

- Methods `TabbedContent.disable_tab` and `TabbedContent.enable_tab` https://github.com/Textualize/textual/pull/3112
- Methods `Tabs.disable` and `Tabs.enable` https://github.com/Textualize/textual/pull/3112
- Messages `Tab.Disabled`, `Tab.Enabled`, `Tabs.TabDisabled` and `Tabs.Enabled` https://github.com/Textualize/textual/pull/3112
- Methods `TabbedContent.hide_tab` and `TabbedContent.show_tab` https://github.com/Textualize/textual/pull/3112
- Methods `Tabs.hide` and `Tabs.show` https://github.com/Textualize/textual/pull/3112
- Messages `Tabs.TabHidden` and `Tabs.TabShown` https://github.com/Textualize/textual/pull/3112
- Added `ListView.extend` method to append multiple items https://github.com/Textualize/textual/pull/3012

### Changed

- grid-columns and grid-rows now accept an `auto` token to detect the optimal size https://github.com/Textualize/textual/pull/3107
- LoadingIndicator now has a minimum height of 1 line.

### Fixed

- Fixed auto height container with default grid-rows https://github.com/Textualize/textual/issues/1597
- Fixed `page_up` and `page_down` bug in `DataTable` when `show_header = False` https://github.com/Textualize/textual/pull/3093
- Fixed issue with visible children inside invisible container when moving focus https://github.com/Textualize/textual/issues/3053

## [0.33.0] - 2023-08-15

### Fixed

- Fixed unintuitive sizing behaviour of TabbedContent https://github.com/Textualize/textual/issues/2411
- Fixed relative units not always expanding auto containers https://github.com/Textualize/textual/pull/3059
- Fixed background refresh https://github.com/Textualize/textual/issues/3055
- Fixed `SelectionList.clear_options` https://github.com/Textualize/textual/pull/3075
- `MouseMove` events bubble up from widgets. `App` and `Screen` receive `MouseMove` events even if there's no Widget under the cursor. https://github.com/Textualize/textual/issues/2905
- Fixed click on double-width char https://github.com/Textualize/textual/issues/2968

### Changed

- Breaking change: `DOMNode.visible` now takes into account full DOM to report whether a node is visible or not.

### Removed

- Property `Widget.focusable_children` https://github.com/Textualize/textual/pull/3070

### Added

- Added an interface for replacing prompt of an individual option in an `OptionList` https://github.com/Textualize/textual/issues/2603
- Added `DirectoryTree.reload_node` method https://github.com/Textualize/textual/issues/2757
- Added widgets.Digit https://github.com/Textualize/textual/pull/3073
- Added `BORDER_TITLE` and `BORDER_SUBTITLE` classvars to Widget https://github.com/Textualize/textual/pull/3097

### Changed

- DescendantBlur and DescendantFocus can now be used with @on decorator

## [0.32.0] - 2023-08-03

### Added

- Added widgets.Log
- Added Widget.is_vertical_scroll_end, Widget.is_horizontal_scroll_end, Widget.is_vertical_scrollbar_grabbed, Widget.is_horizontal_scrollbar_grabbed

### Changed

- Breaking change: Renamed TextLog to RichLog

## [0.31.0] - 2023-08-01

### Added

- Added App.begin_capture_print, App.end_capture_print, Widget.begin_capture_print, Widget.end_capture_print https://github.com/Textualize/textual/issues/2952
- Added the ability to run async methods as thread workers https://github.com/Textualize/textual/pull/2938
- Added `App.stop_animation` https://github.com/Textualize/textual/issues/2786
- Added `Widget.stop_animation` https://github.com/Textualize/textual/issues/2786

### Changed

- Breaking change: Creating a thread worker now requires that a `thread=True` keyword argument is passed https://github.com/Textualize/textual/pull/2938
- Breaking change: `Markdown.load` no longer captures all errors and returns a `bool`, errors now propagate https://github.com/Textualize/textual/issues/2956
- Breaking change: the default style of a `DataTable` now has `max-height: 100%` https://github.com/Textualize/textual/issues/2959

### Fixed

- Fixed a crash when a `SelectionList` had a prompt wider than itself https://github.com/Textualize/textual/issues/2900
- Fixed a bug where `Click` events were bubbling up from `Switch` widgets https://github.com/Textualize/textual/issues/2366
- Fixed a crash when using empty CSS variables https://github.com/Textualize/textual/issues/1849
- Fixed issue with tabs in TextLog https://github.com/Textualize/textual/issues/3007
- Fixed a bug with `DataTable` hover highlighting https://github.com/Textualize/textual/issues/2909

## [0.30.0] - 2023-07-17

### Added

- Added `DataTable.remove_column` method https://github.com/Textualize/textual/pull/2899
- Added notifications https://github.com/Textualize/textual/pull/2866
- Added `on_complete` callback to scroll methods https://github.com/Textualize/textual/pull/2903

### Fixed

- Fixed CancelledError issue with timer https://github.com/Textualize/textual/issues/2854
- Fixed Toggle Buttons issue with not being clickable/hoverable https://github.com/Textualize/textual/pull/2930


## [0.29.0] - 2023-07-03

### Changed

- Factored dev tools (`textual` command) in to external lib (`textual-dev`).

### Added

- Updated `DataTable.get_cell` type hints to accept string keys https://github.com/Textualize/textual/issues/2586
- Added `DataTable.get_cell_coordinate` method
- Added `DataTable.get_row_index` method https://github.com/Textualize/textual/issues/2587
- Added `DataTable.get_column_index` method
- Added can-focus pseudo-class to target widgets that may receive focus
- Make `Markdown.update` optionally awaitable https://github.com/Textualize/textual/pull/2838
- Added `default` parameter to `DataTable.add_column` for populating existing rows https://github.com/Textualize/textual/pull/2836
- Added can-focus pseudo-class to target widgets that may receive focus

### Fixed

- Fixed crash when columns were added to populated `DataTable` https://github.com/Textualize/textual/pull/2836
- Fixed issues with opacity on Screens https://github.com/Textualize/textual/issues/2616
- Fixed style problem with selected selections in a non-focused selection list https://github.com/Textualize/textual/issues/2768
- Fixed sys.stdout and sys.stderr being None https://github.com/Textualize/textual/issues/2879

## [0.28.1] - 2023-06-20

### Fixed

- Fixed indented code blocks not showing up in `Markdown` https://github.com/Textualize/textual/issues/2781
- Fixed inline code blocks in lists showing out of order in `Markdown` https://github.com/Textualize/textual/issues/2676
- Fixed list items in a `Markdown` being added to the focus chain https://github.com/Textualize/textual/issues/2380
- Fixed `Tabs` posting unnecessary messages when removing non-active tabs https://github.com/Textualize/textual/issues/2807
- call_after_refresh will preserve the sender within the callback https://github.com/Textualize/textual/pull/2806

### Added

- Added a method of allowing third party code to handle unhandled tokens in `Markdown` https://github.com/Textualize/textual/pull/2803
- Added `MarkdownBlock` as an exported symbol in `textual.widgets.markdown` https://github.com/Textualize/textual/pull/2803

### Changed

- Tooltips are now inherited, so will work with compound widgets


## [0.28.0] - 2023-06-19

### Added

- The devtools console now confirms when CSS files have been successfully loaded after a previous error https://github.com/Textualize/textual/pull/2716
- Class variable `CSS` to screens https://github.com/Textualize/textual/issues/2137
- Class variable `CSS_PATH` to screens https://github.com/Textualize/textual/issues/2137
- Added `cursor_foreground_priority` and `cursor_background_priority` to `DataTable` https://github.com/Textualize/textual/pull/2736
- Added Region.center
- Added `center` parameter to `Widget.scroll_to_region`
- Added `origin_visible` parameter to `Widget.scroll_to_region`
- Added `origin_visible` parameter to `Widget.scroll_to_center`
- Added `TabbedContent.tab_count` https://github.com/Textualize/textual/pull/2751
- Added `TabbedContent.add_pane` https://github.com/Textualize/textual/pull/2751
- Added `TabbedContent.remove_pane` https://github.com/Textualize/textual/pull/2751
- Added `TabbedContent.clear_panes` https://github.com/Textualize/textual/pull/2751
- Added `TabbedContent.Cleared` https://github.com/Textualize/textual/pull/2751

### Fixed

- Fixed setting `TreeNode.label` on an existing `Tree` node not immediately refreshing https://github.com/Textualize/textual/pull/2713
- Correctly implement `__eq__` protocol in DataTable https://github.com/Textualize/textual/pull/2705
- Fixed exceptions in Pilot tests being silently ignored https://github.com/Textualize/textual/pull/2754
- Fixed issue where internal data of `OptionList` could be invalid for short window after `clear_options` https://github.com/Textualize/textual/pull/2754
- Fixed `Tooltip` causing a `query_one` on a lone `Static` to fail https://github.com/Textualize/textual/issues/2723
- Nested widgets wouldn't lose focus when parent is disabled https://github.com/Textualize/textual/issues/2772
- Fixed the `Tabs` `Underline` highlight getting "lost" in some extreme situations https://github.com/Textualize/textual/pull/2751

### Changed

- Breaking change: The `@on` decorator will now match a message class and any child classes https://github.com/Textualize/textual/pull/2746
- Breaking change: Styles update to checkbox, radiobutton, OptionList, Select, SelectionList, Switch https://github.com/Textualize/textual/pull/2777
- `Tabs.add_tab` is now optionally awaitable https://github.com/Textualize/textual/pull/2778
- `Tabs.add_tab` now takes `before` and `after` arguments to position a new tab https://github.com/Textualize/textual/pull/2778
- `Tabs.remove_tab` is now optionally awaitable https://github.com/Textualize/textual/pull/2778
- Breaking change: `Tabs.clear` has been changed from returning `self` to being optionally awaitable https://github.com/Textualize/textual/pull/2778

## [0.27.0] - 2023-06-01

### Fixed

- Fixed zero division error https://github.com/Textualize/textual/issues/2673
- Fix `scroll_to_center` when there were nested layers out of view (Compositor full_map not populated fully) https://github.com/Textualize/textual/pull/2684
- Fix crash when `Select` widget value attribute was set in `compose` https://github.com/Textualize/textual/pull/2690
- Issue with computing progress in workers https://github.com/Textualize/textual/pull/2686
- Issues with `switch_screen` not updating the results callback appropriately https://github.com/Textualize/textual/issues/2650
- Fixed incorrect mount order https://github.com/Textualize/textual/pull/2702

### Added

- `work` decorator accepts `description` parameter to add debug string https://github.com/Textualize/textual/issues/2597
- Added `SelectionList` widget https://github.com/Textualize/textual/pull/2652
- `App.AUTO_FOCUS` to set auto focus on all screens https://github.com/Textualize/textual/issues/2594
- Option to `scroll_to_center` to ensure we don't scroll such that the top left corner of the widget is not visible https://github.com/Textualize/textual/pull/2682
- Added `Widget.tooltip` property https://github.com/Textualize/textual/pull/2670
- Added `Region.inflect` https://github.com/Textualize/textual/pull/2670
- `Suggester` API to compose with widgets for automatic suggestions https://github.com/Textualize/textual/issues/2330
- `SuggestFromList` class to let widgets get completions from a fixed set of options https://github.com/Textualize/textual/pull/2604
- `Input` has a new component class `input--suggestion` https://github.com/Textualize/textual/pull/2604
- Added `Widget.remove_children` https://github.com/Textualize/textual/pull/2657
- Added `Validator` framework and validation for `Input` https://github.com/Textualize/textual/pull/2600
- Ability to have private and public validate methods https://github.com/Textualize/textual/pull/2708
- Ability to have private compute methods https://github.com/Textualize/textual/pull/2708
- Added `message_hook` to App.run_test https://github.com/Textualize/textual/pull/2702
- Added `Sparkline` widget https://github.com/Textualize/textual/pull/2631

### Changed

- `Placeholder` now sets its color cycle per app https://github.com/Textualize/textual/issues/2590
- Footer now clears key highlight regardless of whether it's in the active screen or not https://github.com/Textualize/textual/issues/2606
- The default Widget repr no longer displays classes and pseudo-classes (to reduce noise in logs). Add them to your `__rich_repr__` method if needed. https://github.com/Textualize/textual/pull/2623
- Setting `Screen.AUTO_FOCUS` to `None` will inherit `AUTO_FOCUS` from the app instead of disabling it https://github.com/Textualize/textual/issues/2594
- Setting `Screen.AUTO_FOCUS` to `""` will disable it on the screen https://github.com/Textualize/textual/issues/2594
- Messages now have a `handler_name` class var which contains the name of the default handler method.
- `Message.control` is now a property instead of a class variable. https://github.com/Textualize/textual/issues/2528
- `Tree` and `DirectoryTree` Messages no longer accept a `tree` parameter, using `self.node.tree` instead. https://github.com/Textualize/textual/issues/2529
- Keybinding <kbd>right</kbd> in `Input` is also used to accept a suggestion if the cursor is at the end of the input https://github.com/Textualize/textual/pull/2604
- `Input.__init__` now accepts a `suggester` attribute for completion suggestions https://github.com/Textualize/textual/pull/2604
- Using `switch_screen` to switch to the currently active screen is now a no-op https://github.com/Textualize/textual/pull/2692
- Breaking change: removed `reactive.py::Reactive.var` in favor of `reactive.py::var` https://github.com/Textualize/textual/pull/2709/

### Removed

- `Placeholder.reset_color_cycle`
- Removed `Widget.reset_focus` (now called `Widget.blur`) https://github.com/Textualize/textual/issues/2642

## [0.26.0] - 2023-05-20

### Added

- Added `Widget.can_view`

### Changed

- Textual will now scroll focused widgets to center if not in view

## [0.25.0] - 2023-05-17

### Changed

- App `title` and `sub_title` attributes can be set to any type https://github.com/Textualize/textual/issues/2521
- `DirectoryTree` now loads directory contents in a worker https://github.com/Textualize/textual/issues/2456
- Only a single error will be written by default, unless in dev mode ("debug" in App.features) https://github.com/Textualize/textual/issues/2480
- Using `Widget.move_child` where the target and the child being moved are the same is now a no-op https://github.com/Textualize/textual/issues/1743
- Calling `dismiss` on a screen that is not at the top of the stack now raises an exception https://github.com/Textualize/textual/issues/2575
- `MessagePump.call_after_refresh` and `MessagePump.call_later` will now return `False` if the callback could not be scheduled. https://github.com/Textualize/textual/pull/2584

### Fixed

- Fixed `ZeroDivisionError` in `resolve_fraction_unit` https://github.com/Textualize/textual/issues/2502
- Fixed `TreeNode.expand` and `TreeNode.expand_all` not posting a `Tree.NodeExpanded` message https://github.com/Textualize/textual/issues/2535
- Fixed `TreeNode.collapse` and `TreeNode.collapse_all` not posting a `Tree.NodeCollapsed` message https://github.com/Textualize/textual/issues/2535
- Fixed `TreeNode.toggle` and `TreeNode.toggle_all` not posting a `Tree.NodeExpanded` or `Tree.NodeCollapsed` message https://github.com/Textualize/textual/issues/2535
- `footer--description` component class was being ignored https://github.com/Textualize/textual/issues/2544
- Pasting empty selection in `Input` would raise an exception https://github.com/Textualize/textual/issues/2563
- `Screen.AUTO_FOCUS` now focuses the first _focusable_ widget that matches the selector https://github.com/Textualize/textual/issues/2578
- `Screen.AUTO_FOCUS` now works on the default screen on startup https://github.com/Textualize/textual/pull/2581
- Fix for setting dark in App `__init__` https://github.com/Textualize/textual/issues/2583
- Fix issue with scrolling and docks https://github.com/Textualize/textual/issues/2525
- Fix not being able to use CSS classes with `Tab` https://github.com/Textualize/textual/pull/2589

### Added

- Class variable `AUTO_FOCUS` to screens https://github.com/Textualize/textual/issues/2457
- Added `NULL_SPACING` and `NULL_REGION` to geometry.py

## [0.24.1] - 2023-05-08

### Fixed

- Fix TypeError in code browser

## [0.24.0] - 2023-05-08

### Fixed

- Fixed crash when creating a `DirectoryTree` starting anywhere other than `.`
- Fixed line drawing in `Tree` when `Tree.show_root` is `True` https://github.com/Textualize/textual/issues/2397
- Fixed line drawing in `Tree` not marking branches as selected when first getting focus https://github.com/Textualize/textual/issues/2397

### Changed

- The DataTable cursor is now scrolled into view when the cursor coordinate is changed programmatically https://github.com/Textualize/textual/issues/2459
- run_worker exclusive parameter is now `False` by default https://github.com/Textualize/textual/pull/2470
- Added `always_update` as an optional argument for `reactive.var`
- Made Binding description default to empty string, which is equivalent to show=False https://github.com/Textualize/textual/pull/2501
- Modified Message to allow it to be used as a dataclass https://github.com/Textualize/textual/pull/2501
- Decorator `@on` accepts arbitrary `**kwargs` to apply selectors to attributes of the message https://github.com/Textualize/textual/pull/2498

### Added

- Property `control` as alias for attribute `tabs` in `Tabs` messages https://github.com/Textualize/textual/pull/2483
- Experimental: Added "overlay" rule https://github.com/Textualize/textual/pull/2501
- Experimental: Added "constrain" rule https://github.com/Textualize/textual/pull/2501
- Added textual.widgets.Select https://github.com/Textualize/textual/pull/2501
- Added Region.translate_inside https://github.com/Textualize/textual/pull/2501
- `TabbedContent` now takes kwargs `id`, `name`, `classes`, and `disabled`, upon initialization, like other widgets https://github.com/Textualize/textual/pull/2497
- Method `DataTable.move_cursor` https://github.com/Textualize/textual/issues/2472
- Added `OptionList.add_options` https://github.com/Textualize/textual/pull/2508
- Added `TreeNode.is_root` https://github.com/Textualize/textual/pull/2510
- Added `TreeNode.remove_children` https://github.com/Textualize/textual/pull/2510
- Added `TreeNode.remove` https://github.com/Textualize/textual/pull/2510
- Added classvar `Message.ALLOW_SELECTOR_MATCH` https://github.com/Textualize/textual/pull/2498
- Added `ALLOW_SELECTOR_MATCH` to all built-in messages associated with widgets https://github.com/Textualize/textual/pull/2498
- Markdown document sub-widgets now reference the container document
- Table of contents of a markdown document now references the document
- Added the `control` property to messages
  - `DirectoryTree.FileSelected`
  - `ListView`
    - `Highlighted`
    - `Selected`
  - `Markdown`
    - `TableOfContentsUpdated`
    - `TableOfContentsSelected`
    - `LinkClicked`
  - `OptionList`
    - `OptionHighlighted`
    - `OptionSelected`
  - `RadioSet.Changed`
  - `TabContent.TabActivated`
  - `Tree`
    - `NodeSelected`
    - `NodeHighlighted`
    - `NodeExpanded`
    - `NodeCollapsed`

## [0.23.0] - 2023-05-03

### Fixed

- Fixed `outline` top and bottom not handling alpha - https://github.com/Textualize/textual/issues/2371
- Fixed `!important` not applying to `align` https://github.com/Textualize/textual/issues/2420
- Fixed `!important` not applying to `border` https://github.com/Textualize/textual/issues/2420
- Fixed `!important` not applying to `content-align` https://github.com/Textualize/textual/issues/2420
- Fixed `!important` not applying to `outline` https://github.com/Textualize/textual/issues/2420
- Fixed `!important` not applying to `overflow` https://github.com/Textualize/textual/issues/2420
- Fixed `!important` not applying to `scrollbar-size` https://github.com/Textualize/textual/issues/2420
- Fixed `outline-right` not being recognised https://github.com/Textualize/textual/issues/2446
- Fixed OSError when a file system is not available https://github.com/Textualize/textual/issues/2468

### Changed

- Setting attributes with a `compute_` method will now raise an `AttributeError` https://github.com/Textualize/textual/issues/2383
- Unknown psuedo-selectors will now raise a tokenizer error (previously they were silently ignored) https://github.com/Textualize/textual/pull/2445
- Breaking change: `DirectoryTree.FileSelected.path` is now always a `Path` https://github.com/Textualize/textual/issues/2448
- Breaking change: `Directorytree.load_directory` renamed to `Directorytree._load_directory` https://github.com/Textualize/textual/issues/2448
- Unknown pseudo-selectors will now raise a tokenizer error (previously they were silently ignored) https://github.com/Textualize/textual/pull/2445

### Added

- Watch methods can now optionally be private https://github.com/Textualize/textual/issues/2382
- Added `DirectoryTree.path` reactive attribute https://github.com/Textualize/textual/issues/2448
- Added `DirectoryTree.FileSelected.node` https://github.com/Textualize/textual/pull/2463
- Added `DirectoryTree.reload` https://github.com/Textualize/textual/issues/2448
- Added textual.on decorator https://github.com/Textualize/textual/issues/2398

## [0.22.3] - 2023-04-29

### Fixed

- Fixed `textual run` on Windows https://github.com/Textualize/textual/issues/2406
- Fixed top border of button hover state

## [0.22.2] - 2023-04-29

### Added

- Added `TreeNode.tree` as a read-only public attribute https://github.com/Textualize/textual/issues/2413

### Fixed

- Fixed superfluous style updates for focus-within pseudo-selector

## [0.22.1] - 2023-04-28

### Fixed

- Fixed timer issue https://github.com/Textualize/textual/issues/2416
- Fixed `textual run` issue https://github.com/Textualize/textual/issues/2391

## [0.22.0] - 2023-04-27

### Fixed

- Fixed broken fr units when there is a min or max dimension https://github.com/Textualize/textual/issues/2378
- Fixed plain text in Markdown code blocks with no syntax being difficult to read https://github.com/Textualize/textual/issues/2400

### Added

- Added `ProgressBar` widget https://github.com/Textualize/textual/pull/2333

### Changed

- All `textual.containers` are now `1fr` in relevant dimensions by default https://github.com/Textualize/textual/pull/2386


## [0.21.0] - 2023-04-26

### Changed

- `textual run` execs apps in a new context.
- Textual console no longer parses console markup.
- Breaking change: `Container` no longer shows required scrollbars by default https://github.com/Textualize/textual/issues/2361
- Breaking change: `VerticalScroll` no longer shows a required horizontal scrollbar by default
- Breaking change: `HorizontalScroll` no longer shows a required vertical scrollbar by default
- Breaking change: Renamed `App.action_add_class_` to `App.action_add_class`
- Breaking change: Renamed `App.action_remove_class_` to `App.action_remove_class`
- Breaking change: `RadioSet` is now a single focusable widget https://github.com/Textualize/textual/pull/2372
- Breaking change: Removed `containers.Content` (use `containers.VerticalScroll` now)

### Added

- Added `-c` switch to `textual run` which runs commands in a Textual dev environment.
- Breaking change: standard keyboard scrollable navigation bindings have been moved off `Widget` and onto a new base class for scrollable containers (see also below addition) https://github.com/Textualize/textual/issues/2332
- `ScrollView` now inherits from `ScrollableContainer` rather than `Widget` https://github.com/Textualize/textual/issues/2332
- Containers no longer inherit any bindings from `Widget` https://github.com/Textualize/textual/issues/2331
- Added `ScrollableContainer`; a container class that binds the common navigation keys to scroll actions (see also above breaking change) https://github.com/Textualize/textual/issues/2332

### Fixed

- Fixed dark mode toggles in a "child" screen not updating a "parent" screen https://github.com/Textualize/textual/issues/1999
- Fixed "panel" border not exposed via CSS
- Fixed `TabbedContent.active` changes not changing the actual content https://github.com/Textualize/textual/issues/2352
- Fixed broken color on macOS Terminal https://github.com/Textualize/textual/issues/2359

## [0.20.1] - 2023-04-18

### Fix

- New fix for stuck tabs underline https://github.com/Textualize/textual/issues/2229

## [0.20.0] - 2023-04-18

### Changed

- Changed signature of Driver. Technically a breaking change, but unlikely to affect anyone.
- Breaking change: Timer.start is now private, and returns None. There was no reason to call this manually, so unlikely to affect anyone.
- A clicked tab will now be scrolled to the center of its tab container https://github.com/Textualize/textual/pull/2276
- Style updates are now done immediately rather than on_idle https://github.com/Textualize/textual/pull/2304
- `ButtonVariant` is now exported from `textual.widgets.button` https://github.com/Textualize/textual/issues/2264
- `HorizontalScroll` and `VerticalScroll` are now focusable by default https://github.com/Textualize/textual/pull/2317

### Added

- Added `DataTable.remove_row` method https://github.com/Textualize/textual/pull/2253
- option `--port` to the command `textual console` to specify which port the console should connect to https://github.com/Textualize/textual/pull/2258
- `Widget.scroll_to_center` method to scroll children to the center of container widget https://github.com/Textualize/textual/pull/2255 and https://github.com/Textualize/textual/pull/2276
- Added `TabActivated` message to `TabbedContent` https://github.com/Textualize/textual/pull/2260
- Added "panel" border style https://github.com/Textualize/textual/pull/2292
- Added `border-title-color`, `border-title-background`, `border-title-style` rules https://github.com/Textualize/textual/issues/2289
- Added `border-subtitle-color`, `border-subtitle-background`, `border-subtitle-style` rules https://github.com/Textualize/textual/issues/2289

### Fixed

- Fixed order styles are applied in DataTable - allows combining of renderable styles and component classes https://github.com/Textualize/textual/pull/2272
- Fixed key combos with up/down keys in some terminals https://github.com/Textualize/textual/pull/2280
- Fix empty ListView preventing bindings from firing https://github.com/Textualize/textual/pull/2281
- Fix `get_component_styles` returning incorrect values on first call when combined with pseudoclasses https://github.com/Textualize/textual/pull/2304
- Fixed `active_message_pump.get` sometimes resulting in a `LookupError` https://github.com/Textualize/textual/issues/2301

## [0.19.1] - 2023-04-10

### Fixed

- Fix viewport units using wrong viewport size  https://github.com/Textualize/textual/pull/2247
- Fixed layout not clearing arrangement cache https://github.com/Textualize/textual/pull/2249


## [0.19.0] - 2023-04-07

### Added

- Added support for filtering a `DirectoryTree` https://github.com/Textualize/textual/pull/2215

### Changed

- Allowed border_title and border_subtitle to accept Text objects
- Added additional line around titles
- When a container is auto, relative dimensions in children stretch the container. https://github.com/Textualize/textual/pull/2221
- DataTable page up / down now move cursor

### Fixed

- Fixed margin not being respected when width or height is "auto" https://github.com/Textualize/textual/issues/2220
- Fixed issue which prevent scroll_visible from working https://github.com/Textualize/textual/issues/2181
- Fixed missing tracebacks on Windows https://github.com/Textualize/textual/issues/2027

## [0.18.0] - 2023-04-04

### Added

- Added Worker API https://github.com/Textualize/textual/pull/2182

### Changed

- Breaking change: Markdown.update is no longer a coroutine https://github.com/Textualize/textual/pull/2182

### Fixed

- `RadioSet` is now far less likely to report `pressed_button` as `None` https://github.com/Textualize/textual/issues/2203

## [0.17.3] - 2023-04-02

### [Fixed]

- Fixed scrollable area not taking in to account dock https://github.com/Textualize/textual/issues/2188

## [0.17.2] - 2023-04-02

### [Fixed]

- Fixed bindings persistance https://github.com/Textualize/textual/issues/1613
- The `Markdown` widget now auto-increments ordered lists https://github.com/Textualize/textual/issues/2002
- Fixed modal bindings https://github.com/Textualize/textual/issues/2194
- Fix binding enter to active button https://github.com/Textualize/textual/issues/2194

### [Changed]

- tab and shift+tab are now defined on Screen.

## [0.17.1] - 2023-03-30

### Fixed

- Fix cursor not hiding on Windows https://github.com/Textualize/textual/issues/2170
- Fixed freeze when ctrl-clicking links https://github.com/Textualize/textual/issues/2167 https://github.com/Textualize/textual/issues/2073

## [0.17.0] - 2023-03-29

### Fixed

- Issue with parsing action strings whose arguments contained quoted closing parenthesis https://github.com/Textualize/textual/pull/2112
- Issues with parsing action strings with tuple arguments https://github.com/Textualize/textual/pull/2112
- Issue with watching for CSS file changes https://github.com/Textualize/textual/pull/2128
- Fix for tabs not invalidating https://github.com/Textualize/textual/issues/2125
- Fixed scrollbar layers issue https://github.com/Textualize/textual/issues/1358
- Fix for interaction between pseudo-classes and widget-level render caches https://github.com/Textualize/textual/pull/2155

### Changed

- DataTable now has height: auto by default. https://github.com/Textualize/textual/issues/2117
- Textual will now render strings within renderables (such as tables) as Console Markup by default. You can wrap your text with rich.Text() if you want the original behavior. https://github.com/Textualize/textual/issues/2120
- Some widget methods now return `self` instead of `None` https://github.com/Textualize/textual/pull/2102:
  - `Widget`: `refresh`, `focus`, `reset_focus`
  - `Button.press`
  - `DataTable`: `clear`, `refresh_coordinate`, `refresh_row`, `refresh_column`, `sort`
  - `Placehoder.cycle_variant`
  - `Switch.toggle`
  - `Tabs.clear`
  - `TextLog`: `write`, `clear`
  - `TreeNode`: `expand`, `expand_all`, `collapse`, `collapse_all`, `toggle`, `toggle_all`
  - `Tree`: `clear`, `reset`
- Screens with alpha in their background color will now blend with the background. https://github.com/Textualize/textual/pull/2139
- Added "thick" border style. https://github.com/Textualize/textual/pull/2139
- message_pump.app will now set the active app if it is not already set.
- DataTable now has max height set to 100vh

### Added

- Added auto_scroll attribute to TextLog https://github.com/Textualize/textual/pull/2127
- Added scroll_end switch to TextLog.write https://github.com/Textualize/textual/pull/2127
- Added `Widget.get_pseudo_class_state` https://github.com/Textualize/textual/pull/2155
- Added Screen.ModalScreen which prevents App from handling bindings. https://github.com/Textualize/textual/pull/2139
- Added TEXTUAL_LOG env var which should be a path that Textual will write verbose logs to (textual devtools is generally preferred) https://github.com/Textualize/textual/pull/2148
- Added textual.logging.TextualHandler logging handler
- Added Query.set_classes, DOMNode.set_classes, and `classes` setter for Widget https://github.com/Textualize/textual/issues/1081
- Added `OptionList` https://github.com/Textualize/textual/pull/2154

## [0.16.0] - 2023-03-22

### Added
- Added `parser_factory` argument to `Markdown` and `MarkdownViewer` constructors https://github.com/Textualize/textual/pull/2075
- Added `HorizontalScroll` https://github.com/Textualize/textual/issues/1957
- Added `Center` https://github.com/Textualize/textual/issues/1957
- Added `Middle` https://github.com/Textualize/textual/issues/1957
- Added `VerticalScroll` (mimicking the old behaviour of `Vertical`) https://github.com/Textualize/textual/issues/1957
- Added `Widget.border_title` and `Widget.border_subtitle` to set border (sub)title for a widget https://github.com/Textualize/textual/issues/1864
- Added CSS styles `border_title_align` and `border_subtitle_align`.
- Added `TabbedContent` widget https://github.com/Textualize/textual/pull/2059
- Added `get_child_by_type` method to widgets / app https://github.com/Textualize/textual/pull/2059
- Added `Widget.render_str` method https://github.com/Textualize/textual/pull/2059
- Added TEXTUAL_DRIVER environment variable

### Changed

- Dropped "loading-indicator--dot" component style from LoadingIndicator https://github.com/Textualize/textual/pull/2050
- Tabs widget now sends Tabs.Cleared when there is no active tab.
- Breaking change: changed default behaviour of `Vertical` (see `VerticalScroll`) https://github.com/Textualize/textual/issues/1957
- The default `overflow` style for `Horizontal` was changed to `hidden hidden` https://github.com/Textualize/textual/issues/1957
- `DirectoryTree` also accepts `pathlib.Path` objects as the path to list https://github.com/Textualize/textual/issues/1438

### Removed

- Removed `sender` attribute from messages. It's now just private (`_sender`). https://github.com/Textualize/textual/pull/2071

### Fixed

- Fixed borders not rendering correctly. https://github.com/Textualize/textual/pull/2074
- Fix for error when removing nodes. https://github.com/Textualize/textual/issues/2079

## [0.15.1] - 2023-03-14

### Fixed

- Fixed how the namespace for messages is calculated to facilitate inheriting messages https://github.com/Textualize/textual/issues/1814
- `Tab` is now correctly made available from `textual.widgets`. https://github.com/Textualize/textual/issues/2044

## [0.15.0] - 2023-03-13

### Fixed

- Fixed container not resizing when a widget is removed https://github.com/Textualize/textual/issues/2007
- Fixes issue where the horizontal scrollbar would be incorrectly enabled https://github.com/Textualize/textual/pull/2024

## [0.15.0] - 2023-03-13

### Changed

- Fixed container not resizing when a widget is removed https://github.com/Textualize/textual/issues/2007
- Fixed issue where the horizontal scrollbar would be incorrectly enabled https://github.com/Textualize/textual/pull/2024
- Fixed `Pilot.click` not correctly creating the mouse events https://github.com/Textualize/textual/issues/2022
- Fixes issue where the horizontal scrollbar would be incorrectly enabled https://github.com/Textualize/textual/pull/2024
- Fixes for tracebacks not appearing on exit https://github.com/Textualize/textual/issues/2027

### Added

- Added a LoadingIndicator widget https://github.com/Textualize/textual/pull/2018
- Added Tabs Widget https://github.com/Textualize/textual/pull/2020

### Changed

- Breaking change: Renamed Widget.action and App.action to Widget.run_action and App.run_action
- Added `shift`, `meta` and `control` arguments to `Pilot.click`.

## [0.14.0] - 2023-03-09

### Changed

- Breaking change: There is now only `post_message` to post events, which is non-async, `post_message_no_wait` was dropped. https://github.com/Textualize/textual/pull/1940
- Breaking change: The Timer class now has just one method to stop it, `Timer.stop` which is non sync https://github.com/Textualize/textual/pull/1940
- Breaking change: Messages don't require a `sender` in their constructor https://github.com/Textualize/textual/pull/1940
- Many messages have grown a `control` property which returns the control they relate to. https://github.com/Textualize/textual/pull/1940
- Updated styling to make it clear DataTable grows horizontally https://github.com/Textualize/textual/pull/1946
- Changed the `Checkbox` character due to issues with Windows Terminal and Windows 10 https://github.com/Textualize/textual/issues/1934
- Changed the `RadioButton` character due to issues with Windows Terminal and Windows 10 and 11 https://github.com/Textualize/textual/issues/1934
- Changed the `Markdown` initial bullet character due to issues with Windows Terminal and Windows 10 and 11 https://github.com/Textualize/textual/issues/1982
- The underscore `_` is no longer a special alias for the method `pilot.press`

### Added

- Added `data_table` attribute to DataTable events https://github.com/Textualize/textual/pull/1940
- Added `list_view` attribute to `ListView` events https://github.com/Textualize/textual/pull/1940
- Added `radio_set` attribute to `RadioSet` events https://github.com/Textualize/textual/pull/1940
- Added `switch` attribute to `Switch` events https://github.com/Textualize/textual/pull/1940
- Added `hover` and `click` methods to `Pilot` https://github.com/Textualize/textual/pull/1966
- Breaking change: Added `toggle_button` attribute to RadioButton and Checkbox events, replaces `input` https://github.com/Textualize/textual/pull/1940
- A percentage alpha can now be applied to a border https://github.com/Textualize/textual/issues/1863
- Added `Color.multiply_alpha`.
- Added `ContentSwitcher` https://github.com/Textualize/textual/issues/1945

### Fixed

- Fixed bug that prevented pilot from pressing some keys https://github.com/Textualize/textual/issues/1815
- DataTable race condition that caused crash https://github.com/Textualize/textual/pull/1962
- Fixed scrollbar getting "stuck" to cursor when cursor leaves window during drag https://github.com/Textualize/textual/pull/1968 https://github.com/Textualize/textual/pull/2003
- DataTable crash when enter pressed when table is empty https://github.com/Textualize/textual/pull/1973

## [0.13.0] - 2023-03-02

### Added

- Added `Checkbox` https://github.com/Textualize/textual/pull/1872
- Added `RadioButton` https://github.com/Textualize/textual/pull/1872
- Added `RadioSet` https://github.com/Textualize/textual/pull/1872

### Changed

- Widget scrolling methods (such as `Widget.scroll_home` and `Widget.scroll_end`) now perform the scroll after the next refresh https://github.com/Textualize/textual/issues/1774
- Buttons no longer accept arbitrary renderables https://github.com/Textualize/textual/issues/1870

### Fixed

- Scrolling with cursor keys now moves just one cell https://github.com/Textualize/textual/issues/1897
- Fix exceptions in watch methods being hidden on startup https://github.com/Textualize/textual/issues/1886
- Fixed scrollbar size miscalculation https://github.com/Textualize/textual/pull/1910
- Fixed slow exit on some terminals https://github.com/Textualize/textual/issues/1920

## [0.12.1] - 2023-02-25

### Fixed

- Fix for batch update glitch https://github.com/Textualize/textual/pull/1880

## [0.12.0] - 2023-02-24

### Added

- Added `App.batch_update` https://github.com/Textualize/textual/pull/1832
- Added horizontal rule to Markdown https://github.com/Textualize/textual/pull/1832
- Added `Widget.disabled` https://github.com/Textualize/textual/pull/1785
- Added `DOMNode.notify_style_update` to replace `messages.StylesUpdated` message https://github.com/Textualize/textual/pull/1861
- Added `DataTable.show_row_labels` reactive to show and hide row labels https://github.com/Textualize/textual/pull/1868
- Added `DataTable.RowLabelSelected` event, which is emitted when a row label is clicked https://github.com/Textualize/textual/pull/1868
- Added `MessagePump.prevent` context manager to temporarily suppress a given message type https://github.com/Textualize/textual/pull/1866

### Changed

- Scrolling by page now adds to current position.
- Markdown lists have been polished: a selection of bullets, better alignment of numbers, style tweaks https://github.com/Textualize/textual/pull/1832
- Added alternative method of composing Widgets https://github.com/Textualize/textual/pull/1847
- Added `label` parameter to `DataTable.add_row` https://github.com/Textualize/textual/pull/1868
- Breaking change: Some `DataTable` component classes were renamed - see PR for details https://github.com/Textualize/textual/pull/1868

### Removed

- Removed `screen.visible_widgets` and `screen.widgets`
- Removed `StylesUpdate` message. https://github.com/Textualize/textual/pull/1861

### Fixed

- Numbers in a descendant-combined selector no longer cause an error https://github.com/Textualize/textual/issues/1836
- Fixed superfluous scrolling when focusing a docked widget https://github.com/Textualize/textual/issues/1816
- Fixes walk_children which was returning more than one screen https://github.com/Textualize/textual/issues/1846
- Fixed issue with watchers fired for detached nodes https://github.com/Textualize/textual/issues/1846

## [0.11.1] - 2023-02-17

### Fixed

- DataTable fix issue where offset cache was not being used https://github.com/Textualize/textual/pull/1810
- DataTable scrollbars resize correctly when header is toggled https://github.com/Textualize/textual/pull/1803
- DataTable location mapping cleared when clear called https://github.com/Textualize/textual/pull/1809

## [0.11.0] - 2023-02-15

### Added

- Added `TreeNode.expand_all` https://github.com/Textualize/textual/issues/1430
- Added `TreeNode.collapse_all` https://github.com/Textualize/textual/issues/1430
- Added `TreeNode.toggle_all` https://github.com/Textualize/textual/issues/1430
- Added the coroutines `Animator.wait_until_complete` and `pilot.wait_for_scheduled_animations` that allow waiting for all current and scheduled animations https://github.com/Textualize/textual/issues/1658
- Added the method `Animator.is_being_animated` that checks if an attribute of an object is being animated or is scheduled for animation
- Added more keyboard actions and related bindings to `Input` https://github.com/Textualize/textual/pull/1676
- Added App.scroll_sensitivity_x and App.scroll_sensitivity_y to adjust how many lines the scroll wheel moves the scroll position https://github.com/Textualize/textual/issues/928
- Added Shift+scroll wheel and ctrl+scroll wheel to scroll horizontally
- Added `Tree.action_toggle_node` to toggle a node without selecting, and bound it to <kbd>Space</kbd> https://github.com/Textualize/textual/issues/1433
- Added `Tree.reset` to fully reset a `Tree` https://github.com/Textualize/textual/issues/1437
- Added `DataTable.sort` to sort rows https://github.com/Textualize/textual/pull/1638
- Added `DataTable.get_cell` to retrieve a cell by column/row keys https://github.com/Textualize/textual/pull/1638
- Added `DataTable.get_cell_at` to retrieve a cell by coordinate https://github.com/Textualize/textual/pull/1638
- Added `DataTable.update_cell` to update a cell by column/row keys https://github.com/Textualize/textual/pull/1638
- Added `DataTable.update_cell_at` to update a cell at a coordinate  https://github.com/Textualize/textual/pull/1638
- Added `DataTable.ordered_rows` property to retrieve `Row`s as they're currently ordered https://github.com/Textualize/textual/pull/1638
- Added `DataTable.ordered_columns` property to retrieve `Column`s as they're currently ordered https://github.com/Textualize/textual/pull/1638
- Added `DataTable.coordinate_to_cell_key` to find the key for the cell at a coordinate https://github.com/Textualize/textual/pull/1638
- Added `DataTable.is_valid_coordinate` https://github.com/Textualize/textual/pull/1638
- Added `DataTable.is_valid_row_index` https://github.com/Textualize/textual/pull/1638
- Added `DataTable.is_valid_column_index` https://github.com/Textualize/textual/pull/1638
- Added attributes to events emitted from `DataTable` indicating row/column/cell keys https://github.com/Textualize/textual/pull/1638
- Added `DataTable.get_row` to retrieve the values from a row by key https://github.com/Textualize/textual/pull/1786
- Added `DataTable.get_row_at` to retrieve the values from a row by index https://github.com/Textualize/textual/pull/1786
- Added `DataTable.get_column` to retrieve the values from a column by key https://github.com/Textualize/textual/pull/1786
- Added `DataTable.get_column_at` to retrieve the values from a column by index https://github.com/Textualize/textual/pull/1786
- Added `DataTable.HeaderSelected` which is posted when header label clicked https://github.com/Textualize/textual/pull/1788
- Added `DOMNode.watch` and `DOMNode.is_attached` methods  https://github.com/Textualize/textual/pull/1750
- Added `DOMNode.css_tree` which is a renderable that shows the DOM and CSS https://github.com/Textualize/textual/pull/1778
- Added `DOMNode.children_view` which is a view on to a nodes children list, use for querying https://github.com/Textualize/textual/pull/1778
- Added `Markdown` and `MarkdownViewer` widgets.
- Added `--screenshot` option to `textual run`

### Changed

- Breaking change: `TreeNode` can no longer be imported from `textual.widgets`; it is now available via `from textual.widgets.tree import TreeNode`. https://github.com/Textualize/textual/pull/1637
- `Tree` now shows a (subdued) cursor for a highlighted node when focus has moved elsewhere https://github.com/Textualize/textual/issues/1471
- `DataTable.add_row` now accepts `key` argument to uniquely identify the row https://github.com/Textualize/textual/pull/1638
- `DataTable.add_column` now accepts `key` argument to uniquely identify the column https://github.com/Textualize/textual/pull/1638
- `DataTable.add_row` and `DataTable.add_column` now return lists of keys identifying the added rows/columns https://github.com/Textualize/textual/pull/1638
- Breaking change: `DataTable.get_cell_value` renamed to `DataTable.get_value_at` https://github.com/Textualize/textual/pull/1638
- `DataTable.row_count` is now a property https://github.com/Textualize/textual/pull/1638
- Breaking change: `DataTable.cursor_cell` renamed to `DataTable.cursor_coordinate` https://github.com/Textualize/textual/pull/1638
  - The method `validate_cursor_cell` was renamed to `validate_cursor_coordinate`.
  - The method `watch_cursor_cell` was renamed to `watch_cursor_coordinate`.
- Breaking change: `DataTable.hover_cell` renamed to `DataTable.hover_coordinate` https://github.com/Textualize/textual/pull/1638
  - The method `validate_hover_cell` was renamed to `validate_hover_coordinate`.
- Breaking change: `DataTable.data` structure changed, and will be made private in upcoming release https://github.com/Textualize/textual/pull/1638
- Breaking change: `DataTable.refresh_cell` was renamed to `DataTable.refresh_coordinate` https://github.com/Textualize/textual/pull/1638
- Breaking change: `DataTable.get_row_height` now takes a `RowKey` argument instead of a row index https://github.com/Textualize/textual/pull/1638
- Breaking change: `DataTable.data` renamed to `DataTable._data` (it's now private) https://github.com/Textualize/textual/pull/1786
- The `_filter` module was made public (now called `filter`) https://github.com/Textualize/textual/pull/1638
- Breaking change: renamed `Checkbox` to `Switch` https://github.com/Textualize/textual/issues/1746
- `App.install_screen` name is no longer optional https://github.com/Textualize/textual/pull/1778
- `App.query` now only includes the current screen https://github.com/Textualize/textual/pull/1778
- `DOMNode.tree` now displays simple DOM structure only https://github.com/Textualize/textual/pull/1778
- `App.install_screen` now returns None rather than AwaitMount https://github.com/Textualize/textual/pull/1778
- `DOMNode.children` is now a simple sequence, the NodesList is exposed as `DOMNode._nodes` https://github.com/Textualize/textual/pull/1778
- `DataTable` cursor can now enter fixed columns https://github.com/Textualize/textual/pull/1799

### Fixed

- Fixed stuck screen  https://github.com/Textualize/textual/issues/1632
- Fixed programmatic style changes not refreshing children layouts when parent widget did not change size https://github.com/Textualize/textual/issues/1607
- Fixed relative units in `grid-rows` and `grid-columns` being computed with respect to the wrong dimension https://github.com/Textualize/textual/issues/1406
- Fixed bug with animations that were triggered back to back, where the second one wouldn't start https://github.com/Textualize/textual/issues/1372
- Fixed bug with animations that were scheduled where all but the first would be skipped https://github.com/Textualize/textual/issues/1372
- Programmatically setting `overflow_x`/`overflow_y` refreshes the layout correctly https://github.com/Textualize/textual/issues/1616
- Fixed double-paste into `Input` https://github.com/Textualize/textual/issues/1657
- Added a workaround for an apparent Windows Terminal paste issue https://github.com/Textualize/textual/issues/1661
- Fixed issue with renderable width calculation https://github.com/Textualize/textual/issues/1685
- Fixed issue with app not processing Paste event https://github.com/Textualize/textual/issues/1666
- Fixed glitch with view position with auto width inputs https://github.com/Textualize/textual/issues/1693
- Fixed `DataTable` "selected" events containing wrong coordinates when mouse was used https://github.com/Textualize/textual/issues/1723

### Removed

- Methods `MessagePump.emit` and `MessagePump.emit_no_wait` https://github.com/Textualize/textual/pull/1738
- Removed `reactive.watch` in favor of DOMNode.watch.

## [0.10.1] - 2023-01-20

### Added

- Added Strip.text property https://github.com/Textualize/textual/issues/1620

### Fixed

- Fixed `textual diagnose` crash on older supported Python versions. https://github.com/Textualize/textual/issues/1622

### Changed

- The default filename for screenshots uses a datetime format similar to ISO8601, but with reserved characters replaced by underscores https://github.com/Textualize/textual/pull/1518


## [0.10.0] - 2023-01-19

### Added

- Added `TreeNode.parent` -- a read-only property for accessing a node's parent https://github.com/Textualize/textual/issues/1397
- Added public `TreeNode` label access via `TreeNode.label` https://github.com/Textualize/textual/issues/1396
- Added read-only public access to the children of a `TreeNode` via `TreeNode.children` https://github.com/Textualize/textual/issues/1398
- Added `Tree.get_node_by_id` to allow getting a node by its ID https://github.com/Textualize/textual/pull/1535
- Added a `Tree.NodeHighlighted` message, giving a `on_tree_node_highlighted` event handler https://github.com/Textualize/textual/issues/1400
- Added a `inherit_component_classes` subclassing parameter to control whether component classes are inherited from base classes https://github.com/Textualize/textual/issues/1399
- Added `diagnose` as a `textual` command https://github.com/Textualize/textual/issues/1542
- Added `row` and `column` cursors to `DataTable` https://github.com/Textualize/textual/pull/1547
- Added an optional parameter `selector` to the methods `Screen.focus_next` and `Screen.focus_previous` that enable using a CSS selector to narrow down which widgets can get focus https://github.com/Textualize/textual/issues/1196

### Changed

- `MouseScrollUp` and `MouseScrollDown` now inherit from `MouseEvent` and have attached modifier keys. https://github.com/Textualize/textual/pull/1458
- Fail-fast and print pretty tracebacks for Widget compose errors https://github.com/Textualize/textual/pull/1505
- Added Widget._refresh_scroll to avoid expensive layout when scrolling https://github.com/Textualize/textual/pull/1524
- `events.Paste` now bubbles https://github.com/Textualize/textual/issues/1434
- Improved error message when style flag `none` is mixed with other flags (e.g., when setting `text-style`) https://github.com/Textualize/textual/issues/1420
- Clock color in the `Header` widget now matches the header color https://github.com/Textualize/textual/issues/1459
- Programmatic calls to scroll now optionally scroll even if overflow styling says otherwise (introduces a new `force` parameter to all the `scroll_*` methods) https://github.com/Textualize/textual/issues/1201
- `COMPONENT_CLASSES` are now inherited from base classes https://github.com/Textualize/textual/issues/1399
- Watch methods may now take no parameters
- Added `compute` parameter to reactive
- A `TypeError` raised during `compose` now carries the full traceback
- Removed base class `NodeMessage` from which all node-related `Tree` events inherited

### Fixed

- The styles `scrollbar-background-active` and `scrollbar-color-hover` are no longer ignored https://github.com/Textualize/textual/pull/1480
- The widget `Placeholder` can now have its width set to `auto` https://github.com/Textualize/textual/pull/1508
- Behavior of widget `Input` when rendering after programmatic value change and related scenarios https://github.com/Textualize/textual/issues/1477 https://github.com/Textualize/textual/issues/1443
- `DataTable.show_cursor` now correctly allows cursor toggling https://github.com/Textualize/textual/pull/1547
- Fixed cursor not being visible on `DataTable` mount when `fixed_columns` were used https://github.com/Textualize/textual/pull/1547
- Fixed `DataTable` cursors not resetting to origin on `clear()` https://github.com/Textualize/textual/pull/1601
- Fixed TextLog wrapping issue https://github.com/Textualize/textual/issues/1554
- Fixed issue with TextLog not writing anything before layout https://github.com/Textualize/textual/issues/1498
- Fixed an exception when populating a child class of `ListView` purely from `compose` https://github.com/Textualize/textual/issues/1588
- Fixed freeze in tests https://github.com/Textualize/textual/issues/1608
- Fixed minus not displaying as symbol https://github.com/Textualize/textual/issues/1482

## [0.9.1] - 2022-12-30

### Added

- Added textual._win_sleep for Python on Windows < 3.11 https://github.com/Textualize/textual/pull/1457

## [0.9.0] - 2022-12-30

### Added

- Added textual.strip.Strip primitive
- Added textual._cache.FIFOCache
- Added an option to clear columns in DataTable.clear() https://github.com/Textualize/textual/pull/1427

### Changed

- Widget.render_line now returns a Strip
- Fix for slow updates on Windows
- Bumped Rich dependency

## [0.8.2] - 2022-12-28

### Fixed

- Fixed issue with TextLog.clear() https://github.com/Textualize/textual/issues/1447

## [0.8.1] - 2022-12-25

### Fixed

- Fix for overflowing tree issue https://github.com/Textualize/textual/issues/1425

## [0.8.0] - 2022-12-22

### Fixed

- Fixed issues with nested auto dimensions https://github.com/Textualize/textual/issues/1402
- Fixed watch method incorrectly running on first set when value hasn't changed and init=False https://github.com/Textualize/textual/pull/1367
- `App.dark` can now be set from `App.on_load` without an error being raised  https://github.com/Textualize/textual/issues/1369
- Fixed setting `visibility` changes needing a `refresh` https://github.com/Textualize/textual/issues/1355

### Added

- Added `textual.actions.SkipAction` exception which can be raised from an action to allow parents to process bindings.
- Added `textual keys` preview.
- Added ability to bind to a character in addition to key name. i.e. you can bind to "." or "full_stop".
- Added TextLog.shrink attribute to allow renderable to reduce in size to fit width.

### Changed

- Deprecated `PRIORITY_BINDINGS` class variable.
- Renamed `char` to `character` on Key event.
- Renamed `key_name` to `name` on Key event.
- Queries/`walk_children` no longer includes self in results by default https://github.com/Textualize/textual/pull/1416

## [0.7.0] - 2022-12-17

### Added

- Added `PRIORITY_BINDINGS` class variable, which can be used to control if a widget's bindings have priority by default. https://github.com/Textualize/textual/issues/1343

### Changed

- Renamed the `Binding` argument `universal` to `priority`. https://github.com/Textualize/textual/issues/1343
- When looking for bindings that have priority, they are now looked from `App` downwards. https://github.com/Textualize/textual/issues/1343
- `BINDINGS` on an `App`-derived class have priority by default. https://github.com/Textualize/textual/issues/1343
- `BINDINGS` on a `Screen`-derived class have priority by default. https://github.com/Textualize/textual/issues/1343
- Added a message parameter to Widget.exit

### Fixed

- Fixed validator not running on first reactive set https://github.com/Textualize/textual/pull/1359
- Ensure only printable characters are used as key_display https://github.com/Textualize/textual/pull/1361


## [0.6.0] - 2022-12-11

https://textual.textualize.io/blog/2022/12/11/version-060

### Added

- Added "inherited bindings" -- BINDINGS classvar will be merged with base classes, unless inherit_bindings is set to False
- Added `Tree` widget which replaces `TreeControl`.
- Added widget `Placeholder` https://github.com/Textualize/textual/issues/1200.
- Added `ListView` and `ListItem` widgets https://github.com/Textualize/textual/pull/1143

### Changed

- Rebuilt `DirectoryTree` with new `Tree` control.
- Empty containers with a dimension set to `"auto"` will now collapse instead of filling up the available space.
- Container widgets now have default height of `1fr`.
- The default `width` of a `Label` is now `auto`.

### Fixed

- Type selectors can now contain numbers https://github.com/Textualize/textual/issues/1253
- Fixed visibility not affecting children https://github.com/Textualize/textual/issues/1313
- Fixed issue with auto width/height and relative children https://github.com/Textualize/textual/issues/1319
- Fixed issue with offset applied to containers https://github.com/Textualize/textual/issues/1256
- Fixed default CSS retrieval for widgets with no `DEFAULT_CSS` that inherited from widgets with `DEFAULT_CSS` https://github.com/Textualize/textual/issues/1335
- Fixed merging of `BINDINGS` when binding inheritance is set to `None` https://github.com/Textualize/textual/issues/1351

## [0.5.0] - 2022-11-20

### Added

- Add get_child_by_id and get_widget_by_id, remove get_child https://github.com/Textualize/textual/pull/1146
- Add easing parameter to Widget.scroll_* methods https://github.com/Textualize/textual/pull/1144
- Added Widget.call_later which invokes a callback on idle.
- `DOMNode.ancestors` no longer includes `self`.
- Added `DOMNode.ancestors_with_self`, which retains the old behaviour of
  `DOMNode.ancestors`.
- Improved the speed of `DOMQuery.remove`.
- Added DataTable.clear
- Added low-level `textual.walk` methods.
- It is now possible to `await` a `Widget.remove`.
  https://github.com/Textualize/textual/issues/1094
- It is now possible to `await` a `DOMQuery.remove`. Note that this changes
  the return value of `DOMQuery.remove`, which used to return `self`.
  https://github.com/Textualize/textual/issues/1094
- Added Pilot.wait_for_animation
- Added `Widget.move_child` https://github.com/Textualize/textual/issues/1121
- Added a `Label` widget https://github.com/Textualize/textual/issues/1190
- Support lazy-instantiated Screens (callables in App.SCREENS) https://github.com/Textualize/textual/pull/1185
- Display of keys in footer has more sensible defaults https://github.com/Textualize/textual/pull/1213
- Add App.get_key_display, allowing custom key_display App-wide https://github.com/Textualize/textual/pull/1213

### Changed

- Watchers are now called immediately when setting the attribute if they are synchronous. https://github.com/Textualize/textual/pull/1145
- Widget.call_later has been renamed to Widget.call_after_refresh.
- Button variant values are now checked at runtime. https://github.com/Textualize/textual/issues/1189
- Added caching of some properties in Styles object

### Fixed

- Fixed DataTable row not updating after add https://github.com/Textualize/textual/issues/1026
- Fixed issues with animation. Now objects of different types may be animated.
- Fixed containers with transparent background not showing borders https://github.com/Textualize/textual/issues/1175
- Fixed auto-width in horizontal containers https://github.com/Textualize/textual/pull/1155
- Fixed Input cursor invisible when placeholder empty https://github.com/Textualize/textual/pull/1202
- Fixed deadlock when removing widgets from the App https://github.com/Textualize/textual/pull/1219

## [0.4.0] - 2022-11-08

https://textual.textualize.io/blog/2022/11/08/version-040/#version-040

### Changed

- Dropped support for mounting "named" and "anonymous" widgets via
  `App.mount` and `Widget.mount`. Both methods now simply take one or more
  widgets as positional arguments.
- `DOMNode.query_one` now raises a `TooManyMatches` exception if there is
  more than one matching node.
  https://github.com/Textualize/textual/issues/1096
- `App.mount` and `Widget.mount` have new `before` and `after` parameters https://github.com/Textualize/textual/issues/778

### Added

- Added `init` param to reactive.watch
- `CSS_PATH` can now be a list of CSS files https://github.com/Textualize/textual/pull/1079
- Added `DOMQuery.only_one` https://github.com/Textualize/textual/issues/1096
- Writes to stdout are now done in a thread, for smoother animation. https://github.com/Textualize/textual/pull/1104

## [0.3.0] - 2022-10-31

### Fixed

- Fixed issue where scrollbars weren't being unmounted
- Fixed fr units for horizontal and vertical layouts https://github.com/Textualize/textual/pull/1067
- Fixed `textual run` breaking sys.argv https://github.com/Textualize/textual/issues/1064
- Fixed footer not updating styles when toggling dark mode
- Fixed how the app title in a `Header` is centred https://github.com/Textualize/textual/issues/1060
- Fixed the swapping of button variants https://github.com/Textualize/textual/issues/1048
- Fixed reserved characters in screenshots https://github.com/Textualize/textual/issues/993
- Fixed issue with TextLog max_lines https://github.com/Textualize/textual/issues/1058

### Changed

- DOMQuery now raises InvalidQueryFormat in response to invalid query strings, rather than cryptic CSS error
- Dropped quit_after, screenshot, and screenshot_title from App.run, which can all be done via auto_pilot
- Widgets are now closed in reversed DOM order
- Input widget justify hardcoded to left to prevent text-align interference
- Changed `textual run` so that it patches `argv` in more situations
- DOM classes and IDs are now always treated fully case-sensitive https://github.com/Textualize/textual/issues/1047

### Added

- Added Unmount event
- Added App.run_async method
- Added App.run_test context manager
- Added auto_pilot to App.run and App.run_async
- Added Widget._get_virtual_dom to get scrollbars
- Added size parameter to run and run_async
- Added always_update to reactive
- Returned an awaitable from push_screen, switch_screen, and install_screen https://github.com/Textualize/textual/pull/1061

## [0.2.1] - 2022-10-23

### Changed

- Updated meta data for PyPI

## [0.2.0] - 2022-10-23

### Added

- CSS support
- Too numerous to mention
## [0.1.18] - 2022-04-30

### Changed

- Bump typing extensions

## [0.1.17] - 2022-03-10

### Changed

- Bumped Rich dependency

## [0.1.16] - 2022-03-10

### Fixed

- Fixed escape key hanging on Windows

## [0.1.15] - 2022-01-31

### Added

- Added Windows Driver

## [0.1.14] - 2022-01-09

### Changed

- Updated Rich dependency to 11.X

## [0.1.13] - 2022-01-01

### Fixed

- Fixed spurious characters when exiting app
- Fixed increasing delay when exiting

## [0.1.12] - 2021-09-20

### Added

- Added geometry.Spacing

### Fixed

- Fixed calculation of virtual size in scroll views

## [0.1.11] - 2021-09-12

### Changed

- Changed message handlers to use prefix handle\_
- Renamed messages to drop the Message suffix
- Events now bubble by default
- Refactor of layout

### Added

- Added App.measure
- Added auto_width to Vertical Layout, WindowView, an ScrollView
- Added big_table.py example
- Added easing.py example

## [0.1.10] - 2021-08-25

### Added

- Added keyboard control of tree control
- Added Widget.gutter to calculate space between renderable and outside edge
- Added margin, padding, and border attributes to Widget

### Changed

- Callbacks may be async or non-async.
- Event handler event argument is optional.
- Fixed exception in clock example https://github.com/willmcgugan/textual/issues/52
- Added Message.wait() which waits for a message to be processed
- Key events are now sent to widgets first, before processing bindings

## [0.1.9] - 2021-08-06

### Added

- Added hover over and mouse click to activate keys in footer
- Added verbosity argument to Widget.log

### Changed

- Simplified events. Remove Startup event (use Mount)
- Changed geometry.Point to geometry.Offset and geometry.Dimensions to geometry.Size

## [0.1.8] - 2021-07-17

### Fixed

- Fixed exiting mouse mode
- Fixed slow animation

### Added

- New log system

## [0.1.7] - 2021-07-14

### Changed

- Added functionality to calculator example.
- Scrollview now shows scrollbars automatically
- New handler system for messages that doesn't require inheritance
- Improved traceback handling

[6.3.0]: https://github.com/Textualize/textual/compare/v6.2.1...v6.3.0
[6.2.1]: https://github.com/Textualize/textual/compare/v6.2.0...v6.2.1
[6.2.0]: https://github.com/Textualize/textual/compare/v6.1.0...v6.2.0
[6.1.0]: https://github.com/Textualize/textual/compare/v6.0.0...v6.1.0
[6.0.0]: https://github.com/Textualize/textual/compare/v5.3.0...v6.0.0
[5.3.0]: https://github.com/Textualize/textual/compare/v5.2.0...v5.3.0
[5.2.0]: https://github.com/Textualize/textual/compare/v5.1.1...v5.2.0
[5.1.1]: https://github.com/Textualize/textual/compare/v5.1.0...v5.1.1
[5.1.0]: https://github.com/Textualize/textual/compare/v5.0.1...v5.1.0
[5.0.1]: https://github.com/Textualize/textual/compare/v5.0.0...v5.0.1
[5.0.0]: https://github.com/Textualize/textual/compare/v4.1.0...v5.0.0
[4.0.0]: https://github.com/Textualize/textual/compare/v3.7.1...v4.0.0
[3.7.1]: https://github.com/Textualize/textual/compare/v3.7.0...v3.7.1
[3.7.0]: https://github.com/Textualize/textual/compare/v3.6.0...v3.7.0
[3.6.0]: https://github.com/Textualize/textual/compare/v3.5.0...v3.6.0
[3.5.0]: https://github.com/Textualize/textual/compare/v3.4.0...v3.5.0
[3.4.0]: https://github.com/Textualize/textual/compare/v3.3.0...v3.4.0
[3.3.0]: https://github.com/Textualize/textual/compare/v3.2.0...v3.3.0
[3.2.0]: https://github.com/Textualize/textual/compare/v3.1.1...v3.2.0
[3.1.1]: https://github.com/Textualize/textual/compare/v3.1.0...v3.1.1
[3.1.0]: https://github.com/Textualize/textual/compare/v3.0.1...v3.1.0
[3.0.1]: https://github.com/Textualize/textual/compare/v3.0.0...v3.0.1
[3.0.0]: https://github.com/Textualize/textual/compare/v2.1.2...v3.0.0
[2.1.2]: https://github.com/Textualize/textual/compare/v2.1.1...v2.1.2
[2.1.1]: https://github.com/Textualize/textual/compare/v2.1.0...v2.1.1
[2.1.0]: https://github.com/Textualize/textual/compare/v2.0.4...v2.1.0
[2.0.4]: https://github.com/Textualize/textual/compare/v2.0.3...v2.0.4
[2.0.3]: https://github.com/Textualize/textual/compare/v2.0.2...v2.0.3
[2.0.2]: https://github.com/Textualize/textual/compare/v2.0.1...v2.0.2
[2.0.1]: https://github.com/Textualize/textual/compare/v2.0.0...v2.0.1
[2.0.0]: https://github.com/Textualize/textual/compare/v1.0.0...v2.0.0
[1.0.0]: https://github.com/Textualize/textual/compare/v0.89.1...v1.0.0
[0.89.1]: https://github.com/Textualize/textual/compare/v0.89.0...v0.89.1
[0.89.0]: https://github.com/Textualize/textual/compare/v0.88.1...v0.89.0
[0.88.1]: https://github.com/Textualize/textual/compare/v0.88.0...v0.88.1
[0.88.0]: https://github.com/Textualize/textual/compare/v0.87.1...v0.88.0
[0.87.1]: https://github.com/Textualize/textual/compare/v0.87.0...v0.87.1
[0.87.0]: https://github.com/Textualize/textual/compare/v0.86.4...v0.87.0
[0.86.3]: https://github.com/Textualize/textual/compare/v0.86.2...v0.86.3
[0.86.2]: https://github.com/Textualize/textual/compare/v0.86.1...v0.86.2
[0.86.1]: https://github.com/Textualize/textual/compare/v0.86.0...v0.86.1
[0.86.0]: https://github.com/Textualize/textual/compare/v0.85.2...v0.86.0
[0.85.2]: https://github.com/Textualize/textual/compare/v0.85.1...v0.85.2
[0.85.1]: https://github.com/Textualize/textual/compare/v0.85.0...v0.85.1
[0.85.0]: https://github.com/Textualize/textual/compare/v0.84.0...v0.85.0
[0.84.0]: https://github.com/Textualize/textual/compare/v0.83.0...v0.84.0
[0.83.0]: https://github.com/Textualize/textual/compare/v0.82.0...v0.83.0
[0.82.0]: https://github.com/Textualize/textual/compare/v0.81.0...v0.82.0
[0.81.0]: https://github.com/Textualize/textual/compare/v0.80.1...v0.81.0
[0.80.1]: https://github.com/Textualize/textual/compare/v0.80.0...v0.80.1
[0.80.0]: https://github.com/Textualize/textual/compare/v0.79.0...v0.80.0
[0.79.0]: https://github.com/Textualize/textual/compare/v0.78.0...v0.79.0
[0.78.0]: https://github.com/Textualize/textual/compare/v0.77.0...v0.78.0
[0.77.0]: https://github.com/Textualize/textual/compare/v0.76.0...v0.77.0
[0.76.0]: https://github.com/Textualize/textual/compare/v0.75.1...v0.76.0
[0.75.1]: https://github.com/Textualize/textual/compare/v0.75.0...v0.75.1
[0.75.0]: https://github.com/Textualize/textual/compare/v0.74.0...v0.75.0
[0.74.0]: https://github.com/Textualize/textual/compare/v0.73.0...v0.74.0
[0.73.0]: https://github.com/Textualize/textual/compare/v0.72.0...v0.73.0
[0.72.0]: https://github.com/Textualize/textual/compare/v0.71.0...v0.72.0
[0.71.0]: https://github.com/Textualize/textual/compare/v0.70.0...v0.71.0
[0.70.0]: https://github.com/Textualize/textual/compare/v0.69.0...v0.70.0
[0.69.0]: https://github.com/Textualize/textual/compare/v0.68.0...v0.69.0
[0.68.0]: https://github.com/Textualize/textual/compare/v0.67.1...v0.68.0
[0.67.1]: https://github.com/Textualize/textual/compare/v0.67.0...v0.67.1
[0.67.0]: https://github.com/Textualize/textual/compare/v0.66.0...v0.67.0
[0.66.0]: https://github.com/Textualize/textual/compare/v0.65.2...v0.66.0
[0.65.2]: https://github.com/Textualize/textual/compare/v0.65.1...v0.65.2
[0.65.1]: https://github.com/Textualize/textual/compare/v0.65.0...v0.65.1
[0.65.0]: https://github.com/Textualize/textual/compare/v0.64.0...v0.65.0
[0.64.0]: https://github.com/Textualize/textual/compare/v0.63.6...v0.64.0
[0.63.6]: https://github.com/Textualize/textual/compare/v0.63.5...v0.63.6
[0.63.5]: https://github.com/Textualize/textual/compare/v0.63.4...v0.63.5
[0.63.4]: https://github.com/Textualize/textual/compare/v0.63.3...v0.63.4
[0.63.3]: https://github.com/Textualize/textual/compare/v0.63.2...v0.63.3
[0.63.2]: https://github.com/Textualize/textual/compare/v0.63.1...v0.63.2
[0.63.1]: https://github.com/Textualize/textual/compare/v0.63.0...v0.63.1
[0.63.0]: https://github.com/Textualize/textual/compare/v0.62.0...v0.63.0
[0.62.0]: https://github.com/Textualize/textual/compare/v0.61.1...v0.62.0
[0.61.1]: https://github.com/Textualize/textual/compare/v0.61.0...v0.61.1
[0.61.0]: https://github.com/Textualize/textual/compare/v0.60.1...v0.61.0
[0.60.1]: https://github.com/Textualize/textual/compare/v0.60.0...v0.60.1
[0.60.0]: https://github.com/Textualize/textual/compare/v0.59.0...v0.60.0
[0.59.0]: https://github.com/Textualize/textual/compare/v0.58.1...v0.59.0
[0.58.1]: https://github.com/Textualize/textual/compare/v0.58.0...v0.58.1
[0.58.0]: https://github.com/Textualize/textual/compare/v0.57.1...v0.58.0
[0.57.1]: https://github.com/Textualize/textual/compare/v0.57.0...v0.57.1
[0.57.0]: https://github.com/Textualize/textual/compare/v0.56.3...v0.57.0
[0.56.3]: https://github.com/Textualize/textual/compare/v0.56.2...v0.56.3
[0.56.2]: https://github.com/Textualize/textual/compare/v0.56.1...v0.56.2
[0.56.1]: https://github.com/Textualize/textual/compare/v0.56.0...v0.56.1
[0.56.0]: https://github.com/Textualize/textual/compare/v0.55.1...v0.56.0
[0.55.1]: https://github.com/Textualize/textual/compare/v0.55.0...v0.55.1
[0.55.0]: https://github.com/Textualize/textual/compare/v0.54.0...v0.55.0
[0.54.0]: https://github.com/Textualize/textual/compare/v0.53.1...v0.54.0
[0.53.1]: https://github.com/Textualize/textual/compare/v0.53.0...v0.53.1
[0.53.0]: https://github.com/Textualize/textual/compare/v0.52.1...v0.53.0
[0.52.1]: https://github.com/Textualize/textual/compare/v0.52.0...v0.52.1
[0.52.0]: https://github.com/Textualize/textual/compare/v0.51.0...v0.52.0
[0.51.0]: https://github.com/Textualize/textual/compare/v0.50.1...v0.51.0
[0.50.1]: https://github.com/Textualize/textual/compare/v0.50.0...v0.50.1
[0.50.0]: https://github.com/Textualize/textual/compare/v0.49.0...v0.50.0
[0.49.1]: https://github.com/Textualize/textual/compare/v0.49.0...v0.49.1
[0.49.0]: https://github.com/Textualize/textual/compare/v0.48.2...v0.49.0
[0.48.2]: https://github.com/Textualize/textual/compare/v0.48.1...v0.48.2
[0.48.1]: https://github.com/Textualize/textual/compare/v0.48.0...v0.48.1
[0.48.0]: https://github.com/Textualize/textual/compare/v0.47.1...v0.48.0
[0.47.1]: https://github.com/Textualize/textual/compare/v0.47.0...v0.47.1
[0.47.0]: https://github.com/Textualize/textual/compare/v0.46.0...v0.47.0
[0.46.0]: https://github.com/Textualize/textual/compare/v0.45.1...v0.46.0
[0.45.1]: https://github.com/Textualize/textual/compare/v0.45.0...v0.45.1
[0.45.0]: https://github.com/Textualize/textual/compare/v0.44.1...v0.45.0
[0.44.1]: https://github.com/Textualize/textual/compare/v0.44.0...v0.44.1
[0.44.0]: https://github.com/Textualize/textual/compare/v0.43.2...v0.44.0
[0.43.2]: https://github.com/Textualize/textual/compare/v0.43.1...v0.43.2
[0.43.1]: https://github.com/Textualize/textual/compare/v0.43.0...v0.43.1
[0.43.0]: https://github.com/Textualize/textual/compare/v0.42.0...v0.43.0
[0.42.0]: https://github.com/Textualize/textual/compare/v0.41.0...v0.42.0
[0.41.0]: https://github.com/Textualize/textual/compare/v0.40.0...v0.41.0
[0.40.0]: https://github.com/Textualize/textual/compare/v0.39.0...v0.40.0
[0.39.0]: https://github.com/Textualize/textual/compare/v0.38.1...v0.39.0
[0.38.1]: https://github.com/Textualize/textual/compare/v0.38.0...v0.38.1
[0.38.0]: https://github.com/Textualize/textual/compare/v0.37.1...v0.38.0
[0.37.1]: https://github.com/Textualize/textual/compare/v0.37.0...v0.37.1
[0.37.0]: https://github.com/Textualize/textual/compare/v0.36.0...v0.37.0
[0.36.0]: https://github.com/Textualize/textual/compare/v0.35.1...v0.36.0
[0.35.1]: https://github.com/Textualize/textual/compare/v0.35.0...v0.35.1
[0.35.0]: https://github.com/Textualize/textual/compare/v0.34.0...v0.35.0
[0.34.0]: https://github.com/Textualize/textual/compare/v0.33.0...v0.34.0
[0.33.0]: https://github.com/Textualize/textual/compare/v0.32.0...v0.33.0
[0.32.0]: https://github.com/Textualize/textual/compare/v0.31.0...v0.32.0
[0.31.0]: https://github.com/Textualize/textual/compare/v0.30.0...v0.31.0
[0.30.0]: https://github.com/Textualize/textual/compare/v0.29.0...v0.30.0
[0.29.0]: https://github.com/Textualize/textual/compare/v0.28.1...v0.29.0
[0.28.1]: https://github.com/Textualize/textual/compare/v0.28.0...v0.28.1
[0.28.0]: https://github.com/Textualize/textual/compare/v0.27.0...v0.28.0
[0.27.0]: https://github.com/Textualize/textual/compare/v0.26.0...v0.27.0
[0.26.0]: https://github.com/Textualize/textual/compare/v0.25.0...v0.26.0
[0.25.0]: https://github.com/Textualize/textual/compare/v0.24.1...v0.25.0
[0.24.1]: https://github.com/Textualize/textual/compare/v0.24.0...v0.24.1
[0.24.0]: https://github.com/Textualize/textual/compare/v0.23.0...v0.24.0
[0.23.0]: https://github.com/Textualize/textual/compare/v0.22.3...v0.23.0
[0.22.3]: https://github.com/Textualize/textual/compare/v0.22.2...v0.22.3
[0.22.2]: https://github.com/Textualize/textual/compare/v0.22.1...v0.22.2
[0.22.1]: https://github.com/Textualize/textual/compare/v0.22.0...v0.22.1
[0.22.0]: https://github.com/Textualize/textual/compare/v0.21.0...v0.22.0
[0.21.0]: https://github.com/Textualize/textual/compare/v0.20.1...v0.21.0
[0.20.1]: https://github.com/Textualize/textual/compare/v0.20.0...v0.20.1
[0.20.0]: https://github.com/Textualize/textual/compare/v0.19.1...v0.20.0
[0.19.1]: https://github.com/Textualize/textual/compare/v0.19.0...v0.19.1
[0.19.0]: https://github.com/Textualize/textual/compare/v0.18.0...v0.19.0
[0.18.0]: https://github.com/Textualize/textual/compare/v0.17.4...v0.18.0
[0.17.3]: https://github.com/Textualize/textual/compare/v0.17.2...v0.17.3
[0.17.2]: https://github.com/Textualize/textual/compare/v0.17.1...v0.17.2
[0.17.1]: https://github.com/Textualize/textual/compare/v0.17.0...v0.17.1
[0.17.0]: https://github.com/Textualize/textual/compare/v0.16.0...v0.17.0
[0.16.0]: https://github.com/Textualize/textual/compare/v0.15.1...v0.16.0
[0.15.1]: https://github.com/Textualize/textual/compare/v0.15.0...v0.15.1
[0.15.0]: https://github.com/Textualize/textual/compare/v0.14.0...v0.15.0
[0.14.0]: https://github.com/Textualize/textual/compare/v0.13.0...v0.14.0
[0.13.0]: https://github.com/Textualize/textual/compare/v0.12.1...v0.13.0
[0.12.1]: https://github.com/Textualize/textual/compare/v0.12.0...v0.12.1
[0.12.0]: https://github.com/Textualize/textual/compare/v0.11.1...v0.12.0
[0.11.1]: https://github.com/Textualize/textual/compare/v0.11.0...v0.11.1
[0.11.0]: https://github.com/Textualize/textual/compare/v0.10.1...v0.11.0
[0.10.1]: https://github.com/Textualize/textual/compare/v0.10.0...v0.10.1
[0.10.0]: https://github.com/Textualize/textual/compare/v0.9.1...v0.10.0
[0.9.1]: https://github.com/Textualize/textual/compare/v0.9.0...v0.9.1
[0.9.0]: https://github.com/Textualize/textual/compare/v0.8.2...v0.9.0
[0.8.2]: https://github.com/Textualize/textual/compare/v0.8.1...v0.8.2
[0.8.1]: https://github.com/Textualize/textual/compare/v0.8.0...v0.8.1
[0.8.0]: https://github.com/Textualize/textual/compare/v0.7.0...v0.8.0
[0.7.0]: https://github.com/Textualize/textual/compare/v0.6.0...v0.7.0
[0.6.0]: https://github.com/Textualize/textual/compare/v0.5.0...v0.6.0
[0.5.0]: https://github.com/Textualize/textual/compare/v0.4.0...v0.5.0
[0.4.0]: https://github.com/Textualize/textual/compare/v0.3.0...v0.4.0
[0.3.0]: https://github.com/Textualize/textual/compare/v0.2.1...v0.3.0
[0.2.1]: https://github.com/Textualize/textual/compare/v0.2.0...v0.2.1
[0.2.0]: https://github.com/Textualize/textual/compare/v0.1.18...v0.2.0
[0.1.18]: https://github.com/Textualize/textual/compare/v0.1.17...v0.1.18
[0.1.17]: https://github.com/Textualize/textual/compare/v0.1.16...v0.1.17
[0.1.16]: https://github.com/Textualize/textual/compare/v0.1.15...v0.1.16
[0.1.15]: https://github.com/Textualize/textual/compare/v0.1.14...v0.1.15
[0.1.14]: https://github.com/Textualize/textual/compare/v0.1.13...v0.1.14
[0.1.13]: https://github.com/Textualize/textual/compare/v0.1.12...v0.1.13
[0.1.12]: https://github.com/Textualize/textual/compare/v0.1.11...v0.1.12
[0.1.11]: https://github.com/Textualize/textual/compare/v0.1.10...v0.1.11
[0.1.10]: https://github.com/Textualize/textual/compare/v0.1.9...v0.1.10
[0.1.9]: https://github.com/Textualize/textual/compare/v0.1.8...v0.1.9
[0.1.8]: https://github.com/Textualize/textual/compare/v0.1.7...v0.1.8
[0.1.7]: https://github.com/Textualize/textual/releases/tag/v0.1.7


--- CONTRIBUTING.md ---
# Contributing to Textual

First of all, thanks for taking the time to contribute to Textual!

## How can I contribute?

You can contribute to Textual in many ways:

 1. [Report a bug](https://github.com/textualize/textual/issues/new?title=%5BBUG%5D%20short%20bug%20description&template=bug_report.md)
 2. Add a new feature
 3. Fix a bug
 4. Improve the documentation


## Setup

To make a code or documentation contribution you will need to set up Textual locally.
You can follow these steps:

 1. Make sure you have Poetry installed ([see instructions here](https://python-poetry.org))
 2. Clone the Textual repository
 3. Run `poetry shell` to create a virtual environment for the dependencies
 4. Run `make setup` to install all dependencies
 5. Make sure the latest version of Textual was installed by running the command `textual --version`
 6. Install the pre-commit hooks with the command `pre-commit install`

([Read this](#makefile-commands) if the command `make` doesn't work for you.)

## Demo

Once you have Textual installed, run the Textual demo to get an impression of what Textual can do and to double check that everything was installed correctly:

```bash
python -m textual
```

## Guidelines

- Read any issue instructions carefully. Feel free to ask for clarification if any details are missing.

- Add docstrings to all of your code (functions, methods, classes, ...). The codebase should have enough examples for you to copy from.

- Write tests for your code.
  - If you are fixing a bug, make sure to add regression tests that link to the original issue.
  - If you are implementing a visual element, make sure to add _snapshot tests_. [See below](#snapshot-testing) for more details.

## Before opening a PR

Before you open your PR, please go through this checklist and make sure you've checked all the items that apply:

 - [ ] Update the `CHANGELOG.md`
 - [ ] Format your code with black (`make format`)
 - [ ] All your code has docstrings in the style of the rest of the codebase
 - [ ] Your code passes all tests (`make test`)

([Read this](#makefile-commands) if the command `make` doesn't work for you.)

## Updating and building the documentation

If you change the documentation, you will want to build the documentation to make sure everything looks like it should.
The command `make docs-serve-offline` should start a server that will let you preview the documentation locally and that should reload whenever you save changes to the documentation or the code files.

([Read this](#makefile-commands) if the command `make` doesn't work for you.)

We strive to write our documentation in a clear and accessible way so, if you find any issues with the documentation, we encourage you to open an issue where you can enumerate the things you think should be changed or added.

Opening an issue or a discussion is typically better than opening a PR directly.
That's because there are many subjective considerations that go into writing documentation and we cannot expect you, a well-intentioned external contributor, to be aware of those subjective considerations that we take into account when writing our documentation.

Of course, this does not apply to objective/technical issues with the documentation like bugs or broken links.

## After opening a PR

When you open a PR, your code will be reviewed by one of the Textual maintainers.
In that review process,

- We will take a look at all of the changes you are making
- We might ask for clarifications (why did you do X or Y?)
- We might ask for more tests/more documentation
- We might ask for some code changes

The sole purpose of those interactions is to make sure that, in the long run, everyone has the best experience possible with Textual and with the feature you are implementing/fixing.

Don't be discouraged if a reviewer asks for code changes.
If you go through our history of pull requests, you will see that every single one of the maintainers has had to make changes following a review.

## Snapshot testing

Snapshot tests ensure that visual things (like widgets) look like they are supposed to.
PR [#1969](https://github.com/Textualize/textual/pull/1969) is a good example of what adding snapshot tests looks like: it amounts to a change in the file `tests/snapshot_tests/test_snapshots.py` that should run an app that you write and compare it against a historic snapshot of what that app should look like.

When you create a new snapshot test, run it with `pytest -vv tests/snapshot_tests/test_snapshots.py`.
Because you just created this snapshot test, there is no history to compare against and the test will fail.
After running the snapshot tests, you should see a link that opens an interface in your browser.
This interface should show all failing snapshot tests and a side-by-side diff between what the app looked like when the test ran versus the historic snapshot.

Make sure your snapshot app looks like it is supposed to and that you didn't break any other snapshot tests.
If everything looks fine, you can run `make test-snapshot-update` to update the snapshot history with your new snapshot.
This will write a new SVG file to the `tests/snapshot_tests/__snapshots__/` directory.
You should NOT modify these files by hand.
If a pre-existing snapshot tests fails, you should carefully inspect the diff and decide if the new snapshot is correct or if the pre-existing one is.
If the new snapshot is correct, you should update the snapshot history with your new snapshot using `make test-snapshot-update`.
If the pre-existing snapshot is correct, your change has likely introduced a bug, and you should try to fix it.
After fixing it, and checking the output of `make test-snapshot` now looks correct, you should run `make test-snapshot-update` to update the snapshot history with your new snapshot.


([Read this](#makefile-commands) if the command `make` doesn't work for you.)

## Join the community

Seems a little overwhelming?
Join our community on [Discord](https://discord.gg/Enf6Z3qhVr) to get help!

## Makefile commands

Textual has a `Makefile` file that contains the most common commands used when developing Textual.
([Read about Make and makefiles on Wikipedia.](https://en.wikipedia.org/wiki/Make_(software)))
If you don't have Make and you're on Windows, you may want to [install Make](https://stackoverflow.com/q/32127524/2828287).


--- CODE_OF_CONDUCT.md ---
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
[will@textualize.io](mailto:will@textualize.io).
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.

--- README.md ---


[![Discord](https://img.shields.io/discord/1026214085173461072)](https://discord.gg/Enf6Z3qhVr)
[![Supported Python Versions](https://img.shields.io/pypi/pyversions/textual)](https://pypi.org/project/textual/)
[![PyPI version](https://badge.fury.io/py/textual.svg?)](https://badge.fury.io/py/textual)
![OS support](https://img.shields.io/badge/OS-macOS%20Linux%20Windows-red)



![textual-splash](https://github.com/user-attachments/assets/4caeb77e-48c0-4cf7-b14d-c53ded855ffd)

# Textual

<img align="right" width="250" alt="clock" src="https://github.com/user-attachments/assets/63e839c3-5b8e-478d-b78e-cf7647eb85e8" />

Build cross-platform user interfaces with a simple Python API. Run your apps in the terminal *or* a web browser.

Textual's API combines modern Python with the best of developments from the web world, for a lean app development experience.
De-coupled components and an advanced [testing](https://textual.textualize.io/guide/testing/) framework ensure you can maintain your app for the long-term.

Want some more examples? See the [examples](https://github.com/Textualize/textual/tree/main/examples) directory.

```python
"""
An App to show the current time.
"""

from datetime import datetime

from textual.app import App, ComposeResult
from textual.widgets import Digits


class ClockApp(App):
    CSS = """
    Screen { align: center middle; }
    Digits { width: auto; }
    """

    def compose(self) -> ComposeResult:
        yield Digits("")

    def on_ready(self) -> None:
        self.update_clock()
        self.set_interval(1, self.update_clock)

    def update_clock(self) -> None:
        clock = datetime.now().time()
        self.query_one(Digits).update(f"{clock:%T}")


if __name__ == "__main__":
    app = ClockApp()
    app.run()
```

> [!TIP]
> Textual is an asynchronous framework under the hood. Which means you can integrate your apps with async libraries &mdash; if you want to.
> If you don't want or need to use async, Textual won't force it on you. 



<img src="https://img.spacergif.org/spacer.gif" width="1" height="64"/>

## Widgets

Textual's library of [widgets](https://textual.textualize.io/widget_gallery/) covers everything from buttons, tree controls, data tables, inputs, text areas, and more…
Combined with a flexible [layout](https://textual.textualize.io/how-to/design-a-layout/) system, you can realize any User Interface you need.

Predefined themes ensure your apps will look good out of the box. 


<table>

<tr>

  <td>
    
  ![buttons](https://github.com/user-attachments/assets/2ac26387-aaa3-41ed-bc00-7d488600343c)
    
  </td>

  <td>
    
![tree](https://github.com/user-attachments/assets/61ccd6e9-97ea-4918-8eda-3ee0f0d3770e)
    
  </td>
  
</tr>


<tr>

  <td>
    
  ![datatables](https://github.com/user-attachments/assets/3e1f9f7a-f965-4901-a114-3c188bd17695)
    
  </td>

  <td>
    
![inputs](https://github.com/user-attachments/assets/b02aa203-7c37-42da-a1bb-2cb244b7d0d3)
    
  </td>
  
</tr>
<tr>

<td>

![listview](https://github.com/user-attachments/assets/963603bc-aa07-4688-bd24-379962ece871)

</td>

<td>

![textarea](https://github.com/user-attachments/assets/cd4ba787-5519-40e2-8d86-8224e1b7e506)
  
</td>

  
</tr>

</table>


<img src="https://img.spacergif.org/spacer.gif" width="1" height="32"/>

## Installing

Install Textual via pip:

```
pip install textual textual-dev
```

See [getting started](https://textual.textualize.io/getting_started/) for details.


<img src="https://img.spacergif.org/spacer.gif" width="1" height="32"/>

## Demo


Run the following command to see a little of what Textual can do:

```
python -m textual
```

Or try the [textual demo](https://github.com/textualize/textual-demo) *without* installing (requires [uv](https://docs.astral.sh/uv/)):

```bash
uvx --python 3.12 textual-demo
```

<img src="https://img.spacergif.org/spacer.gif" width="1" height="32"/>

## Dev Console

<img align="right" width="40%" alt="devtools" src="https://github.com/user-attachments/assets/12c60d65-e342-4b2f-9372-bae0459a7552" />


How do you debug an app in the terminal that is also running in the terminal?

The `textual-dev` package supplies a dev console that connects to your application from another terminal.
In addition to system messages and events, your logged messages and print statements will appear in the dev console.

See [the guide](https://textual.textualize.io/guide/devtools/) for other helpful tools provided by the `textual-dev` package.

<img src="https://img.spacergif.org/spacer.gif" width="1" height="32"/>

## Command Palette


Textual apps have a *fuzzy search* command palette.
Hit `ctrl+p` to open the command palette.

It is easy to extend the command palette with [custom commands](https://textual.textualize.io/guide/command_palette/) for your application.


![Command Palette](https://github.com/user-attachments/assets/94d8ec5d-b668-4033-a5cb-bf820e1b8d60)

<img src="https://img.spacergif.org/spacer.gif" width="1" height="32"/>

# Textual ❤️ Web

<img align="right" width="40%" alt="textual-serve" src="https://github.com/user-attachments/assets/a25820fb-87ae-433a-858b-ac3940169242">


Textual apps are equally at home in the browser as they are the terminal. Any Textual app may be served with `textual serve` &mdash; so you can share your creations on the web.
Here's how to serve the demo app:

```
textual serve "python -m textual"
```

In addition to serving your apps locally, you can serve apps with [Textual Web](https://github.com/Textualize/textual-web).

Textual Web's firewall-busting technology can serve an unlimited number of applications.

Since Textual apps have low system requirements, you can install them anywhere Python also runs. Turning any device into a connected device.
No desktop required!


<img src="https://img.spacergif.org/spacer.gif" width="1" height="32"/>


## Join us on Discord

Join the Textual developers and community on our [Discord Server](https://discord.gg/Enf6Z3qhVr).


--- .github/PULL_REQUEST_TEMPLATE.md ---

**Please review the following checklist.**

- [ ] Docstrings on all new or modified functions / classes 
- [ ] Updated documentation
- [ ] Updated CHANGELOG.md (where appropriate)


--- .github/ISSUE_TEMPLATE/bug_report.md ---
---
name: Report a bug
about: Report a crash or something not working as described in the docs
title: ''
labels: ''
assignees: ''

---

Have you checked closed issues? (https://github.com/Textualize/textual/issues?q=is%3Aissue+is%3Aclosed)

Have you checked against the most recent version of Textual? (https://pypi.org/search/?q=textual)

## Consider discussions!

Issues are for actionable items only.
If Textual crashes or behaves differently from the docs, then submit an issue.
If you want to know how to do something, or you have a suggestion for a new feature, then open a discussion (https://github.com/Textualize/textual/discussions/).

For realtime help, join our discord server (https://discord.gg/Enf6Z3qhVr)

## The bug

Please give a brief but clear explanation of the issue.
If you can, include a complete working example that demonstrates the bug. **Check it can run without modifications.**

It will be helpful if you run the following command and paste the results:

```
textual diagnose
```

If you don't have the `textual` command on your path, you may have forgotten to install the `textual-dev` package.

Feel free to add screenshots and / or videos. These can be very helpful!


--- notes/README.md ---
# Developer notes

These are notes made by the developer, and _not_ to be considered documentation.


--- notes/refresh.md ---
# Refresh system

This note describes how Textual updates widgets on-screen.

if widget has made some changes and wishes to update visuals it can call Widget.refresh. There are two flags on this method; `repaint` which will repaint just the widget, and `layout` which will re-layout the screen. A layout must be done if the widget has changed size / position / visibility. Otherwise repaint will refresh just the widget area.

A refresh won't happen immediately when `refresh()` is called, rather it sets internal flags. The `on_idle` method of Widget checks these flags. This is so that multiple changes made to the UI while processing events don't cause excessive repainting of the screen (which makes the UI slow and jumpy).

In the case of a repaint. The Widget.on_idle handler will emit (send to the parent) an UpdateMessage. This message will be handled by the parent view, which will update the widget (a particular part of the screen).

In the case of a layout. The Widget.on_idle handler will emit a LayoutMessage. This message will be handled by the parent view, which calls refresh_layout on the root view, which will layout and repaint the entire screen.


--- notes/snapshot_testing.md ---
# Snapshot Testing


## What is snapshot testing?

Some tests that run for Textual are snapshot tests.
When you first run a snapshot test, a screenshot of an app is taken and saved to disk.
Next time you run it, another screenshot is taken and compared with the original one.

If the screenshots don't match, it means something has changed.
It's up to you to tell the test system whether that change is expected or not.

This allows us to easily catch regressions in how Textual outputs to the terminal.

Snapshot tests run alongside normal unit tests.

## How do I write a snapshot test?

1. Inject the `snap_compare` fixture into your test.
2. Pass in the path to the file which contains the Textual app.

```python
def test_grid_layout_basic_overflow(snap_compare):
    assert snap_compare("docs/examples/guide/layout/grid_layout2.py")
```

`snap_compare` can take additional arguments such as `press`, which allows
you to simulate key presses etc.
See the signature of `snap_compare` for more info.

## A snapshot test failed, what do I do?

When a snapshot test fails, a report will be created on your machine, and you
can use this report to visually compare the output from your test with the historical output for that test.

This report will be visible at the bottom of the terminal after the `pytest` session completes,
or, if running in CI, it will be available as an artifact attached to the GitHub Actions summary.

If you're happy that the new output of the app is correct, you can run `pytest` with the
`--snapshot-update` flag. This flag will update the snapshots for any test that is executed in the run,
so to update a snapshot for a single test, run only that test.

With your snapshot on disk updated to match the new output, running the test again should result in a pass.


--- questions/README.md ---

# Questions

Your questions should go in this directory.

Question files should be named with the extension ".question.md".

To build the FAQ, install [faqtory](https://github.com/willmcgugan/faqtory) if you haven't already.
Faqtory is best installed via [pipx](https://github.com/pypa/pipx) to avoid any dependency conflicts:

```
pipx install faqtory
```

Then run the following from the top of the repository:

```
faqtory build
```


--- questions/align-center-middle.question.md ---
---
title: "How do I center a widget in a screen?"
alt_titles:
  - "centre a widget"
  - "centre widgets"
  - "center a control"
  - "centre a control"
  - "center controls"
  - "centre controls"
---

!!! tip

    See [*How To Center Things*](https://textual.textualize.io/how-to/center-things/) in the
    Textual documentation for a more comprehensive answer to this question.

To center a widget within a container use
[`align`](https://textual.textualize.io/styles/align/). But remember that
`align` works on the *children* of a container, it isn't something you use
on the child you want centered.

For example, here's an app that shows a `Button` in the middle of a
`Screen`:

```python
from textual.app import App, ComposeResult
from textual.widgets import Button

class ButtonApp(App):

    CSS = """
    Screen {
        align: center middle;
    }
    """

    def compose(self) -> ComposeResult:
        yield Button("PUSH ME!")

if __name__ == "__main__":
    ButtonApp().run()
```

If you use the above on multiple widgets, you'll find they appear to
"left-align" in the center of the screen, like this:

```
+-----+
|     |
+-----+

+---------+
|         |
+---------+

+---------------+
|               |
+---------------+
```

If you want them more like this:

```
     +-----+
     |     |
     +-----+

   +---------+
   |         |
   +---------+

+---------------+
|               |
+---------------+
```

The best approach is to wrap each widget in a [`Center`
container](https://textual.textualize.io/api/containers/#textual.containers.Center)
that individually centers it. For example:

```python
from textual.app import App, ComposeResult
from textual.containers import Center
from textual.widgets import Button

class ButtonApp(App):

    CSS = """
    Screen {
        align: center middle;
    }
    """

    def compose(self) -> ComposeResult:
        yield Center(Button("PUSH ME!"))
        yield Center(Button("AND ME!"))
        yield Center(Button("ALSO PLEASE PUSH ME!"))
        yield Center(Button("HEY ME ALSO!!"))

if __name__ == "__main__":
    ButtonApp().run()
```


--- questions/compose-result.question.md ---
---
title: "How can I fix ImportError cannot import name ComposeResult from textual.app ?"
alt_titles:
  - "Can't import ComposeResult"
  - "Error about missing ComposeResult from textual.app"
---

You likely have an older version of Textual. You can install the latest version by adding the `-U` switch which will force pip to upgrade.

The following should do it:

```
pip install textual-dev -U
```


--- questions/copy-text.question.md ---
---
title: "How can I select and copy text in a Textual app?"
alt_titles:
  - "Can't copy text"
  - "Highlighting and copy text not working"
---

Textual supports text selection for most widgets, via click and drag. Press ctrl+c to copy.

For widgets that don't yet support text selection, you can try and use your terminal's builtin support.
Most terminal emulators offer a modifier key which you can hold while you click and drag to restore the behavior you
may expect from the command line. The exact modifier key depends on the terminal and platform you are running on.

- **iTerm** Hold the OPTION key.
- **Gnome Terminal** Hold the SHIFT key.
- **Windows Terminal** Hold the SHIFT key.

Refer to the documentation for your terminal emulator, if it is not listed above.


--- questions/images.question.md ---
---
title: "Does Textual support images?"
alt_titles:
  - "Can Textual display PNG / SVG files?"
  - "Render images"
---

Textual doesn't have built-in support for images yet, but it is on the [Roadmap](https://textual.textualize.io/roadmap/).

See also the [rich-pixels](https://github.com/darrenburns/rich-pixels) project for a Rich renderable for images that works with Textual.


--- questions/pass-args-to-app.question.md ---
---
title: "How do I pass arguments to an app?"
alt_titles:
  - "pass arguments to an application"
  - "pass parameters to an app"
  - "pass parameters to an application"
---

When creating your `App` class, override `__init__` as you would when
inheriting normally. For example:

```python
from textual.app import App, ComposeResult
from textual.widgets import Static

class Greetings(App[None]):

    def __init__(self, greeting: str="Hello", to_greet: str="World") -> None:
        self.greeting = greeting
        self.to_greet = to_greet
        super().__init__()

    def compose(self) -> ComposeResult:
        yield Static(f"{self.greeting}, {self.to_greet}")
```

Then the app can be run, passing in various arguments; for example:

```python
# Running with default arguments.
Greetings().run()

# Running with a keyword argument.
Greetings(to_greet="davep").run()

# Running with both positional arguments.
Greetings("Well hello", "there").run()
```


--- questions/transparent-background.question.md ---
---
title: "How can I set a translucent app background?"
alt_titles:
  - "Transparent background not working"
  - "Translucent background not working"
  - "Can't see desktop underneath terminal"
---

Some terminal emulators have a translucent background feature which allows the desktop underneath to be partially visible.

This feature is unlikely to work with Textual, as the translucency effect requires the use of ANSI background colors, which Textual doesn't use.
Textual uses 16.7 million colors where available which enables consistent colors across all platforms and additional effects which aren't possible with ANSI colors.

For more information on ANSI colors in Textual, see [Why no ANSI Themes?](#why-doesnt-textual-support-ansi-themes).


--- questions/why-do-some-keys-not-make-it-to-my-app.question.md ---
---
title: "Why do some key combinations never make it to my app?"
alt_titles:
  - "Cmd key isn't working"
  - "Command key isn't working"
  - "Alt key isn't working"
  - "Ctrl and function key isn't working"
  - "Control and function key isn't working"
---

Textual can only ever support key combinations that are passed on by your
terminal application. Which keys get passed on can differ from terminal to
terminal, and from operating system to operating system.

Because of this it's best to stick to key combinations that are known to be
universally-supported; these include the likes of:

- Letters
- Numbers
- Numbered function keys (especially F1 through F10)
- Space
- Return
- Arrow, home, end and page keys
- Control
- Shift

When [creating bindings for your
application](https://textual.textualize.io/guide/input/#bindings) we
recommend picking keys and key combinations from the above.

Keys that aren't normally passed through by terminals include Cmd and Option
on macOS, and the Windows key on Windows.

If you need to test what [key
combinations](https://textual.textualize.io/guide/input/#keyboard-input)
work in different environments you can try them out with `textual keys`.


--- questions/why-looks-bad-on-macos.question.md ---
---
title: "Why doesn't Textual look good on macOS?"
alt_titles:
  - "looks bad on macOS"
  - "dashed lines on macOS"
  - "broken borders on macOS"
  - "pale colors on macOS"
  - "pale colours on macOS"
  - "mac terminal"
  - "macOS terminal"
---

You may find that the default macOS Terminal.app doesn't render Textual apps (and likely other TUIs) very well, particularly when it comes to box characters.
For instance, you may find it displays misaligned blocks and lines like this:

<img width="1042" alt="Screenshot 2023-06-19 at 10 43 02" src="https://github.com/Textualize/textual/assets/554369/e61f3876-3dd1-4ac8-b380-22922c89c7d6">

You can (mostly) fix this by opening settings -> profiles > Text tab, and changing the font settings.
We have found that Menlo Regular font, with a character spacing of 1 and line spacing of 0.805 produces reasonable results.
If you want to use another font, you may have to tweak the line spacing until you get good results.

<img width="737" alt="Screenshot 2023-06-19 at 10 44 00" src="https://github.com/Textualize/textual/assets/554369/0a052a93-b1fd-4327-9d33-d954b51a9ad2">

With these changes, Textual apps render more as intended:

<img width="1042" alt="Screenshot 2023-06-19 at 10 43 23" src="https://github.com/Textualize/textual/assets/554369/a0c4aa05-c509-4ac1-b0b8-e68ce4433f70">

Even with this *fix*, Terminal.app has a few limitations.
It is limited to 256 colors, and can be a little slow compared to more modern alternatives.
Fortunately there are a number of free terminal emulators for macOS which produces high quality results.

We recommend any of the following terminals:

- [iTerm2](https://iterm2.com/)
- [Kitty](https://sw.kovidgoyal.net/kitty/)
- [WezTerm](https://wezfurlong.org/wezterm/)

### Terminal.app colors

<img width="762" alt="Screenshot 2023-06-19 at 11 00 12" src="https://github.com/Textualize/textual/assets/554369/e0555d23-e141-4069-b318-f3965c880208">

### iTerm2 colors

<img width="1002" alt="Screenshot 2023-06-19 at 11 00 25" src="https://github.com/Textualize/textual/assets/554369/9a8cde57-5121-49a7-a2e0-5f6fc871b7a6">


--- questions/why-no-ansi-themes.question.md ---
---
title: "Why doesn't Textual support ANSI themes?"
alt_titles:
  - "Textual should use system terminal colors for cyan, etc"
  - "ANSI theme colors not working"
---

Textual will not generate escape sequences for the 16 themeable *ANSI* colors.

This is an intentional design decision we took for the following reasons:

- Not everyone has a carefully chosen ANSI color theme. Color combinations which may look fine on your system, may be unreadable on another machine. There is very little an app author or Textual can do to resolve this. Asking users to simply pick a better theme is not a good solution, since not all users will know how.
- ANSI colors can't be manipulated in the way Textual can do with other colors. Textual can blend colors and produce light and dark shades from an original color, which is used to create more readable text and user interfaces. Color blending will also be used to power future accessibility features.

Textual has a design system which guarantees apps will be readable on all platforms and terminals, and produces better results than ANSI colors.

There is currently a light and dark version of the design system, but more are planned. It will also be possible for users to customize the source colors on a per-app or per-system basis. This means that in the future you will be able to modify the core colors to blend in with your chosen terminal theme.

!!! tip "Changed in version 0.80.0"

    Textual added an `ansi_color` boolean to App. If you set this to `True`, then Textual will not attempt to convert ANSI colors. Note that you will lose transparency effects if you enable this setting.


--- src/textual/actions.py ---
from __future__ import annotations

import ast
import re
from functools import lru_cache
from typing import Any

from typing_extensions import TypeAlias

ActionParseResult: TypeAlias = "tuple[str, str, tuple[object, ...]]"
"""An action is its name and the arbitrary tuple of its arguments."""


class SkipAction(Exception):
    """Raise in an action to skip the action (and allow any parent bindings to run)."""


class ActionError(Exception):
    pass


re_action_args = re.compile(r"([\w\.]+)\((.*)\)")


@lru_cache(maxsize=1024)
def parse(action: str) -> ActionParseResult:
    """Parses an action string.

    Args:
        action: String containing action.

    Raises:
        ActionError: If the action has invalid syntax.

    Returns:
        Action name and arguments.
    """
    args_match = re_action_args.match(action)
    if args_match is not None:
        action_name, action_args_str = args_match.groups()
        if action_args_str:
            try:
                # We wrap `action_args_str` to be able to disambiguate the cases where
                # the list of arguments is a comma-separated list of values from the
                # case where the argument is a single tuple.
                action_args: tuple[Any, ...] = ast.literal_eval(f"({action_args_str},)")
            except Exception:
                raise ActionError(
                    f"unable to parse {action_args_str!r} in action {action!r}"
                )
        else:
            action_args = ()
    else:
        action_name = action
        action_args = ()

    namespace, _, action_name = action_name.rpartition(".")

    return namespace, action_name, action_args


--- src/textual/_animator.py ---
from __future__ import annotations

import asyncio
from abc import ABC, abstractmethod
from dataclasses import dataclass
from functools import partial
from typing import TYPE_CHECKING, Any, Callable, TypeVar

from typing_extensions import Protocol, runtime_checkable

from textual import _time
from textual._callback import invoke
from textual._compat import cached_property
from textual._easing import DEFAULT_EASING, EASING
from textual._types import AnimationLevel, CallbackType
from textual.timer import Timer

if TYPE_CHECKING:
    from textual.app import App

    AnimationKey = tuple[int, str]
    """Animation keys are the id of the object and the attribute being animated."""

EasingFunction = Callable[[float], float]
"""Signature for a function that parametrizes animation speed.

An easing function must map the interval [0, 1] into the interval [0, 1].
"""


class AnimationError(Exception):
    """An issue prevented animation from starting."""


ReturnType = TypeVar("ReturnType")


@runtime_checkable
class Animatable(Protocol):
    """Protocol for objects that can have their intrinsic values animated.

    For example, the transition between two colors can be animated
    because the class [`Color`][textual.color.Color.blend] satisfies this protocol.
    """

    def blend(
        self: ReturnType, destination: ReturnType, factor: float
    ) -> ReturnType:  # pragma: no cover
        ...


class Animation(ABC):
    on_complete: CallbackType | None = None
    """Callback to run after animation completes"""

    @abstractmethod
    def __call__(
        self,
        time: float,
        app_animation_level: AnimationLevel = "full",
    ) -> bool:  # pragma: no cover
        """Call the animation, return a boolean indicating whether animation is in-progress or complete.

        Args:
            time: The current timestamp

        Returns:
            True if the animation has finished, otherwise False.
        """
        raise NotImplementedError("")

    async def invoke_callback(self) -> None:
        """Calls the [`on_complete`][Animation.on_complete] callback if one is provided."""
        if self.on_complete is not None:
            await invoke(self.on_complete)

    @abstractmethod
    async def stop(self, complete: bool = True) -> None:
        """Stop the animation.

        Args:
            complete: Flag to say if the animation should be taken to completion.
        """
        raise NotImplementedError

    def __eq__(self, other: object) -> bool:
        return False


@dataclass
class SimpleAnimation(Animation):
    obj: object
    attribute: str
    start_time: float
    duration: float
    start_value: float | Animatable
    end_value: float | Animatable
    final_value: object
    easing: EasingFunction
    on_complete: CallbackType | None = None
    level: AnimationLevel = "full"
    """Minimum level required for the animation to take place (inclusive)."""

    def __call__(
        self, time: float, app_animation_level: AnimationLevel = "full"
    ) -> bool:
        if (
            self.duration == 0
            or app_animation_level == "none"
            or app_animation_level == "basic"
            and self.level == "full"
        ):
            setattr(self.obj, self.attribute, self.final_value)
            return True

        factor = min(1.0, (time - self.start_time) / self.duration)
        eased_factor = self.easing(factor)

        if factor == 1.0:
            value = self.final_value
        elif isinstance(self.start_value, Animatable):
            assert isinstance(
                self.end_value, Animatable
            ), "end_value must be animatable"
            value = self.start_value.blend(self.end_value, eased_factor)
        else:
            assert isinstance(
                self.start_value, (int, float)
            ), f"`start_value` must be float, not {self.start_value!r}"
            assert isinstance(
                self.end_value, (int, float)
            ), f"`end_value` must be float, not {self.end_value!r}"

            if self.end_value > self.start_value:
                eased_factor = self.easing(factor)
                value = (
                    self.start_value
                    + (self.end_value - self.start_value) * eased_factor
                )
            else:
                eased_factor = 1 - self.easing(factor)
                value = (
                    self.end_value + (self.start_value - self.end_value) * eased_factor
                )
        setattr(self.obj, self.attribute, value)
        return factor >= 1

    async def stop(self, complete: bool = True) -> None:
        """Stop the animation.

        Args:
            complete: Flag to say if the animation should be taken to completion.

        Note:
            [`on_complete`][Animation.on_complete] will be called regardless
            of the value provided for `complete`.
        """
        if complete:
            setattr(self.obj, self.attribute, self.end_value)
        await self.invoke_callback()

    def __eq__(self, other: object) -> bool:
        if isinstance(other, SimpleAnimation):
            return (
                self.final_value == other.final_value
                and self.duration == other.duration
            )
        return False


class BoundAnimator:
    def __init__(self, animator: Animator, obj: object) -> None:
        self._animator = animator
        self._obj = obj

    def __call__(
        self,
        attribute: str,
        value: str | float | Animatable,
        *,
        final_value: object = ...,
        duration: float | None = None,
        speed: float | None = None,
        delay: float = 0.0,
        easing: EasingFunction | str = DEFAULT_EASING,
        on_complete: CallbackType | None = None,
        level: AnimationLevel = "full",
    ) -> None:
        """Animate an attribute.

        Args:
            attribute: Name of the attribute to animate.
            value: The value to animate to.
            final_value: The final value of the animation. Defaults to `value` if not set.
            duration: The duration (in seconds) of the animation.
            speed: The speed of the animation.
            delay: A delay (in seconds) before the animation starts.
            easing: An easing method.
            on_complete: A callable to invoke when the animation is finished.
            level: Minimum level required for the animation to take place (inclusive).
        """
        start_value = getattr(self._obj, attribute)
        if isinstance(value, str) and hasattr(start_value, "parse"):
            # Color and Scalar have a parse method
            # I'm exploiting a coincidence here, but I think this should be a first-class concept
            # TODO: add a `Parsable` protocol
            value = start_value.parse(value)
        easing_function = EASING[easing] if isinstance(easing, str) else easing
        return self._animator.animate(
            self._obj,
            attribute=attribute,
            value=value,
            final_value=final_value,
            duration=duration,
            speed=speed,
            delay=delay,
            easing=easing_function,
            on_complete=on_complete,
            level=level,
        )


class Animator:
    """An object to manage updates to a given attribute over a period of time."""

    def __init__(self, app: App, frames_per_second: int = 60) -> None:
        """Initialise the animator object.

        Args:
            app: The application that owns the animator.
            frames_per_second: The number of frames/second to run the animation at.
        """
        self._animations: dict[AnimationKey, Animation] = {}
        """Dictionary that maps animation keys to the corresponding animation instances."""
        self._scheduled: dict[AnimationKey, Timer] = {}
        """Dictionary of scheduled animations, comprising of their keys and the timer objects."""
        self.app = app
        """The app that owns the animator object."""
        self._timer = Timer(
            app,
            1 / frames_per_second,
            name="Animator",
            callback=self,
            pause=True,
        )

    @cached_property
    def _idle_event(self) -> asyncio.Event:
        """The timer that runs the animator."""
        return asyncio.Event()

    @cached_property
    def _complete_event(self) -> asyncio.Event:
        """Flag if no animations are currently taking place."""
        return asyncio.Event()

    async def start(self) -> None:
        """Start the animator task."""
        self._idle_event.set()
        self._complete_event.set()
        self._timer._start()

    async def stop(self) -> None:
        """Stop the animator task."""
        try:
            self._timer.stop()
        except asyncio.CancelledError:
            pass
        finally:
            self._idle_event.set()
            self._complete_event.set()

    def bind(self, obj: object) -> BoundAnimator:
        """Bind the animator to a given object.

        Args:
            obj: The object to bind to.

        Returns:
            The bound animator.
        """
        return BoundAnimator(self, obj)

    def is_being_animated(self, obj: object, attribute: str) -> bool:
        """Does the object/attribute pair have an ongoing or scheduled animation?

        Args:
            obj: An object to check for.
            attribute: The attribute on the object to test for.

        Returns:
            `True` if that attribute is being animated for that object, `False` if not.
        """
        key = (id(obj), attribute)
        return key in self._animations or key in self._scheduled

    def animate(
        self,
        obj: object,
        attribute: str,
        value: Any,
        *,
        final_value: object = ...,
        duration: float | None = None,
        speed: float | None = None,
        easing: EasingFunction | str = DEFAULT_EASING,
        delay: float = 0.0,
        on_complete: CallbackType | None = None,
        level: AnimationLevel = "full",
    ) -> None:
        """Animate an attribute to a new value.

        Args:
            obj: The object containing the attribute.
            attribute: The name of the attribute.
            value: The destination value of the attribute.
            final_value: The final value, or ellipsis if it is the same as ``value``.
            duration: The duration of the animation, or ``None`` to use speed.
            speed: The speed of the animation.
            easing: An easing function.
            delay: Number of seconds to delay the start of the animation by.
            on_complete: Callback to run after the animation completes.
            level: Minimum level required for the animation to take place (inclusive).
        """
        self._record_animation(attribute)
        animate_callback = partial(
            self._animate,
            obj,
            attribute,
            value,
            final_value=final_value,
            duration=duration,
            speed=speed,
            easing=easing,
            on_complete=on_complete,
            level=level,
        )
        if delay:
            self._complete_event.clear()
            self._scheduled[(id(obj), attribute)] = self.app.set_timer(
                delay, animate_callback
            )
        else:
            animate_callback()

    def _record_animation(self, attribute: str) -> None:
        """Called when an attribute is to be animated.

        Args:
            attribute: Attribute being animated.
        """

    def _animate(
        self,
        obj: object,
        attribute: str,
        value: Any,
        *,
        final_value: object = ...,
        duration: float | None = None,
        speed: float | None = None,
        easing: EasingFunction | str = DEFAULT_EASING,
        on_complete: CallbackType | None = None,
        level: AnimationLevel = "full",
    ) -> None:
        """Animate an attribute to a new value.

        Args:
            obj: The object containing the attribute.
            attribute: The name of the attribute.
            value: The destination value of the attribute.
            final_value: The final value, or ellipsis if it is the same as ``value``.
            duration: The duration of the animation, or ``None`` to use speed.
            speed: The speed of the animation.
            easing: An easing function.
            on_complete: Callback to run after the animation completes.
            level: Minimum level required for the animation to take place (inclusive).
        """
        if not hasattr(obj, attribute):
            raise AttributeError(
                f"Can't animate attribute {attribute!r} on {obj!r}; attribute does not exist"
            )
        assert (duration is not None and speed is None) or (
            duration is None and speed is not None
        ), "An Animation should have a duration OR a speed"

        # If an animation is already scheduled for this attribute, unschedule it.
        animation_key = (id(obj), attribute)
        try:
            del self._scheduled[animation_key]
        except KeyError:
            pass

        if final_value is ...:
            final_value = value

        start_time = self._get_time()
        easing_function = EASING[easing] if isinstance(easing, str) else easing
        animation: Animation | None = None

        if hasattr(obj, "__textual_animation__"):
            animation = getattr(obj, "__textual_animation__")(
                attribute,
                getattr(obj, attribute),
                value,
                start_time,
                duration=duration,
                speed=speed,
                easing=easing_function,
                on_complete=on_complete,
                level=level,
            )

        if animation is None:
            if not isinstance(value, (int, float)) and not isinstance(
                value, Animatable
            ):
                raise AnimationError(
                    f"Don't know how to animate {value!r}; "
                    "Can only animate <int>, <float>, or objects with a blend method"
                )

            start_value = getattr(obj, attribute)

            if start_value == value:
                self._animations.pop(animation_key, None)
                return

            if duration is not None:
                animation_duration = duration
            else:
                if hasattr(value, "get_distance_to"):
                    animation_duration = value.get_distance_to(start_value) / (
                        speed or 50
                    )
                else:
                    animation_duration = abs(value - start_value) / (speed or 50)

            animation = SimpleAnimation(
                obj,
                attribute=attribute,
                start_time=start_time,
                duration=animation_duration,
                start_value=start_value,
                end_value=value,
                final_value=final_value,
                easing=easing_function,
                on_complete=(
                    partial(self.app.call_later, on_complete)
                    if on_complete is not None
                    else None
                ),
                level=level,
            )

        assert animation is not None, "animation expected to be non-None"

        current_animation = self._animations.get(animation_key)
        if current_animation is not None and current_animation == animation:
            return

        self._animations[animation_key] = animation
        self._timer.resume()
        self._idle_event.clear()
        self._complete_event.clear()

    async def _stop_scheduled_animation(
        self, key: AnimationKey, complete: bool
    ) -> None:
        """Stop a scheduled animation.

        Args:
            key: The key for the animation to stop.
            complete: Should the animation be moved to its completed state?
        """
        # First off, pull the timer out of the schedule and stop it; it
        # won't be needed.
        try:
            schedule = self._scheduled.pop(key)
        except KeyError:
            return
        schedule.stop()
        # If we've been asked to complete (there's no point in making the
        # animation only to then do nothing with it), and if there was a
        # callback (there will be, but this just keeps type checkers happy
        # really)...
        if complete and schedule._callback is not None:
            # ...invoke it to get the animator created and in the running
            # animations. Yes, this does mean that a stopped scheduled
            # animation will start running early...
            await invoke(schedule._callback)
            # ...but only so we can call on it to run right to the very end
            # right away.
            await self._stop_running_animation(key, complete)

    async def _stop_running_animation(self, key: AnimationKey, complete: bool) -> None:
        """Stop a running animation.

        Args:
            key: The key for the animation to stop.
            complete: Should the animation be moved to its completed state?
        """
        try:
            animation = self._animations.pop(key)
        except KeyError:
            return
        await animation.stop(complete)

    async def stop_animation(
        self, obj: object, attribute: str, complete: bool = True
    ) -> None:
        """Stop an animation on an attribute.

        Args:
            obj: The object containing the attribute.
            attribute: The name of the attribute.
            complete: Should the animation be set to its final value?

        Note:
            If there is no animation scheduled or running, this is a no-op.
        """
        key = (id(obj), attribute)
        if key in self._scheduled:
            await self._stop_scheduled_animation(key, complete)
        elif key in self._animations:
            await self._stop_running_animation(key, complete)

    def force_stop_animation(self, obj: object, attribute: str) -> None:
        """Force stop an animation on an attribute. This will immediately stop the animation,
        without running any associated callbacks, setting the attribute to its final value.

        Args:
            obj: The object containing the attribute.
            attribute: The name of the attribute.

        Note:
            If there is no animation scheduled or running, this is a no-op.
        """
        from textual.css.scalar_animation import ScalarAnimation

        animation_key = (id(obj), attribute)
        try:
            animation = self._animations.pop(animation_key)
        except KeyError:
            return

        if isinstance(animation, SimpleAnimation):
            setattr(obj, attribute, animation.end_value)
        elif isinstance(animation, ScalarAnimation):
            setattr(obj, attribute, animation.final_value)

        if animation.on_complete is not None:
            animation.on_complete()

    def __call__(self) -> None:
        if not self._animations:
            self._timer.pause()
            self._idle_event.set()
            if not self._scheduled:
                self._complete_event.set()
        else:
            app_animation_level = self.app.animation_level
            animation_time = self._get_time()
            animation_keys = list(self._animations.keys())
            for animation_key in animation_keys:
                animation = self._animations[animation_key]
                animation_complete = animation(animation_time, app_animation_level)
                if animation_complete:
                    del self._animations[animation_key]
                    if animation.on_complete is not None:
                        animation.on_complete()

    def _get_time(self) -> float:
        """Get the current wall clock time, via the internal Timer.

        Returns:
            The wall clock time.
        """
        # N.B. We could remove this method and always call `self._timer.get_time()` internally,
        # but it's handy to have in mocking situations.
        return _time.get_time()

    async def wait_for_idle(self) -> None:
        """Wait for any animations to complete."""
        await self._idle_event.wait()

    async def wait_until_complete(self) -> None:
        """Wait for any current and scheduled animations to complete."""
        await self._complete_event.wait()


--- src/textual/_ansi_sequences.py ---
from __future__ import annotations

from typing import Mapping, Tuple

from typing_extensions import Final

from textual.keys import Keys


class IgnoredSequence:
    """Class used to mark that a sequence should be ignored."""


IGNORE_SEQUENCE: Final[IgnoredSequence] = IgnoredSequence()
"""Constant to indicate that a sequence should be ignored."""


# Mapping of vt100 escape codes to Keys.
ANSI_SEQUENCES_KEYS: Mapping[str, Tuple[Keys, ...] | str | IgnoredSequence] = {
    # Control keys.
    " ": (Keys.Space,),
    "\r": (Keys.Enter,),
    "\x00": (Keys.ControlAt,),  # Control-At (Also for Ctrl-Space)
    "\x01": (Keys.ControlA,),  # Control-A (home)
    "\x02": (Keys.ControlB,),  # Control-B (emacs cursor left)
    "\x03": (Keys.ControlC,),  # Control-C (interrupt)
    "\x04": (Keys.ControlD,),  # Control-D (exit)
    "\x05": (Keys.ControlE,),  # Control-E (end)
    "\x06": (Keys.ControlF,),  # Control-F (cursor forward)
    "\x07": (Keys.ControlG,),  # Control-G
    "\x08": (Keys.Backspace,),  # Control-H (8) (Identical to '\b')
    "\x09": (Keys.Tab,),  # Control-I (9) (Identical to '\t')
    "\x0a": (Keys.ControlJ,),  # Control-J (10) (Identical to '\n')
    "\x0b": (Keys.ControlK,),  # Control-K (delete until end of line; vertical tab)
    "\x0c": (Keys.ControlL,),  # Control-L (clear; form feed)
    # "\x0d": (Keys.ControlM,),  # Control-M (13) (Identical to '\r')
    "\x0e": (Keys.ControlN,),  # Control-N (14) (history forward)
    "\x0f": (Keys.ControlO,),  # Control-O (15)
    "\x10": (Keys.ControlP,),  # Control-P (16) (history back)
    "\x11": (Keys.ControlQ,),  # Control-Q
    "\x12": (Keys.ControlR,),  # Control-R (18) (reverse search)
    "\x13": (Keys.ControlS,),  # Control-S (19) (forward search)
    "\x14": (Keys.ControlT,),  # Control-T
    "\x15": (Keys.ControlU,),  # Control-U
    "\x16": (Keys.ControlV,),  # Control-V
    "\x17": (Keys.ControlW,),  # Control-W
    "\x18": (Keys.ControlX,),  # Control-X
    "\x19": (Keys.ControlY,),  # Control-Y (25)
    "\x1a": (Keys.ControlZ,),  # Control-Z
    "\x1b": (Keys.Escape,),  # Also Control-[
    "\x1b\x1b": (
        Keys.Escape,
    ),  # Windows issues esc esc for a single press of escape key
    "\x9b": (Keys.ShiftEscape,),
    "\x1c": (Keys.ControlBackslash,),  # Both Control-\ (also Ctrl-| )
    "\x1d": (Keys.ControlSquareClose,),  # Control-]
    "\x1e": (Keys.ControlCircumflex,),  # Control-^
    "\x1f": (Keys.ControlUnderscore,),  # Control-underscore (Also for Ctrl-hyphen.)
    # ASCII Delete (0x7f)
    # Vt220 (and Linux terminal) send this when pressing backspace. We map this
    # to ControlH, because that will make it easier to create key bindings that
    # work everywhere, with the trade-off that it's no longer possible to
    # handle backspace and control-h individually for the few terminals that
    # support it. (Most terminals send ControlH when backspace is pressed.)
    # See: http://www.ibb.net/~anne/keyboard.html
    "\x7f": (Keys.Backspace,),
    "\x1b\x7f": (Keys.ControlW,),
    # Various
    "\x1b[1~": (Keys.Home,),  # tmux
    "\x1b[2~": (Keys.Insert,),
    "\x1b[3~": (Keys.Delete,),
    "\x1b[4~": (Keys.End,),  # tmux
    "\x1b[5~": (Keys.PageUp,),
    "\x1b[6~": (Keys.PageDown,),
    "\x1b[7~": (Keys.Home,),  # xrvt
    "\x1b[8~": (Keys.End,),  # xrvt
    "\x1b[Z": (Keys.BackTab,),  # shift + tab
    "\x1b\x09": (Keys.BackTab,),  # Linux console
    "\x1b[~": (Keys.BackTab,),  # Windows console
    # --
    # Function keys.
    "\x1bOP": (Keys.F1,),
    "\x1bOQ": (Keys.F2,),
    "\x1bOR": (Keys.F3,),
    "\x1bOS": (Keys.F4,),
    "\x1b[[A": (Keys.F1,),  # Linux console.
    "\x1b[[B": (Keys.F2,),  # Linux console.
    "\x1b[[C": (Keys.F3,),  # Linux console.
    "\x1b[[D": (Keys.F4,),  # Linux console.
    "\x1b[[E": (Keys.F5,),  # Linux console.
    "\x1b[11~": (Keys.F1,),  # rxvt-unicode
    "\x1b[12~": (Keys.F2,),  # rxvt-unicode
    "\x1b[13~": (Keys.F3,),  # rxvt-unicode
    "\x1b[14~": (Keys.F4,),  # rxvt-unicode
    "\x1b[15~": (Keys.F5,),
    "\x1b[17~": (Keys.F6,),
    "\x1b[18~": (Keys.F7,),
    "\x1b[19~": (Keys.F8,),
    "\x1b[20~": (Keys.F9,),
    "\x1b[21~": (Keys.F10,),
    "\x1b[23~": (Keys.F11,),
    "\x1b[24~": (Keys.F12,),
    "\x1b[25~": (Keys.F13,),
    "\x1b[26~": (Keys.F14,),
    "\x1b[28~": (Keys.F15,),
    "\x1b[29~": (Keys.F16,),
    "\x1b[31~": (Keys.F17,),
    "\x1b[32~": (Keys.F18,),
    "\x1b[33~": (Keys.F19,),
    "\x1b[34~": (Keys.F20,),
    # Xterm
    "\x1b[1;2P": (Keys.F13,),
    "\x1b[1;2Q": (Keys.F14,),
    "\x1b[1;2R": (
        Keys.F15,
    ),  # Conflicts with CPR response; enabled after https://github.com/Textualize/textual/issues/3440.
    "\x1b[1;2S": (Keys.F16,),
    "\x1b[15;2~": (Keys.F17,),
    "\x1b[17;2~": (Keys.F18,),
    "\x1b[18;2~": (Keys.F19,),
    "\x1b[19;2~": (Keys.F20,),
    "\x1b[20;2~": (Keys.F21,),
    "\x1b[21;2~": (Keys.F22,),
    "\x1b[23;2~": (Keys.F23,),
    "\x1b[24;2~": (Keys.F24,),
    "\x1b[23$": (Keys.F23,),  # rxvt
    "\x1b[24$": (Keys.F24,),  # rxvt
    # --
    # Control + function keys.
    "\x1b[1;5P": (Keys.ControlF1,),
    "\x1b[1;5Q": (Keys.ControlF2,),
    "\x1b[1;5R": (
        Keys.ControlF3,
    ),  # Conflicts with CPR response; enabled after https://github.com/Textualize/textual/issues/3440.
    "\x1b[1;5S": (Keys.ControlF4,),
    "\x1b[15;5~": (Keys.ControlF5,),
    "\x1b[17;5~": (Keys.ControlF6,),
    "\x1b[18;5~": (Keys.ControlF7,),
    "\x1b[19;5~": (Keys.ControlF8,),
    "\x1b[20;5~": (Keys.ControlF9,),
    "\x1b[21;5~": (Keys.ControlF10,),
    "\x1b[23;5~": (Keys.ControlF11,),
    "\x1b[24;5~": (Keys.ControlF12,),
    "\x1b[1;6P": (Keys.ControlF13,),
    "\x1b[1;6Q": (Keys.ControlF14,),
    "\x1b[1;6R": (
        Keys.ControlF15,
    ),  # Conflicts with CPR response; enabled after https://github.com/Textualize/textual/issues/3440.
    "\x1b[1;6S": (Keys.ControlF16,),
    "\x1b[15;6~": (Keys.ControlF17,),
    "\x1b[17;6~": (Keys.ControlF18,),
    "\x1b[18;6~": (Keys.ControlF19,),
    "\x1b[19;6~": (Keys.ControlF20,),
    "\x1b[20;6~": (Keys.ControlF21,),
    "\x1b[21;6~": (Keys.ControlF22,),
    "\x1b[23;6~": (Keys.ControlF23,),
    "\x1b[24;6~": (Keys.ControlF24,),
    # rxvt-unicode control function keys:
    "\x1b[11^": (Keys.ControlF1,),
    "\x1b[12^": (Keys.ControlF2,),
    "\x1b[13^": (Keys.ControlF3,),
    "\x1b[14^": (Keys.ControlF4,),
    "\x1b[15^": (Keys.ControlF5,),
    "\x1b[17^": (Keys.ControlF6,),
    "\x1b[18^": (Keys.ControlF7,),
    "\x1b[19^": (Keys.ControlF8,),
    "\x1b[20^": (Keys.ControlF9,),
    "\x1b[21^": (Keys.ControlF10,),
    "\x1b[23^": (Keys.ControlF11,),
    "\x1b[24^": (Keys.ControlF12,),
    # rxvt-unicode control+shift function keys:
    "\x1b[25^": (Keys.ControlF13,),
    "\x1b[26^": (Keys.ControlF14,),
    "\x1b[28^": (Keys.ControlF15,),
    "\x1b[29^": (Keys.ControlF16,),
    "\x1b[31^": (Keys.ControlF17,),
    "\x1b[32^": (Keys.ControlF18,),
    "\x1b[33^": (Keys.ControlF19,),
    "\x1b[34^": (Keys.ControlF20,),
    "\x1b[23@": (Keys.ControlF21,),
    "\x1b[24@": (Keys.ControlF22,),
    # --
    # Tmux (Win32 subsystem) sends the following scroll events.
    "\x1b[62~": (Keys.ScrollUp,),
    "\x1b[63~": (Keys.ScrollDown,),
    # Meta/control/escape + pageup/pagedown/insert/delete.
    "\x1b[3;2~": (Keys.ShiftDelete,),  # xterm, gnome-terminal.
    "\x1b[3$": (Keys.ShiftDelete,),  # rxvt
    "\x1b[5;2~": (Keys.ShiftPageUp,),
    "\x1b[6;2~": (Keys.ShiftPageDown,),
    "\x1b[2;3~": (Keys.Escape, Keys.Insert),
    "\x1b[3;3~": (Keys.Escape, Keys.Delete),
    "\x1b[5;3~": (Keys.Escape, Keys.PageUp),
    "\x1b[6;3~": (Keys.Escape, Keys.PageDown),
    "\x1b[2;4~": (Keys.Escape, Keys.ShiftInsert),
    "\x1b[3;4~": (Keys.Escape, Keys.ShiftDelete),
    "\x1b[5;4~": (Keys.Escape, Keys.ShiftPageUp),
    "\x1b[6;4~": (Keys.Escape, Keys.ShiftPageDown),
    "\x1b[3;5~": (Keys.ControlDelete,),  # xterm, gnome-terminal.
    "\x1b[3^": (Keys.ControlDelete,),  # rxvt
    "\x1b[5;5~": (Keys.ControlPageUp,),
    "\x1b[6;5~": (Keys.ControlPageDown,),
    "\x1b[5^": (Keys.ControlPageUp,),  # rxvt
    "\x1b[6^": (Keys.ControlPageDown,),  # rxvt
    "\x1b[3;6~": (Keys.ControlShiftDelete,),
    "\x1b[5;6~": (Keys.ControlShiftPageUp,),
    "\x1b[6;6~": (Keys.ControlShiftPageDown,),
    "\x1b[2;7~": (Keys.Escape, Keys.ControlInsert),
    "\x1b[5;7~": (Keys.Escape, Keys.ControlPageDown),
    "\x1b[6;7~": (Keys.Escape, Keys.ControlPageDown),
    "\x1b[2;8~": (Keys.Escape, Keys.ControlShiftInsert),
    "\x1b[5;8~": (Keys.Escape, Keys.ControlShiftPageDown),
    "\x1b[6;8~": (Keys.Escape, Keys.ControlShiftPageDown),
    # --
    # Arrows.
    # (Normal cursor mode).
    "\x1b[A": (Keys.Up,),
    "\x1b[B": (Keys.Down,),
    "\x1b[C": (Keys.Right,),
    "\x1b[D": (Keys.Left,),
    "\x1b[H": (Keys.Home,),
    "\x1b[F": (Keys.End,),
    # Tmux sends following keystrokes when control+arrow is pressed, but for
    # Emacs ansi-term sends the same sequences for normal arrow keys. Consider
    # it a normal arrow press, because that's more important.
    # (Application cursor mode).
    "\x1bOA": (Keys.Up,),
    "\x1bOB": (Keys.Down,),
    "\x1bOC": (Keys.Right,),
    "\x1bOD": (Keys.Left,),
    "\x1bOF": (Keys.End,),
    "\x1bOH": (Keys.Home,),
    # Shift + arrows.
    "\x1b[1;2A": (Keys.ShiftUp,),
    "\x1b[1;2B": (Keys.ShiftDown,),
    "\x1b[1;2C": (Keys.ShiftRight,),
    "\x1b[1;2D": (Keys.ShiftLeft,),
    "\x1b[1;2F": (Keys.ShiftEnd,),
    "\x1b[1;2H": (Keys.ShiftHome,),
    # Shift+navigation in rxvt
    "\x1b[a": (Keys.ShiftUp,),
    "\x1b[b": (Keys.ShiftDown,),
    "\x1b[c": (Keys.ShiftRight,),
    "\x1b[d": (Keys.ShiftLeft,),
    "\x1b[7$": (Keys.ShiftHome,),
    "\x1b[8$": (Keys.ShiftEnd,),
    # Meta + arrow keys. Several terminals handle this differently.
    # The following sequences are for xterm and gnome-terminal.
    #     (Iterm sends ESC followed by the normal arrow_up/down/left/right
    #     sequences, and the OSX Terminal sends ESCb and ESCf for "alt
    #     arrow_left" and "alt arrow_right." We don't handle these
    #     explicitly, in here, because would could not distinguish between
    #     pressing ESC (to go to Vi navigation mode), followed by just the
    #     'b' or 'f' key. These combinations are handled in
    #     the input processor.)
    "\x1b[1;3A": (Keys.Escape, Keys.Up),
    "\x1b[1;3B": (Keys.Escape, Keys.Down),
    "\x1b[1;3C": (Keys.Escape, Keys.Right),
    "\x1b[1;3D": (Keys.Escape, Keys.Left),
    "\x1b[1;3F": (Keys.Escape, Keys.End),
    "\x1b[1;3H": (Keys.Escape, Keys.Home),
    # Alt+shift+number.
    "\x1b[1;4A": (Keys.Escape, Keys.ShiftUp),
    "\x1b[1;4B": (Keys.Escape, Keys.ShiftDown),
    "\x1b[1;4C": (Keys.Escape, Keys.ShiftRight),
    "\x1b[1;4D": (Keys.Escape, Keys.ShiftLeft),
    "\x1b[1;4F": (Keys.Escape, Keys.ShiftEnd),
    "\x1b[1;4H": (Keys.Escape, Keys.ShiftHome),
    # Control + arrows.
    "\x1b[1;5A": (Keys.ControlUp,),  # Cursor Mode
    "\x1b[1;5B": (Keys.ControlDown,),  # Cursor Mode
    "\x1b[1;5C": (Keys.ControlRight,),  # Cursor Mode
    "\x1b[1;5D": (Keys.ControlLeft,),  # Cursor Mode
    "\x1bf": (Keys.ControlRight,),  # iTerm natural editing keys
    "\x1bb": (Keys.ControlLeft,),  # iTerm natural editing keys
    "\x1b[1;5F": (Keys.ControlEnd,),
    "\x1b[1;5H": (Keys.ControlHome,),
    # rxvt
    "\x1b[7^": (Keys.ControlEnd,),
    "\x1b[8^": (Keys.ControlHome,),
    # Tmux sends following keystrokes when control+arrow is pressed, but for
    # Emacs ansi-term sends the same sequences for normal arrow keys. Consider
    # it a normal arrow press, because that's more important.
    "\x1b[5A": (Keys.ControlUp,),
    "\x1b[5B": (Keys.ControlDown,),
    "\x1b[5C": (Keys.ControlRight,),
    "\x1b[5D": (Keys.ControlLeft,),
    # Control arrow keys in rxvt
    "\x1bOa": (Keys.ControlUp,),
    "\x1bOb": (Keys.ControlUp,),
    "\x1bOc": (Keys.ControlRight,),
    "\x1bOd": (Keys.ControlLeft,),
    # Control + shift + arrows.
    "\x1b[1;6A": (Keys.ControlShiftUp,),
    "\x1b[1;6B": (Keys.ControlShiftDown,),
    "\x1b[1;6C": (Keys.ControlShiftRight,),
    "\x1b[1;6D": (Keys.ControlShiftLeft,),
    "\x1b[1;6F": (Keys.ControlShiftEnd,),
    "\x1b[1;6H": (Keys.ControlShiftHome,),
    # Control + Meta + arrows.
    "\x1b[1;7A": (Keys.Escape, Keys.ControlUp),
    "\x1b[1;7B": (Keys.Escape, Keys.ControlDown),
    "\x1b[1;7C": (Keys.Escape, Keys.ControlRight),
    "\x1b[1;7D": (Keys.Escape, Keys.ControlLeft),
    "\x1b[1;7F": (Keys.Escape, Keys.ControlEnd),
    "\x1b[1;7H": (Keys.Escape, Keys.ControlHome),
    # Meta + Shift + arrows.
    "\x1b[1;8A": (Keys.Escape, Keys.ControlShiftUp),
    "\x1b[1;8B": (Keys.Escape, Keys.ControlShiftDown),
    "\x1b[1;8C": (Keys.Escape, Keys.ControlShiftRight),
    "\x1b[1;8D": (Keys.Escape, Keys.ControlShiftLeft),
    "\x1b[1;8F": (Keys.Escape, Keys.ControlShiftEnd),
    "\x1b[1;8H": (Keys.Escape, Keys.ControlShiftHome),
    # Meta + arrow on (some?) Macs when using iTerm defaults (see issue #483).
    "\x1b[1;9A": (Keys.Escape, Keys.Up),
    "\x1b[1;9B": (Keys.Escape, Keys.Down),
    "\x1b[1;9C": (Keys.Escape, Keys.Right),
    "\x1b[1;9D": (Keys.Escape, Keys.Left),
    # --
    # Control/shift/meta + number in mintty.
    # (c-2 will actually send c-@ and c-6 will send c-^.)
    "\x1b[1;5p": (Keys.Control0,),
    "\x1b[1;5q": (Keys.Control1,),
    "\x1b[1;5r": (Keys.Control2,),
    "\x1b[1;5s": (Keys.Control3,),
    "\x1b[1;5t": (Keys.Control4,),
    "\x1b[1;5u": (Keys.Control5,),
    "\x1b[1;5v": (Keys.Control6,),
    "\x1b[1;5w": (Keys.Control7,),
    "\x1b[1;5x": (Keys.Control8,),
    "\x1b[1;5y": (Keys.Control9,),
    "\x1b[1;6p": (Keys.ControlShift0,),
    "\x1b[1;6q": (Keys.ControlShift1,),
    "\x1b[1;6r": (Keys.ControlShift2,),
    "\x1b[1;6s": (Keys.ControlShift3,),
    "\x1b[1;6t": (Keys.ControlShift4,),
    "\x1b[1;6u": (Keys.ControlShift5,),
    "\x1b[1;6v": (Keys.ControlShift6,),
    "\x1b[1;6w": (Keys.ControlShift7,),
    "\x1b[1;6x": (Keys.ControlShift8,),
    "\x1b[1;6y": (Keys.ControlShift9,),
    "\x1b[1;7p": (Keys.Escape, Keys.Control0),
    "\x1b[1;7q": (Keys.Escape, Keys.Control1),
    "\x1b[1;7r": (Keys.Escape, Keys.Control2),
    "\x1b[1;7s": (Keys.Escape, Keys.Control3),
    "\x1b[1;7t": (Keys.Escape, Keys.Control4),
    "\x1b[1;7u": (Keys.Escape, Keys.Control5),
    "\x1b[1;7v": (Keys.Escape, Keys.Control6),
    "\x1b[1;7w": (Keys.Escape, Keys.Control7),
    "\x1b[1;7x": (Keys.Escape, Keys.Control8),
    "\x1b[1;7y": (Keys.Escape, Keys.Control9),
    "\x1b[1;8p": (Keys.Escape, Keys.ControlShift0),
    "\x1b[1;8q": (Keys.Escape, Keys.ControlShift1),
    "\x1b[1;8r": (Keys.Escape, Keys.ControlShift2),
    "\x1b[1;8s": (Keys.Escape, Keys.ControlShift3),
    "\x1b[1;8t": (Keys.Escape, Keys.ControlShift4),
    "\x1b[1;8u": (Keys.Escape, Keys.ControlShift5),
    "\x1b[1;8v": (Keys.Escape, Keys.ControlShift6),
    "\x1b[1;8w": (Keys.Escape, Keys.ControlShift7),
    "\x1b[1;8x": (Keys.Escape, Keys.ControlShift8),
    "\x1b[1;8y": (Keys.Escape, Keys.ControlShift9),
    # Simplify some sequences that appear to be unique to rxvt; see
    # https://github.com/Textualize/textual/issues/3741 for context.
    "\x1bOj": "*",
    "\x1bOk": "+",
    "\x1bOm": "-",
    "\x1bOn": ".",
    "\x1bOo": "/",
    "\x1bOp": "0",
    "\x1bOq": "1",
    "\x1bOr": "2",
    "\x1bOs": "3",
    "\x1bOt": "4",
    "\x1bOu": "5",
    "\x1bOv": "6",
    "\x1bOw": "7",
    "\x1bOx": "8",
    "\x1bOy": "9",
    "\x1bOM": (Keys.Enter,),
    # WezTerm on macOS emits sequences for Opt and keys on the top numeric
    # row; whereas other terminals provide various characters. The following
    # swallow up those sequences and turns them into characters the same as
    # the other terminals.
    "\x1b§": "§",
    "\x1b1": "¡",
    "\x1b2": "™",
    "\x1b3": "£",
    "\x1b4": "¢",
    "\x1b5": "∞",
    "\x1b6": "§",
    "\x1b7": "¶",
    "\x1b8": "•",
    "\x1b9": "ª",
    "\x1b0": "º",
    "\x1b-": "–",
    "\x1b=": "≠",
    # Ctrl+§ on kitty is different from most other terminals on macOS.
    "\x1b[167;5u": "0",
    ############################################################################
    # The ignore section. Only add sequences here if they are going to be
    # ignored. Also, when adding a sequence here, please include a note as
    # to why it is being ignored; ideally citing sources if possible.
    ############################################################################
    # The following 2 are inherited from prompt toolkit. They relate to a
    # press of 5 on the numeric keypad, when *not* in number mode.
    "\x1b[E": IGNORE_SEQUENCE,  # Xterm.
    "\x1b[G": IGNORE_SEQUENCE,  # Linux console.
    # Various ctrl+cmd+ keys under Kitty on macOS.
    "\x1b[3;13~": IGNORE_SEQUENCE,  # ctrl-cmd-del
    "\x1b[1;13H": IGNORE_SEQUENCE,  # ctrl-cmd-home
    "\x1b[1;13F": IGNORE_SEQUENCE,  # ctrl-cmd-end
    "\x1b[5;13~": IGNORE_SEQUENCE,  # ctrl-cmd-pgup
    "\x1b[6;13~": IGNORE_SEQUENCE,  # ctrl-cmd-pgdn
    "\x1b[49;13u": IGNORE_SEQUENCE,  # ctrl-cmd-1
    "\x1b[50;13u": IGNORE_SEQUENCE,  # ctrl-cmd-2
    "\x1b[51;13u": IGNORE_SEQUENCE,  # ctrl-cmd-3
    "\x1b[52;13u": IGNORE_SEQUENCE,  # ctrl-cmd-4
    "\x1b[53;13u": IGNORE_SEQUENCE,  # ctrl-cmd-5
    "\x1b[54;13u": IGNORE_SEQUENCE,  # ctrl-cmd-6
    "\x1b[55;13u": IGNORE_SEQUENCE,  # ctrl-cmd-7
    "\x1b[56;13u": IGNORE_SEQUENCE,  # ctrl-cmd-8
    "\x1b[57;13u": IGNORE_SEQUENCE,  # ctrl-cmd-9
    "\x1b[48;13u": IGNORE_SEQUENCE,  # ctrl-cmd-0
    "\x1b[45;13u": IGNORE_SEQUENCE,  # ctrl-cmd--
    "\x1b[61;13u": IGNORE_SEQUENCE,  # ctrl-cmd-+
    "\x1b[91;13u": IGNORE_SEQUENCE,  # ctrl-cmd-[
    "\x1b[93;13u": IGNORE_SEQUENCE,  # ctrl-cmd-]
    "\x1b[92;13u": IGNORE_SEQUENCE,  # ctrl-cmd-\
    "\x1b[39;13u": IGNORE_SEQUENCE,  # ctrl-cmd-'
    "\x1b[59;13u": IGNORE_SEQUENCE,  # ctrl-cmd-;
    "\x1b[47;13u": IGNORE_SEQUENCE,  # ctrl-cmd-/
    "\x1b[46;13u": IGNORE_SEQUENCE,  # ctrl-cmd-.
}

# https://gist.github.com/christianparpart/d8a62cc1ab659194337d73e399004036
SYNC_START = "\x1b[?2026h"
SYNC_END = "\x1b[?2026l"


--- src/textual/_ansi_theme.py ---
from __future__ import annotations

from rich.terminal_theme import TerminalTheme


def rgb(red: int, green: int, blue: int) -> tuple[int, int, int]:
    """Define an RGB color.

    This exists mainly so that a VSCode extension can render the colors inline.

    Args:
        red: Red component.
        green: Green component.
        blue: Blue component.

    Returns:
        Color triplet.
    """
    return red, green, blue


MONOKAI = TerminalTheme(
    rgb(12, 12, 12),
    rgb(217, 217, 217),
    [
        rgb(26, 26, 26),
        rgb(244, 0, 95),
        rgb(152, 224, 36),
        rgb(253, 151, 31),
        rgb(157, 101, 255),
        rgb(244, 0, 95),
        rgb(88, 209, 235),
        rgb(196, 197, 181),
        rgb(98, 94, 76),
    ],
    [
        rgb(244, 0, 95),
        rgb(152, 224, 36),
        rgb(224, 213, 97),
        rgb(157, 101, 255),
        rgb(244, 0, 95),
        rgb(88, 209, 235),
        rgb(246, 246, 239),
    ],
)

ALABASTER = TerminalTheme(
    rgb(247, 247, 247),
    rgb(0, 0, 0),
    [
        rgb(0, 0, 0),
        rgb(170, 55, 49),
        rgb(68, 140, 39),
        rgb(203, 144, 0),
        rgb(50, 92, 192),
        rgb(122, 62, 157),
        rgb(0, 131, 178),
        rgb(247, 247, 247),
        rgb(119, 119, 119),
    ],
    [
        rgb(240, 80, 80),
        rgb(96, 203, 0),
        rgb(255, 188, 93),
        rgb(0, 122, 204),
        rgb(230, 76, 230),
        rgb(0, 170, 203),
        rgb(247, 247, 247),
    ],
)

DEFAULT_TERMINAL_THEME = MONOKAI


--- src/textual/app.py ---
"""

Here you will find the [App][textual.app.App] class, which is the base class for Textual apps.

See [app basics](/guide/app) for how to build Textual apps.
"""

from __future__ import annotations

import asyncio
import importlib
import inspect
import io
import mimetypes
import os
import signal
import sys
import threading
import uuid
import warnings
from asyncio import AbstractEventLoop, Task, create_task
from concurrent.futures import Future
from contextlib import (
    asynccontextmanager,
    contextmanager,
    redirect_stderr,
    redirect_stdout,
)
from functools import partial
from pathlib import Path
from time import perf_counter
from typing import (
    TYPE_CHECKING,
    Any,
    AsyncGenerator,
    Awaitable,
    BinaryIO,
    Callable,
    ClassVar,
    Generator,
    Generic,
    Iterable,
    Iterator,
    Mapping,
    NamedTuple,
    Sequence,
    TextIO,
    Type,
    TypeVar,
    overload,
)
from weakref import WeakKeyDictionary, WeakSet

import rich
import rich.repr
from platformdirs import user_downloads_path
from rich.console import Console, ConsoleDimensions, ConsoleOptions, RenderableType
from rich.control import Control
from rich.protocol import is_renderable
from rich.segment import Segment, Segments
from rich.terminal_theme import TerminalTheme

from textual import (
    Logger,
    LogGroup,
    LogVerbosity,
    actions,
    constants,
    events,
    log,
    messages,
    on,
)
from textual._animator import DEFAULT_EASING, Animatable, Animator, EasingFunction
from textual._ansi_sequences import SYNC_END, SYNC_START
from textual._ansi_theme import ALABASTER, MONOKAI
from textual._callback import invoke
from textual._compat import cached_property
from textual._compositor import CompositorUpdate
from textual._context import active_app, active_message_pump
from textual._context import message_hook as message_hook_context_var
from textual._dispatch_key import dispatch_key
from textual._event_broker import NoHandler, extract_handler_actions
from textual._files import generate_datetime_filename
from textual._path import (
    CSSPathType,
    _css_path_type_as_list,
    _make_path_object_relative,
)
from textual._types import AnimationLevel
from textual._wait import wait_for_idle
from textual.actions import ActionParseResult, SkipAction
from textual.await_complete import AwaitComplete
from textual.await_remove import AwaitRemove
from textual.binding import Binding, BindingsMap, BindingType, Keymap
from textual.command import CommandListItem, CommandPalette, Provider, SimpleProvider
from textual.compose import compose
from textual.content import Content
from textual.css.errors import StylesheetError
from textual.css.query import NoMatches
from textual.css.stylesheet import RulesMap, Stylesheet
from textual.dom import DOMNode, NoScreen
from textual.driver import Driver
from textual.errors import NoWidget
from textual.features import FeatureFlag, parse_features
from textual.file_monitor import FileMonitor
from textual.filter import ANSIToTruecolor, DimFilter, Monochrome, NoColor
from textual.geometry import Offset, Region, Size
from textual.keys import (
    REPLACED_KEYS,
    _character_to_key,
    _get_unicode_name_from_key,
    _normalize_key_list,
    format_key,
)
from textual.messages import CallbackType, Prune
from textual.notifications import Notification, Notifications, Notify, SeverityLevel
from textual.reactive import Reactive
from textual.renderables.blank import Blank
from textual.screen import (
    ActiveBinding,
    Screen,
    ScreenResultCallbackType,
    ScreenResultType,
    SystemModalScreen,
)
from textual.signal import Signal
from textual.theme import BUILTIN_THEMES, Theme, ThemeProvider
from textual.timer import Timer
from textual.visual import SupportsVisual, Visual
from textual.widget import AwaitMount, Widget
from textual.widgets._toast import ToastRack
from textual.worker import NoActiveWorker, get_current_worker
from textual.worker_manager import WorkerManager

if TYPE_CHECKING:
    from textual_dev.client import DevtoolsClient
    from typing_extensions import Coroutine, Literal, Self, TypeAlias

    from textual._types import MessageTarget

    # Unused & ignored imports are needed for the docs to link to these objects:
    from textual.css.query import WrongType  # type: ignore  # noqa: F401
    from textual.filter import LineFilter
    from textual.message import Message
    from textual.pilot import Pilot
    from textual.system_commands import SystemCommandsProvider
    from textual.widget import MountError  # type: ignore  # noqa: F401

WINDOWS = sys.platform == "win32"

# asyncio will warn against resources not being cleared
if constants.DEBUG:
    warnings.simplefilter("always", ResourceWarning)

# `asyncio.get_event_loop()` is deprecated since Python 3.10:
_ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED = sys.version_info >= (3, 10, 0)

ComposeResult = Iterable[Widget]
RenderResult: TypeAlias = "RenderableType | Visual | SupportsVisual"
"""Result of Widget.render()"""

AutopilotCallbackType: TypeAlias = (
    "Callable[[Pilot[object]], Coroutine[Any, Any, None]]"
)
"""Signature for valid callbacks that can be used to control apps."""

CommandCallback: TypeAlias = "Callable[[], Awaitable[Any]] | Callable[[], Any]"
"""Signature for callbacks used in [`get_system_commands`][textual.app.App.get_system_commands]"""

ScreenType = TypeVar("ScreenType", bound=Screen)
"""Type var for a Screen, used in [`get_screen`][textual.app.App.get_screen]."""


class SystemCommand(NamedTuple):
    """Defines a system command used in the command palette (yielded from [`get_system_commands`][textual.app.App.get_system_commands])."""

    title: str
    """The title of the command (used in search)."""
    help: str
    """Additional help text, shown under the title."""
    callback: CommandCallback
    """A callback to invoke when the command is selected."""
    discover: bool = True
    """Should the command show when the search is empty?"""


def get_system_commands_provider() -> type[SystemCommandsProvider]:
    """Callable to lazy load the system commands.

    Returns:
        System commands class.
    """
    from textual.system_commands import SystemCommandsProvider

    return SystemCommandsProvider


class AppError(Exception):
    """Base class for general App related exceptions."""


class ActionError(Exception):
    """Base class for exceptions relating to actions."""


class ScreenError(Exception):
    """Base class for exceptions that relate to screens."""


class ScreenStackError(ScreenError):
    """Raised when trying to manipulate the screen stack incorrectly."""


class ModeError(Exception):
    """Base class for exceptions related to modes."""


class InvalidModeError(ModeError):
    """Raised if there is an issue with a mode name."""


class UnknownModeError(ModeError):
    """Raised when attempting to use a mode that is not known."""


class ActiveModeError(ModeError):
    """Raised when attempting to remove the currently active mode."""


class SuspendNotSupported(Exception):
    """Raised if suspending the application is not supported.

    This exception is raised if [`App.suspend`][textual.app.App.suspend] is called while
    the application is running in an environment where this isn't supported.
    """


class InvalidThemeError(Exception):
    """Raised when an invalid theme is set."""


ReturnType = TypeVar("ReturnType")
CallThreadReturnType = TypeVar("CallThreadReturnType")


class _NullFile:
    """A file-like where writes go nowhere."""

    def write(self, text: str) -> None:
        pass

    def flush(self) -> None:
        pass

    def isatty(self) -> bool:
        return True


class _PrintCapture:
    """A file-like which captures output."""

    def __init__(self, app: App, stderr: bool = False) -> None:
        """

        Args:
            app: App instance.
            stderr: Write from stderr.
        """
        self.app = app
        self.stderr = stderr

    def write(self, text: str) -> None:
        """Called when writing to stdout or stderr.

        Args:
            text: Text that was "printed".
        """
        self.app._print(text, stderr=self.stderr)

    def flush(self) -> None:
        """Called when stdout or stderr was flushed."""
        self.app._flush(stderr=self.stderr)

    def isatty(self) -> bool:
        """Pretend we're a terminal."""
        # TODO: should this be configurable?
        return True

    def fileno(self) -> int:
        """Return invalid fileno."""
        return -1


@rich.repr.auto
class App(Generic[ReturnType], DOMNode):
    """The base class for Textual Applications."""

    CSS: ClassVar[str] = ""
    """Inline CSS, useful for quick scripts. This is loaded after CSS_PATH,
    and therefore takes priority in the event of a specificity clash."""

    # Default (the lowest priority) CSS
    DEFAULT_CSS: ClassVar[str]
    DEFAULT_CSS = """
    App {
        background: $background;
        color: $foreground;

        &:ansi {
            background: ansi_default;
            color: ansi_default;

            .-ansi-scrollbar {
                scrollbar-background: ansi_default;
                scrollbar-background-hover: ansi_default;
                scrollbar-background-active: ansi_default;
                scrollbar-color: ansi_blue;
                scrollbar-color-active: ansi_bright_blue;
                scrollbar-color-hover: ansi_bright_blue;    
                scrollbar-corner-color: ansi_default;           
            }

            .bindings-table--key {
                color: ansi_magenta;
            }
            .bindings-table--description {
                color: ansi_default;
            }

            .bindings-table--header {
                color: ansi_default;
            }

            .bindings-table--divider {
                color: transparent;
                text-style: dim;
            }
        }

        /* When a widget is maximized */
        Screen.-maximized-view {                    
            layout: vertical !important;
            hatch: right $panel;
            overflow-y: auto !important;
            align: center middle;
            .-maximized {
                dock: initial !important;                
            }
        }
        /* Fade the header title when app is blurred */
        &:blur HeaderTitle {           
            text-opacity: 50%;           
        }
    }
    *:disabled:can-focus {
        opacity: 0.7;
    }
    """

    MODES: ClassVar[dict[str, str | Callable[[], Screen]]] = {}
    """Modes associated with the app and their base screens.

    The base screen is the screen at the bottom of the mode stack. You can think of
    it as the default screen for that stack.
    The base screens can be names of screens listed in [SCREENS][textual.app.App.SCREENS],
    [`Screen`][textual.screen.Screen] instances, or callables that return screens.

    Example:
        ```py
        class HelpScreen(Screen[None]):
            ...

        class MainAppScreen(Screen[None]):
            ...

        class MyApp(App[None]):
            MODES = {
                "default": "main",
                "help": HelpScreen,
            }

            SCREENS = {
                "main": MainAppScreen,
            }

            ...
        ```
    """
    DEFAULT_MODE: ClassVar[str] = "_default"
    """Name of the default mode."""

    SCREENS: ClassVar[dict[str, Callable[[], Screen[Any]]]] = {}
    """Screens associated with the app for the lifetime of the app."""

    AUTO_FOCUS: ClassVar[str | None] = "*"
    """A selector to determine what to focus automatically when a screen is activated.

    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).
    Setting to `None` or `""` disables auto focus.
    """

    ALLOW_SELECT: ClassVar[bool] = True
    """A switch to toggle arbitrary text selection for the app.
    
    Note that this doesn't apply to Input and TextArea which have builtin support for selection.
    """

    _BASE_PATH: str | None = None
    CSS_PATH: ClassVar[CSSPathType | None] = None
    """File paths to load CSS from."""

    TITLE: str | None = None
    """A class variable to set the *default* title for the application.

    To update the title while the app is running, you can set the [title][textual.app.App.title] attribute.
    See also [the `Screen.TITLE` attribute][textual.screen.Screen.TITLE].
    """

    SUB_TITLE: str | None = None
    """A class variable to set the default sub-title for the application.

    To update the sub-title while the app is running, you can set the [sub_title][textual.app.App.sub_title] attribute.
    See also [the `Screen.SUB_TITLE` attribute][textual.screen.Screen.SUB_TITLE].
    """

    ENABLE_COMMAND_PALETTE: ClassVar[bool] = True
    """Should the [command palette][textual.command.CommandPalette] be enabled for the application?"""

    NOTIFICATION_TIMEOUT: ClassVar[float] = 5
    """Default number of seconds to show notifications before removing them."""

    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = {
        get_system_commands_provider
    }
    """Command providers used by the [command palette](/guide/command_palette).

    Should be a set of [command.Provider][textual.command.Provider] classes.
    """

    COMMAND_PALETTE_BINDING: ClassVar[str] = "ctrl+p"
    """The key that launches the command palette (if enabled by [`App.ENABLE_COMMAND_PALETTE`][textual.app.App.ENABLE_COMMAND_PALETTE])."""

    COMMAND_PALETTE_DISPLAY: ClassVar[str | None] = None
    """How the command palette key should be displayed in the footer (or `None` for default)."""

    ALLOW_IN_MAXIMIZED_VIEW: ClassVar[str] = "Footer"
    """The default value of [Screen.ALLOW_IN_MAXIMIZED_VIEW][textual.screen.Screen.ALLOW_IN_MAXIMIZED_VIEW]."""

    CLICK_CHAIN_TIME_THRESHOLD: ClassVar[float] = 0.5
    """The maximum number of seconds between clicks to upgrade a single click to a double click, 
    a double click to a triple click, etc."""

    BINDINGS: ClassVar[list[BindingType]] = [
        Binding(
            "ctrl+q",
            "quit",
            "Quit",
            tooltip="Quit the app and return to the command prompt.",
            show=False,
            priority=True,
        ),
        Binding("ctrl+c", "help_quit", show=False, system=True),
    ]
    """The default key bindings."""

    CLOSE_TIMEOUT: float | None = 5.0
    """Timeout waiting for widget's to close, or `None` for no timeout."""

    TOOLTIP_DELAY: float = 0.5
    """The time in seconds after which a tooltip gets displayed."""

    BINDING_GROUP_TITLE: str | None = None
    """Set to text to show in the key panel."""

    ESCAPE_TO_MINIMIZE: ClassVar[bool] = True
    """Use escape key to minimize widgets (potentially overriding bindings).
    
    This is the default value, used if the active screen's `ESCAPE_TO_MINIMIZE` is not changed from `None`.
    """

    INLINE_PADDING: ClassVar[int] = 1
    """Number of blank lines above an inline app."""

    SUSPENDED_SCREEN_CLASS: ClassVar[str] = ""
    """Class to apply to suspended screens, or empty string for no class."""

    HORIZONTAL_BREAKPOINTS: ClassVar[list[tuple[int, str]]] | None = []
    """List of horizontal breakpoints for responsive classes.

    This allows for styles to be responsive to the dimensions of the terminal.
    For instance, you might want to show less information, or fewer columns on a narrow displays -- or more information when the terminal is sized wider than usual.
    
    A breakpoint consists of a tuple containing the minimum width where the class should applied, and the name of the class to set.

    Note that only one class name is set, and if you should avoid having more than one breakpoint set for the same size.

    Example:
        ```python
        # Up to 80 cells wide, the app has the class "-normal"
        # 80 - 119 cells wide, the app has the class "-wide"
        # 120 cells or wider, the app has the class "-very-wide"
        HORIZONTAL_BREAKPOINTS = [(0, "-normal"), (80, "-wide"), (120, "-very-wide")]
        ```
    
    """
    VERTICAL_BREAKPOINTS: ClassVar[list[tuple[int, str]]] | None = []
    """List of vertical breakpoints for responsive classes.
    
    Contents are the same as [`HORIZONTAL_BREAKPOINTS`][textual.app.App.HORIZONTAL_BREAKPOINTS], but the integer is compared to the height, rather than the width.
    """

    _PSEUDO_CLASSES: ClassVar[dict[str, Callable[[App[Any]], bool]]] = {
        "focus": lambda app: app.app_focus,
        "blur": lambda app: not app.app_focus,
        "dark": lambda app: app.current_theme.dark,
        "light": lambda app: not app.current_theme.dark,
        "inline": lambda app: app.is_inline,
        "ansi": lambda app: app.ansi_color,
        "nocolor": lambda app: app.no_color,
    }  # type: ignore[assignment]

    title: Reactive[str] = Reactive("", compute=False)
    """The title of the app, displayed in the header."""
    sub_title: Reactive[str] = Reactive("", compute=False)
    """The app's sub-title, combined with [`title`][textual.app.App.title] in the header."""

    app_focus = Reactive(True, compute=False)
    """Indicates if the app has focus.

    When run in the terminal, the app always has focus. When run in the web, the app will
    get focus when the terminal widget has focus.
    """

    theme: Reactive[str] = Reactive(constants.DEFAULT_THEME)
    """The name of the currently active theme."""

    ansi_theme_dark = Reactive(MONOKAI, init=False)
    """Maps ANSI colors to hex colors using a Rich TerminalTheme object while using a dark theme."""

    ansi_theme_light = Reactive(ALABASTER, init=False)
    """Maps ANSI colors to hex colors using a Rich TerminalTheme object while using a light theme."""

    ansi_color = Reactive(False)
    """Allow ANSI colors in UI?"""

    def __init__(
        self,
        driver_class: Type[Driver] | None = None,
        css_path: CSSPathType | None = None,
        watch_css: bool = False,
        ansi_color: bool = False,
    ):
        """Create an instance of an app.

        Args:
            driver_class: Driver class or `None` to auto-detect.
                This will be used by some Textual tools.
            css_path: Path to CSS or `None` to use the `CSS_PATH` class variable.
                To load multiple CSS files, pass a list of strings or paths which
                will be loaded in order.
            watch_css: Reload CSS if the files changed. This is set automatically if
                you are using `textual run` with the `dev` switch.
            ansi_color: Allow ANSI colors if `True`, or convert ANSI colors to to RGB if `False`.

        Raises:
            CssPathError: When the supplied CSS path(s) are an unexpected type.
        """
        self._start_time = perf_counter()
        super().__init__(classes=self.DEFAULT_CLASSES)
        self.features: frozenset[FeatureFlag] = parse_features(os.getenv("TEXTUAL", ""))

        self._registered_themes: dict[str, Theme] = {}
        """Themes that have been registered with the App using `App.register_theme`.
        
        This excludes the built-in themes."""

        for theme in BUILTIN_THEMES.values():
            self.register_theme(theme)

        ansi_theme = (
            self.ansi_theme_dark if self.current_theme.dark else self.ansi_theme_light
        )
        self.set_reactive(App.ansi_color, ansi_color)
        self._filters: list[LineFilter] = [
            ANSIToTruecolor(ansi_theme, enabled=not ansi_color)
        ]
        environ = dict(os.environ)
        self.no_color = environ.pop("NO_COLOR", None) is not None
        if self.no_color:
            self._filters.append(NoColor() if self.ansi_color else Monochrome())

        for filter_name in constants.FILTERS.split(","):
            filter = filter_name.lower().strip()
            if filter == "dim":
                self._filters.append(DimFilter())

        self.console = Console(
            color_system=constants.COLOR_SYSTEM,
            file=_NullFile(),
            markup=True,
            highlight=False,
            emoji=False,
            legacy_windows=False,
            _environ=environ,
            force_terminal=True,
            safe_box=False,
            soft_wrap=False,
        )
        self._workers = WorkerManager(self)
        self.error_console = Console(markup=False, highlight=False, stderr=True)
        self.driver_class = driver_class or self.get_driver_class()
        self._screen_stacks: dict[str, list[Screen[Any]]] = {self.DEFAULT_MODE: []}
        """A stack of screens per mode."""
        self._current_mode: str = self.DEFAULT_MODE
        """The current mode the app is in."""
        self._sync_available = False

        self.mouse_over: Widget | None = None
        """The widget directly under the mouse."""
        self.hover_over: Widget | None = None
        """The first widget with a hover style under the mouse."""
        self.mouse_captured: Widget | None = None
        self._driver: Driver | None = None
        self._exit_renderables: list[RenderableType] = []

        self._action_targets = {"app", "screen", "focused"}
        self._animator = Animator(self)
        self._animate = self._animator.bind(self)
        self.mouse_position = Offset(0, 0)

        self._mouse_down_widget: Widget | None = None
        """The widget that was most recently mouse downed (used to create click events)."""

        self._click_chain_last_offset: Offset | None = None
        """The last offset at which a Click occurred, in screen-space."""

        self._click_chain_last_time: float | None = None
        """The last time at which a Click occurred."""

        self._chained_clicks: int = 1
        """Counter which tracks the number of clicks received in a row."""

        self._previous_cursor_position = Offset(0, 0)
        """The previous cursor position"""

        self.cursor_position = Offset(0, 0)
        """The position of the terminal cursor in screen-space.

        This can be set by widgets and is useful for controlling the
        positioning of OS IME and emoji popup menus."""

        self._exception: Exception | None = None
        """The unhandled exception which is leading to the app shutting down,
        or None if the app is still running with no unhandled exceptions."""

        self.title = (
            self.TITLE if self.TITLE is not None else f"{self.__class__.__name__}"
        )
        """The title for the application.

        The initial value for `title` will be set to the `TITLE` class variable if it exists, or
        the name of the app if it doesn't.

        Assign a new value to this attribute to change the title.
        The new value is always converted to string.
        """

        self.sub_title = self.SUB_TITLE if self.SUB_TITLE is not None else ""
        """The sub-title for the application.

        The initial value for `sub_title` will be set to the `SUB_TITLE` class variable if it exists, or
        an empty string if it doesn't.

        Sub-titles are typically used to show the high-level state of the app, such as the current mode, or path to
        the file being worked on.

        Assign a new value to this attribute to change the sub-title.
        The new value is always converted to string.
        """

        self.use_command_palette: bool = self.ENABLE_COMMAND_PALETTE
        """A flag to say if the application should use the command palette.

        If set to `False` any call to
        [`action_command_palette`][textual.app.App.action_command_palette]
        will be ignored.
        """

        self._logger = Logger(self._log, app=self)

        self._css_has_errors = False

        self.theme_variables: dict[str, str] = {}
        """Variables generated from the current theme."""

        # Note that the theme must be set *before* self.get_css_variables() is called
        # to ensure that the variables are retrieved from the currently active theme.
        self.stylesheet = Stylesheet(variables=self.get_css_variables())

        css_path = css_path or self.CSS_PATH
        css_paths = [
            _make_path_object_relative(css_path, self)
            for css_path in (
                _css_path_type_as_list(css_path) if css_path is not None else []
            )
        ]
        self.css_path = css_paths

        self._registry: WeakSet[DOMNode] = WeakSet()

        self._keymap: Keymap = {}

        # Sensitivity on X is double the sensitivity on Y to account for
        # cells being twice as tall as wide
        self.scroll_sensitivity_x: float = 4.0
        """Number of columns to scroll in the X direction with wheel or trackpad."""
        self.scroll_sensitivity_y: float = 2.0
        """Number of lines to scroll in the Y direction with wheel or trackpad."""

        self._installed_screens: dict[str, Screen | Callable[[], Screen]] = {}
        self._installed_screens.update(**self.SCREENS)
        self._modes: dict[str, str | Callable[[], Screen]] = self.MODES.copy()
        """Contains the working-copy of the `MODES` for each instance."""

        self._compose_stacks: list[list[Widget]] = []
        self._composed: list[list[Widget]] = []
        self._recompose_required = False

        self.devtools: DevtoolsClient | None = None
        self._devtools_redirector: StdoutRedirector | None = None
        if "devtools" in self.features:
            try:
                from textual_dev.client import DevtoolsClient
                from textual_dev.redirect_output import StdoutRedirector
            except ImportError:
                # Dev dependencies not installed
                pass
            else:
                self.devtools = DevtoolsClient(constants.DEVTOOLS_HOST)
                self._devtools_redirector = StdoutRedirector(self.devtools)

        self._loop: asyncio.AbstractEventLoop | None = None
        self._return_value: ReturnType | None = None
        """Internal attribute used to set the return value for the app."""
        self._return_code: int | None = None
        """Internal attribute used to set the return code for the app."""
        self._exit = False
        self._disable_tooltips = False
        self._disable_notifications = False

        self.css_monitor = (
            FileMonitor(self.css_path, self._on_css_change)
            if watch_css or self.debug
            else None
        )
        self._screenshot: str | None = None
        self._dom_ready = False
        self._batch_count = 0
        self._notifications = Notifications()

        self._capture_print: WeakKeyDictionary[MessageTarget, tuple[bool, bool]] = (
            WeakKeyDictionary()
        )
        """Registry of the MessageTargets which are capturing output at any given time."""
        self._capture_stdout = _PrintCapture(self, stderr=False)
        """File-like object capturing data written to stdout."""
        self._capture_stderr = _PrintCapture(self, stderr=True)
        """File-like object capturing data written to stderr."""
        self._original_stdout = sys.__stdout__
        """The original stdout stream (before redirection etc)."""
        self._original_stderr = sys.__stderr__
        """The original stderr stream (before redirection etc)."""

        self.theme_changed_signal: Signal[Theme] = Signal(self, "theme-changed")
        """Signal that is published when the App's theme is changed.
        
        Subscribers will receive the new theme object as an argument to the callback.
        """

        self.app_suspend_signal: Signal[App] = Signal(self, "app-suspend")
        """The signal that is published when the app is suspended.

        When [`App.suspend`][textual.app.App.suspend] is called this signal
        will be [published][textual.signal.Signal.publish];
        [subscribe][textual.signal.Signal.subscribe] to this signal to
        perform work before the suspension takes place.
        """
        self.app_resume_signal: Signal[App] = Signal(self, "app-resume")
        """The signal that is published when the app is resumed after a suspend.

        When the app is resumed after a
        [`App.suspend`][textual.app.App.suspend] call this signal will be
        [published][textual.signal.Signal.publish];
        [subscribe][textual.signal.Signal.subscribe] to this signal to
        perform work after the app has resumed.
        """

        self.set_class(self.current_theme.dark, "-dark-mode", update=False)
        self.set_class(not self.current_theme.dark, "-light-mode", update=False)

        self.animation_level: AnimationLevel = constants.TEXTUAL_ANIMATIONS
        """Determines what type of animations the app will display.

        See [`textual.constants.TEXTUAL_ANIMATIONS`][textual.constants.TEXTUAL_ANIMATIONS].
        """

        self._last_focused_on_app_blur: Widget | None = None
        """The widget that had focus when the last `AppBlur` happened.

        This will be used to restore correct focus when an `AppFocus`
        happens.
        """

        self._previous_inline_height: int | None = None
        """Size of previous inline update."""

        self._resize_event: events.Resize | None = None
        """A pending resize event, sent on idle."""

        self._size: Size | None = None

        self._css_update_count: int = 0
        """Incremented when CSS is invalidated."""

        self._clipboard: str = ""
        """Contents of local clipboard."""

        self.supports_smooth_scrolling: bool = False
        """Does the terminal support smooth scrolling?"""

        self._compose_screen: Screen | None = None
        """The screen composed by App.compose."""

        if self.ENABLE_COMMAND_PALETTE:
            for _key, binding in self._bindings:
                if binding.action in {"command_palette", "app.command_palette"}:
                    break
            else:
                self._bindings._add_binding(
                    Binding(
                        self.COMMAND_PALETTE_BINDING,
                        "command_palette",
                        "palette",
                        show=False,
                        key_display=self.COMMAND_PALETTE_DISPLAY,
                        priority=True,
                        tooltip="Open the command palette",
                    )
                )

    def get_line_filters(self) -> Sequence[LineFilter]:
        """Get currently enabled line filters.

        Returns:
            A list of [LineFilter][textual.filters.LineFilter] instances.
        """
        return [filter for filter in self._filters if filter.enabled]

    @property
    def _is_devtools_connected(self) -> bool:
        """Is the app connected to the devtools?"""
        return self.devtools is not None and self.devtools.is_connected

    @cached_property
    def _exception_event(self) -> asyncio.Event:
        """An event that will be set when the first exception is encountered."""
        return asyncio.Event()

    def __init_subclass__(cls, *args, **kwargs) -> None:
        for variable_name, screen_collection in (
            ("SCREENS", cls.SCREENS),
            ("MODES", cls.MODES),
        ):
            for screen_name, screen_object in screen_collection.items():
                if not (isinstance(screen_object, str) or callable(screen_object)):
                    if isinstance(screen_object, Screen):
                        raise ValueError(
                            f"{variable_name} should contain a Screen type or callable, not an instance"
                            f" (got instance of {type(screen_object).__name__} for {screen_name!r})"
                        )
                    raise TypeError(
                        f"expected a callable or string, got {screen_object!r}"
                    )

        return super().__init_subclass__(*args, **kwargs)

    def _thread_init(self):
        """Initialize threading primitives for the current thread.

        https://github.com/Textualize/textual/issues/5845

        """
        self._message_queue
        self._mounted_event
        self._exception_event
        self._thread_id = threading.get_ident()

    def _get_dom_base(self) -> DOMNode:
        """When querying from the app, we want to query the default screen."""
        return self.default_screen

    def validate_title(self, title: Any) -> str:
        """Make sure the title is set to a string."""
        return str(title)

    def validate_sub_title(self, sub_title: Any) -> str:
        """Make sure the subtitle is set to a string."""
        return str(sub_title)

    @property
    def default_screen(self) -> Screen:
        """The default screen instance."""
        return self.screen if self._compose_screen is None else self._compose_screen

    @property
    def workers(self) -> WorkerManager:
        """The [worker](/guide/workers/) manager.

        Returns:
            An object to manage workers.
        """
        return self._workers

    @property
    def return_value(self) -> ReturnType | None:
        """The return value of the app, or `None` if it has not yet been set.

        The return value is set when calling [exit][textual.app.App.exit].
        """
        return self._return_value

    @property
    def return_code(self) -> int | None:
        """The return code with which the app exited.

        Non-zero codes indicate errors.
        A value of 1 means the app exited with a fatal error.
        If the app hasn't exited yet, this will be `None`.

        Example:
            The return code can be used to exit the process via `sys.exit`.
            ```py
            my_app.run()
            sys.exit(my_app.return_code)
            ```
        """
        return self._return_code

    @property
    def children(self) -> Sequence["Widget"]:
        """A view onto the app's immediate children.

        This attribute exists on all widgets.
        In the case of the App, it will only ever contain a single child, which will
        be the currently active screen.

        Returns:
            A sequence of widgets.
        """
        try:
            return (
                next(
                    screen
                    for screen in reversed(self._screen_stack)
                    if not isinstance(screen, SystemModalScreen)
                ),
            )
        except StopIteration:
            return ()

    @property
    def clipboard(self) -> str:
        """The value of the local clipboard.

        Note, that this only contains text copied in the app, and not
        text copied from elsewhere in the OS.
        """
        return self._clipboard

    def format_title(self, title: str, sub_title: str) -> Content:
        """Format the title for display.

        Args:
            title: The title.
            sub_title: The sub title.

        Returns:
            Content instance with title and subtitle.
        """
        title_content = Content(title)
        sub_title_content = Content(sub_title)
        if sub_title_content:
            return Content.assemble(
                title_content,
                (" — ", "dim"),
                sub_title_content.stylize("dim"),
            )
        else:
            return title_content

    @contextmanager
    def batch_update(self) -> Generator[None, None, None]:
        """A context manager to suspend all repaints until the end of the batch."""
        self._begin_batch()
        try:
            yield
        finally:
            self._end_batch()

    def _begin_batch(self) -> None:
        """Begin a batch update."""
        self._batch_count += 1

    def _end_batch(self) -> None:
        """End a batch update."""
        self._batch_count -= 1
        assert self._batch_count >= 0, "This won't happen if you use `batch_update`"
        if not self._batch_count:
            self.check_idle()

    def delay_update(self, delay: float = 0.05) -> None:
        """Delay updates for a short period of time.

        May be used to mask a brief transition.
        Consider this method only if you aren't able to use `App.batch_update`.

        Args:
            delay: Delay before updating.
        """
        self._begin_batch()

        def end_batch() -> None:
            """Re-enable updates, and refresh screen."""
            self._end_batch()
            if not self._batch_count:
                self.screen.refresh()

        self.set_timer(delay, end_batch, name="delay_update")

    @contextmanager
    def _context(self) -> Generator[None, None, None]:
        """Context manager to set ContextVars."""
        app_reset_token = active_app.set(self)
        message_pump_reset_token = active_message_pump.set(self)
        try:
            yield
        finally:
            active_message_pump.reset(message_pump_reset_token)
            active_app.reset(app_reset_token)

    def _watch_ansi_color(self, ansi_color: bool) -> None:
        """Enable or disable the truecolor filter when the reactive changes"""
        for filter in self._filters:
            if isinstance(filter, ANSIToTruecolor):
                filter.enabled = not ansi_color

    def animate(
        self,
        attribute: str,
        value: float | Animatable,
        *,
        final_value: object = ...,
        duration: float | None = None,
        speed: float | None = None,
        delay: float = 0.0,
        easing: EasingFunction | str = DEFAULT_EASING,
        on_complete: CallbackType | None = None,
        level: AnimationLevel = "full",
    ) -> None:
        """Animate an attribute.

        See the guide for how to use the [animation](/guide/animation) system.

        Args:
            attribute: Name of the attribute to animate.
            value: The value to animate to.
            final_value: The final value of the animation.
            duration: The duration (in seconds) of the animation.
            speed: The speed of the animation.
            delay: A delay (in seconds) before the animation starts.
            easing: An easing method.
            on_complete: A callable to invoke when the animation is finished.
            level: Minimum level required for the animation to take place (inclusive).
        """
        self._animate(
            attribute,
            value,
            final_value=final_value,
            duration=duration,
            speed=speed,
            delay=delay,
            easing=easing,
            on_complete=on_complete,
            level=level,
        )

    async def stop_animation(self, attribute: str, complete: bool = True) -> None:
        """Stop an animation on an attribute.

        Args:
            attribute: Name of the attribute whose animation should be stopped.
            complete: Should the animation be set to its final value?

        Note:
            If there is no animation scheduled or running, this is a no-op.
        """
        await self._animator.stop_animation(self, attribute, complete)

    @property
    def is_dom_root(self) -> bool:
        """Is this a root node (i.e. the App)?"""
        return True

    @property
    def is_attached(self) -> bool:
        """Is this node linked to the app through the DOM?"""
        return True

    @property
    def debug(self) -> bool:
        """Is debug mode enabled?"""
        return "debug" in self.features or constants.DEBUG

    @property
    def is_headless(self) -> bool:
        """Is the app running in 'headless' mode?

        Headless mode is used when running tests with [run_test][textual.app.App.run_test].
        """
        return False if self._driver is None else self._driver.is_headless

    @property
    def is_inline(self) -> bool:
        """Is the app running in 'inline' mode?"""
        return False if self._driver is None else self._driver.is_inline

    @property
    def is_web(self) -> bool:
        """Is the app running in 'web' mode via a browser?"""
        return False if self._driver is None else self._driver.is_web

    @property
    def screen_stack(self) -> list[Screen[Any]]:
        """A snapshot of the current screen stack.

        Returns:
            A snapshot of the current state of the screen stack.
        """
        return self._screen_stacks[self._current_mode].copy()

    @property
    def _screen_stack(self) -> list[Screen[Any]]:
        """A reference to the current screen stack.

        Note:
            Consider using [`screen_stack`][textual.app.App.screen_stack] instead.

        Returns:
            A reference to the current screen stack.
        """
        return self._screen_stacks[self._current_mode]

    @property
    def current_mode(self) -> str:
        """The name of the currently active mode."""
        return self._current_mode

    @property
    def console_options(self) -> ConsoleOptions:
        """Get options for the Rich console.

        Returns:
            Console options (same object returned from `console.options`).
        """
        size = ConsoleDimensions(*self.size)
        console = self.console
        return ConsoleOptions(
            max_height=size.height,
            size=size,
            legacy_windows=console.legacy_windows,
            min_width=1,
            max_width=size.width,
            encoding=console.encoding,
            is_terminal=console.is_terminal,
        )

    def exit(
        self,
        result: ReturnType | None = None,
        return_code: int = 0,
        message: RenderableType | None = None,
    ) -> None:
        """Exit the app, and return the supplied result.

        Args:
            result: Return value.
            return_code: The return code. Use non-zero values for error codes.
            message: Optional message to display on exit.
        """
        self._exit = True
        self._return_value = result
        self._return_code = return_code
        self.post_message(messages.ExitApp())
        if message:
            self._exit_renderables.append(message)

    @property
    def focused(self) -> Widget | None:
        """The widget that is focused on the currently active screen, or `None`.

        Focused widgets receive keyboard input.

        Returns:
            The currently focused widget, or `None` if nothing is focused.
        """
        focused = self.screen.focused
        if focused is not None and focused.loading:
            return None
        return focused

    @property
    def active_bindings(self) -> dict[str, ActiveBinding]:
        """Get currently active bindings.

        If no widget is focused, then app-level bindings are returned.
        If a widget is focused, then any bindings present in the active screen and app are merged and returned.

        This property may be used to inspect current bindings.

        Returns:
            A dict that maps keys on to binding information.
        """
        return self.screen.active_bindings

    def get_system_commands(self, screen: Screen) -> Iterable[SystemCommand]:
        """A generator of system commands used in the command palette.

        Args:
            screen: The screen where the command palette was invoked from.

        Implement this method in your App subclass if you want to add custom commands.
        Here is an example:

        ```python
        def get_system_commands(self, screen: Screen) -> Iterable[SystemCommand]:
            yield from super().get_system_commands(screen)
            yield SystemCommand("Bell", "Ring the bell", self.bell)
        ```

        !!! note
            Requires that [`SystemCommandsProvider`][textual.system_commands.SystemCommandsProvider] is in `App.COMMANDS` class variable.

        Yields:
            [SystemCommand][textual.app.SystemCommand] instances.
        """
        if not self.ansi_color:
            yield SystemCommand(
                "Change theme",
                "Change the current theme",
                self.action_change_theme,
            )
        yield SystemCommand(
            "Quit the application",
            "Quit the application as soon as possible",
            self.action_quit,
        )

        if screen.query("HelpPanel"):
            yield SystemCommand(
                "Hide keys and help panel",
                "Hide the keys and widget help panel",
                self.action_hide_help_panel,
            )
        else:
            yield SystemCommand(
                "Show keys and help panel",
                "Show help for the focused widget and a summary of available keys",
                self.action_show_help_panel,
            )

        if screen.maximized is not None:
            yield SystemCommand(
                "Minimize",
                "Minimize the widget and restore to normal size",
                screen.action_minimize,
            )
        elif screen.focused is not None and screen.focused.allow_maximize:
            yield SystemCommand(
                "Maximize", "Maximize the focused widget", screen.action_maximize
            )

        yield SystemCommand(
            "Save screenshot",
            "Save an SVG 'screenshot' of the current screen",
            lambda: self.set_timer(0.1, self.deliver_screenshot),
        )

    def get_default_screen(self) -> Screen:
        """Get the default screen.

        This is called when the App is first composed. The returned screen instance
        will be the first screen on the stack.

        Implement this method if you would like to use a custom Screen as the default screen.

        Returns:
            A screen instance.
        """
        return Screen(id="_default")

    def compose(self) -> ComposeResult:
        """Yield child widgets for a container.

        This method should be implemented in a subclass.
        """
        yield from ()

    def get_theme_variable_defaults(self) -> dict[str, str]:
        """Get the default values for the `variables` used in a theme.

        If the currently specified theme doesn't define a value for a variable,
        the value specified here will be used as a fallback.

        If a variable is referenced in CSS but does not appear either here
        or in the theme, the CSS will fail to parse on startup.

        This method allows applications to define their own variables, beyond
        those offered by Textual, which can then be overridden by a Theme.

        Returns:
            A mapping of variable name (e.g. "my-button-background-color") to value.
            Values can be any valid CSS value, e.g. "red 50%", "auto 90%",
            "#ff0000", "rgb(255, 0, 0)", etc.
        """
        return {}

    def get_css_variables(self) -> dict[str, str]:
        """Get a mapping of variables used to pre-populate CSS.

        May be implemented in a subclass to add new CSS variables.

        Returns:
            A mapping of variable name to value.
        """
        theme = self.current_theme
        # Build the Textual color system from the theme.
        # This will contain $secondary, $primary, $background, etc.
        variables = theme.to_color_system().generate()
        # Apply the additional variables from the theme
        variables = {**variables, **(theme.variables)}
        theme_variables = self.get_theme_variable_defaults()

        combined_variables = {**theme_variables, **variables}
        self.theme_variables = combined_variables
        return combined_variables

    def get_theme(self, theme_name: str) -> Theme | None:
        """Get a theme by name.

        Args:
            theme_name: The name of the theme to get. May also be a comma
                separated list of names, to pick the first available theme.

        Returns:
            A Theme instance and None if the theme doesn't exist.
        """
        theme_names = [token.strip() for token in theme_name.split(",")]
        for theme_name in theme_names:
            if theme_name in self.available_themes:
                return self.available_themes[theme_name]
        return None

    def register_theme(self, theme: Theme) -> None:
        """Register a theme with the app.

        If the theme already exists, it will be overridden.

        After registering a theme, you can activate it by setting the
        `App.theme` attribute. To retrieve a registered theme, use the
        `App.get_theme` method.

        Args:
            theme: The theme to register.
        """
        self._registered_themes[theme.name] = theme

    def unregister_theme(self, theme_name: str) -> None:
        """Unregister a theme with the app.

        Args:
            theme_name: The name of the theme to unregister.
        """
        if theme_name in self._registered_themes:
            del self._registered_themes[theme_name]

    @property
    def available_themes(self) -> dict[str, Theme]:
        """All available themes (all built-in themes plus any that have been registered).

        A dictionary mapping theme names to Theme instances.
        """
        return {**self._registered_themes}

    @property
    def current_theme(self) -> Theme:
        theme = self.get_theme(self.theme)
        if theme is None:
            theme = self.get_theme("textual-dark")
        assert theme is not None  # validated by _validate_theme
        return theme

    def _validate_theme(self, theme_name: str) -> str:
        if theme_name not in self.available_themes:
            message = (
                f"Theme {theme_name!r} has not been registered. "
                "Call 'App.register_theme' before setting the 'App.theme' attribute."
            )
            raise InvalidThemeError(message)
        return theme_name

    def _watch_theme(self, theme_name: str) -> None:
        """Apply a theme to the application.

        This method is called when the theme reactive attribute is set.
        """
        theme = self.current_theme
        dark = theme.dark
        self.ansi_color = theme_name == "textual-ansi"
        self.set_class(dark, "-dark-mode", update=False)
        self.set_class(not dark, "-light-mode", update=False)
        self._refresh_truecolor_filter(self.ansi_theme)
        self._invalidate_css()
        self.call_next(partial(self.refresh_css, animate=False))
        self.call_next(self.theme_changed_signal.publish, theme)

    def _invalidate_css(self) -> None:
        """Invalidate CSS, so it will be refreshed."""
        self._css_update_count += 1

    def watch_ansi_theme_dark(self, theme: TerminalTheme) -> None:
        if self.current_theme.dark:
            self._refresh_truecolor_filter(theme)
            self._invalidate_css()
            self.call_next(self.refresh_css)

    def watch_ansi_theme_light(self, theme: TerminalTheme) -> None:
        if not self.current_theme.dark:
            self._refresh_truecolor_filter(theme)
            self._invalidate_css()
            self.call_next(self.refresh_css)

    @property
    def ansi_theme(self) -> TerminalTheme:
        """The ANSI TerminalTheme currently being used.

        Defines how colors defined as ANSI (e.g. `magenta`) inside Rich renderables
        are mapped to hex codes.
        """
        return (
            self.ansi_theme_dark if self.current_theme.dark else self.ansi_theme_light
        )

    def _refresh_truecolor_filter(self, theme: TerminalTheme) -> None:
        """Update the ANSI to Truecolor filter, if available, with a new theme mapping.

        Args:
            theme: The new terminal theme to use for mapping ANSI to truecolor.
        """
        filters = self._filters
        for index, filter in enumerate(filters):
            if isinstance(filter, ANSIToTruecolor):
                filters[index] = ANSIToTruecolor(theme, enabled=not self.ansi_color)
                return

    def get_driver_class(self) -> Type[Driver]:
        """Get a driver class for this platform.

        This method is called by the constructor, and unlikely to be required when
        building a Textual app.

        Returns:
            A Driver class which manages input and display.
        """

        driver_class: Type[Driver]

        driver_import = constants.DRIVER
        if driver_import is not None:
            # The driver class is set from the environment
            # Syntax should be foo.bar.baz:MyDriver
            module_import, _, driver_symbol = driver_import.partition(":")
            driver_module = importlib.import_module(module_import)
            driver_class = getattr(driver_module, driver_symbol)
            if not inspect.isclass(driver_class) or not issubclass(
                driver_class, Driver
            ):
                raise RuntimeError(
                    f"Unable to import {driver_import!r}; {driver_class!r} is not a Driver class "
                )
            return driver_class

        if WINDOWS:
            from textual.drivers.windows_driver import WindowsDriver

            driver_class = WindowsDriver
        else:
            from textual.drivers.linux_driver import LinuxDriver

            driver_class = LinuxDriver
        return driver_class

    def __rich_repr__(self) -> rich.repr.Result:
        yield "title", self.title
        yield "id", self.id, None
        if self.name:
            yield "name", self.name
        if self.classes:
            yield "classes", set(self.classes)
        pseudo_classes = self.pseudo_classes
        if pseudo_classes:
            yield "pseudo_classes", set(pseudo_classes)

    @property
    def animator(self) -> Animator:
        """The animator object."""
        return self._animator

    @property
    def screen(self) -> Screen[object]:
        """The current active screen.

        Returns:
            The currently active (visible) screen.

        Raises:
            ScreenStackError: If there are no screens on the stack.
        """
        try:
            return self._screen_stack[-1]
        except KeyError:
            raise UnknownModeError(f"No known mode {self._current_mode!r}") from None
        except IndexError:
            raise ScreenStackError("No screens on stack") from None

    @property
    def _background_screens(self) -> list[Screen]:
        """A list of screens that may be visible due to background opacity (top-most first, not including current screen)."""
        screens: list[Screen] = []
        for screen in reversed(self._screen_stack[:-1]):
            screens.append(screen)
            if screen.styles.background.a == 1:
                break
        background_screens = screens[::-1]
        return background_screens

    @property
    def size(self) -> Size:
        """The size of the terminal.

        Returns:
            Size of the terminal.
        """
        if self._size is not None:
            return self._size
        if self._driver is not None and self._driver._size is not None:
            width, height = self._driver._size
        else:
            width, height = self.console.size
        return Size(width, height)

    @property
    def viewport_size(self) -> Size:
        """Get the viewport size (size of the screen)."""
        try:
            return self.screen.size
        except (ScreenStackError, NoScreen):
            return self.size

    def _get_inline_height(self) -> int:
        """Get the inline height (height when in inline mode).

        Returns:
            Height in lines.
        """
        size = self.size
        return max(screen._get_inline_height(size) for screen in self._screen_stack)

    @property
    def log(self) -> Logger:
        """The textual logger.

        Example:
            ```python
            self.log("Hello, World!")
            self.log(self.tree)
            ```

        Returns:
            A Textual logger.
        """
        return self._logger

    def _log(
        self,
        group: LogGroup,
        verbosity: LogVerbosity,
        _textual_calling_frame: inspect.Traceback,
        *objects: Any,
        **kwargs,
    ) -> None:
        """Write to logs or devtools.

        Positional args will be logged. Keyword args will be prefixed with the key.

        Example:
            ```python
            data = [1,2,3]
            self.log("Hello, World", state=data)
            self.log(self.tree)
            self.log(locals())
            ```

        Args:
            verbosity: Verbosity level 0-3.
        """

        devtools = self.devtools
        if devtools is None or not devtools.is_connected:
            return

        if verbosity.value > LogVerbosity.NORMAL.value and not devtools.verbose:
            return

        try:
            from textual_dev.client import DevtoolsLog

            if len(objects) == 1 and not kwargs:
                devtools.log(
                    DevtoolsLog(objects, caller=_textual_calling_frame),
                    group,
                    verbosity,
                )
            else:
                output = " ".join(str(arg) for arg in objects)
                if kwargs:
                    key_values = " ".join(
                        f"{key}={value!r}" for key, value in kwargs.items()
                    )
                    output = f"{output} {key_values}" if output else key_values
                devtools.log(
                    DevtoolsLog(output, caller=_textual_calling_frame),
                    group,
                    verbosity,
                )
        except Exception as error:
            self._handle_exception(error)

    def get_loading_widget(self) -> Widget:
        """Get a widget to be used as a loading indicator.

        Extend this method if you want to display the loading state a little differently.

        Returns:
            A widget to display a loading state.
        """
        from textual.widgets import LoadingIndicator

        return LoadingIndicator()

    def copy_to_clipboard(self, text: str) -> None:
        """Copy text to the clipboard.

        !!! note

            This does not work on macOS Terminal, but will work on most other terminals.

        Args:
            text: Text you wish to copy to the clipboard.
        """
        self._clipboard = text
        if self._driver is None:
            return
        import base64

        base64_text = base64.b64encode(text.encode("utf-8")).decode("utf-8")
        self._driver.write(f"\x1b]52;c;{base64_text}\a")

    def call_from_thread(
        self,
        callback: Callable[..., CallThreadReturnType | Awaitable[CallThreadReturnType]],
        *args: Any,
        **kwargs: Any,
    ) -> CallThreadReturnType:
        """Run a callable from another thread, and return the result.

        Like asyncio apps in general, Textual apps are not thread-safe. If you call methods
        or set attributes on Textual objects from a thread, you may get unpredictable results.

        This method will ensure that your code runs within the correct context.

        !!! tip

            Consider using [post_message][textual.message_pump.MessagePump.post_message] which is also thread-safe.

        Args:
            callback: A callable to run.
            *args: Arguments to the callback.
            **kwargs: Keyword arguments for the callback.

        Raises:
            RuntimeError: If the app isn't running or if this method is called from the same
                thread where the app is running.

        Returns:
            The result of the callback.
        """

        if self._loop is None:
            raise RuntimeError("App is not running")

        if self._thread_id == threading.get_ident():
            raise RuntimeError(
                "The `call_from_thread` method must run in a different thread from the app"
            )

        callback_with_args = partial(callback, *args, **kwargs)

        async def run_callback() -> CallThreadReturnType:
            """Run the callback, set the result or error on the future."""
            with self._context():
                return await invoke(callback_with_args)

        # Post the message to the main loop
        future: Future[CallThreadReturnType] = asyncio.run_coroutine_threadsafe(
            run_callback(), loop=self._loop
        )
        result = future.result()
        return result

    def action_change_theme(self) -> None:
        """An [action](/guide/actions) to change the current theme."""
        self.search_themes()

    def action_screenshot(
        self, filename: str | None = None, path: str | None = None
    ) -> None:
        """This [action](/guide/actions) will save an SVG file containing the current contents of the screen.

        Args:
            filename: Filename of screenshot, or None to auto-generate.
            path: Path to directory. Defaults to the user's Downloads directory.
        """
        self.deliver_screenshot(filename, path)

    def export_screenshot(
        self,
        *,
        title: str | None = None,
        simplify: bool = False,
    ) -> str:
        """Export an SVG screenshot of the current screen.

        See also [save_screenshot][textual.app.App.save_screenshot] which writes the screenshot to a file.

        Args:
            title: The title of the exported screenshot or None
                to use app title.
            simplify: Simplify the segments by combining contiguous segments with the same style.
        """
        assert self._driver is not None, "App must be running"
        width, height = self.size

        console = Console(
            width=width,
            height=height,
            file=io.StringIO(),
            force_terminal=True,
            color_system="truecolor",
            record=True,
            legacy_windows=False,
            safe_box=False,
        )
        screen_render = self.screen._compositor.render_update(
            full=True, screen_stack=self.app._background_screens, simplify=simplify
        )
        console.print(screen_render)
        return console.export_svg(title=title or self.title)

    def save_screenshot(
        self,
        filename: str | None = None,
        path: str | None = None,
        time_format: str | None = None,
    ) -> str:
        """Save an SVG screenshot of the current screen.

        Args:
            filename: Filename of SVG screenshot, or None to auto-generate
                a filename with the date and time.
            path: Path to directory for output. Defaults to current working directory.
            time_format: Date and time format to use if filename is None.
                Defaults to a format like ISO 8601 with some reserved characters replaced with underscores.

        Returns:
            Filename of screenshot.
        """
        path = path or "./"
        if not filename:
            svg_filename = generate_datetime_filename(self.title, ".svg", time_format)
        else:
            svg_filename = filename
        svg_path = os.path.expanduser(os.path.join(path, svg_filename))
        screenshot_svg = self.export_screenshot()
        with open(svg_path, "w", encoding="utf-8") as svg_file:
            svg_file.write(screenshot_svg)
        return svg_path

    def deliver_screenshot(
        self,
        filename: str | None = None,
        path: str | None = None,
        time_format: str | None = None,
    ) -> str | None:
        """Deliver a screenshot of the app.

        This with save the screenshot when running locally, or serve it when the app
        is running in a web browser.

        Args:
            filename: Filename of SVG screenshot, or None to auto-generate
                a filename with the date and time.
            path: Path to directory for output when saving locally (not used when app is running in the browser).
                Defaults to current working directory.
            time_format: Date and time format to use if filename is None.
                Defaults to a format like ISO 8601 with some reserved characters replaced with underscores.

        Returns:
            The delivery key that uniquely identifies the file delivery.
        """
        if not filename:
            svg_filename = generate_datetime_filename(self.title, ".svg", time_format)
        else:
            svg_filename = filename
        screenshot_svg = self.export_screenshot()
        return self.deliver_text(
            io.StringIO(screenshot_svg),
            save_directory=path,
            save_filename=svg_filename,
            open_method="browser",
            mime_type="image/svg+xml",
            name="screenshot",
        )

    def search_commands(
        self,
        commands: Sequence[CommandListItem],
        placeholder: str = "Search for commands…",
    ) -> AwaitMount:
        """Show a list of commands in the app.

        Args:
            commands: A list of SimpleCommand instances.
            placeholder: Placeholder text for the search field.

        Returns:
            AwaitMount: An awaitable that resolves when the commands are shown.
        """
        return self.push_screen(
            CommandPalette(
                providers=[SimpleProvider(self.screen, commands)],
                placeholder=placeholder,
            )
        )

    def search_themes(self) -> None:
        """Show a fuzzy search command palette containing all registered themes.

        Selecting a theme in the list will change the app's theme.
        """
        self.push_screen(
            CommandPalette(
                providers=[ThemeProvider],
                placeholder="Search for themes…",
            ),
        )

    def bind(
        self,
        keys: str,
        action: str,
        *,
        description: str = "",
        show: bool = True,
        key_display: str | None = None,
    ) -> None:
        """Bind a key to an action.

        !!! warning
            This method may be private or removed in a future version of Textual.
            See [dynamic actions](/guide/actions#dynamic-actions) for a more flexible alternative to updating bindings.

        Args:
            keys: A comma separated list of keys, i.e.
            action: Action to bind to.
            description: Short description of action.
            show: Show key in UI.
            key_display: Replacement text for key, or None to use default.
        """
        self._bindings.bind(
            keys, action, description, show=show, key_display=key_display
        )

    def get_key_display(self, binding: Binding) -> str:
        """Format a bound key for display in footer / key panel etc.

        !!! note
            You can implement this in a subclass if you want to change how keys are displayed in your app.

        Args:
            binding: A Binding.

        Returns:
            A string used to represent the key.
        """
        # Dev has overridden the key display, so use that
        if binding.key_display:
            return binding.key_display

        # Extract modifiers
        modifiers, key = binding.parse_key()

        # Format the key (replace unicode names with character)
        key = format_key(key)

        # Convert ctrl modifier to caret
        if "ctrl" in modifiers:
            modifiers.pop(modifiers.index("ctrl"))
            key = f"^{key}"
        # Join everything with +
        key_tokens = modifiers + [key]
        return "+".join(key_tokens)

    async def _press_keys(self, keys: Iterable[str]) -> None:
        """A task to send key events."""
        import unicodedata

        app = self
        driver = app._driver
        assert driver is not None
        for key in keys:
            if key.startswith("wait:"):
                _, wait_ms = key.split(":")
                await asyncio.sleep(float(wait_ms) / 1000)
                await app._animator.wait_until_complete()
            else:
                if len(key) == 1 and not key.isalnum():
                    key = _character_to_key(key)
                original_key = REPLACED_KEYS.get(key, key)
                char: str | None
                try:
                    char = unicodedata.lookup(_get_unicode_name_from_key(original_key))
                except KeyError:
                    char = key if len(key) == 1 else None
                key_event = events.Key(key, char)
                key_event.set_sender(app)
                driver.send_message(key_event)
                await wait_for_idle(0)
                await app._animator.wait_until_complete()
                await wait_for_idle(0)

    def _flush(self, stderr: bool = False) -> None:
        """Called when stdout or stderr is flushed.

        Args:
            stderr: True if the print was to stderr, or False for stdout.

        """
        if self._devtools_redirector is not None:
            self._devtools_redirector.flush()

    def _print(self, text: str, stderr: bool = False) -> None:
        """Called with captured print.

        Dispatches printed content to appropriate destinations: devtools,
        widgets currently capturing output, stdout/stderr.

        Args:
            text: Text that has been printed.
            stderr: True if the print was to stderr, or False for stdout.
        """
        if self._devtools_redirector is not None:
            current_frame = inspect.currentframe()
            self._devtools_redirector.write(
                text, current_frame.f_back if current_frame is not None else None
            )

        # If we're in headless mode, we want printed text to still reach stdout/stderr.
        if self.is_headless:
            target_stream = self._original_stderr if stderr else self._original_stdout
            target_stream.write(text)

        # Send Print events to all widgets that are currently capturing output.
        for target, (_stdout, _stderr) in self._capture_print.items():
            if (_stderr and stderr) or (_stdout and not stderr):
                target.post_message(events.Print(text, stderr=stderr))

    def begin_capture_print(
        self, target: MessageTarget, stdout: bool = True, stderr: bool = True
    ) -> None:
        """Capture content that is printed (or written to stdout / stderr).

        If printing is captured, the `target` will be sent an [events.Print][textual.events.Print] message.

        Args:
            target: The widget where print content will be sent.
            stdout: Capture stdout.
            stderr: Capture stderr.
        """
        if not stdout and not stderr:
            self.end_capture_print(target)
        else:
            self._capture_print[target] = (stdout, stderr)

    def end_capture_print(self, target: MessageTarget) -> None:
        """End capturing of prints.

        Args:
            target: The widget that was capturing prints.
        """
        self._capture_print.pop(target)

    @asynccontextmanager
    async def run_test(
        self,
        *,
        headless: bool = True,
        size: tuple[int, int] | None = (80, 24),
        tooltips: bool = False,
        notifications: bool = False,
        message_hook: Callable[[Message], None] | None = None,
    ) -> AsyncGenerator[Pilot[ReturnType], None]:
        """An asynchronous context manager for testing apps.

        !!! tip

            See the guide for [testing](/guide/testing) Textual apps.

        Use this to run your app in "headless" mode (no output) and drive the app via a [Pilot][textual.pilot.Pilot] object.

        Example:

            ```python
            async with app.run_test() as pilot:
                await pilot.click("#Button.ok")
                assert ...
            ```

        Args:
            headless: Run in headless mode (no output or input).
            size: Force terminal size to `(WIDTH, HEIGHT)`,
                or None to auto-detect.
            tooltips: Enable tooltips when testing.
            notifications: Enable notifications when testing.
            message_hook: An optional callback that will be called each time any message arrives at any
                message pump in the app.
        """
        from textual.pilot import Pilot

        app = self
        app._disable_tooltips = not tooltips
        app._disable_notifications = not notifications
        app_ready_event = asyncio.Event()

        def on_app_ready() -> None:
            """Called when app is ready to process events."""
            app_ready_event.set()

        async def run_app(app: App[ReturnType]) -> None:
            """Run the apps message loop.

            Args:
                app: App to run.
            """

            with app._context():
                try:
                    if message_hook is not None:
                        message_hook_context_var.set(message_hook)
                    app._loop = asyncio.get_running_loop()
                    app._thread_id = threading.get_ident()
                    await app._process_messages(
                        ready_callback=on_app_ready,
                        headless=headless,
                        terminal_size=size,
                    )
                finally:
                    app_ready_event.set()

        # Launch the app in the "background"

        self._task = app_task = create_task(run_app(app), name=f"run_test {app}")

        # Wait until the app has performed all startup routines.
        await app_ready_event.wait()
        with app._context():
            # Context manager returns pilot object to manipulate the app
            try:
                pilot = Pilot(app)
                await pilot._wait_for_screen()
                yield pilot
            finally:
                await asyncio.sleep(0)
                # Shutdown the app cleanly
                await app._shutdown()
                await app_task
                # Re-raise the exception which caused panic so test frameworks are aware
                if self._exception:
                    raise self._exception

    async def run_async(
        self,
        *,
        headless: bool = False,
        inline: bool = False,
        inline_no_clear: bool = False,
        mouse: bool = True,
        size: tuple[int, int] | None = None,
        auto_pilot: AutopilotCallbackType | None = None,
    ) -> ReturnType | None:
        """Run the app asynchronously.

        Args:
            headless: Run in headless mode (no output).
            inline: Run the app inline (under the prompt).
            inline_no_clear: Don't clear the app output when exiting an inline app.
            mouse: Enable mouse support.
            size: Force terminal size to `(WIDTH, HEIGHT)`,
                or None to auto-detect.
            auto_pilot: An autopilot coroutine.

        Returns:
            App return value.
        """
        from textual.pilot import Pilot

        app = self
        auto_pilot_task: Task | None = None

        if auto_pilot is None and constants.PRESS:
            keys = constants.PRESS.split(",")

            async def press_keys(pilot: Pilot[ReturnType]) -> None:
                """Auto press keys."""
                await pilot.press(*keys)

            auto_pilot = press_keys

        async def app_ready() -> None:
            """Called by the message loop when the app is ready."""
            nonlocal auto_pilot_task

            if auto_pilot is not None:

                async def run_auto_pilot(
                    auto_pilot: AutopilotCallbackType, pilot: Pilot
                ) -> None:
                    with self._context():
                        try:
                            await auto_pilot(pilot)
                        except Exception:
                            app.exit()
                            raise

                pilot = Pilot(app)
                auto_pilot_task = create_task(
                    run_auto_pilot(auto_pilot, pilot), name=repr(pilot)
                )

        self._thread_init()

        loop = app._loop = asyncio.get_running_loop()
        if hasattr(asyncio, "eager_task_factory"):
            loop.set_task_factory(asyncio.eager_task_factory)
        with app._context():
            try:
                await app._process_messages(
                    ready_callback=None if auto_pilot is None else app_ready,
                    headless=headless,
                    inline=inline,
                    inline_no_clear=inline_no_clear,
                    mouse=mouse,
                    terminal_size=size,
                )
            finally:
                try:
                    if auto_pilot_task is not None:
                        await auto_pilot_task
                finally:
                    try:
                        await asyncio.shield(app._shutdown())
                    except asyncio.CancelledError:
                        pass
                app._loop = None
                app._thread_id = 0

        return app.return_value

    def run(
        self,
        *,
        headless: bool = False,
        inline: bool = False,
        inline_no_clear: bool = False,
        mouse: bool = True,
        size: tuple[int, int] | None = None,
        auto_pilot: AutopilotCallbackType | None = None,
        loop: AbstractEventLoop | None = None,
    ) -> ReturnType | None:
        """Run the app.

        Args:
            headless: Run in headless mode (no output).
            inline: Run the app inline (under the prompt).
            inline_no_clear: Don't clear the app output when exiting an inline app.
            mouse: Enable mouse support.
            size: Force terminal size to `(WIDTH, HEIGHT)`,
                or None to auto-detect.
            auto_pilot: An auto pilot coroutine.
            loop: Asyncio loop instance, or `None` to use default.
        Returns:
            App return value.
        """

        async def run_app() -> ReturnType | None:
            """Run the app."""
            return await self.run_async(
                headless=headless,
                inline=inline,
                inline_no_clear=inline_no_clear,
                mouse=mouse,
                size=size,
                auto_pilot=auto_pilot,
            )

        if loop is None:
            if _ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED:
                # N.B. This does work with Python<3.10, but global Locks, Events, etc
                # eagerly bind the event loop, and result in Future bound to wrong
                # loop errors.
                return asyncio.run(run_app())
            try:
                global_loop = asyncio.get_event_loop()
            except RuntimeError:
                # the global event loop may have been destroyed by someone running
                # asyncio.run(), or asyncio.set_event_loop(None), in which case
                # we need to use asyncio.run() also. (We run this outside the
                # context of an exception handler)
                pass
            else:
                return global_loop.run_until_complete(run_app())
            return asyncio.run(run_app())
        return loop.run_until_complete(run_app())

    async def _on_css_change(self) -> None:
        """Callback for the file monitor, called when CSS files change."""
        css_paths = (
            self.css_monitor._paths if self.css_monitor is not None else self.css_path
        )
        if css_paths:
            try:
                time = perf_counter()
                stylesheet = self.stylesheet.copy()
                try:
                    stylesheet.read_all(css_paths)
                except StylesheetError as error:
                    # If one of the CSS paths is no longer available (or perhaps temporarily unavailable),
                    #  we'll end up with partial CSS, which is probably confusing more than anything. We opt to do
                    #  nothing here, knowing that we'll retry again very soon, on the next file monitor invocation.
                    #  Related issue: https://github.com/Textualize/textual/issues/3996
                    self.log.warning(str(error))
                    return
                stylesheet.parse()
                elapsed = (perf_counter() - time) * 1000
                if self._css_has_errors:
                    from rich.panel import Panel

                    self.log.system(
                        Panel(
                            "CSS files successfully loaded after previous error:\n\n- "
                            + "\n- ".join(str(path) for path in css_paths),
                            style="green",
                            border_style="green",
                        )
                    )
                self.log.system(
                    f"<stylesheet> loaded {len(css_paths)} CSS files in {elapsed:.0f} ms"
                )
            except Exception as error:
                # TODO: Catch specific exceptions
                self._css_has_errors = True
                self.log.error(error)
                self.bell()
            else:
                self._css_has_errors = False
                self.stylesheet = stylesheet
                self.stylesheet.update(self)
                for screen in self.screen_stack:
                    self.stylesheet.update(screen)

    def render(self) -> RenderResult:
        """Render method, inherited from widget, to render the screen's background.

        May be overridden to customize background visuals.

        """
        return Blank(self.styles.background)

    ExpectType = TypeVar("ExpectType", bound=Widget)

    if TYPE_CHECKING:

        @overload
        def get_child_by_id(self, id: str) -> Widget: ...

        @overload
        def get_child_by_id(
            self, id: str, expect_type: type[ExpectType]
        ) -> ExpectType: ...

    def get_child_by_id(
        self, id: str, expect_type: type[ExpectType] | None = None
    ) -> ExpectType | Widget:
        """Get the first child (immediate descendant) of this DOMNode with the given ID.

        Args:
            id: The ID of the node to search for.
            expect_type: Require the object be of the supplied type,
                or use `None` to apply no type restriction.

        Returns:
            The first child of this node with the specified ID.

        Raises:
            NoMatches: If no children could be found for this ID.
            WrongType: If the wrong type was found.
        """
        return (
            self.screen.get_child_by_id(id)
            if expect_type is None
            else self.screen.get_child_by_id(id, expect_type)
        )

    if TYPE_CHECKING:

        @overload
        def get_widget_by_id(self, id: str) -> Widget: ...

        @overload
        def get_widget_by_id(
            self, id: str, expect_type: type[ExpectType]
        ) -> ExpectType: ...

    def get_widget_by_id(
        self, id: str, expect_type: type[ExpectType] | None = None
    ) -> ExpectType | Widget:
        """Get the first descendant widget with the given ID.

        Performs a breadth-first search rooted at the current screen.
        It will not return the Screen if that matches the ID.
        To get the screen, use `self.screen`.

        Args:
            id: The ID to search for in the subtree
            expect_type: Require the object be of the supplied type, or None for any type.
                Defaults to None.

        Returns:
            The first descendant encountered with this ID.

        Raises:
            NoMatches: if no children could be found for this ID
            WrongType: if the wrong type was found.
        """
        return (
            self.screen.get_widget_by_id(id)
            if expect_type is None
            else self.screen.get_widget_by_id(id, expect_type)
        )

    def get_child_by_type(self, expect_type: type[ExpectType]) -> ExpectType:
        """Get a child of a give type.

        Args:
            expect_type: The type of the expected child.

        Raises:
            NoMatches: If no valid child is found.

        Returns:
            A widget.
        """
        return self.screen.get_child_by_type(expect_type)

    def update_styles(self, node: DOMNode) -> None:
        """Immediately update the styles of this node and all descendant nodes.

        Should be called whenever CSS classes / pseudo classes change.
        For example, when you hover over a button, the :hover pseudo class
        will be added, and this method is called to apply the corresponding
        :hover styles.
        """
        descendants = node.walk_children(with_self=True)
        self.stylesheet.update_nodes(descendants, animate=True)

    def mount(
        self,
        *widgets: Widget,
        before: int | str | Widget | None = None,
        after: int | str | Widget | None = None,
    ) -> AwaitMount:
        """Mount the given widgets relative to the app's screen.

        Args:
            *widgets: The widget(s) to mount.
            before: Optional location to mount before. An `int` is the index
                of the child to mount before, a `str` is a `query_one` query to
                find the widget to mount before.
            after: Optional location to mount after. An `int` is the index
                of the child to mount after, a `str` is a `query_one` query to
                find the widget to mount after.

        Returns:
            An awaitable object that waits for widgets to be mounted.

        Raises:
            MountError: If there is a problem with the mount request.

        Note:
            Only one of ``before`` or ``after`` can be provided. If both are
            provided a ``MountError`` will be raised.
        """
        return self.screen.mount(*widgets, before=before, after=after)

    def mount_all(
        self,
        widgets: Iterable[Widget],
        *,
        before: int | str | Widget | None = None,
        after: int | str | Widget | None = None,
    ) -> AwaitMount:
        """Mount widgets from an iterable.

        Args:
            widgets: An iterable of widgets.
            before: Optional location to mount before. An `int` is the index
                of the child to mount before, a `str` is a `query_one` query to
                find the widget to mount before.
            after: Optional location to mount after. An `int` is the index
                of the child to mount after, a `str` is a `query_one` query to
                find the widget to mount after.

        Returns:
            An awaitable object that waits for widgets to be mounted.

        Raises:
            MountError: If there is a problem with the mount request.

        Note:
            Only one of ``before`` or ``after`` can be provided. If both are
            provided a ``MountError`` will be raised.
        """
        return self.mount(*widgets, before=before, after=after)

    def _init_mode(self, mode: str) -> AwaitMount:
        """Do internal initialization of a new screen stack mode.

        Args:
            mode: Name of the mode.

        Returns:
            An optionally awaitable object which can be awaited until the screen
            associated with the mode has been mounted.
        """

        stack = self._screen_stacks.get(mode, [])
        if stack:
            # Mode already exists
            # Return an dummy await
            return AwaitMount(stack[0], [])

        if mode in self._modes:
            # Mode is defined in MODES
            _screen = self._modes[mode]
            if isinstance(_screen, Screen):
                raise TypeError(
                    "MODES cannot contain instances, use a type instead "
                    f"(got instance of {type(_screen).__name__} for {mode!r})"
                )
            new_screen: Screen | str = _screen() if callable(_screen) else _screen
            screen, await_mount = self._get_screen(new_screen)
            stack.append(screen)
            self._load_screen_css(screen)
            if screen._css_update_count != self._css_update_count:
                self.refresh_css()

            screen.post_message(events.ScreenResume())
        else:
            # Mode is not defined
            screen = self.get_default_screen()
            stack.append(screen)
            self._register(self, screen)
            screen.post_message(events.ScreenResume())
            await_mount = AwaitMount(stack[0], [])

        screen._screen_resized(self.size)

        self._screen_stacks[mode] = stack
        return await_mount

    def switch_mode(self, mode: str) -> AwaitMount:
        """Switch to a given mode.

        Args:
            mode: The mode to switch to.

        Returns:
            An optionally awaitable object which waits for the screen associated
                with the mode to be mounted.

        Raises:
            UnknownModeError: If trying to switch to an unknown mode.

        """

        if mode == self._current_mode:
            return AwaitMount(self.screen, [])

        if mode not in self._modes:
            raise UnknownModeError(f"No known mode {mode!r}")

        self.screen.post_message(events.ScreenSuspend())
        self.screen.refresh()

        if mode not in self._screen_stacks:
            await_mount = self._init_mode(mode)
        else:
            await_mount = AwaitMount(self.screen, [])

        self._current_mode = mode
        if self.screen._css_update_count != self._css_update_count:
            self.refresh_css()
        self.screen._screen_resized(self.size)
        self.screen.post_message(events.ScreenResume())

        self.log.system(f"{self._current_mode!r} is the current mode")
        self.log.system(f"{self.screen} is active")

        return await_mount

    def add_mode(self, mode: str, base_screen: str | Callable[[], Screen]) -> None:
        """Adds a mode and its corresponding base screen to the app.

        Args:
            mode: The new mode.
            base_screen: The base screen associated with the given mode.

        Raises:
            InvalidModeError: If the name of the mode is not valid/duplicated.
        """
        if mode == "_default":
            raise InvalidModeError("Cannot use '_default' as a custom mode.")
        elif mode in self._modes:
            raise InvalidModeError(f"Duplicated mode name {mode!r}.")

        if isinstance(base_screen, Screen):
            raise TypeError(
                "add_mode() must be called with a Screen type, not an instance"
                f" (got instance of {type(base_screen).__name__})"
            )
        self._modes[mode] = base_screen

    def remove_mode(self, mode: str) -> AwaitComplete:
        """Removes a mode from the app.

        Screens that are running in the stack of that mode are scheduled for pruning.

        Args:
            mode: The mode to remove. It can't be the active mode.

        Raises:
            ActiveModeError: If trying to remove the active mode.
            UnknownModeError: If trying to remove an unknown mode.
        """
        if mode == self._current_mode:
            raise ActiveModeError(f"Can't remove active mode {mode!r}")
        elif mode not in self._modes:
            raise UnknownModeError(f"Unknown mode {mode!r}")
        else:
            del self._modes[mode]

        if mode not in self._screen_stacks:
            return AwaitComplete.nothing()

        stack = self._screen_stacks[mode]
        del self._screen_stacks[mode]

        async def remove_screens() -> None:
            """Remove screens."""
            for screen in reversed(stack):
                await self._replace_screen(screen)

        return AwaitComplete(remove_screens()).call_next(self)

    def is_screen_installed(self, screen: Screen | str) -> bool:
        """Check if a given screen has been installed.

        Args:
            screen: Either a Screen object or screen name (the `name` argument when installed).

        Returns:
            True if the screen is currently installed,
        """
        if isinstance(screen, str):
            return screen in self._installed_screens
        else:
            return screen in self._installed_screens.values()

    @overload
    def get_screen(self, screen: ScreenType) -> ScreenType: ...

    @overload
    def get_screen(self, screen: str) -> Screen: ...

    @overload
    def get_screen(
        self, screen: str, screen_class: Type[ScreenType] | None = None
    ) -> ScreenType: ...

    @overload
    def get_screen(
        self, screen: ScreenType, screen_class: Type[ScreenType] | None = None
    ) -> ScreenType: ...

    def get_screen(
        self, screen: Screen | str, screen_class: Type[Screen] | None = None
    ) -> Screen:
        """Get an installed screen.

        Example:
            ```python
            my_screen = self.get_screen("settings", MyScreen)
            ```

        Args:
            screen: Either a Screen object or screen name (the `name` argument when installed).
            screen_class: Class of expected screen, or `None` for any screen class.

        Raises:
            KeyError: If the named screen doesn't exist.

        Returns:
            A screen instance.
        """
        if isinstance(screen, str):
            try:
                next_screen = self._installed_screens[screen]
            except KeyError:
                raise KeyError(f"No screen called {screen!r} installed") from None
            if callable(next_screen):
                next_screen = next_screen()
                self._installed_screens[screen] = next_screen
        else:
            next_screen = screen
        if screen_class is not None and not isinstance(next_screen, screen_class):
            raise TypeError(
                f"Expected a screen of type {screen_class}, got {type(next_screen)}"
            )
        return next_screen

    def _get_screen(self, screen: Screen | str) -> tuple[Screen, AwaitMount]:
        """Get an installed screen and an AwaitMount object.

        If the screen isn't running, it will be registered before it is run.

        Args:
            screen: Either a Screen object or screen name (the `name` argument when installed).

        Raises:
            KeyError: If the named screen doesn't exist.

        Returns:
            A screen instance and an awaitable that awaits the children mounting.
        """
        _screen = self.get_screen(screen)
        if not _screen.is_running:
            widgets = self._register(self, _screen)
            await_mount = AwaitMount(_screen, widgets)
            self.call_next(await_mount)
            return (_screen, await_mount)
        else:
            await_mount = AwaitMount(_screen, [])
            self.call_next(await_mount)
            return (_screen, await_mount)

    def _load_screen_css(self, screen: Screen):
        """Loads the CSS associated with a screen."""

        if self.css_monitor is not None:
            self.css_monitor.add_paths(screen.css_path)

        update = False
        for path in screen.css_path:
            if not self.stylesheet.has_source(str(path), ""):
                self.stylesheet.read(path)
                update = True
        if screen.CSS:
            try:
                screen_path = inspect.getfile(screen.__class__)
            except (TypeError, OSError):
                screen_path = ""
            screen_class_var = f"{screen.__class__.__name__}.CSS"
            read_from = (screen_path, screen_class_var)
            if not self.stylesheet.has_source(screen_path, screen_class_var):
                self.stylesheet.add_source(
                    screen.CSS,
                    read_from=read_from,
                    is_default_css=False,
                    scope=screen._css_type_name if screen.SCOPED_CSS else "",
                )
                update = True
        if update:
            self.stylesheet.reparse()
            self.stylesheet.update(self)

    async def _replace_screen(self, screen: Screen) -> Screen:
        """Handle the replaced screen.

        Args:
            screen: A screen object.

        Returns:
            The screen that was replaced.
        """
        if self._screen_stack:
            self.screen.refresh()
        screen.post_message(events.ScreenSuspend())
        self.log.system(f"{screen} SUSPENDED")
        if not self.is_screen_installed(screen) and all(
            screen not in stack for stack in self._screen_stacks.values()
        ):
            self.capture_mouse(None)
            await screen.remove()
            self.log.system(f"{screen} REMOVED")
        return screen

    if TYPE_CHECKING:

        @overload
        def push_screen(
            self,
            screen: Screen[ScreenResultType] | str,
            callback: ScreenResultCallbackType[ScreenResultType] | None = None,
            wait_for_dismiss: Literal[False] = False,
        ) -> AwaitMount: ...

        @overload
        def push_screen(
            self,
            screen: Screen[ScreenResultType] | str,
            callback: ScreenResultCallbackType[ScreenResultType] | None = None,
            wait_for_dismiss: Literal[True] = True,
        ) -> asyncio.Future[ScreenResultType]: ...

    def push_screen(
        self,
        screen: Screen[ScreenResultType] | str,
        callback: ScreenResultCallbackType[ScreenResultType] | None = None,
        wait_for_dismiss: bool = False,
    ) -> AwaitMount | asyncio.Future[ScreenResultType]:
        """Push a new [screen](/guide/screens) on the screen stack, making it the current screen.

        Args:
            screen: A Screen instance or the name of an installed screen.
            callback: An optional callback function that will be called if the screen is [dismissed][textual.screen.Screen.dismiss] with a result.
            wait_for_dismiss: If `True`, awaiting this method will return the dismiss value from the screen. When set to `False`, awaiting
                this method will wait for the screen to be mounted. Note that `wait_for_dismiss` should only be set to `True` when running in a worker.

        Raises:
            NoActiveWorker: If using `wait_for_dismiss` outside of a worker.

        Returns:
            An optional awaitable that awaits the mounting of the screen and its children, or an asyncio Future
                to await the result of the screen.
        """
        if not isinstance(screen, (Screen, str)):
            raise TypeError(
                f"push_screen requires a Screen instance or str; not {screen!r}"
            )

        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            # Mainly for testing, when push_screen isn't called in an async context
            future: asyncio.Future[ScreenResultType] = asyncio.Future()
        else:
            future = loop.create_future()

        self.app.capture_mouse(None)
        if self._screen_stack:
            self.screen.post_message(events.ScreenSuspend())
            self.screen.refresh()
        next_screen, await_mount = self._get_screen(screen)
        try:
            message_pump = active_message_pump.get()
        except LookupError:
            message_pump = self.app

        next_screen._push_result_callback(message_pump, callback, future)
        self._load_screen_css(next_screen)
        next_screen._update_auto_focus()
        self._screen_stack.append(next_screen)
        next_screen.post_message(events.ScreenResume())
        self.log.system(f"{self.screen} is current (PUSHED)")
        if wait_for_dismiss:
            try:
                get_current_worker()
            except NoActiveWorker:
                raise NoActiveWorker(
                    "push_screen must be run from a worker when `wait_for_dismiss` is True"
                ) from None
            return future
        else:
            return await_mount

    if TYPE_CHECKING:

        @overload
        async def push_screen_wait(
            self, screen: Screen[ScreenResultType]
        ) -> ScreenResultType: ...

        @overload
        async def push_screen_wait(self, screen: str) -> Any: ...

    async def push_screen_wait(
        self, screen: Screen[ScreenResultType] | str
    ) -> ScreenResultType | Any:
        """Push a screen and wait for the result (received from [`Screen.dismiss`][textual.screen.Screen.dismiss]).

        Note that this method may only be called when running in a worker.

        Args:
            screen: A screen or the name of an installed screen.

        Returns:
            The screen's result.
        """
        await self._flush_next_callbacks()
        # The shield prevents the cancellation of the current task from canceling the push_screen awaitable
        return await asyncio.shield(self.push_screen(screen, wait_for_dismiss=True))

    def switch_screen(self, screen: Screen | str) -> AwaitComplete:
        """Switch to another [screen](/guide/screens) by replacing the top of the screen stack with a new screen.

        Args:
            screen: Either a Screen object or screen name (the `name` argument when installed).
        """
        if not isinstance(screen, (Screen, str)):
            raise TypeError(
                f"switch_screen requires a Screen instance or str; not {screen!r}"
            )

        next_screen, await_mount = self._get_screen(screen)
        if screen is self.screen or next_screen is self.screen:
            self.log.system(f"Screen {screen} is already current.")
            return AwaitComplete.nothing()

        self.app.capture_mouse(None)
        top_screen = self._screen_stack.pop()

        top_screen._pop_result_callback()
        self._load_screen_css(next_screen)
        self._screen_stack.append(next_screen)
        self.screen.post_message(events.ScreenResume())
        self.screen._push_result_callback(self.screen, None)
        self.log.system(f"{self.screen} is current (SWITCHED)")

        async def do_switch() -> None:
            """Task to perform switch."""

            await await_mount()
            await self._replace_screen(top_screen)

        return AwaitComplete(do_switch()).call_next(self)

    def install_screen(self, screen: Screen, name: str) -> None:
        """Install a screen.

        Installing a screen prevents Textual from destroying it when it is no longer on the screen stack.
        Note that you don't need to install a screen to use it. See [push_screen][textual.app.App.push_screen]
        or [switch_screen][textual.app.App.switch_screen] to make a new screen current.

        Args:
            screen: Screen to install.
            name: Unique name to identify the screen.

        Raises:
            ScreenError: If the screen can't be installed.

        Returns:
            An awaitable that awaits the mounting of the screen and its children.
        """
        if name in self._installed_screens:
            raise ScreenError(f"Can't install screen; {name!r} is already installed")
        if screen in self._installed_screens.values():
            raise ScreenError(
                f"Can't install screen; {screen!r} has already been installed"
            )
        self._installed_screens[name] = screen
        self.log.system(f"{screen} INSTALLED name={name!r}")

    def uninstall_screen(self, screen: Screen | str) -> str | None:
        """Uninstall a screen.

        If the screen was not previously installed, then this method is a null-op.
        Uninstalling a screen allows Textual to delete it when it is popped or switched.
        Note that uninstalling a screen is only required if you have previously installed it
        with [install_screen][textual.app.App.install_screen].
        Textual will also uninstall screens automatically on exit.

        Args:
            screen: The screen to uninstall or the name of an installed screen.

        Returns:
            The name of the screen that was uninstalled, or None if no screen was uninstalled.
        """
        if isinstance(screen, str):
            if screen not in self._installed_screens:
                return None
            uninstall_screen = self._installed_screens[screen]
            if any(uninstall_screen in stack for stack in self._screen_stacks.values()):
                raise ScreenStackError("Can't uninstall screen in screen stack")
            del self._installed_screens[screen]
            self.log.system(f"{uninstall_screen} UNINSTALLED name={screen!r}")
            return screen
        else:
            if any(screen in stack for stack in self._screen_stacks.values()):
                raise ScreenStackError("Can't uninstall screen in screen stack")
            for name, installed_screen in self._installed_screens.items():
                if installed_screen is screen:
                    self._installed_screens.pop(name)
                    self.log.system(f"{screen} UNINSTALLED name={name!r}")
                    return name
        return None

    def pop_screen(self) -> AwaitComplete:
        """Pop the current [screen](/guide/screens) from the stack, and switch to the previous screen.

        Returns:
            The screen that was replaced.
        """

        screen_stack = self._screen_stack
        if len(screen_stack) <= 1:
            raise ScreenStackError(
                "Can't pop screen; there must be at least one screen on the stack"
            )

        previous_screen = screen_stack.pop()
        previous_screen._pop_result_callback()
        self.screen.post_message(events.ScreenResume())
        self.log.system(f"{self.screen} is active")

        async def do_pop() -> None:
            """Task to pop the screen."""
            await self._replace_screen(previous_screen)

        return AwaitComplete(do_pop()).call_next(self)

    def _pop_to_screen(self, screen: Screen) -> None:
        """Pop screens until the given screen is active.

        Args:
            screen: desired active screen

        Raises:
            ScreenError: If the screen doesn't exist in the stack.
        """
        screens_to_pop: list[Screen] = []
        for pop_screen in reversed(self.screen_stack):
            if pop_screen is not screen:
                screens_to_pop.append(pop_screen)
            else:
                break
        else:
            raise ScreenError(f"Screen {screen!r} not in screen stack")

        async def pop_screens() -> None:
            """Pop any screens in `screens_to_pop`."""
            with self.batch_update():
                for screen in screens_to_pop:
                    await screen.dismiss()

        if screens_to_pop:
            self.call_later(pop_screens)

    def set_focus(self, widget: Widget | None, scroll_visible: bool = True) -> None:
        """Focus (or unfocus) a widget. A focused widget will receive key events first.

        Args:
            widget: Widget to focus.
            scroll_visible: Scroll widget into view.
        """
        self.screen.set_focus(widget, scroll_visible)

    def _set_mouse_over(
        self, widget: Widget | None, hover_widget: Widget | None
    ) -> None:
        """Called when the mouse is over another widget.

        Args:
            widget: Widget under mouse, or None for no widgets.
        """
        if widget is None:
            if self.mouse_over is not None:
                try:
                    self.mouse_over.post_message(events.Leave(self.mouse_over))
                finally:
                    self.mouse_over = None
        else:
            if self.mouse_over is not widget:
                try:
                    if self.mouse_over is not None:
                        self.mouse_over.post_message(events.Leave(self.mouse_over))
                    if widget is not None:
                        widget.post_message(events.Enter(widget))
                finally:
                    self.mouse_over = widget
        if self.hover_over is not None:
            self.hover_over.mouse_hover = False
        if hover_widget is not None:
            hover_widget.mouse_hover = True

        self.hover_over = hover_widget

    def _update_mouse_over(self, screen: Screen) -> None:
        """Updates the mouse over after the next refresh.

        This method is called whenever a widget is added or removed, which may change
        the widget under the mouse.

        """

        if self.mouse_over is None or not screen.is_active:
            return

        async def check_mouse() -> None:
            """Check if the mouse over widget has changed."""
            try:
                hover_widgets = screen.get_hover_widgets_at(*self.mouse_position)
            except NoWidget:
                pass
            else:
                mouse_over, hover_over = hover_widgets.widgets
                if (
                    mouse_over is not self.mouse_over
                    or hover_over is not self.hover_over
                ):
                    self._set_mouse_over(mouse_over, hover_over)

        self.call_after_refresh(check_mouse)

    def capture_mouse(self, widget: Widget | None) -> None:
        """Send all mouse events to the given widget or disable mouse capture.

        Normally mouse events are sent to the widget directly under the pointer.
        Capturing the mouse allows a widget to receive mouse events even when the pointer is over another widget.

        Args:
            widget: Widget to capture mouse events, or `None` to end mouse capture.
        """
        if widget == self.mouse_captured:
            return
        if self.mouse_captured is not None:
            self.mouse_captured.post_message(events.MouseRelease(self.mouse_position))
        self.mouse_captured = widget
        if widget is not None:
            widget.post_message(events.MouseCapture(self.mouse_position))

    def panic(self, *renderables: RenderableType) -> None:
        """Exits the app and display error message(s).

        Used in response to unexpected errors.
        For a more graceful exit, see the [exit][textual.app.App.exit] method.

        Args:
            *renderables: Text or Rich renderable(s) to display on exit.
        """
        assert all(
            is_renderable(renderable) for renderable in renderables
        ), "Can only call panic with strings or Rich renderables"

        def render(renderable: RenderableType) -> list[Segment]:
            """Render a panic renderables."""
            segments = list(self.console.render(renderable, self.console.options))
            return segments

        pre_rendered = [Segments(render(renderable)) for renderable in renderables]
        self._exit_renderables.extend(pre_rendered)

        self._close_messages_no_wait()

    def _handle_exception(self, error: Exception) -> None:
        """Called with an unhandled exception.

        Always results in the app exiting.

        Args:
            error: An exception instance.
        """
        self._return_code = 1
        # If we're running via pilot and this is the first exception encountered,
        # take note of it so that we can re-raise for test frameworks later.
        if self._exception is None:
            self._exception = error
            self._exception_event.set()

        if hasattr(error, "__rich__"):
            # Exception has a rich method, so we can defer to that for the rendering
            self.panic(error)
        else:
            # Use default exception rendering
            self._fatal_error()

    def _fatal_error(self) -> None:
        """Exits the app after an unhandled exception."""
        from rich.traceback import Traceback

        self.bell()
        traceback = Traceback(
            show_locals=True, width=None, locals_max_length=5, suppress=[rich]
        )
        self._exit_renderables.append(
            Segments(self.console.render(traceback, self.console.options))
        )
        self._close_messages_no_wait()

    def _print_error_renderables(self) -> None:
        """Print and clear exit renderables."""
        error_count = len(self._exit_renderables)
        if "debug" in self.features:
            for renderable in self._exit_renderables:
                self.error_console.print(renderable)
            if error_count > 1:
                self.error_console.print(
                    f"\n[b]NOTE:[/b] {error_count} errors shown above.", markup=True
                )
        elif self._exit_renderables:
            self.error_console.print(self._exit_renderables[0])
            if error_count > 1:
                self.error_console.print(
                    f"\n[b]NOTE:[/b] 1 of {error_count} errors shown. Run with [b]textual run --dev[/] to see all errors.",
                    markup=True,
                )

        self._exit_renderables.clear()

    def _build_driver(
        self, headless: bool, inline: bool, mouse: bool, size: tuple[int, int] | None
    ) -> Driver:
        """Construct a driver instance.

        Args:
            headless: Request headless driver.
            inline: Request inline driver.
            mouse: Request mouse support.
            size: Initial size.

        Returns:
            Driver instance.
        """
        driver: Driver
        driver_class: type[Driver]
        if headless:
            from textual.drivers.headless_driver import HeadlessDriver

            driver_class = HeadlessDriver
        elif inline and not WINDOWS:
            from textual.drivers.linux_inline_driver import LinuxInlineDriver

            driver_class = LinuxInlineDriver
        else:
            driver_class = self.driver_class

        driver = self._driver = driver_class(
            self,
            debug=constants.DEBUG,
            mouse=mouse,
            size=size,
        )
        return driver

    async def _init_devtools(self):
        """Initialize developer tools."""
        if self.devtools is not None:
            from textual_dev.client import DevtoolsConnectionError

            try:
                await self.devtools.connect()
                self.log.system(f"Connected to devtools ( {self.devtools.url} )")
            except DevtoolsConnectionError:
                self.log.system(f"Couldn't connect to devtools ( {self.devtools.url} )")

    async def _process_messages(
        self,
        ready_callback: CallbackType | None = None,
        headless: bool = False,
        inline: bool = False,
        inline_no_clear: bool = False,
        mouse: bool = True,
        terminal_size: tuple[int, int] | None = None,
        message_hook: Callable[[Message], None] | None = None,
    ) -> None:
        self._thread_init()

        async def app_prelude() -> bool:
            """Work required before running the app.

            Returns:
                `True` if the app should continue, or `False` if there was a problem starting.
            """
            await self._init_devtools()
            self.log.system("---")
            self.log.system(loop=asyncio.get_running_loop())
            self.log.system(features=self.features)
            if constants.LOG_FILE is not None:
                _log_path = os.path.abspath(constants.LOG_FILE)
                self.log.system(f"Writing logs to {_log_path!r}")

            try:
                if self.css_path:
                    self.stylesheet.read_all(self.css_path)
                for read_from, css, tie_breaker, scope in self._get_default_css():
                    self.stylesheet.add_source(
                        css,
                        read_from=read_from,
                        is_default_css=True,
                        tie_breaker=tie_breaker,
                        scope=scope,
                    )
                if self.CSS:
                    try:
                        app_path = inspect.getfile(self.__class__)
                    except (TypeError, OSError):
                        app_path = ""
                    read_from = (app_path, f"{self.__class__.__name__}.CSS")
                    self.stylesheet.add_source(
                        self.CSS, read_from=read_from, is_default_css=False
                    )
            except Exception as error:
                self._handle_exception(error)
                self._print_error_renderables()
                return False

            if self.css_monitor:
                self.set_interval(0.25, self.css_monitor, name="css monitor")
                self.log.system("STARTED", self.css_monitor)
            return True

        async def run_process_messages():
            """The main message loop, invoke below."""

            async def invoke_ready_callback() -> None:
                if ready_callback is not None:
                    ready_result = ready_callback()
                    if inspect.isawaitable(ready_result):
                        await ready_result

            with self.batch_update():
                try:
                    try:
                        await self._dispatch_message(events.Compose())
                        await self._dispatch_message(
                            events.Resize.from_dimensions(self.size, None)
                        )
                        default_screen = self.screen
                        self.stylesheet.apply(self)
                        await self._dispatch_message(events.Mount())
                        self.check_idle()
                    finally:
                        self._mounted_event.set()
                        self._is_mounted = True

                    Reactive._initialize_object(self)

                    if self.screen is not default_screen:
                        self.stylesheet.apply(default_screen)

                    await self.animator.start()

                except Exception:
                    await self.animator.stop()
                    raise

                finally:
                    self._running = True
                    await self._ready()
                    await invoke_ready_callback()

            try:
                await self._process_messages_loop()
            except asyncio.CancelledError:
                pass
            finally:
                self.workers.cancel_all()
                self._running = False
                try:
                    await self.animator.stop()
                finally:
                    await Timer._stop_all(self._timers)

        with self._context():
            if not await app_prelude():
                return
            self._running = True
            try:
                load_event = events.Load()
                await self._dispatch_message(load_event)

                driver = self._driver = self._build_driver(
                    headless=headless,
                    inline=inline,
                    mouse=mouse,
                    size=terminal_size,
                )
                self.log(driver=driver)

                if not self._exit:
                    driver.start_application_mode()
                    try:
                        with redirect_stdout(self._capture_stdout):
                            with redirect_stderr(self._capture_stderr):
                                await run_process_messages()

                    finally:
                        Reactive._clear_watchers(self)
                        if self._driver.is_inline:
                            cursor_x, cursor_y = self._previous_cursor_position
                            self._driver.write(
                                Control.move(-cursor_x, -cursor_y).segment.text
                            )
                            self._driver.flush()
                            if inline_no_clear and not self.app._exit_renderables:
                                console = Console()
                                try:
                                    console.print(self.screen._compositor)
                                except ScreenStackError:
                                    console.print()
                            else:
                                self._driver.write(
                                    Control.move(0, -self.INLINE_PADDING).segment.text
                                )

                        driver.stop_application_mode()
            except Exception as error:
                self._handle_exception(error)

    async def _pre_process(self) -> bool:
        """Special case for the app, which doesn't need the functionality in MessagePump."""
        return True

    async def _ready(self) -> None:
        """Called immediately prior to processing messages.

        May be used as a hook for any operations that should run first.
        """

        ready_time = (perf_counter() - self._start_time) * 1000
        self.log.system(f"ready in {ready_time:0.0f} milliseconds")

        async def take_screenshot() -> None:
            """Take a screenshot and exit."""
            self.save_screenshot(
                path=constants.SCREENSHOT_LOCATION,
                filename=constants.SCREENSHOT_FILENAME,
            )
            self.exit()

        if constants.SCREENSHOT_DELAY >= 0:
            self.set_timer(
                constants.SCREENSHOT_DELAY, take_screenshot, name="screenshot timer"
            )

    async def _on_compose(self) -> None:
        _rich_traceback_omit = True
        self._compose_screen = self.screen
        try:
            widgets = [*self.screen._nodes, *compose(self)]
        except TypeError as error:
            raise TypeError(
                f"{self!r} compose() method returned an invalid result; {error}"
            ) from error

        await self.mount_all(widgets)

    async def _check_recompose(self) -> None:
        """Check if a recompose is required."""
        if self._recompose_required:
            self._recompose_required = False
            await self.recompose()

    async def recompose(self) -> None:
        """Recompose the widget.

        Recomposing will remove children and call `self.compose` again to remount.
        """
        if self._exit:
            return
        try:
            async with self.screen.batch():
                await self.screen.query("*").exclude(".-textual-system").remove()
                await self.screen.mount_all(compose(self))
        except ScreenStackError:
            pass

    def _register_child(
        self, parent: DOMNode, child: Widget, before: int | None, after: int | None
    ) -> None:
        """Register a widget as a child of another.

        Args:
            parent: Parent node.
            child: The child widget to register.
            widgets: The widget to register.
            before: A location to mount before.
            after: A location to mount after.
        """

        # Let's be 100% sure that we've not been asked to do a before and an
        # after at the same time. It's possible that we can remove this
        # check later on, but for the purposes of development right now,
        # it's likely a good idea to keep it here to check assumptions in
        # the rest of the code.
        if before is not None and after is not None:
            raise AppError("Only one of 'before' and 'after' may be specified.")

        # If we don't already know about this widget...
        if child not in self._registry:
            # Now to figure out where to place it. If we've got a `before`...
            if before is not None:
                # ...it's safe to NodeList._insert before that location.
                parent._nodes._insert(before, child)
            elif after is not None and after != -1:
                # In this case we've got an after. -1 holds the special
                # position (for now) of meaning "okay really what I mean is
                # do an append, like if I'd asked to add with no before or
                # after". So... we insert before the next item in the node
                # list, if after isn't -1.
                parent._nodes._insert(after + 1, child)
            else:
                # At this point we appear to not be adding before or after,
                # or we've got a before/after value that really means
                # "please append". So...
                parent._nodes._append(child)

            # Now that the widget is in the NodeList of its parent, sort out
            # the rest of the admin.
            self._registry.add(child)
            child._attach(parent)
            child._post_register(self)

    def _register(
        self,
        parent: DOMNode,
        *widgets: Widget,
        before: int | None = None,
        after: int | None = None,
        cache: dict[tuple, RulesMap] | None = None,
    ) -> list[Widget]:
        """Register widget(s) so they may receive events.

        Args:
            parent: Parent node.
            *widgets: The widget(s) to register.
            before: A location to mount before.
            after: A location to mount after.
            cache: Optional rules map cache.

        Returns:
            List of modified widgets.
        """

        if not widgets:
            return []

        if cache is None:
            cache = {}
        widget_list: Iterable[Widget]
        if before is not None or after is not None:
            # There's a before or after, which means there's going to be an
            # insertion, so make it easier to get the new things in the
            # correct order.
            widget_list = reversed(widgets)
        else:
            widget_list = widgets

        apply_stylesheet = self.stylesheet.apply
        new_widgets: list[Widget] = []
        add_new_widget = new_widgets.append
        for widget in widget_list:
            widget._closing = False
            widget._closed = False
            widget._pruning = False
            if not isinstance(widget, Widget):
                raise AppError(f"Can't register {widget!r}; expected a Widget instance")
            if widget not in self._registry:
                add_new_widget(widget)
                self._register_child(parent, widget, before, after)
                if widget._nodes:
                    self._register(widget, *widget._nodes, cache=cache)
        for widget in new_widgets:
            apply_stylesheet(widget, cache=cache)
            widget._start_messages()

        if not self._running:
            # If the app is not running, prevent awaiting of the widget tasks
            return []

        return list(widgets)

    def _unregister(self, widget: Widget) -> None:
        """Unregister a widget.

        Args:
            widget: A Widget to unregister
        """
        widget.blur()
        if isinstance(widget._parent, Widget):
            widget._parent._nodes._remove(widget)
            widget._detach()
        self._registry.discard(widget)

    async def _disconnect_devtools(self):
        if self.devtools is not None:
            await self.devtools.disconnect()

    def _start_widget(self, parent: Widget, widget: Widget) -> None:
        """Start a widget (run its task) so that it can receive messages.

        Args:
            parent: The parent of the Widget.
            widget: The Widget to start.
        """

        widget._attach(parent)
        widget._start_messages()
        self.app._registry.add(widget)

    def is_mounted(self, widget: Widget) -> bool:
        """Check if a widget is mounted.

        Args:
            widget: A widget.

        Returns:
            True of the widget is mounted.
        """
        return widget in self._registry

    async def _close_all(self) -> None:
        """Close all message pumps."""

        # Close all screens on all stacks:
        for stack in self._screen_stacks.values():
            for stack_screen in reversed(stack):
                if stack_screen._running:
                    await self._prune(stack_screen)
            stack.clear()
        self._installed_screens.clear()
        self._modes.clear()

        # Close any remaining nodes
        # Should be empty by now
        remaining_nodes = list(self._registry)
        for child in remaining_nodes:
            await child._close_messages()

    async def _shutdown(self) -> None:
        self._begin_batch()  # Prevents any layout / repaint while shutting down
        driver = self._driver
        self._running = False
        if driver is not None:
            driver.disable_input()

        await self._close_all()
        await self._close_messages()
        await self._dispatch_message(events.Unmount())

        if self._driver is not None:
            self._driver.close()

        self._nodes._clear()

        if self.devtools is not None and self.devtools.is_connected:
            await self._disconnect_devtools()

        self._print_error_renderables()

        if constants.SHOW_RETURN:
            from rich.console import Console
            from rich.pretty import Pretty

            console = Console()
            console.print("[b]The app returned:")
            console.print(Pretty(self._return_value))

    async def _on_exit_app(self) -> None:
        self._begin_batch()  # Prevent repaint / layout while shutting down
        self._message_queue.put_nowait(None)

    def refresh(
        self,
        *,
        repaint: bool = True,
        layout: bool = False,
        recompose: bool = False,
    ) -> Self:
        """Refresh the entire screen.

        Args:
            repaint: Repaint the widget (will call render() again).
            layout: Also layout widgets in the view.
            recompose: Re-compose the widget (will remove and re-mount children).

        Returns:
            The `App` instance.
        """
        if recompose:
            self._recompose_required = recompose
            self.call_next(self._check_recompose)
            return self

        if self._screen_stack:
            self.screen.refresh(repaint=repaint, layout=layout)
        self.check_idle()
        return self

    def refresh_css(self, animate: bool = True) -> None:
        """Refresh CSS.

        Args:
            animate: Also execute CSS animations.
        """
        stylesheet = self.app.stylesheet
        stylesheet.set_variables(self.get_css_variables())
        stylesheet.reparse()
        stylesheet.update(self.app, animate=animate)
        try:
            if self.screen.is_mounted:
                self.screen._refresh_layout(self.size)
                self.screen._css_update_count = self._css_update_count
        except ScreenError:
            pass
        # The other screens in the stack will need to know about some style
        # changes, as a final pass let's check in on every screen that isn't
        # the current one and update them too.
        for screen in self.screen_stack:
            if screen != self.screen:
                stylesheet.update(screen, animate=animate)
                screen._css_update_count = self._css_update_count

    def _display(self, screen: Screen, renderable: RenderableType | None) -> None:
        """Display a renderable within a sync.

        Args:
            screen: Screen instance
            renderable: A Rich renderable.
        """

        try:
            if renderable is None:
                return
            if self._batch_count:
                return
            if (
                self._running
                and not self._closed
                and not self.is_headless
                and self._driver is not None
            ):
                console = self.console
                self._begin_update()
                try:
                    try:
                        if isinstance(renderable, CompositorUpdate):
                            cursor_position = self.screen.outer_size.clamp_offset(
                                self.cursor_position
                            )
                            if self._driver.is_inline:
                                terminal_sequence = Control.move(
                                    *(-self._previous_cursor_position)
                                ).segment.text
                                terminal_sequence += renderable.render_segments(console)
                                terminal_sequence += Control.move(
                                    *cursor_position
                                ).segment.text
                            else:
                                terminal_sequence = renderable.render_segments(console)
                                terminal_sequence += Control.move_to(
                                    *cursor_position
                                ).segment.text
                            self._previous_cursor_position = cursor_position
                        else:
                            segments = console.render(renderable)
                            terminal_sequence = console._render_buffer(segments)
                    except Exception as error:
                        self._handle_exception(error)
                    else:
                        if WINDOWS:
                            # Combat a problem with Python on Windows.
                            #
                            # https://github.com/Textualize/textual/issues/2548
                            # https://github.com/python/cpython/issues/82052
                            CHUNK_SIZE = 8192
                            write = self._driver.write
                            for chunk in (
                                terminal_sequence[offset : offset + CHUNK_SIZE]
                                for offset in range(
                                    0, len(terminal_sequence), CHUNK_SIZE
                                )
                            ):
                                write(chunk)
                        else:
                            self._driver.write(terminal_sequence)
                finally:
                    self._end_update()

                self._driver.flush()

        finally:
            self.post_display_hook()

    def post_display_hook(self) -> None:
        """Called immediately after a display is done. Used in tests."""

    def get_widget_at(self, x: int, y: int) -> tuple[Widget, Region]:
        """Get the widget under the given coordinates.

        Args:
            x: X coordinate.
            y: Y coordinate.

        Returns:
            The widget and the widget's screen region.
        """
        return self.screen.get_widget_at(x, y)

    def bell(self) -> None:
        """Play the console 'bell'.

        For terminals that support a bell, this typically makes a notification or error sound.
        Some terminals may make no sound or display a visual bell indicator, depending on configuration.
        """
        if not self.is_headless and self._driver is not None:
            self._driver.write("\07")

    @property
    def _binding_chain(self) -> list[tuple[DOMNode, BindingsMap]]:
        """Get a chain of nodes and bindings to consider.

        If no widget is focused, returns the bindings from both the screen and the app level bindings.
        Otherwise, combines all the bindings from the currently focused node up the DOM to the root App.
        """
        focused = self.focused
        namespace_bindings: list[tuple[DOMNode, BindingsMap]]

        if focused is None:
            namespace_bindings = [
                (self.screen, self.screen._bindings),
                (self, self._bindings),
            ]
        else:
            namespace_bindings = [
                (node, node._bindings) for node in focused.ancestors_with_self
            ]

        return namespace_bindings

    def simulate_key(self, key: str) -> None:
        """Simulate a key press.

        This will perform the same action as if the user had pressed the key.

        Args:
            key: Key to simulate. May also be the name of a key, e.g. "space".
        """
        self.post_message(events.Key(key, None))

    async def _check_bindings(self, key: str, priority: bool = False) -> bool:
        """Handle a key press.

        This method is used internally by the bindings system.

        Args:
            key: A key.
            priority: If `True` check from `App` down, otherwise from focused up.

        Returns:
            True if the key was handled by a binding, otherwise False
        """
        for namespace, bindings in (
            reversed(self.screen._binding_chain)
            if priority
            else self.screen._modal_binding_chain
        ):
            key_bindings = bindings.key_to_bindings.get(key, ())
            for binding in key_bindings:
                if binding.priority == priority:
                    if await self.run_action(binding.action, namespace):
                        return True
        return False

    def action_help_quit(self) -> None:
        """Bound to ctrl+C to alert the user that it no longer quits."""
        # Doing this because users will reflexively hit ctrl+C to exit
        # Ctrl+C is now bound to copy if an input / textarea is focused.
        # This makes is possible, even likely, that a user may do it accidentally -- which would be maddening.
        # Rather than do nothing, we can make an educated guess the user was trying
        # to quit, and inform them how you really quit.
        for key, active_binding in self.active_bindings.items():
            if active_binding.binding.action in ("quit", "app.quit"):
                self.notify(
                    f"Press [b]{key}[/b] to quit the app", title="Do you want to quit?"
                )
                return

    @classmethod
    def _normalize_keymap(cls, keymap: Keymap) -> Keymap:
        """Normalizes the keys in a keymap, so they use long form, i.e. "question_mark" rather than "?"."""
        return {
            binding_id: _normalize_key_list(keys) for binding_id, keys in keymap.items()
        }

    def set_keymap(self, keymap: Keymap) -> None:
        """Set the keymap, a mapping of binding IDs to key strings.

        Bindings in the keymap are used to override default key bindings,
        i.e. those defined in `BINDINGS` class variables.

        Bindings with IDs that are present in the keymap will have
        their key string replaced with the value from the keymap.

        Args:
            keymap: A mapping of binding IDs to key strings.
        """

        self._keymap = self._normalize_keymap(keymap)
        self.refresh_bindings()

    def update_keymap(self, keymap: Keymap) -> None:
        """Update the App's keymap, merging with `keymap`.

        If a Binding ID exists in both the App's keymap and the `keymap`
        argument, the `keymap` argument takes precedence.

        Args:
            keymap: A mapping of binding IDs to key strings.
        """

        self._keymap = {**self._keymap, **self._normalize_keymap(keymap)}
        self.refresh_bindings()

    def handle_bindings_clash(
        self, clashed_bindings: set[Binding], node: DOMNode
    ) -> None:
        """Handle a clash between bindings.

        Bindings clashes are likely due to users setting conflicting
        keys via their keymap.

        This method is intended to be overridden by subclasses.

        Textual will call this each time a clash is encountered -
        which may be on each keypress if a clashing widget is focused
        or is in the bindings chain.

        Args:
            clashed_bindings: The bindings that are clashing.
            node: The node that has the clashing bindings.
        """
        pass

    async def on_event(self, event: events.Event) -> None:
        # Handle input events that haven't been forwarded
        # If the event has been forwarded it may have bubbled up back to the App
        if isinstance(event, events.Compose):
            await self._init_mode(self._current_mode)
            await super().on_event(event)
        elif isinstance(event, events.InputEvent) and not event.is_forwarded:
            if not self.app_focus and isinstance(event, (events.Key, events.MouseDown)):
                self.app_focus = True
            if isinstance(event, events.MouseEvent):
                # Record current mouse position on App
                self.mouse_position = Offset(event.x, event.y)
                if isinstance(event, events.MouseDown):
                    try:
                        self._mouse_down_widget, _ = self.get_widget_at(
                            event.x, event.y
                        )
                    except NoWidget:
                        # Shouldn't occur, since at the very least this will find the Screen
                        self._mouse_down_widget = None

                self.screen._forward_event(event)

                # If a MouseUp occurs at the same widget as a MouseDown, then we should
                # consider it a click, and produce a Click event.
                if (
                    isinstance(event, events.MouseUp)
                    and self._mouse_down_widget is not None
                ):
                    try:
                        screen_offset = event.screen_offset
                        mouse_down_widget = self._mouse_down_widget
                        mouse_up_widget, _ = self.get_widget_at(*screen_offset)
                        if mouse_up_widget is mouse_down_widget:
                            same_offset = (
                                self._click_chain_last_offset is not None
                                and self._click_chain_last_offset == screen_offset
                            )
                            within_time_threshold = (
                                self._click_chain_last_time is not None
                                and event.time - self._click_chain_last_time
                                <= self.CLICK_CHAIN_TIME_THRESHOLD
                            )

                            if same_offset and within_time_threshold:
                                self._chained_clicks += 1
                            else:
                                self._chained_clicks = 1

                            click_event = events.Click.from_event(
                                mouse_down_widget, event, chain=self._chained_clicks
                            )

                            self._click_chain_last_time = event.time
                            self._click_chain_last_offset = screen_offset

                            self.screen._forward_event(click_event)
                    except NoWidget:
                        pass

            elif isinstance(event, events.Key):
                # Special case for maximized widgets
                # If something is maximized, then escape should minimize
                if (
                    self.screen.maximized is not None
                    and event.key == "escape"
                    and self.escape_to_minimize
                ):
                    self.screen.minimize()
                    return
                if self.focused:
                    try:
                        self.screen._clear_tooltip()
                    except NoScreen:
                        pass
                if not await self._check_bindings(event.key, priority=True):
                    forward_target = self.focused or self.screen
                    forward_target._forward_event(event)
            else:
                self.screen._forward_event(event)

        elif isinstance(event, events.Paste) and not event.is_forwarded:
            if self.focused is not None:
                self.focused._forward_event(event)
            else:
                self.screen._forward_event(event)
        else:
            await super().on_event(event)

    @property
    def escape_to_minimize(self) -> bool:
        """Use the escape key to minimize?

        When a widget is [maximized][textual.screen.Screen.maximize], this boolean determines if the `escape` key will
        minimize the widget (potentially overriding any bindings).

        The default logic is to use the screen's `ESCAPE_TO_MINIMIZE` classvar if it is set to `True` or `False`.
        If the classvar on the screen is *not* set (and left as `None`), then the app's `ESCAPE_TO_MINIMIZE` is used.

        """
        return bool(
            self.ESCAPE_TO_MINIMIZE
            if self.screen.ESCAPE_TO_MINIMIZE is None
            else self.screen.ESCAPE_TO_MINIMIZE
        )

    def _parse_action(
        self,
        action: str | ActionParseResult,
        default_namespace: DOMNode,
        namespaces: Mapping[str, DOMNode] | None = None,
    ) -> tuple[DOMNode, str, tuple[object, ...]]:
        """Parse an action.

        Args:
            action: An action string.
            default_namespace: Namespace to user when none is supplied in the action.
            namespaces: Mapping of namespaces.

        Raises:
            ActionError: If there are any errors parsing the action string.

        Returns:
            A tuple of (node or None, action name, tuple of parameters).
        """
        if isinstance(action, tuple):
            destination, action_name, params = action
        else:
            destination, action_name, params = actions.parse(action)

        action_target: DOMNode | None = (
            None if namespaces is None else namespaces.get(destination)
        )
        if destination and action_target is None:
            if destination not in self._action_targets:
                raise ActionError(f"Action namespace {destination} is not known")
            action_target = getattr(self, destination, None)
            if action_target is None:
                raise ActionError(f"Action target {destination!r} not available")
        return (
            (default_namespace if action_target is None else action_target),
            action_name,
            params,
        )

    def _check_action_state(
        self, action: str, default_namespace: DOMNode
    ) -> bool | None:
        """Check if an action is enabled.

        Args:
            action: An action string.
            default_namespace: The default namespace if one is not specified in the action.

        Returns:
            State of an action.
        """
        action_target, action_name, parameters = self._parse_action(
            action, default_namespace
        )
        return action_target.check_action(action_name, parameters)

    async def run_action(
        self,
        action: str | ActionParseResult,
        default_namespace: DOMNode | None = None,
        namespaces: Mapping[str, DOMNode] | None = None,
    ) -> bool:
        """Perform an [action](/guide/actions).

        Actions are typically associated with key bindings, where you wouldn't need to call this method manually.

        Args:
            action: Action encoded in a string.
            default_namespace: Namespace to use if not provided in the action,
                or None to use app.
            namespaces: Mapping of namespaces.

        Returns:
            True if the event has been handled.
        """
        action_target, action_name, params = self._parse_action(
            action, self if default_namespace is None else default_namespace, namespaces
        )
        if action_target.check_action(action_name, params):
            return await self._dispatch_action(action_target, action_name, params)
        else:
            return False

    async def _dispatch_action(
        self, namespace: DOMNode, action_name: str, params: Any
    ) -> bool:
        """Dispatch an action to an action method.

        Args:
            namespace: Namespace (object) of action.
            action_name: Name of the action.
            params: Action parameters.

        Returns:
            True if handled, otherwise False.
        """
        _rich_traceback_guard = True

        log.system(
            "<action>",
            namespace=namespace,
            action_name=action_name,
            params=params,
        )

        try:
            private_method = getattr(namespace, f"_action_{action_name}", None)
            if callable(private_method):
                await invoke(private_method, *params)
                return True
            public_method = getattr(namespace, f"action_{action_name}", None)
            if callable(public_method):
                await invoke(public_method, *params)
                return True
            log.system(
                f"<action> {action_name!r} has no target."
                f" Could not find methods '_action_{action_name}' or 'action_{action_name}'"
            )
        except SkipAction:
            # The action method raised this to explicitly not handle the action
            log.system(f"<action> {action_name!r} skipped.")

        return False

    async def _broker_event(
        self, event_name: str, event: events.Event, default_namespace: DOMNode
    ) -> bool:
        """Allow the app an opportunity to dispatch events to action system.

        Args:
            event_name: _description_
            event: An event object.
            default_namespace: The default namespace, where one isn't supplied.

        Returns:
            True if an action was processed.
        """
        try:
            style = getattr(event, "style")
        except AttributeError:
            return False
        try:
            _modifiers, action = extract_handler_actions(event_name, style.meta)
        except NoHandler:
            return False
        else:
            event.stop()

        if isinstance(action, str):
            await self.run_action(action, default_namespace)
        elif isinstance(action, tuple) and len(action) == 2:
            action_name, action_params = action
            namespace, parsed_action, _ = actions.parse(action_name)
            await self.run_action(
                (namespace, parsed_action, action_params),
                default_namespace,
            )
        else:
            if isinstance(action, tuple) and self.debug:
                # It's a tuple and made it this far, which means it'll be a
                # malformed action. This is a no-op, but let's log that
                # anyway.
                log.warning(
                    f"Can't parse @{event_name} action from style meta; check your console markup syntax"
                )
            return False
        return True

    async def _on_update(self, message: messages.Update) -> None:
        message.stop()

    async def _on_layout(self, message: messages.Layout) -> None:
        message.stop()

    async def _on_key(self, event: events.Key) -> None:
        if not (await self._check_bindings(event.key)):
            await dispatch_key(self, event)

    async def _on_resize(self, event: events.Resize) -> None:
        event.stop()
        self._size = event.size
        self._resize_event = event

    async def _on_app_focus(self, event: events.AppFocus) -> None:
        """App has focus."""
        # Required by textual-web to manage focus in a web page.
        self.app_focus = True
        self.screen.refresh_bindings()

    async def _on_app_blur(self, event: events.AppBlur) -> None:
        """App has lost focus."""
        # Required by textual-web to manage focus in a web page.
        self.app_focus = False
        self.screen.refresh_bindings()

    def _prune(self, *nodes: Widget, parent: DOMNode | None = None) -> AwaitRemove:
        """Prune nodes from DOM.

        Args:
            parent: Parent node.

        Returns:
            Optional awaitable.
        """
        if not nodes:
            return AwaitRemove([])
        pruning_nodes: set[Widget] = {*nodes}
        for node in nodes:
            node.post_message(Prune())
            pruning_nodes.update(node.walk_children(with_self=True))

        try:
            screen = nodes[0].screen
        except (ScreenStackError, NoScreen):
            pass
        else:
            if screen.focused and screen.focused in pruning_nodes:
                screen._reset_focus(screen.focused, list(pruning_nodes))

        for node in pruning_nodes:
            node._pruning = True

        def post_mount() -> None:
            """Called after removing children."""

            if parent is not None:
                try:
                    screen = parent.screen
                except (ScreenStackError, NoScreen):
                    pass
                else:
                    if screen._running:
                        self._update_mouse_over(screen)
                finally:
                    parent.refresh(layout=True)

        await_complete = AwaitRemove(
            [task for node in nodes if (task := node._task) is not None],
            post_mount,
        )
        self.call_next(await_complete)
        return await_complete

    def _watch_app_focus(self, focus: bool) -> None:
        """Respond to changes in app focus."""
        self.screen._update_styles()
        if focus:
            # If we've got a last-focused widget, if it still has a screen,
            # and if the screen is still the current screen and if nothing
            # is focused right now...
            try:
                if (
                    self._last_focused_on_app_blur is not None
                    and self._last_focused_on_app_blur.screen is self.screen
                    and self.screen.focused is None
                ):
                    # ...settle focus back on that widget.
                    # Don't scroll the newly focused widget, as this can be quite jarring
                    self.screen.set_focus(
                        self._last_focused_on_app_blur,
                        scroll_visible=False,
                        from_app_focus=True,
                    )
            except NoScreen:
                pass
            # Now that we have focus back on the app and we don't need the
            # widget reference any more, don't keep it hanging around here.
            self._last_focused_on_app_blur = None
        else:
            # Remember which widget has focus, when the app gets focus back
            # we'll want to try and focus it again.
            self._last_focused_on_app_blur = self.screen.focused
            # Remove focus for now.
            self.screen.set_focus(None)

    async def action_simulate_key(self, key: str) -> None:
        """An [action](/guide/actions) to simulate a key press.

        This will invoke the same actions as if the user had pressed the key.

        Args:
            key: The key to process.
        """
        self.simulate_key(key)

    async def action_quit(self) -> None:
        """An [action](/guide/actions) to quit the app as soon as possible."""
        self.exit()

    async def action_bell(self) -> None:
        """An [action](/guide/actions) to play the terminal 'bell'."""
        self.bell()

    async def action_focus(self, widget_id: str) -> None:
        """An [action](/guide/actions) to focus the given widget.

        Args:
            widget_id: ID of widget to focus.
        """
        try:
            node = self.query(f"#{widget_id}").first()
        except NoMatches:
            pass
        else:
            if isinstance(node, Widget):
                self.set_focus(node)

    async def action_switch_screen(self, screen: str) -> None:
        """An [action](/guide/actions) to switch screens.

        Args:
            screen: Name of the screen.
        """
        self.switch_screen(screen)

    async def action_push_screen(self, screen: str) -> None:
        """An [action](/guide/actions) to push a new screen on to the stack and make it active.

        Args:
            screen: Name of the screen.
        """
        self.push_screen(screen)

    async def action_pop_screen(self) -> None:
        """An [action](/guide/actions) to remove the topmost screen and makes the new topmost screen active."""
        self.pop_screen()

    async def action_switch_mode(self, mode: str) -> None:
        """An [action](/guide/actions) that switches to the given mode."""
        self.switch_mode(mode)

    async def action_back(self) -> None:
        """An [action](/guide/actions) to go back to the previous screen (pop the current screen).

        Note:
            If there is no screen to go back to, this is a non-operation (in
            other words it's safe to call even if there are no other screens
            on the stack.)
        """
        try:
            self.pop_screen()
        except ScreenStackError:
            pass

    async def action_add_class(self, selector: str, class_name: str) -> None:
        """An [action](/guide/actions) to add a CSS class to the selected widget.

        Args:
            selector: Selects the widget to add the class to.
            class_name: The class to add to the selected widget.
        """
        self.screen.query(selector).add_class(class_name)

    async def action_remove_class(self, selector: str, class_name: str) -> None:
        """An [action](/guide/actions) to remove a CSS class from the selected widget.

        Args:
            selector: Selects the widget to remove the class from.
            class_name: The class to remove from  the selected widget."""
        self.screen.query(selector).remove_class(class_name)

    async def action_toggle_class(self, selector: str, class_name: str) -> None:
        """An [action](/guide/actions) to toggle a CSS class on the selected widget.

        Args:
            selector: Selects the widget to toggle the class on.
            class_name: The class to toggle on the selected widget.
        """
        self.screen.query(selector).toggle_class(class_name)

    def action_toggle_dark(self) -> None:
        """An [action](/guide/actions) to toggle the theme between textual-light
        and textual-dark. This is offered as a convenience to simplify backwards
        compatibility with previous versions of Textual which only had light mode
        and dark mode."""
        self.theme = (
            "textual-dark" if self.theme == "textual-light" else "textual-light"
        )

    def action_focus_next(self) -> None:
        """An [action](/guide/actions) to focus the next widget."""
        self.screen.focus_next()

    def action_focus_previous(self) -> None:
        """An [action](/guide/actions) to focus the previous widget."""
        self.screen.focus_previous()

    def action_hide_help_panel(self) -> None:
        """Hide the keys panel (if present)."""
        self.screen.query("HelpPanel").remove()

    def action_show_help_panel(self) -> None:
        """Show the keys panel."""
        from textual.widgets import HelpPanel

        try:
            self.screen.query_one(HelpPanel)
        except NoMatches:
            self.screen.mount(HelpPanel())

    def action_notify(
        self, message: str, title: str = "", severity: str = "information"
    ) -> None:
        """Show a notification."""
        self.notify(message, title=title, severity=severity)

    def _on_terminal_supports_synchronized_output(
        self, message: messages.TerminalSupportsSynchronizedOutput
    ) -> None:
        log.system("SynchronizedOutput mode is supported")
        if self._driver is not None and not self._driver.is_inline:
            self._sync_available = True

    def _begin_update(self) -> None:
        if self._sync_available and self._driver is not None:
            self._driver.write(SYNC_START)

    def _end_update(self) -> None:
        if self._sync_available and self._driver is not None:
            self._driver.write(SYNC_END)

    def _refresh_notifications(self) -> None:
        """Refresh the notifications on the current screen, if one is available."""
        # If we've got a screen to hand...
        try:
            screen = self.screen
        except ScreenStackError:
            pass
        else:
            try:
                # ...see if it has a toast rack.
                toast_rack = screen.get_child_by_type(ToastRack)
            except NoMatches:
                # It doesn't. That's fine. Either there won't ever be one,
                # or one will turn up. Things will work out later.
                return
            # Update the toast rack.
            self.call_later(toast_rack.show, self._notifications)

    def clear_selection(self) -> None:
        """Clear text selection on the active screen."""
        try:
            self.screen.clear_selection()
        except NoScreen:
            pass

    def notify(
        self,
        message: str,
        *,
        title: str = "",
        severity: SeverityLevel = "information",
        timeout: float | None = None,
        markup: bool = True,
    ) -> None:
        """Create a notification.

        !!! tip

            This method is thread-safe.


        Args:
            message: The message for the notification.
            title: The title for the notification.
            severity: The severity of the notification.
            timeout: The timeout (in seconds) for the notification, or `None` for default.
            markup: Render the message as content markup?

        The `notify` method is used to create an application-wide
        notification, shown in a [`Toast`][textual.widgets._toast.Toast],
        normally originating in the bottom right corner of the display.

        Notifications can have the following severity levels:

        - `information`
        - `warning`
        - `error`

        The default is `information`.

        Example:
            ```python
            # Show an information notification.
            self.notify("It's an older code, sir, but it checks out.")

            # Show a warning. Note that Textual's notification system allows
            # for the use of Rich console markup.
            self.notify(
                "Now witness the firepower of this fully "
                "[b]ARMED[/b] and [i][b]OPERATIONAL[/b][/i] battle station!",
                title="Possible trap detected",
                severity="warning",
            )

            # Show an error. Set a longer timeout so it's noticed.
            self.notify("It's a trap!", severity="error", timeout=10)

            # Show an information notification, but without any sort of title.
            self.notify("It's against my programming to impersonate a deity.", title="")
            ```
        """
        if timeout is None:
            timeout = self.NOTIFICATION_TIMEOUT
        notification = Notification(message, title, severity, timeout, markup=markup)
        self.post_message(Notify(notification))

    def _on_notify(self, event: Notify) -> None:
        """Handle notification message."""
        self._notifications.add(event.notification)
        self._refresh_notifications()

    def _unnotify(self, notification: Notification, refresh: bool = True) -> None:
        """Remove a notification from the notification collection.

        Args:
            notification: The notification to remove.
            refresh: Flag to say if the display of notifications should be refreshed.
        """
        del self._notifications[notification]
        if refresh:
            self._refresh_notifications()

    def clear_notifications(self) -> None:
        """Clear all the current notifications."""
        self._notifications.clear()
        self._refresh_notifications()

    def action_command_palette(self) -> None:
        """Show the Textual command palette."""
        if self.use_command_palette and not CommandPalette.is_open(self):
            self.push_screen(CommandPalette(id="--command-palette"))

    def _suspend_signal(self) -> None:
        """Signal that the application is being suspended."""
        self.app_suspend_signal.publish(self)

    @on(Driver.SignalResume)
    def _resume_signal(self) -> None:
        """Signal that the application is being resumed from a suspension."""
        self.app_resume_signal.publish(self)

    @contextmanager
    def suspend(self) -> Iterator[None]:
        """A context manager that temporarily suspends the app.

        While inside the `with` block, the app will stop reading input and
        emitting output. Other applications will have full control of the
        terminal, configured as it was before the app started running. When
        the `with` block ends, the application will start reading input and
        emitting output again.

        Example:
            ```python
            with self.suspend():
                os.system("emacs -nw")
            ```

        Raises:
            SuspendNotSupported: If the environment doesn't support suspending.

        !!! note
            Suspending the application is currently only supported on
            Unix-like operating systems and Microsoft Windows. Suspending is
            not supported in Textual Web.
        """
        if self._driver is None:
            return
        if self._driver.can_suspend:
            # Publish a suspend signal *before* we suspend application mode.
            self._suspend_signal()
            self._driver.suspend_application_mode()
            # We're going to handle the start of the driver again so mark
            # this next part as such; the reason for this is that the code
            # the developer may be running could be in this process, and on
            # Unix-like systems the user may `action_suspend_process` the
            # app, and we don't want to have the driver auto-restart
            # application mode when the application comes back to the
            # foreground, in this context.
            with (
                self._driver.no_automatic_restart(),
                redirect_stdout(sys.__stdout__),
                redirect_stderr(sys.__stderr__),
            ):
                yield
            # We're done with the dev's code so resume application mode.
            self._driver.resume_application_mode()
            # ...and publish a resume signal.
            self._resume_signal()
            self.refresh(layout=True)
        else:
            raise SuspendNotSupported(
                "App.suspend is not supported in this environment."
            )

    def action_suspend_process(self) -> None:
        """Suspend the process into the background.

        Note:
            On Unix and Unix-like systems a `SIGTSTP` is sent to the
            application's process. Currently on Windows and when running
            under Textual Web this is a non-operation.
        """
        # Check if we're in an environment that permits this kind of
        # suspend.
        if not WINDOWS and self._driver is not None and self._driver.can_suspend:
            # First, ensure that the suspend signal gets published while
            # we're still in application mode.
            self._suspend_signal()
            # With that out of the way, send the SIGTSTP signal.
            os.kill(os.getpid(), signal.SIGTSTP)
            # NOTE: There is no call to publish the resume signal here, this
            # will be handled by the driver posting a SignalResume event
            # (see the event handler on App._resume_signal) above.

    def open_url(self, url: str, *, new_tab: bool = True) -> None:
        """Open a URL in the default web browser.

        Args:
            url: The URL to open.
            new_tab: Whether to open the URL in a new tab.
        """
        if self._driver is not None:
            self._driver.open_url(url, new_tab)

    def deliver_text(
        self,
        path_or_file: str | Path | TextIO,
        *,
        save_directory: str | Path | None = None,
        save_filename: str | None = None,
        open_method: Literal["browser", "download"] = "download",
        encoding: str | None = None,
        mime_type: str | None = None,
        name: str | None = None,
    ) -> str | None:
        """Deliver a text file to the end-user of the application.

        If a TextIO object is supplied, it will be closed by this method
        and *must not be used* after this method is called.

        If running in a terminal, this will save the file to the user's
        downloads directory.

        If running via a web browser, this will initiate a download via
        a single-use URL.

        After the file has been delivered, a `DeliveryComplete` message will be posted
        to this `App`, which contains the `delivery_key` returned by this method. By
        handling this message, you can add custom logic to your application that fires
        only after the file has been delivered.

        Args:
            path_or_file: The path or file-like object to save.
            save_directory: The directory to save the file to.
            save_filename: The filename to save the file to.  If `path_or_file`
                is a file-like object, the filename will be generated from
                the `name` attribute if available. If `path_or_file` is a path
                the filename will be generated from the path.
            encoding: The encoding to use when saving the file. If `None`,
                the encoding will be determined by supplied file-like object
                (if possible). If this is not possible, 'utf-8' will be used.
            mime_type: The MIME type of the file or None to guess based on file extension.
                If no MIME type is supplied and we cannot guess the MIME type, from the
                file extension, the MIME type will be set to "text/plain".
            name: A user-defined named which will be returned in [`DeliveryComplete`][textual.events.DeliveryComplete]
                and [`DeliveryComplete`][textual.events.DeliveryComplete].

        Returns:
            The delivery key that uniquely identifies the file delivery.
        """
        # Ensure `path_or_file` is a file-like object - convert if needed.
        if isinstance(path_or_file, (str, Path)):
            binary_path = Path(path_or_file)
            binary = binary_path.open("rb")
            file_name = save_filename or binary_path.name
        else:
            encoding = encoding or getattr(path_or_file, "encoding", None) or "utf-8"
            binary = path_or_file
            file_name = save_filename or getattr(path_or_file, "name", None)

        # If we could infer a filename, and no MIME type was supplied, guess the MIME type.
        if file_name and not mime_type:
            mime_type, _ = mimetypes.guess_type(file_name)

        # Still no MIME type? Default it to "text/plain".
        if mime_type is None:
            mime_type = "text/plain"

        return self._deliver_binary(
            binary,
            save_directory=save_directory,
            save_filename=file_name,
            open_method=open_method,
            encoding=encoding,
            mime_type=mime_type,
            name=name,
        )

    def deliver_binary(
        self,
        path_or_file: str | Path | BinaryIO,
        *,
        save_directory: str | Path | None = None,
        save_filename: str | None = None,
        open_method: Literal["browser", "download"] = "download",
        mime_type: str | None = None,
        name: str | None = None,
    ) -> str | None:
        """Deliver a binary file to the end-user of the application.

        If an IO object is supplied, it will be closed by this method
        and *must not be used* after it is supplied to this method.

        If running in a terminal, this will save the file to the user's
        downloads directory.

        If running via a web browser, this will initiate a download via
        a single-use URL.

        This operation runs in a thread when running on web, so this method
        returning does not indicate that the file has been delivered.

        After the file has been delivered, a `DeliveryComplete` message will be posted
        to this `App`, which contains the `delivery_key` returned by this method. By
        handling this message, you can add custom logic to your application that fires
        only after the file has been delivered.

        Args:
            path_or_file: The path or file-like object to save.
            save_directory: The directory to save the file to. If None,
                the default "downloads" directory will be used. This
                argument is ignored when running via the web.
            save_filename: The filename to save the file to. If None, the following logic
                applies to generate the filename:
                - If `path_or_file` is a file-like object, the filename will be taken from
                  the `name` attribute if available.
                - If `path_or_file` is a path, the filename will be taken from the path.
                - If a filename is not available, a filename will be generated using the
                  App's title and the current date and time.
            open_method: The method to use to open the file. "browser" will open the file in the
                web browser, "download" will initiate a download. Note that this can sometimes
                be impacted by the browser's settings.
            mime_type: The MIME type of the file or None to guess based on file extension.
                If no MIME type is supplied and we cannot guess the MIME type, from the
                file extension, the MIME type will be set to "application/octet-stream".
            name: A user-defined named which will be returned in [`DeliveryComplete`][textual.events.DeliveryComplete]
                and [`DeliveryComplete`][textual.events.DeliveryComplete].

        Returns:
            The delivery key that uniquely identifies the file delivery.
        """
        # Ensure `path_or_file` is a file-like object - convert if needed.
        if isinstance(path_or_file, (str, Path)):
            binary_path = Path(path_or_file)
            binary = binary_path.open("rb")
            file_name = save_filename or binary_path.name
        else:  # IO object
            binary = path_or_file
            file_name = save_filename or getattr(path_or_file, "name", None)

        # If we could infer a filename, and no MIME type was supplied, guess the MIME type.
        if file_name and not mime_type:
            mime_type, _ = mimetypes.guess_type(file_name)

        # Still no MIME type? Default it to "application/octet-stream".
        if mime_type is None:
            mime_type = "application/octet-stream"

        return self._deliver_binary(
            binary,
            save_directory=save_directory,
            save_filename=file_name,
            open_method=open_method,
            mime_type=mime_type,
            encoding=None,
            name=name,
        )

    def _deliver_binary(
        self,
        binary: BinaryIO | TextIO,
        *,
        save_directory: str | Path | None,
        save_filename: str | None,
        open_method: Literal["browser", "download"],
        encoding: str | None = None,
        mime_type: str | None = None,
        name: str | None = None,
    ) -> str | None:
        """Deliver a binary file to the end-user of the application."""
        if self._driver is None:
            return None

        # Generate a filename if the file-like object doesn't have one.
        if save_filename is None:
            save_filename = generate_datetime_filename(self.title, "")

        # Find the appropriate save location if not specified.
        save_directory = (
            user_downloads_path() if save_directory is None else Path(save_directory)
        )

        # Generate a unique key for this delivery
        delivery_key = str(uuid.uuid4().hex)

        # Save the file. The driver will determine the appropriate action
        # to take here. It could mean simply writing to the save_path, or
        # sending the file to the web browser for download.
        self._driver.deliver_binary(
            binary,
            delivery_key=delivery_key,
            save_path=save_directory / save_filename,
            encoding=encoding,
            open_method=open_method,
            mime_type=mime_type,
            name=name,
        )

        return delivery_key

    @on(events.DeliveryComplete)
    def _on_delivery_complete(self, event: events.DeliveryComplete) -> None:
        """Handle a successfully delivered screenshot."""
        if event.name == "screenshot":
            if event.path is None:
                self.notify("Saved screenshot", title="Screenshot")
            else:
                self.notify(
                    f"Saved screenshot to [green]{str(event.path)!r}",
                    title="Screenshot",
                )

    @on(events.DeliveryFailed)
    def _on_delivery_failed(self, event: events.DeliveryComplete) -> None:
        """Handle a failure to deliver the screenshot."""
        if event.name == "screenshot":
            self.notify(
                "Failed to save screenshot", title="Screenshot", severity="error"
            )

    @on(messages.InBandWindowResize)
    def _on_in_band_window_resize(self, message: messages.InBandWindowResize) -> None:
        """In band window resize enables smooth scrolling."""
        self.supports_smooth_scrolling = message.enabled
        self.log.debug(message)

    def _on_idle(self) -> None:
        """Send app resize events on idle, so we don't do more resizing that necessary."""
        event = self._resize_event
        if event is not None:
            self._resize_event = None
            self.screen.post_message(event)
            for screen in self._background_screens:
                screen.post_message(event)


--- src/textual/_arrange.py ---
from __future__ import annotations

from collections import defaultdict
from fractions import Fraction
from operator import attrgetter
from typing import TYPE_CHECKING, Iterable, Mapping, Sequence

from textual._partition import partition
from textual.geometry import NULL_OFFSET, NULL_SPACING, Region, Size, Spacing
from textual.layout import DockArrangeResult, WidgetPlacement

if TYPE_CHECKING:
    from textual.widget import Widget

# TODO: This is a bit of a fudge, need to ensure it is impossible for layouts to generate this value
TOP_Z = 2**31 - 1


def _build_layers(widgets: Iterable[Widget]) -> Mapping[str, Sequence[Widget]]:
    """Organize widgets into layers.

    Args:
        widgets: The widgets.

    Returns:
        A mapping of layer name onto the widgets within the layer.
    """
    layers: defaultdict[str, list[Widget]] = defaultdict(list)
    for widget in widgets:
        layers[widget.layer].append(widget)
    return layers


_get_dock = attrgetter("styles.is_docked")
_get_split = attrgetter("styles.is_split")
_get_display = attrgetter("display")


def arrange(
    widget: Widget,
    children: Sequence[Widget],
    size: Size,
    viewport: Size,
    optimal: bool = False,
) -> DockArrangeResult:
    """Arrange widgets by applying docks and calling layouts

    Args:
        widget: The parent (container) widget.
        size: The size of the available area.
        viewport: The size of the viewport (terminal).

    Returns:
        Widget arrangement information.
    """
    placements: list[WidgetPlacement] = []
    scroll_spacing = NULL_SPACING
    styles = widget.styles

    # Widgets which will be displayed
    display_widgets = list(filter(_get_display, children))
    # Widgets organized into layers
    layers = _build_layers(display_widgets)

    for widgets in layers.values():
        # Partition widgets into split widgets and non-split widgets
        non_split_widgets, split_widgets = partition(_get_split, widgets)
        if split_widgets:
            _split_placements, dock_region = _arrange_split_widgets(
                split_widgets, size, viewport
            )
            placements.extend(_split_placements)
        else:
            dock_region = size.region

        split_spacing = size.region.get_spacing_between(dock_region)

        # Partition widgets into "layout" widgets (those that appears in the normal 'flow' of the
        # document), and "dock" widgets which are positioned relative to an edge
        layout_widgets, dock_widgets = partition(_get_dock, non_split_widgets)

        # Arrange docked widgets
        if dock_widgets:
            _dock_placements, dock_spacing = _arrange_dock_widgets(
                dock_widgets, dock_region, viewport, greedy=not optimal
            )
            placements.extend(_dock_placements)
            dock_region = dock_region.shrink(dock_spacing)
        else:
            dock_spacing = Spacing()

        dock_spacing += split_spacing

        if layout_widgets:
            # Arrange layout widgets (i.e. not docked)
            layout_placements = widget.process_layout(
                widget.layout.arrange(
                    widget, layout_widgets, dock_region.size, greedy=not optimal
                )
            )
            scroll_spacing = scroll_spacing.grow_maximum(dock_spacing)
            placement_offset = dock_region.offset
            # Perform any alignment of the widgets.
            if styles.align_horizontal != "left" or styles.align_vertical != "top":
                bounding_region = WidgetPlacement.get_bounds(layout_placements)
                container_width, container_height = dock_region.size
                placement_offset += styles._align_size(
                    bounding_region.size,
                    widget._extrema.apply_dimensions(
                        0 if styles.is_auto_width else container_width,
                        0 if styles.is_auto_height else container_height,
                    ),
                ).clamped

            if placement_offset:
                # Translate placements if required.
                layout_placements = WidgetPlacement.translate(
                    layout_placements, placement_offset
                )

            WidgetPlacement.apply_absolute(layout_placements)
            placements.extend(layout_placements)

    return DockArrangeResult(placements, set(display_widgets), scroll_spacing)


def _arrange_dock_widgets(
    dock_widgets: Sequence[Widget], region: Region, viewport: Size, greedy: bool = True
) -> tuple[list[WidgetPlacement], Spacing]:
    """Arrange widgets which are *docked*.

    Args:
        dock_widgets: Widgets with a non-empty dock.
        region: Region to dock within.
        viewport: Size of the viewport.

    Returns:
        A tuple of widget placements, and additional spacing around them.
    """
    _WidgetPlacement = WidgetPlacement
    top_z = TOP_Z
    region_offset = region.offset
    size = region.size
    width, height = size
    null_spacing = NULL_SPACING

    top = right = bottom = left = 0

    placements: list[WidgetPlacement] = []
    append_placement = placements.append

    for dock_widget in dock_widgets:
        edge = dock_widget.styles.dock

        box_model = dock_widget._get_box_model(
            size, viewport, Fraction(size.width), Fraction(size.height), greedy=greedy
        )
        widget_width_fraction, widget_height_fraction, margin = box_model
        widget_width = int(widget_width_fraction) + margin.width
        widget_height = int(widget_height_fraction) + margin.height

        if edge == "bottom":
            dock_region = Region(0, height - widget_height, widget_width, widget_height)
            bottom = max(bottom, widget_height)
        elif edge == "top":
            dock_region = Region(0, 0, widget_width, widget_height)
            top = max(top, widget_height)
        elif edge == "left":
            dock_region = Region(0, 0, widget_width, widget_height)
            left = max(left, widget_width)
        elif edge == "right":
            dock_region = Region(width - widget_width, 0, widget_width, widget_height)
            right = max(right, widget_width)
        else:
            # Should not occur, mainly to keep Mypy happy
            raise AssertionError("invalid value for dock edge")  # pragma: no-cover

        dock_region = dock_region.shrink(margin)
        styles = dock_widget.styles
        offset = (
            styles.offset.resolve(
                size,
                viewport,
            )
            if styles.has_rule("offset")
            else NULL_OFFSET
        )
        append_placement(
            _WidgetPlacement(
                dock_region.translate(region_offset),
                offset,
                null_spacing,
                dock_widget,
                top_z,
                True,
                False,
            )
        )

    dock_spacing = Spacing(top, right, bottom, left)
    return (placements, dock_spacing)


def _arrange_split_widgets(
    split_widgets: Sequence[Widget], size: Size, viewport: Size
) -> tuple[list[WidgetPlacement], Region]:
    """Arrange split widgets.

    Split widgets are "docked" but also reduce the area available for regular widgets.

    Args:
        split_widgets: Widgets to arrange.
        size: Available area to arrange.
        viewport: Viewport (size of terminal).

    Returns:
        A tuple of widget placements, and the remaining view area.
    """
    _WidgetPlacement = WidgetPlacement
    placements: list[WidgetPlacement] = []
    append_placement = placements.append
    view_region = size.region
    null_spacing = NULL_SPACING
    null_offset = NULL_OFFSET

    for split_widget in split_widgets:
        split = split_widget.styles.split
        box_model = split_widget._get_box_model(
            size, viewport, Fraction(size.width), Fraction(size.height)
        )
        widget_width_fraction, widget_height_fraction, margin = box_model
        if split == "bottom":
            widget_height = int(widget_height_fraction) + margin.height
            view_region, split_region = view_region.split_horizontal(-widget_height)
        elif split == "top":
            widget_height = int(widget_height_fraction) + margin.height
            split_region, view_region = view_region.split_horizontal(widget_height)
        elif split == "left":
            widget_width = int(widget_width_fraction) + margin.width
            split_region, view_region = view_region.split_vertical(widget_width)
        elif split == "right":
            widget_width = int(widget_width_fraction) + margin.width
            view_region, split_region = view_region.split_vertical(-widget_width)
        else:
            raise AssertionError("invalid value for split edge")  # pragma: no-cover

        append_placement(
            _WidgetPlacement(
                split_region, null_offset, null_spacing, split_widget, 1, True, False
            )
        )

    return placements, view_region


--- src/textual/await_complete.py ---
from __future__ import annotations

from asyncio import Future, gather
from typing import TYPE_CHECKING, Any, Awaitable, Generator

import rich.repr
from typing_extensions import Self

from textual._debug import get_caller_file_and_line
from textual.message_pump import MessagePump

if TYPE_CHECKING:
    from textual.types import CallbackType


@rich.repr.auto(angular=True)
class AwaitComplete:
    """An 'optionally-awaitable' object which runs one or more coroutines (or other awaitables) concurrently."""

    def __init__(
        self, *awaitables: Awaitable, pre_await: CallbackType | None = None
    ) -> None:
        """Create an AwaitComplete.

        Args:
            awaitables: One or more awaitables to run concurrently.
        """
        self._awaitables = awaitables
        self._future: Future[Any] = gather(*awaitables)
        self._pre_await: CallbackType | None = pre_await
        self._caller = get_caller_file_and_line()

    def __rich_repr__(self) -> rich.repr.Result:
        yield self._awaitables
        yield "pre_await", self._pre_await, None
        yield "caller", self._caller, None

    def set_pre_await_callback(self, pre_await: CallbackType | None) -> None:
        """Set a callback to run prior to awaiting.

        This is used by Textual, mainly to check for possible deadlocks.
        You are unlikely to need to call this method in an app.

        Args:
            pre_await: A callback.
        """
        self._pre_await = pre_await

    def call_next(self, node: MessagePump) -> Self:
        """Await after the next message.

        Args:
            node: The node which created the object.
        """
        node.call_next(self)
        return self

    async def __call__(self) -> Any:
        return await self

    def __await__(self) -> Generator[Any, None, Any]:
        _rich_traceback_omit = True
        if self._pre_await is not None:
            self._pre_await()
        return self._future.__await__()

    @property
    def is_done(self) -> bool:
        """`True` if the task has completed."""
        return self._future.done()

    @property
    def exception(self) -> BaseException | None:
        """An exception if the awaitables failed."""
        if self._future.done():
            return self._future.exception()
        return None

    @classmethod
    def nothing(cls):
        """Returns an already completed instance of AwaitComplete."""
        instance = cls()
        instance._future = Future()
        instance._future.set_result(None)  # Mark it as completed with no result
        return instance


--- src/textual/await_remove.py ---
"""
An *optionally* awaitable object returned by methods that remove widgets.
"""

from __future__ import annotations

import asyncio
from asyncio import Task, gather
from typing import Generator

import rich.repr

from textual._callback import invoke
from textual._debug import get_caller_file_and_line
from textual._types import CallbackType


@rich.repr.auto
class AwaitRemove:
    """An awaitable that waits for nodes to be removed."""

    def __init__(
        self, tasks: list[Task], post_remove: CallbackType | None = None
    ) -> None:
        self._tasks = tasks
        self._post_remove = post_remove
        self._caller = get_caller_file_and_line()

    def __rich_repr__(self) -> rich.repr.Result:
        yield "tasks", self._tasks
        yield "post_remove", self._post_remove
        yield "caller", self._caller, None

    async def __call__(self) -> None:
        await self

    def __await__(self) -> Generator[None, None, None]:
        current_task = asyncio.current_task()
        tasks = [task for task in self._tasks if task is not current_task]

        async def await_prune() -> None:
            """Wait for the prune operation to finish."""
            await gather(*tasks)
            if self._post_remove is not None:
                await invoke(self._post_remove)

        return await_prune().__await__()


--- src/textual/_binary_encode.py ---
"""
An encoding / decoding format suitable for serializing data structures to binary.

This is based on https://en.wikipedia.org/wiki/Bencode with some extensions.

The following data types may be encoded:

- None
- int
- bool
- bytes
- str
- list
- tuple
- dict

"""

from __future__ import annotations

from typing import Any, Callable


class DecodeError(Exception):
    """A problem decoding data."""


def dump(data: object) -> bytes:
    """Encodes a data structure into bytes.

    Args:
        data: Data structure

    Returns:
        A byte string encoding the data.
    """

    def encode_none(_datum: None) -> bytes:
        """
        Encodes a None value.

        Args:
            datum: Always None.

        Returns:
            None encoded.
        """
        return b"N"

    def encode_bool(datum: bool) -> bytes:
        """
        Encode a boolean value.

        Args:
            datum: The boolean value to encode.

        Returns:
            The encoded bytes.
        """
        return b"T" if datum else b"F"

    def encode_int(datum: int) -> bytes:
        """
        Encode an integer value.

        Args:
            datum: The integer value to encode.

        Returns:
            The encoded bytes.
        """
        return b"i%ie" % datum

    def encode_bytes(datum: bytes) -> bytes:
        """
        Encode a bytes value.

        Args:
            datum: The bytes value to encode.

        Returns:
            The encoded bytes.
        """
        return b"%i:%s" % (len(datum), datum)

    def encode_string(datum: str) -> bytes:
        """
        Encode a string value.

        Args:
            datum: The string value to encode.

        Returns:
            The encoded bytes.
        """
        encoded_data = datum.encode("utf-8")
        return b"s%i:%s" % (len(encoded_data), encoded_data)

    def encode_list(datum: list) -> bytes:
        """
        Encode a list value.

        Args:
            datum: The list value to encode.

        Returns:
            The encoded bytes.
        """
        return b"l%se" % b"".join(encode(element) for element in datum)

    def encode_tuple(datum: tuple) -> bytes:
        """
        Encode a tuple value.

        Args:
            datum: The tuple value to encode.

        Returns:
            The encoded bytes.
        """
        return b"t%se" % b"".join(encode(element) for element in datum)

    def encode_dict(datum: dict) -> bytes:
        """
        Encode a dictionary value.

        Args:
            datum: The dictionary value to encode.

        Returns:
            The encoded bytes.
        """
        return b"d%se" % b"".join(
            b"%s%s" % (encode(key), encode(value)) for key, value in datum.items()
        )

    ENCODERS: dict[type, Callable[[Any], Any]] = {
        type(None): encode_none,
        bool: encode_bool,
        int: encode_int,
        bytes: encode_bytes,
        str: encode_string,
        list: encode_list,
        tuple: encode_tuple,
        dict: encode_dict,
    }

    def encode(datum: object) -> bytes:
        """Recursively encode data.

        Args:
            datum: Data suitable for encoding.

        Raises:
            TypeError: If `datum` is not one of the supported types.

        Returns:
            Encoded data bytes.
        """
        try:
            decoder = ENCODERS[type(datum)]
        except KeyError:
            raise TypeError("Can't encode {datum!r}") from None
        return decoder(datum)

    return encode(data)


def load(encoded: bytes) -> object:
    """Load an encoded data structure from bytes.

    Args:
        encoded: Encoded data in bytes.

    Raises:
        DecodeError: If an error was encountered decoding the string.

    Returns:
        Decoded data.
    """
    if not isinstance(encoded, bytes):
        raise TypeError("must be bytes")
    max_position = len(encoded)
    position = 0

    def get_byte() -> bytes:
        """Get an encoded byte and advance position.

        Raises:
            DecodeError: If the end of the data was reached

        Returns:
            A bytes object with a single byte.
        """
        nonlocal position
        if position >= max_position:
            raise DecodeError("More data expected")
        character = encoded[position : position + 1]
        position += 1
        return character

    def peek_byte() -> bytes:
        """Get the byte at the current position, but don't advance position.

        Returns:
            A bytes object with a single byte.
        """
        return encoded[position : position + 1]

    def get_bytes(size: int) -> bytes:
        """Get a number of bytes of encode data.

        Args:
            size: Number of bytes to retrieve.

        Raises:
            DecodeError: If there aren't enough bytes.

        Returns:
            A bytes object.
        """
        nonlocal position
        bytes_data = encoded[position : position + size]
        if len(bytes_data) != size:
            raise DecodeError(b"Missing bytes in {bytes_data!r}")
        position += size
        return bytes_data

    def decode_int() -> int:
        """Decode an int from the encoded data.

        Returns:
            An integer.
        """
        int_bytes = b""
        while (byte := get_byte()) != b"e":
            int_bytes += byte
        return int(int_bytes)

    def decode_bytes(size_bytes: bytes) -> bytes:
        """Decode a bytes string from the encoded data.

        Returns:
            A bytes object.
        """
        while (byte := get_byte()) != b":":
            size_bytes += byte
        bytes_string = get_bytes(int(size_bytes))
        return bytes_string

    def decode_string() -> str:
        """Decode a (utf-8 encoded) string from the encoded data.

        Returns:
            A string.
        """
        size_bytes = b""
        while (byte := get_byte()) != b":":
            size_bytes += byte
        bytes_string = get_bytes(int(size_bytes))
        decoded_string = bytes_string.decode("utf-8", errors="replace")
        return decoded_string

    def decode_list() -> list[object]:
        """Decode a list.

        Returns:
            A list of data.
        """
        elements: list[object] = []
        add_element = elements.append
        while peek_byte() != b"e":
            add_element(decode())
        get_byte()
        return elements

    def decode_tuple() -> tuple[object, ...]:
        """Decode a tuple.

        Returns:
            A tuple of decoded data.
        """
        elements: list[object] = []
        add_element = elements.append
        while peek_byte() != b"e":
            add_element(decode())
        get_byte()
        return tuple(elements)

    def decode_dict() -> dict[object, object]:
        """Decode a dict.

        Returns:
            A dict of decoded data.
        """
        elements: dict[object, object] = {}
        add_element = elements.__setitem__
        while peek_byte() != b"e":
            add_element(decode(), decode())
        get_byte()
        return elements

    DECODERS = {
        b"i": decode_int,
        b"s": decode_string,
        b"l": decode_list,
        b"t": decode_tuple,
        b"d": decode_dict,
        b"T": lambda: True,
        b"F": lambda: False,
        b"N": lambda: None,
    }

    def decode() -> object:
        """Recursively decode data.

        Returns:
            Decoded data.
        """
        decoder = DECODERS.get(initial := get_byte(), None)
        if decoder is None:
            return decode_bytes(initial)
        return decoder()

    return decode()


--- src/textual/binding.py ---
"""

This module contains the `Binding` class and related objects.

See [bindings](/guide/input#bindings) in the guide for details.
"""

from __future__ import annotations

import dataclasses
from dataclasses import dataclass
from typing import TYPE_CHECKING, Iterable, Iterator, Mapping, NamedTuple

import rich.repr

from textual.keys import _character_to_key

if TYPE_CHECKING:
    from typing_extensions import TypeAlias

    from textual.dom import DOMNode

BindingType: TypeAlias = "Binding | tuple[str, str] | tuple[str, str, str]"
"""The possible types of a binding found in the `BINDINGS` class variable."""

BindingIDString: TypeAlias = str
"""The ID of a Binding defined somewhere in the application.

Corresponds to the `id` parameter of the `Binding` class.
"""

KeyString: TypeAlias = str
"""A string that represents a key binding.

For example, "x", "ctrl+i", "ctrl+shift+a", "ctrl+j,space,x", etc.
"""

Keymap = Mapping[BindingIDString, KeyString]
"""A mapping of binding IDs to key strings, used for overriding default key bindings."""


class BindingError(Exception):
    """A binding related error."""


class NoBinding(Exception):
    """A binding was not found."""


class InvalidBinding(Exception):
    """Binding key is in an invalid format."""


@dataclass(frozen=True)
class Binding:
    """The configuration of a key binding."""

    key: str
    """Key to bind. This can also be a comma-separated list of keys to map multiple keys to a single action."""
    action: str
    """Action to bind to."""
    description: str = ""
    """Description of action."""
    show: bool = True
    """Show the action in Footer, or False to hide."""
    key_display: str | None = None
    """How the key should be shown in footer.

    If `None`, the display of the key will use the result of `App.get_key_display`.

    If overridden in a keymap then this value is ignored.
    """
    priority: bool = False
    """Enable priority binding for this key."""
    tooltip: str = ""
    """Optional tooltip to show in footer."""

    id: str | None = None
    """ID of the binding. Intended to be globally unique, but uniqueness is not enforced.

    If specified in the App's keymap then Textual will use this ID to lookup the binding,
    and substitute the `key` property of the Binding with the key specified in the keymap.
    """
    system: bool = False
    """Make this binding a system binding, which removes it from the key panel."""

    @dataclass(frozen=True)
    class Group:
        """A binding group causes the keys to be grouped under a single description."""

        description: str = ""
        """Description of the group."""

        compact: bool = False
        """Show keys in compact form (no spaces)."""

    group: Group | None = None
    """Optional binding group (used to group related bindings in the footer)."""

    def parse_key(self) -> tuple[list[str], str]:
        """Parse a key into a list of modifiers, and the actual key.

        Returns:
            A tuple of (MODIFIER LIST, KEY).
        """
        *modifiers, key = self.key.split("+")
        return modifiers, key

    def with_key(self, key: str, key_display: str | None = None) -> Binding:
        """Return a new binding with the key and key_display set to the specified values.

        Args:
            key: The new key to set.
            key_display: The new key display to set.

        Returns:
            A new binding with the key set to the specified value.
        """
        return dataclasses.replace(self, key=key, key_display=key_display)

    @classmethod
    def make_bindings(cls, bindings: Iterable[BindingType]) -> Iterable[Binding]:
        """Convert a list of BindingType (the types that can be specified in BINDINGS)
        into an Iterable[Binding].

        Compound bindings like "j,down" will be expanded into 2 Binding instances.

        Args:
            bindings: An iterable of BindingType.

        Returns:
            An iterable of Binding.
        """
        bindings = list(bindings)
        for binding in bindings:
            # If it's a tuple of length 3, convert into a Binding first
            if isinstance(binding, tuple):
                if len(binding) not in (2, 3):
                    raise BindingError(
                        f"BINDINGS must contain a tuple of two or three strings, not {binding!r}"
                    )
                # `binding` is a tuple of 2 or 3 values at this point
                binding = Binding(*binding)  # type: ignore[reportArgumentType]

            # At this point we have a Binding instance, but the key may
            # be a list of keys, so now we unroll that single Binding
            # into a (potential) collection of Binding instances.
            for key in binding.key.split(","):
                key = key.strip()
                if not key:
                    raise InvalidBinding(
                        f"Can not bind empty string in {binding.key!r}"
                    )
                if len(key) == 1:
                    key = _character_to_key(key)

                yield Binding(
                    key=key,
                    action=binding.action,
                    description=binding.description,
                    show=bool(binding.description and binding.show),
                    key_display=binding.key_display,
                    priority=binding.priority,
                    tooltip=binding.tooltip,
                    id=binding.id,
                    system=binding.system,
                    group=binding.group,
                )


class ActiveBinding(NamedTuple):
    """Information about an active binding (returned from [active_bindings][textual.screen.Screen.active_bindings])."""

    node: DOMNode
    """The node where the binding is defined."""
    binding: Binding
    """The binding information."""
    enabled: bool
    """Is the binding enabled? (enabled bindings are typically rendered dim)"""
    tooltip: str = ""
    """Optional tooltip shown in Footer."""


@rich.repr.auto
class BindingsMap:
    """Manage a set of bindings."""

    def __init__(
        self,
        bindings: Iterable[BindingType] | None = None,
    ) -> None:
        """Initialise a collection of bindings.

        Args:
            bindings: An optional set of initial bindings.

        Note:
            The iterable of bindings can contain either a `Binding`
            instance, or a tuple of 3 values mapping to the first three
            properties of a `Binding`.
        """

        self.key_to_bindings: dict[str, list[Binding]] = {}
        """Mapping of key (e.g. "ctrl+a") to list of bindings for that key."""

        for binding in Binding.make_bindings(bindings or {}):
            self.key_to_bindings.setdefault(binding.key, []).append(binding)

    def _add_binding(self, binding: Binding) -> None:
        """Add a new binding.

        Args:
            binding: New Binding to add.
        """
        self.key_to_bindings.setdefault(binding.key, []).append(binding)

    def __iter__(self) -> Iterator[tuple[str, Binding]]:
        """Iterating produces a sequence of (KEY, BINDING) tuples."""
        return iter(
            [
                (key, binding)
                for key, bindings in self.key_to_bindings.items()
                for binding in bindings
            ]
        )

    @classmethod
    def from_keys(cls, keys: dict[str, list[Binding]]) -> BindingsMap:
        """Construct a BindingsMap from a dict of keys and bindings.

        Args:
            keys: A dict that maps a key on to a list of `Binding` objects.

        Returns:
            New `BindingsMap`
        """
        bindings = cls()
        bindings.key_to_bindings = keys
        return bindings

    def copy(self) -> BindingsMap:
        """Return a copy of this instance.

        Return:
            New bindings object.
        """
        copy = BindingsMap()
        copy.key_to_bindings = self.key_to_bindings.copy()
        return copy

    def __rich_repr__(self) -> rich.repr.Result:
        yield self.key_to_bindings

    @classmethod
    def merge(cls, bindings: Iterable[BindingsMap]) -> BindingsMap:
        """Merge a bindings.

        Args:
            bindings: A number of bindings.

        Returns:
            New `BindingsMap`.
        """
        keys: dict[str, list[Binding]] = {}
        for _bindings in bindings:
            for key, key_bindings in _bindings.key_to_bindings.items():
                keys.setdefault(key, []).extend(key_bindings)
        return BindingsMap.from_keys(keys)

    def apply_keymap(self, keymap: Keymap) -> KeymapApplyResult:
        """Replace bindings for keys that are present in `keymap`.

        Preserves existing bindings for keys that are not in `keymap`.

        Args:
            keymap: A keymap to overlay.

        Returns:
            KeymapApplyResult: The result of applying the keymap, including any clashed bindings.
        """
        clashed_bindings: set[Binding] = set()
        new_bindings: dict[str, list[Binding]] = {}

        key_to_bindings = list(self.key_to_bindings.items())
        for key, bindings in key_to_bindings:
            for binding in bindings:
                binding_id = binding.id
                if binding_id is None:
                    # Bindings without an ID are irrelevant when applying a keymap
                    continue

                # If the keymap has an override for this binding ID
                if keymap_key_string := keymap.get(binding_id):
                    keymap_keys = keymap_key_string.split(",")

                    # Remove the old binding
                    for key, key_bindings in key_to_bindings:
                        key = key.strip()
                        if any(binding.id == binding_id for binding in key_bindings):
                            if key in self.key_to_bindings:
                                del self.key_to_bindings[key]

                    for keymap_key in keymap_keys:
                        if (
                            keymap_key in self.key_to_bindings
                            or keymap_key in new_bindings
                        ):
                            # The key is already mapped either by default or by the keymap,
                            # so there's a clash unless the existing binding is being rebound
                            # to a different key.
                            clashing_bindings = self.key_to_bindings.get(
                                keymap_key, []
                            ) + new_bindings.get(keymap_key, [])
                            for clashed_binding in clashing_bindings:
                                # If the existing binding is not being rebound, it's a clash
                                if not (
                                    clashed_binding.id
                                    and keymap.get(clashed_binding.id)
                                    != clashed_binding.key
                                ):
                                    clashed_bindings.add(clashed_binding)

                            if keymap_key in self.key_to_bindings:
                                del self.key_to_bindings[keymap_key]

                    for keymap_key in keymap_keys:
                        new_bindings.setdefault(keymap_key, []).append(
                            binding.with_key(key=keymap_key, key_display=None)
                        )

        # Update the key_to_bindings with the new bindings
        self.key_to_bindings.update(new_bindings)
        return KeymapApplyResult(clashed_bindings)

    @property
    def shown_keys(self) -> list[Binding]:
        """A list of bindings for shown keys."""
        keys = [
            binding
            for bindings in self.key_to_bindings.values()
            for binding in bindings
            if binding.show
        ]
        return keys

    def bind(
        self,
        keys: str,
        action: str,
        description: str = "",
        show: bool = True,
        key_display: str | None = None,
        priority: bool = False,
    ) -> None:
        """Bind keys to an action.

        Args:
            keys: The keys to bind. Can be a comma-separated list of keys.
            action: The action to bind the keys to.
            description: An optional description for the binding.
            show: A flag to say if the binding should appear in the footer.
            key_display: Optional string to display in the footer for the key.
            priority: Is this a priority binding, checked form app down to focused widget?
        """
        all_keys = [key.strip() for key in keys.split(",")]
        for key in all_keys:
            self.key_to_bindings.setdefault(key, []).append(
                Binding(
                    key,
                    action,
                    description,
                    show=bool(description and show),
                    key_display=key_display,
                    priority=priority,
                )
            )

    def get_bindings_for_key(self, key: str) -> list[Binding]:
        """Get a list of bindings for a given key.

        Args:
            key: Key to look up.

        Raises:
            NoBinding: If the binding does not exist.

        Returns:
            A list of bindings associated with the key.
        """
        try:
            return self.key_to_bindings[key]
        except KeyError:
            raise NoBinding(f"No binding for {key}") from None


class KeymapApplyResult(NamedTuple):
    """The result of applying a keymap."""

    clashed_bindings: set[Binding]
    """A list of bindings that were clashed and replaced by the keymap."""


--- tests/deadlock.py ---
"""
Called by test_pipe.py

"""

from textual.app import App
from textual.binding import Binding
from textual.widgets import Footer


class MyApp(App[None]):
    BINDINGS = [
        Binding(key="q", action="quit", description="Quit the app"),
    ]

    def compose(self):
        yield Footer()


app = MyApp()
app.run()


--- tests/__init__.py ---


--- tests/test_actions.py ---
from __future__ import annotations

from typing import Any

import pytest

from textual.actions import ActionError, parse


@pytest.mark.parametrize(
    ("action_string", "expected_namespace", "expected_name", "expected_arguments"),
    [
        ("spam", "", "spam", ()),
        ("hypothetical_action()", "", "hypothetical_action", ()),
        ("another_action(1)", "", "another_action", (1,)),
        ("foo(True, False)", "", "foo", (True, False)),
        ("foo.bar.baz(3, 3.15, 'Python')", "foo.bar", "baz", (3, 3.15, "Python")),
        ("m1234.n5678(None, [1, 2])", "m1234", "n5678", (None, [1, 2])),
    ],
)
def test_parse_action(
    action_string: str,
    expected_namespace: str,
    expected_name: str,
    expected_arguments: tuple[Any],
) -> None:
    namespace, action_name, action_arguments = parse(action_string)
    assert namespace == expected_namespace
    assert action_name == expected_name
    assert action_arguments == expected_arguments


@pytest.mark.parametrize(
    ("action_string", "expected_arguments"),
    [
        ("f()", ()),
        ("f(())", ((),)),
        ("f((1, 2, 3))", ((1, 2, 3),)),
        ("f((1, 2, 3), (1, 2, 3))", ((1, 2, 3), (1, 2, 3))),
        ("f(((1, 2), (), None), 0)", (((1, 2), (), None), 0)),
        ("f((((((1))))))", (1,)),
        ("f(((((((((1, 2)))))))))", ((1, 2),)),
        ("f((1, 2), (3, 4))", ((1, 2), (3, 4))),
        ("f((((((1, 2), (3, 4))))))", (((1, 2), (3, 4)),)),
    ],
)
def test_nested_and_convoluted_tuple_arguments(
    action_string: str, expected_arguments: tuple[Any]
) -> None:
    """Test that tuple arguments are parsed correctly."""
    _, _, args = parse(action_string)
    assert args == expected_arguments


@pytest.mark.parametrize(
    ["action_string", "expected_arguments"],
    [
        ("f('')", ("",)),
        ('f("")', ("",)),
        ("f('''''')", ("",)),
        ('f("""""")', ("",)),
        ("f('(')", ("(",)),
        ("f(')')", (")",)),  # Regression test for #2088
        ("f('f()')", ("f()",)),
    ],
)
def test_parse_action_nested_special_character_arguments(
    action_string: str, expected_arguments: tuple[Any]
) -> None:
    """Test that special characters nested in strings are handled correctly.

    See also: https://github.com/Textualize/textual/issues/2088
    """
    _, _, args = parse(action_string)
    assert args == expected_arguments


@pytest.mark.parametrize(
    "action_string",
    [
        "foo(,,,,,)",
        "bar(1 2 3 4 5)",
        "baz.spam(Tru, Fals, in)",
        "ham(not)",
        "cheese((((()",
    ],
)
def test_parse_action_raises_error(action_string: str) -> None:
    with pytest.raises(ActionError):
        parse(action_string)


--- tests/test_animation.py ---
from time import perf_counter

from textual.app import App, ComposeResult
from textual.reactive import var
from textual.widgets import Static


class AnimApp(App):
    CSS = """
    #foo {
        height: 1;
    }
    """

    def compose(self) -> ComposeResult:
        yield Static("foo", id="foo")


async def test_animate_height() -> None:
    """Test animating styles.height works."""

    # Styles.height is a scalar, which makes it more complicated to animate

    app = AnimApp()

    async with app.run_test() as pilot:
        static = app.query_one(Static)
        assert static.size.height == 1
        assert static.styles.height.value == 1
        static.styles.animate("height", 100, duration=0.5, easing="linear")
        start = perf_counter()

        # Wait for the animation to finished
        await pilot.wait_for_animation()
        elapsed = perf_counter() - start
        # Check that the full time has elapsed
        assert elapsed >= 0.5
        # Check the height reached the maximum
        assert static.styles.height.value == 100


async def test_scheduling_animation() -> None:
    """Test that scheduling an animation works."""

    app = AnimApp()
    delay = 0.1

    async with app.run_test() as pilot:
        styles = app.query_one(Static).styles
        styles.background = "black"

        styles.animate("background", "white", delay=delay, duration=0)

        # Still black immediately after call, animation hasn't started yet due to `delay`
        assert styles.background.rgb == (0, 0, 0)

        await pilot.wait_for_scheduled_animations()
        assert styles.background.rgb == (255, 255, 255)


async def test_wait_for_current_animations() -> None:
    """Test that we can wait only for the current animations taking place."""

    app = AnimApp()

    delay = 10

    async with app.run_test() as pilot:
        styles = app.query_one(Static).styles
        styles.animate("height", 100, duration=0.1)
        start = perf_counter()
        styles.animate("height", 200, duration=0.1, delay=delay)

        # Wait for the first animation to finish
        await pilot.wait_for_animation()
        elapsed = perf_counter() - start
        assert elapsed < (delay / 2)


async def test_wait_for_current_and_scheduled_animations() -> None:
    """Test that we can wait for current and scheduled animations."""

    app = AnimApp()

    async with app.run_test() as pilot:
        styles = app.query_one(Static).styles

        start = perf_counter()
        styles.animate("height", 50, duration=0.01)
        styles.animate("background", "black", duration=0.01, delay=0.05)

        await pilot.wait_for_scheduled_animations()
        elapsed = perf_counter() - start
        assert elapsed >= 0.06
        assert styles.background.rgb == (0, 0, 0)


async def test_reverse_animations() -> None:
    """Test that you can create reverse animations.

    Regression test for #1372 https://github.com/Textualize/textual/issues/1372
    """

    app = AnimApp()

    async with app.run_test() as pilot:
        static = app.query_one(Static)
        styles = static.styles

        # Starting point.
        styles.background = "black"
        assert styles.background.rgb == (0, 0, 0)

        # First, make sure we can go from black to white and back, step by step.
        styles.animate("background", "white", duration=0.01)
        await pilot.wait_for_animation()
        assert styles.background.rgb == (255, 255, 255)

        styles.animate("background", "black", duration=0.01)
        await pilot.wait_for_animation()
        assert styles.background.rgb == (0, 0, 0)

        # Now, the actual test is to make sure we go back to black if creating both at once.
        styles.animate("background", "white", duration=0.01)
        styles.animate("background", "black", duration=0.01)
        await pilot.wait_for_animation()
        assert styles.background.rgb == (0, 0, 0)


async def test_schedule_reverse_animations() -> None:
    """Test that you can schedule reverse animations.

    Regression test for #1372 https://github.com/Textualize/textual/issues/1372
    """

    app = AnimApp()

    async with app.run_test() as pilot:
        static = app.query_one(Static)
        styles = static.styles

        # Starting point.
        styles.background = "black"
        assert styles.background.rgb == (0, 0, 0)

        # First, make sure we can go from black to white and back, step by step.
        styles.animate("background", "white", delay=0.01, duration=0.01)
        await pilot.wait_for_scheduled_animations()
        assert styles.background.rgb == (255, 255, 255)

        styles.animate("background", "black", delay=0.01, duration=0.01)
        await pilot.wait_for_scheduled_animations()
        assert styles.background.rgb == (0, 0, 0)

        # Now, the actual test is to make sure we go back to black if scheduling both at once.
        styles.animate("background", "white", delay=0.025, duration=0.05)
        # While the black -> white animation runs, start the white -> black animation.
        styles.animate("background", "black", delay=0.05, duration=0.01)
        await pilot.wait_for_scheduled_animations()
        assert styles.background.rgb == (0, 0, 0)


class CancelAnimWidget(Static):
    counter: var[float] = var(23)


class CancelAnimApp(App[None]):
    counter: var[float] = var(23)

    def compose(self) -> ComposeResult:
        yield CancelAnimWidget()


async def test_cancel_app_animation() -> None:
    """It should be possible to cancel a running app animation."""

    async with CancelAnimApp().run_test() as pilot:
        pilot.app.animate("counter", value=0, final_value=1000, duration=60)
        await pilot.pause()
        assert pilot.app.animator.is_being_animated(pilot.app, "counter")
        await pilot.app.stop_animation("counter")
        assert not pilot.app.animator.is_being_animated(pilot.app, "counter")


async def test_cancel_app_non_animation() -> None:
    """It should be possible to attempt to cancel a non-running app animation."""

    async with CancelAnimApp().run_test() as pilot:
        assert not pilot.app.animator.is_being_animated(pilot.app, "counter")
        await pilot.app.stop_animation("counter")
        assert not pilot.app.animator.is_being_animated(pilot.app, "counter")


async def test_cancel_widget_animation() -> None:
    """It should be possible to cancel a running widget animation."""

    async with CancelAnimApp().run_test() as pilot:
        widget = pilot.app.query_one(CancelAnimWidget)
        widget.animate("counter", value=0, final_value=1000, duration=60)
        await pilot.pause()
        assert pilot.app.animator.is_being_animated(widget, "counter")
        await widget.stop_animation("counter")
        assert not pilot.app.animator.is_being_animated(widget, "counter")


async def test_cancel_widget_non_animation() -> None:
    """It should be possible to attempt to cancel a non-running widget animation."""

    async with CancelAnimApp().run_test() as pilot:
        widget = pilot.app.query_one(CancelAnimWidget)
        assert not pilot.app.animator.is_being_animated(widget, "counter")
        await widget.stop_animation("counter")
        assert not pilot.app.animator.is_being_animated(widget, "counter")


--- tests/test_animator.py ---
from __future__ import annotations

from dataclasses import dataclass
from unittest.mock import Mock

import pytest

from textual._animator import Animator, SimpleAnimation
from textual._easing import DEFAULT_EASING, EASING


class Animatable:
    """An animatable object."""

    def __init__(self, value):
        self.value = value

    def blend(self, destination: Animatable, factor: float) -> Animatable:
        return Animatable(self.value + (destination.value - self.value) * factor)


@dataclass
class AnimateTest:
    """An object with animatable properties."""

    foo: float | None = 0.0  # Plain float that may be set to None on final_value
    bar: Animatable = Animatable(0)  # A mock object supporting the animatable protocol


def test_simple_animation():
    """Test an animation from one float to another."""

    # Thing that may be animated
    animate_test = AnimateTest()

    # Fake wall-clock time
    time = 100.0

    # Object that does the animation
    animation = SimpleAnimation(
        animate_test,
        "foo",
        time,
        3.0,
        start_value=20.0,
        end_value=50.0,
        final_value=None,
        easing=lambda x: x,
    )

    assert animate_test.foo == 0.0

    assert animation(time) is False
    assert animate_test.foo == 20.0

    assert animation(time + 1.0) is False
    assert animate_test.foo == 30.0

    assert animation(time + 2.0) is False
    assert animate_test.foo == 40.0

    assert animation(time + 2.9) is False  # Not quite final value
    assert animate_test.foo == pytest.approx(49.0)

    assert animation(time + 3.0) is True  # True to indicate animation is complete
    assert animate_test.foo is None  # This is final_value

    assert animation(time + 3.0) is True
    assert animate_test.foo is None


def test_simple_animation_duration_zero():
    """Test animation handles duration of 0."""

    # Thing that may be animated
    animatable = AnimateTest()

    # Fake wall-clock time
    time = 100.0

    # Object that does the animation
    animation = SimpleAnimation(
        animatable,
        "foo",
        time,
        0.0,
        start_value=20.0,
        end_value=50.0,
        final_value=50.0,
        easing=lambda x: x,
    )

    assert animation(time) is True  # Duration is 0, so this is last value
    assert animatable.foo == 50.0

    assert animation(time + 1.0) is True
    assert animatable.foo == 50.0


def test_simple_animation_reverse():
    """Test an animation from one float to another, where the end value is less than the start."""

    # Thing that may be animated
    animate_Test = AnimateTest()

    # Fake wall-clock time
    time = 100.0

    # Object that does the animation
    animation = SimpleAnimation(
        animate_Test,
        "foo",
        time,
        3.0,
        start_value=50.0,
        end_value=20.0,
        final_value=20.0,
        easing=lambda x: x,
    )

    assert animation(time) is False
    assert animate_Test.foo == 50.0

    assert animation(time + 1.0) is False
    assert animate_Test.foo == 40.0

    assert animation(time + 2.0) is False
    assert animate_Test.foo == 30.0

    assert animation(time + 3.0) is True
    assert animate_Test.foo == 20.0


def test_animatable():
    """Test SimpleAnimation works with the Animatable protocol"""

    animate_test = AnimateTest()

    # Fake wall-clock time
    time = 100.0

    # Object that does the animation
    animation = SimpleAnimation(
        animate_test,
        "bar",
        time,
        3.0,
        start_value=Animatable(20.0),
        end_value=Animatable(50.0),
        final_value=Animatable(50.0),
        easing=lambda x: x,
    )

    assert animation(time) is False
    assert animate_test.bar.value == 20.0

    assert animation(time + 1.0) is False
    assert animate_test.bar.value == 30.0

    assert animation(time + 2.0) is False
    assert animate_test.bar.value == 40.0

    assert animation(time + 2.9) is False
    assert animate_test.bar.value == pytest.approx(49.0)

    assert animation(time + 3.0) is True  # True to indicate animation is complete
    assert animate_test.bar.value == 50.0


class MockAnimator(Animator):
    """A mock animator."""

    def __init__(self, *args) -> None:
        super().__init__(*args)
        self._time = 0.0
        self._on_animation_frame_called = False

    def on_animation_frame(self):
        self._on_animation_frame_called = True

    def _get_time(self):
        return self._time


async def test_animator():
    target = Mock()
    animator = MockAnimator(target)
    animate_test = AnimateTest()

    # Animate attribute "foo" on animate_test to 100.0 in 10 seconds
    animator.animate(animate_test, "foo", 100.0, duration=10.0)

    expected = SimpleAnimation(
        animate_test,
        "foo",
        0.0,
        duration=10.0,
        start_value=0.0,
        end_value=100.0,
        final_value=100.0,
        easing=EASING[DEFAULT_EASING],
    )
    assert animator._animations[(id(animate_test), "foo")] == expected
    assert not animator._on_animation_frame_called

    animator()
    assert animate_test.foo == 0

    animator._time = 5
    animator()
    assert animate_test.foo == 50

    # New animation in the middle of an existing one
    animator.animate(animate_test, "foo", 200, duration=1)
    assert animate_test.foo == 50

    animator._time = 6
    animator()
    assert animate_test.foo == 200


def test_bound_animator():
    target = Mock()
    animator = MockAnimator(target)
    animate_test = AnimateTest()

    # Bind an animator so it animates attributes on the given object
    bound_animator = animator.bind(animate_test)

    # Animate attribute "foo" on animate_test to 100.0 in 10 seconds
    bound_animator("foo", 100.0, duration=10)

    expected = SimpleAnimation(
        animate_test,
        "foo",
        0,
        duration=10,
        start_value=0,
        end_value=100,
        final_value=100,
        easing=EASING[DEFAULT_EASING],
    )
    assert animator._animations[(id(animate_test), "foo")] == expected


async def test_animator_on_complete_callback_not_fired_before_duration_ends():
    callback = Mock()
    animate_test = AnimateTest()
    animator = MockAnimator(Mock())

    animator.animate(animate_test, "foo", 200, duration=10, on_complete=callback)

    animator._time = 9
    animator()

    assert not callback.called


async def test_animator_on_complete_callback_fired_at_duration():
    callback = Mock()
    animate_test = AnimateTest()
    mock_app = Mock()
    animator = MockAnimator(mock_app)

    animator.animate(animate_test, "foo", 200, duration=10, on_complete=callback)

    animator._time = 10
    animator()

    # Ensure that the callback is scheduled to run after the duration is up.
    mock_app.call_later.assert_called_once_with(callback)


def test_force_stop_animation():
    callback = Mock()
    animate_test = AnimateTest()
    mock_app = Mock()
    animator = MockAnimator(mock_app)

    animator.animate(animate_test, "foo", 200, duration=10, on_complete=callback)

    assert animator.is_being_animated(animate_test, "foo")
    assert animate_test.foo != 200

    animator.force_stop_animation(animate_test, "foo")

    # The animation of the attribute was force cancelled.
    assert not animator.is_being_animated(animate_test, "foo")
    assert animate_test.foo == 200

    # The on_complete callback was scheduled.
    mock_app.call_later.assert_called_once_with(callback)


--- tests/test_app.py ---
import asyncio
import contextlib

import pytest
from rich.terminal_theme import DIMMED_MONOKAI, MONOKAI, NIGHT_OWLISH

from textual import events
from textual.app import App, ComposeResult
from textual.command import SimpleCommand
from textual.pilot import Pilot, _get_mouse_message_arguments
from textual.widgets import Button, Input, Label, Static


def test_batch_update():
    """Test `batch_update` context manager"""
    app = App()
    assert app._batch_count == 0  # Start at zero

    with app.batch_update():
        assert app._batch_count == 1  # Increments in context manager

        with app.batch_update():
            assert app._batch_count == 2  # Nested updates

        assert app._batch_count == 1  # Exiting decrements

    assert app._batch_count == 0  # Back to zero


class MyApp(App):
    def compose(self) -> ComposeResult:
        yield Input()
        yield Button("Click me!")


async def test_hover_update_styles():
    app = MyApp(ansi_color=False)
    async with app.run_test() as pilot:
        button = app.query_one(Button)
        assert button.pseudo_classes == {
            "blur",
            "can-focus",
            "dark",
            "enabled",
            "first-of-type",
            "last-of-type",
            "last-child",
            "even",
            "empty",
        }

        # Take note of the initial background colour
        initial_background = button.styles.background
        await pilot.hover(Button)

        # We've hovered, so ensure the pseudoclass is present and background changed
        assert button.pseudo_classes == {
            "blur",
            "can-focus",
            "dark",
            "enabled",
            "hover",
            "first-of-type",
            "last-of-type",
            "last-child",
            "even",
            "empty",
        }
        assert button.styles.background != initial_background


def test_setting_title():
    app = MyApp()
    app.title = None
    assert app.title == "None"

    app.title = ""
    assert app.title == ""

    app.title = 0.125
    assert app.title == "0.125"

    app.title = [True, False, 2]
    assert app.title == "[True, False, 2]"


def test_setting_sub_title():
    app = MyApp()
    app.sub_title = None
    assert app.sub_title == "None"

    app.sub_title = ""
    assert app.sub_title == ""

    app.sub_title = 0.125
    assert app.sub_title == "0.125"

    app.sub_title = [True, False, 2]
    assert app.sub_title == "[True, False, 2]"


async def test_default_return_code_is_zero():
    app = App()
    async with app.run_test():
        app.exit()
    assert app.return_code == 0


async def test_return_code_is_one_after_crash():
    class MyApp(App):
        def key_p(self):
            1 / 0

    app = MyApp()
    with contextlib.suppress(ZeroDivisionError):
        async with app.run_test() as pilot:
            await pilot.press("p")
    assert app.return_code == 1


async def test_set_return_code():
    app = App()
    async with app.run_test():
        app.exit(return_code=42)
    assert app.return_code == 42


def test_no_return_code_before_running():
    app = App()
    assert app.return_code is None


async def test_no_return_code_while_running():
    app = App()
    async with app.run_test():
        assert app.return_code is None


async def test_ansi_theme():
    app = App()
    async with app.run_test():
        app.ansi_theme_dark = NIGHT_OWLISH
        assert app.ansi_theme == NIGHT_OWLISH

        app.theme = "textual-light"
        assert app.ansi_theme != NIGHT_OWLISH

        app.ansi_theme_light = MONOKAI
        assert app.ansi_theme == MONOKAI

        # Ensure if we change the dark theme while on light mode,
        # then change back to dark mode, the dark theme is updated.
        app.ansi_theme_dark = DIMMED_MONOKAI
        assert app.ansi_theme == MONOKAI

        app.theme = "textual-dark"
        assert app.ansi_theme == DIMMED_MONOKAI


async def test_early_exit():
    """Test exiting early doesn't cause issues."""
    from textual.app import App

    class AppExit(App):
        def compose(self):
            yield Static("Hello")

        def on_mount(self) -> None:
            # Exit after creating app
            self.exit()

    app = AppExit()
    async with app.run_test():
        pass


def test_early_exit_inline():
    """Test exiting early in inline mode doesn't break."""

    class AppExit(App[None]):
        def compose(self):
            yield Static("Hello")

        def on_mount(self) -> None:
            # Exit after creating app
            self.exit()

    app = AppExit()
    app.run(inline=True, inline_no_clear=True)


async def test_search_with_simple_commands():
    """Test search with a list of SimpleCommands and ensure callbacks are invoked."""
    called = False

    def callback():
        nonlocal called
        called = True

    app = App[None]()
    commands = [
        SimpleCommand("Test Command", callback, "A test command"),
        SimpleCommand("Another Command", callback, "Another test command"),
    ]
    async with app.run_test() as pilot:
        await app.search_commands(commands)
        await pilot.press("enter", "enter")
        assert called


async def test_search_with_tuples():
    """Test search with a list of tuples and ensure callbacks are invoked.
    In this case we also have no help text in the tuples.
    """
    called = False

    def callback():
        nonlocal called
        called = True

    app = App[None]()
    commands = [
        ("Test Command", callback),
        ("Another Command", callback),
    ]
    async with app.run_test() as pilot:
        await app.search_commands(commands)
        await pilot.press("enter", "enter")
        assert called


async def test_search_with_empty_list():
    """Test search with an empty command list doesn't crash."""
    app = App[None]()
    async with app.run_test():
        await app.search_commands([])


async def raw_click(pilot: Pilot, selector: str, times: int = 1):
    """A lower level click function that doesn't use the Pilot,
    and so doesn't bypass the click chain logic in App.on_event."""
    app = pilot.app
    kwargs = _get_mouse_message_arguments(app.query_one(selector))
    for _ in range(times):
        app.post_message(events.MouseDown(**kwargs))
        app.post_message(events.MouseUp(**kwargs))
        await pilot.pause()


@pytest.mark.parametrize("number_of_clicks,final_count", [(1, 1), (2, 3), (3, 6)])
async def test_click_chain_initial_repeated_clicks(
    number_of_clicks: int, final_count: int
):
    click_count = 0

    class MyApp(App[None]):
        # Ensure clicks are always within the time threshold
        CLICK_CHAIN_TIME_THRESHOLD = 1000.0

        def compose(self) -> ComposeResult:
            yield Label("Click me!", id="one")

        def on_click(self, event: events.Click) -> None:
            nonlocal click_count
            print(f"event: {event}")
            click_count += event.chain

    async with MyApp().run_test() as pilot:
        # Clicking the same Label at the same offset creates a double and triple click.
        for _ in range(number_of_clicks):
            await raw_click(pilot, "#one")

        assert click_count == final_count


async def test_click_chain_different_offset():
    click_count = 0

    class MyApp(App[None]):
        # Ensure clicks are always within the time threshold
        CLICK_CHAIN_TIME_THRESHOLD = 1000.0

        def compose(self) -> ComposeResult:
            yield Label("One!", id="one")
            yield Label("Two!", id="two")
            yield Label("Three!", id="three")

        def on_click(self, event: events.Click) -> None:
            nonlocal click_count
            click_count += event.chain

    async with MyApp().run_test() as pilot:
        # Clicking on different offsets in quick-succession doesn't qualify as a double or triple click.
        await raw_click(pilot, "#one")
        assert click_count == 1
        await raw_click(pilot, "#two")
        assert click_count == 2
        await raw_click(pilot, "#three")
        assert click_count == 3


async def test_click_chain_offset_changes_mid_chain():
    """If we're in the middle of a click chain (e.g. we've double clicked), and the third click
    comes in at a different offset, that third click should be considered a single click.
    """

    click_count = 0

    class MyApp(App[None]):
        # Ensure clicks are always within the time threshold
        CLICK_CHAIN_TIME_THRESHOLD = 1000.0

        def compose(self) -> ComposeResult:
            yield Label("Click me!", id="one")
            yield Label("Another button!", id="two")

        def on_click(self, event: events.Click) -> None:
            nonlocal click_count
            click_count = event.chain

    async with MyApp().run_test() as pilot:
        await raw_click(pilot, "#one", times=2)  # Double click
        assert click_count == 2
        await raw_click(pilot, "#two")  # Single click (because different widget)
        assert click_count == 1


async def test_click_chain_time_outwith_threshold():
    click_count = 0

    class MyApp(App[None]):
        # Intentionally set the threshold to 0.0 to ensure we always exceed it
        # and can confirm that a click chain is never created
        CLICK_CHAIN_TIME_THRESHOLD = 0.0

        def compose(self) -> ComposeResult:
            yield Label("Click me!", id="one")

        def on_click(self, event: events.Click) -> None:
            nonlocal click_count
            click_count += event.chain

    async with MyApp().run_test() as pilot:
        for i in range(1, 4):
            # Each click is outwith the time threshold, so a click chain is never created.
            await raw_click(pilot, "#one")
            assert click_count == i


def test_app_loop() -> None:
    """Test that App.run accepts a loop argument."""

    class MyApp(App[int]):
        def on_mount(self) -> None:
            self.exit(42)

    app = MyApp()
    result = app.run(loop=asyncio.new_event_loop())
    assert result == 42


async def test_app_run_async() -> None:
    """Check run_async runs without issues."""

    class MyApp(App[int]):
        def on_mount(self) -> None:
            self.exit(42)

    app = MyApp()
    result = await app.run_async()
    assert result == 42


def test_app_loop_run_after_asyncio_run() -> None:
    """Test that App.run runs after asyncio.run has run."""

    class MyApp(App[int]):
        def on_mount(self) -> None:
            self.exit(42)

    async def amain():
        pass

    asyncio.run(amain())

    app = MyApp()
    result = app.run()
    assert result == 42


--- tests/test_app_focus_blur.py ---
"""Test the workings of reacting to AppFocus and AppBlur."""

from textual.app import App, ComposeResult
from textual.events import AppBlur, AppFocus
from textual.widgets import Input


class FocusBlurApp(App[None]):

    AUTO_FOCUS = "#input-4"

    def compose(self) -> ComposeResult:
        for n in range(10):
            yield Input(id=f"input-{n}")


async def test_app_blur() -> None:
    """Test that AppBlur removes focus."""
    async with FocusBlurApp().run_test() as pilot:
        assert pilot.app.focused is not None
        assert pilot.app.focused.id == "input-4"
        pilot.app.post_message(AppBlur())
        await pilot.pause()
        assert pilot.app.focused is None


async def test_app_focus_restores_focus() -> None:
    """Test that AppFocus restores the correct focus."""
    async with FocusBlurApp().run_test() as pilot:
        assert pilot.app.focused is not None
        assert pilot.app.focused.id == "input-4"
        pilot.app.post_message(AppBlur())
        await pilot.pause()
        assert pilot.app.focused is None
        pilot.app.post_message(AppFocus())
        await pilot.pause()
        assert pilot.app.focused is not None
        assert pilot.app.focused.id == "input-4"


async def test_app_focus_restores_none_focus() -> None:
    """Test that AppFocus doesn't set focus if nothing was focused."""
    async with FocusBlurApp().run_test() as pilot:
        pilot.app.screen.focused = None
        pilot.app.post_message(AppBlur())
        await pilot.pause()
        assert pilot.app.focused is None
        pilot.app.post_message(AppFocus())
        await pilot.pause()
        assert pilot.app.focused is None


async def test_app_focus_handles_missing_widget() -> None:
    """Test that AppFocus works even when the last-focused widget has gone away."""
    async with FocusBlurApp().run_test() as pilot:
        assert pilot.app.focused is not None
        assert pilot.app.focused.id == "input-4"
        pilot.app.post_message(AppBlur())
        await pilot.pause()
        assert pilot.app.focused is None
        await pilot.app.query_one("#input-4").remove()
        pilot.app.post_message(AppFocus())
        await pilot.pause()
        assert pilot.app.focused is None


async def test_app_focus_defers_to_new_focus() -> None:
    """Test that AppFocus doesn't undo a fresh focus done while the app is in AppBlur state."""
    async with FocusBlurApp().run_test() as pilot:
        assert pilot.app.focused is not None
        assert pilot.app.focused.id == "input-4"
        pilot.app.post_message(AppBlur())
        await pilot.pause()
        assert pilot.app.focused is None
        pilot.app.query_one("#input-1").focus()
        await pilot.pause()
        pilot.app.post_message(AppFocus())
        await pilot.pause()
        assert pilot.app.focused.id == "input-1"


--- tests/test_arrange.py ---
import pytest

from textual._arrange import TOP_Z, arrange
from textual.app import App
from textual.geometry import NULL_OFFSET, Region, Size, Spacing
from textual.layout import WidgetPlacement
from textual.widget import Widget


async def test_arrange_empty():
    container = Widget(id="container")

    result = arrange(container, [], Size(80, 24), Size(80, 24))
    assert result.placements == []
    assert result.widgets == set()


async def test_arrange_dock_top():
    container = Widget(id="container")
    container._parent = App()
    child = Widget(id="child")
    header = Widget(id="header")
    header.styles.dock = "top"
    header.styles.height = "1"

    result = arrange(container, [child, header], Size(80, 24), Size(80, 24))

    assert result.placements == [
        WidgetPlacement(
            Region(0, 0, 80, 1), NULL_OFFSET, Spacing(), header, order=TOP_Z, fixed=True
        ),
        WidgetPlacement(
            Region(0, 1, 80, 23), NULL_OFFSET, Spacing(), child, order=0, fixed=False
        ),
    ]
    assert result.widgets == {child, header}


async def test_arrange_dock_left():
    container = Widget(id="container")
    container._parent = App()
    child = Widget(id="child")
    header = Widget(id="header")
    header.styles.dock = "left"
    header.styles.width = "10"

    result = arrange(container, [child, header], Size(80, 24), Size(80, 24))
    assert result.placements == [
        WidgetPlacement(
            Region(0, 0, 10, 24),
            NULL_OFFSET,
            Spacing(),
            header,
            order=TOP_Z,
            fixed=True,
        ),
        WidgetPlacement(
            Region(10, 0, 70, 24), NULL_OFFSET, Spacing(), child, order=0, fixed=False
        ),
    ]
    assert result.widgets == {child, header}


async def test_arrange_dock_right():
    container = Widget(id="container")
    container._parent = App()
    child = Widget(id="child")
    header = Widget(id="header")
    header.styles.dock = "right"
    header.styles.width = "10"

    result = arrange(container, [child, header], Size(80, 24), Size(80, 24))
    assert result.placements == [
        WidgetPlacement(
            Region(70, 0, 10, 24),
            NULL_OFFSET,
            Spacing(),
            header,
            order=TOP_Z,
            fixed=True,
        ),
        WidgetPlacement(
            Region(0, 0, 70, 24), NULL_OFFSET, Spacing(), child, order=0, fixed=False
        ),
    ]
    assert result.widgets == {child, header}


async def test_arrange_dock_bottom():
    container = Widget(id="container")
    container._parent = App()
    child = Widget(id="child")
    header = Widget(id="header")
    header.styles.dock = "bottom"
    header.styles.height = "1"

    result = arrange(container, [child, header], Size(80, 24), Size(80, 24))
    assert result.placements == [
        WidgetPlacement(
            Region(0, 23, 80, 1),
            NULL_OFFSET,
            Spacing(),
            header,
            order=TOP_Z,
            fixed=True,
        ),
        WidgetPlacement(
            Region(0, 0, 80, 23), NULL_OFFSET, Spacing(), child, order=0, fixed=False
        ),
    ]
    assert result.widgets == {child, header}


async def test_arrange_dock_badly():
    child = Widget(id="child")
    child.styles.dock = "nowhere"
    with pytest.raises(AssertionError):
        _ = arrange(Widget(), [child], Size(80, 24), Size(80, 24))


--- tests/test_auto_refresh.py ---
import asyncio
from time import time

from textual.app import App
from textual.pilot import Pilot


class RefreshApp(App[float]):
    def __init__(self):
        self.count = 0
        super().__init__()

    def on_mount(self):
        self.start = time()
        self.auto_refresh = 0.1

    def automatic_refresh(self):
        self.count += 1
        if self.count == 3:
            self.exit(time() - self.start)
        super().automatic_refresh()


def test_auto_refresh():
    app = RefreshApp()

    async def quit_after(pilot: Pilot) -> None:
        await asyncio.sleep(1)

    elapsed = app.run(auto_pilot=quit_after, headless=True)
    assert elapsed is not None
    # CI can run slower, so we need to give this a bit of margin
    assert 0.2 <= elapsed < 0.8


--- tests/test_await_remove.py ---
from textual.app import App
from textual.widgets import Label


class SelfRemovingLabel(Label):
    async def on_mount(self) -> None:
        await self.remove()


class RemoveOnTimerApp(App[None]):
    def on_mount(self):
        for _ in range(5):
            self.mount(SelfRemovingLabel("I will remove myself!"))


async def test_multiple_simultaneous_removals():
    """Regression test for https://github.com/Textualize/textual/issues/2854."""
    # The app should run and finish without raising any errors.
    async with RemoveOnTimerApp().run_test() as pilot:
        # Sanity check to ensure labels were removed.
        assert len(pilot.app.query(Label)) == 0


--- tools/gen_easings_tests.ts ---
/*
 * Easing functions from http://easings.net;
 * This script generates the data points used to test the _easing.py functions.
 */

function easeInSine(x: number): number {
    return 1 - cos((x * PI) / 2);
}

function easeOutSine(x: number): number {
    return sin((x * PI) / 2);
}

function easeInOutSine(x: number): number {
    return -(cos(PI * x) - 1) / 2;
}


function easeInQuad(x: number): number {
    return x * x;
}

function easeOutQuad(x: number): number {
    return 1 - (1 - x) * (1 - x);
}

function easeInOutQuad(x: number): number {
    return x < 0.5 ? 2 * x * x : 1 - pow(-2 * x + 2, 2) / 2;
}


function easeInCubic(x: number): number {
    return x * x * x;
}

function easeOutCubic(x: number): number {
    return 1 - pow(1 - x, 3);
}

function easeInOutCubic(x: number): number {
    return x < 0.5 ? 4 * x * x * x : 1 - pow(-2 * x + 2, 3) / 2;
}


function easeInQuart(x: number): number {
    return x * x * x * x;
}

function easeOutQuart(x: number): number {
    return 1 - pow(1 - x, 4);
}

function easeInOutQuart(x: number): number {
    return x < 0.5 ? 8 * x * x * x * x : 1 - pow(-2 * x + 2, 4) / 2;
}


function easeInQuint(x: number): number {
    return x * x * x * x * x;
}

function easeOutQuint(x: number): number {
    return 1 - pow(1 - x, 5);
}

function easeInOutQuint(x: number): number {
    return x < 0.5 ? 16 * x * x * x * x * x : 1 - pow(-2 * x + 2, 5) / 2;
}


function easeInExpo(x: number): number {
    return x === 0 ? 0 : pow(2, 10 * x - 10);
}

function easeOutExpo(x: number): number {
    return x === 1 ? 1 : 1 - pow(2, -10 * x);
}

function easeInOutExpo(x: number): number {
    return x === 0
      ? 0
      : x === 1
      ? 1
      : x < 0.5 ? pow(2, 20 * x - 10) / 2
      : (2 - pow(2, -20 * x + 10)) / 2;
}


function easeInCirc(x: number): number {
    return 1 - sqrt(1 - pow(x, 2));
}

function easeOutCirc(x: number): number {
    return sqrt(1 - pow(x - 1, 2));
}

function easeInOutCirc(x: number): number {
    return x < 0.5
      ? (1 - sqrt(1 - pow(2 * x, 2))) / 2
      : (sqrt(1 - pow(-2 * x + 2, 2)) + 1) / 2;
}


function easeInBack(x: number): number {
    const c1 = 1.70158;
    const c3 = c1 + 1;

    return c3 * x * x * x - c1 * x * x;
}

function easeOutBack(x: number): number {
    const c1 = 1.70158;
    const c3 = c1 + 1;

    return 1 + c3 * pow(x - 1, 3) + c1 * pow(x - 1, 2);
}

function easeInOutBack(x: number): number {
    const c1 = 1.70158;
    const c2 = c1 * 1.525;

    return x < 0.5
      ? (pow(2 * x, 2) * ((c2 + 1) * 2 * x - c2)) / 2
      : (pow(2 * x - 2, 2) * ((c2 + 1) * (x * 2 - 2) + c2) + 2) / 2;
}


function easeInElastic(x: number): number {
    const c4 = (2 * Math.PI) / 3;

    return x === 0
      ? 0
      : x === 1
      ? 1
      : -pow(2, 10 * x - 10) * sin((x * 10 - 10.75) * c4);
}

function easeOutElastic(x: number): number {
    const c4 = (2 * Math.PI) / 3;

    return x === 0
      ? 0
      : x === 1
      ? 1
      : pow(2, -10 * x) * sin((x * 10 - 0.75) * c4) + 1;
}

function easeInOutElastic(x: number): number {
    const c5 = (2 * Math.PI) / 4.5;

    return x === 0
      ? 0
      : x === 1
      ? 1
      : x < 0.5
      ? -(pow(2, 20 * x - 10) * sin((20 * x - 11.125) * c5)) / 2
      : (pow(2, -20 * x + 10) * sin((20 * x - 11.125) * c5)) / 2 + 1;

}


function easeInBounce(x: number): number {
    return 1 - easeOutBounce(1 - x);
}

function easeOutBounce(x: number): number {
    const n1 = 7.5625;
    const d1 = 2.75;

    if (x < 1 / d1) {
        return n1 * x * x;
    } else if (x < 2 / d1) {
        return n1 * (x -= 1.5 / d1) * x + 0.75;
    } else if (x < 2.5 / d1) {
        return n1 * (x -= 2.25 / d1) * x + 0.9375;
    } else {
        return n1 * (x -= 2.625 / d1) * x + 0.984375;
    }
}

function easeInOutBounce(x: number): number {
    return x < 0.5
      ? (1 - easeOutBounce(1 - 2 * x)) / 2
      : (1 + easeOutBounce(2 * x - 1)) / 2;
}


const funcs = [
    easeInSine, easeOutSine, easeInOutSine, easeInQuad, easeOutQuad, easeInOutQuad,
    easeInCubic, easeOutCubic, easeInOutCubic, easeInQuart, easeOutQuart, easeInOutQuart,
    easeInQuint, easeOutQuint, easeInOutQuint, easeInExpo, easeOutExpo, easeInOutExpo,
    easeInCirc, easeOutCirc, easeInOutCirc, easeInBack, easeOutBack, easeInOutBack,
    easeInElastic, easeOutElastic, easeInOutElastic, easeInBounce, easeOutBounce, easeInOutBounce
];

const points = [
    0.0,
    0.05,
    0.1,
    0.15,
    0.2,
    0.25,
    0.3,
    0.35,
    0.4,
    0.45,
    0.5,
    0.55,
    0.6,
    0.65,
    0.7,
    0.75,
    0.8,
    0.85,
    0.9,
    0.95,
    1.0
];

let PI = Math.PI;
let cos = Math.cos;
let sin = Math.sin;
let pow = Math.pow;
let sqrt = Math.sqrt;

for (func of funcs) {
    console.log(func.name);
    let values = [];
    for (point of points) {
        values.push(func(point));
    }
    console.log(values);
}


--- tools/widget_documentation.py ---
"""
Helper script to help document all widgets.

This goes through the widgets listed in textual.widgets and prints the scaffolding
for the tables that are used to document the classvars BINDINGS and COMPONENT_CLASSES.
"""
from __future__ import annotations

from typing import TYPE_CHECKING

import textual.widgets

if TYPE_CHECKING:
    from textual.binding import Binding


def print_bindings(widget: str, bindings: list[Binding]) -> None:
    """Print a table summarising the bindings.

    The table contains columns for the key(s) that trigger the binding,
    the method that it calls (and tries to link it to the widget itself),
    and the description of the binding.
    """
    if bindings:
        print("BINDINGS")
        print('"""')
        print("| Key(s) | Description |")
        print("| :- | :- |")

    for binding in bindings:
        print(f"| {binding.key} | {binding.description} |")

    if bindings:
        print('"""')


def print_component_classes(classes: set[str]) -> None:
    """Print a table to document these component classes.

    The table contains two columns, one with the component class name and another
    for the description of what the component class is for.
    The second column is always empty.
    """
    if classes:
        print("COMPONENT_CLASSES")
        print('"""')
        print("| Class | Description |")
        print("| :- | :- |")

    for cls in sorted(classes):
        print(f"| `{cls}` | XXX |")

    if classes:
        print('"""')


def main() -> None:
    """Main entrypoint.

    Iterates over all widgets and prints docs tables.
    """

    widgets: list[str] = textual.widgets.__all__

    for widget in widgets:
        w = getattr(textual.widgets, widget)
        bindings: list[Binding] = w.__dict__.get("BINDINGS", [])
        component_classes: set[str] = getattr(w, "COMPONENT_CLASSES", set())

        if bindings or component_classes:
            print(widget)
            print()
            print_bindings(widget, bindings)
            print_component_classes(component_classes)
            print()


if __name__ == "__main__":
    main()


--- src/lmstudiotxt_generator/analyzer.py ---
from __future__ import annotations

import logging
import re
from collections import defaultdict
from typing import Dict, Iterable, List, Tuple

import dspy
import requests

from .github import construct_raw_url, owner_repo_from_url
from .signatures import (
    AnalyzeCodeStructure,
    AnalyzeRepository,
    GenerateLLMsTxt,
    GenerateUsageExamples,
)

logger = logging.getLogger(__name__)

_URL_VALIDATION_TIMEOUT = 5
_URL_SESSION = requests.Session()
_URL_HEADERS = {"User-Agent": "lmstudio-llmstxt-generator"}


def _nicify_title(path: str) -> str:
    base = path.rsplit("/", 1)[-1]
    base = re.sub(r"\.(md|rst|txt|py|ipynb|js|ts|html|mdx)$", "", base, flags=re.I)
    base = base.replace("-", " ").replace("_", " ")
    title = base.strip().title() or path
    if re.search(r"(^|/)index(\.mdx?|\.html?)?$", path, flags=re.I):
        parts = path.strip("/").split("/")
        if len(parts) > 1:
            title = parts[-2].replace("-", " ").replace("_", " ").title()
    return title


def _short_note(path: str) -> str:
    lower = path.lower()
    if any(
        hint in lower
        for hint in ["getting-started", "quickstart", "install", "overview", "/readme"]
    ):
        return "install & quickstart"
    if any(hint in lower for hint in ["reference", "/api"]):
        return "API reference"
    if any(hint in lower for hint in ["tutorial", "example", "how-to", "demo"]):
        return "worked example"
    if any(hint in lower for hint in ["concept", "architecture", "faq"]):
        return "core concept"
    if "changelog" in lower or "release" in lower:
        return "version history"
    if "license" in lower:
        return "usage terms"
    if "security" in lower:
        return "security policy"
    return "docs page"


def _score(path: str) -> float:
    score = 0.0
    lower = path.lower()
    if any(
        hint in lower
        for hint in ["quickstart", "getting-started", "install", "overview", "/readme"]
    ):
        score += 5
    if any(hint in lower for hint in ["tutorial", "example", "how-to", "demo"]):
        score += 3
    if re.search(r"(^|/)index(\.mdx?|\.html?)?$", lower):
        score += 2
    score -= lower.count("/") * 0.1
    return score


TAXONOMY: List[Tuple[str, re.Pattern]] = [
    (
        "Docs",
        re.compile(r"(docs|guide|getting[-_ ]?started|quickstart|install|overview)", re.I),
    ),
    ("Tutorials", re.compile(r"(tutorial|example|how[-_ ]?to|cookbook|demos?)", re.I)),
    ("API", re.compile(r"(api|reference|sdk|class|module)", re.I)),
    ("Concepts", re.compile(r"(concept|architecture|design|faq)", re.I)),
    (
        "Optional",
        re.compile(r"(contributing|changelog|release|security|license|benchmark)", re.I),
    ),
]


def _url_alive(url: str) -> bool:
    try:
        response = _URL_SESSION.head(
            url, allow_redirects=True, timeout=_URL_VALIDATION_TIMEOUT, headers=_URL_HEADERS
        )
        status = response.status_code
        if status and status < 400:
            return True
        response = _URL_SESSION.get(
            url,
            stream=True,
            timeout=_URL_VALIDATION_TIMEOUT,
            headers=_URL_HEADERS,
        )
        response.close()
        return response.status_code < 400
    except requests.RequestException:
        return False


def build_dynamic_buckets(
    repo_url: str,
    file_tree: str,
    default_ref: str | None = None,
    validate_urls: bool = True,
) -> List[Tuple[str, List[Tuple[str, str, str]]]]:
    paths = [p.strip() for p in file_tree.splitlines() if p.strip()]
    pages = []
    for path in paths:
        if not re.search(r"\.(md|mdx|py|ipynb|js|ts|rst|txt|html)$", path, flags=re.I):
            continue
        pages.append(
            {
                "path": path,
                "url": construct_raw_url(repo_url, path, ref=default_ref),
                "title": (
                    "README"
                    if re.search(r"(^|/)README\.md$", path, flags=re.I)
                    else _nicify_title(path)
                ),
                "note": _short_note(path),
                "score": _score(path),
            }
        )

    buckets: Dict[str, List[dict]] = defaultdict(list)
    for page in pages:
        matched = False
        for name, regex in TAXONOMY:
            if regex.search(page["path"]) or regex.search(page["title"]):
                buckets[name].append(page)
                matched = True
                break
        if not matched:
            top = page["path"].strip("/").split("/")[0] or "Misc"
            buckets[top.replace("-", " ").replace("_", " ").title()].append(page)

    for name, items in list(buckets.items()):
        items.sort(key=lambda item: (-item["score"], item["title"]))
        buckets[name] = items[:10]
        if not buckets[name]:
            buckets.pop(name, None)

    if validate_urls:
        for name, items in list(buckets.items()):
            filtered = []
            for page in items:
                if _url_alive(page["url"]):
                    filtered.append(page)
                else:
                    logger.debug("Dropping %s due to missing resource.", page["url"])
            if filtered:
                buckets[name] = filtered
            else:
                buckets.pop(name, None)

    reserved = {name for name, _ in TAXONOMY}
    for name in list(buckets.keys()):
        if name not in reserved and len(buckets[name]) <= 1:
            buckets["Optional"].extend(buckets.pop(name))

    ordered: List[Tuple[str, List[Tuple[str, str, str]]]] = []
    seen = set()
    for name, _ in TAXONOMY:
        if name in buckets:
            ordered.append(
                (
                    name,
                    [(pg["title"], pg["url"], pg["note"]) for pg in buckets[name]],
                )
            )
            seen.add(name)
    for name in sorted(k for k in buckets.keys() if k not in seen):
        ordered.append((name, [(pg["title"], pg["url"], pg["note"]) for pg in buckets[name]]))
    return ordered


def render_llms_markdown(
    project_name: str,
    project_purpose: str,
    remember_bullets: Iterable[str],
    buckets: List[Tuple[str, List[Tuple[str, str, str]]]],
) -> str:
    bullets = [str(b).strip().rstrip(".") for b in remember_bullets if str(b).strip()]
    bullets = bullets[:6] or [
        "Install + Quickstart first",
        "Core concepts & API surface",
        "Use Tutorials for worked examples",
    ]
    if len(bullets) < 3:
        bullets += ["Review API reference", "See Optional for meta docs"][: 3 - len(bullets)]
    purpose_line = (project_purpose or "").strip().replace("\n", " ")

    def fmt(items: Iterable[Tuple[str, str, str]]) -> str:
        return "\n".join(f"- [{title}]({url}): {note}." for title, url, note in items)

    out = [
        f"# {project_name}",
        "",
        f"> {purpose_line or 'Project overview unavailable.'}",
        "",
        "**Remember:**",
        *[f"- {bullet}" for bullet in bullets],
        "",
    ]
    for name, items in buckets:
        if not items:
            continue
        out.append(f"## {name}")
        out.append(fmt(items) or "- _No curated links yet_.")
        out.append("")
    return "\n".join(out).strip()


class RepositoryAnalyzer(dspy.Module):
    """DSPy module that synthesizes an llms.txt summary for a GitHub repository."""

    def __init__(self) -> None:
        super().__init__()
        self.analyze_repo = dspy.ChainOfThought(AnalyzeRepository)
        self.analyze_structure = dspy.ChainOfThought(AnalyzeCodeStructure)
        self.generate_examples = dspy.ChainOfThought(GenerateUsageExamples)
        self.generate_llms_txt = dspy.ChainOfThought(GenerateLLMsTxt)

    def forward(
        self,
        repo_url: str,
        file_tree: str,
        readme_content: str,
        package_files: str,
        default_branch: str | None = None,
    ):
        repo_analysis = self.analyze_repo(
            repo_url=repo_url,
            file_tree=file_tree,
            readme_content=readme_content,
        )
        structure_analysis = self.analyze_structure(
            file_tree=file_tree, package_files=package_files
        )

        self.generate_examples(
            repo_info=(
                f"Purpose: {repo_analysis.project_purpose}\n\n"
                f"Concepts: {', '.join(repo_analysis.key_concepts or [])}\n\n"
                f"Entry points: {', '.join(structure_analysis.entry_points or [])}\n"
            )
        )

        try:
            _, repo = owner_repo_from_url(repo_url)
            project_name = repo.replace("-", " ").replace("_", " ").title()
        except Exception:
            project_name = "Project"

        buckets = build_dynamic_buckets(
            repo_url,
            file_tree,
            default_ref=default_branch,
        )

        llms_txt_content = render_llms_markdown(
            project_name=project_name,
            project_purpose=repo_analysis.project_purpose or "",
            remember_bullets=repo_analysis.key_concepts or [],
            buckets=buckets,
        )

        return dspy.Prediction(
            llms_txt_content=llms_txt_content,
            analysis=repo_analysis,
            structure=structure_analysis,
        )


--- src/lmstudiotxt_generator/cli.py ---
import argparse
import logging
import sys
from pathlib import Path
from textwrap import dedent

from .config import AppConfig
from .pipeline import run_generation


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="lmstudio-llmstxt",
        description="Generate llms.txt artifacts for a GitHub repository using LM Studio.",
    )
    parser.add_argument("repo", help="GitHub repository URL (https://github.com/<owner>/<repo>)")
    parser.add_argument(
        "--model",
        help="LM Studio model identifier (overrides LMSTUDIO_MODEL).",
    )
    parser.add_argument(
        "--api-base",
        help="LM Studio API base URL (overrides LMSTUDIO_BASE_URL).",
    )
    parser.add_argument(
        "--api-key",
        help="LM Studio API key (overrides LMSTUDIO_API_KEY).",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        help="Directory where artifacts will be written (default: OUTPUT_DIR or ./artifacts).",
    )
    parser.add_argument(
        "--stamp",
        action="store_true",
        help="Append a UTC timestamp comment to generated files.",
    )
    parser.add_argument(
        "--no-ctx",
        action="store_true",
        help="Skip generating llms-ctx.txt even if ENABLE_CTX is set.",
    )
    parser.add_argument(
        "--cache-lm",
        action="store_true",
        help="Enable DSPy's LM cache (useful for repeated experiments).",
    )
    return parser


def main(argv: list[str] | None = None) -> int:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
    )
    parser = build_parser()
    args = parser.parse_args(argv)

    config = AppConfig()
    if args.model:
        config.lm_model = args.model
    if args.api_base:
        config.lm_api_base = str(args.api_base)
    if args.api_key:
        config.lm_api_key = args.api_key
    if args.output_dir:
        config.output_dir = args.output_dir
    if args.no_ctx:
        config.enable_ctx = False

    try:
        artifacts = run_generation(
            repo_url=args.repo,
            config=config,
            stamp=bool(args.stamp),
            cache_lm=bool(args.cache_lm),
        )
    except Exception as exc:
        parser.error(str(exc))
        return 2

    summary = dedent(
        f"""\
        Artifacts written:
          - {artifacts.llms_txt_path}
          - {artifacts.llms_full_path}
        """
    ).rstrip()

    if artifacts.ctx_path:
        summary += f"\n  - {artifacts.ctx_path}"
    if artifacts.json_path:
        summary += f"\n  - {artifacts.json_path}"
    if artifacts.used_fallback:
        summary += "\n(note) LM call failed; fallback JSON/schema output was used."

    print(summary)
    return 0


if __name__ == "__main__":
    sys.exit(main())


--- src/lmstudiotxt_generator/config.py ---
from __future__ import annotations

import os
from dataclasses import dataclass, field
from pathlib import Path

from dotenv import load_dotenv

load_dotenv()


def _env_flag(name: str, default: bool = False) -> bool:
    raw = os.getenv(name)
    if raw is None:
        return default
    return raw.strip().lower() in {"1", "true", "yes", "on"}


@dataclass(slots=True)
class AppConfig:
    """
    Runtime configuration for the LM Studio llms.txt generator.

    Users can override defaults through environment variables:
      - ``LMSTUDIO_MODEL``: LM Studio model identifier.
      - ``LMSTUDIO_BASE_URL``: API base URL (defaults to http://localhost:1234/v1).
      - ``LMSTUDIO_API_KEY``: Optional API key (LM Studio accepts any string).
      - ``OUTPUT_DIR``: Root folder for generated artifacts.
      - ``ENABLE_CTX``: Set truthy to emit llms-ctx.txt files when llms_txt.create_ctx
        is available.
    """
    lm_model: str = field(
        default_factory=lambda: os.getenv(
            "LMSTUDIO_MODEL", "qwen3-4b-instruct-2507@q6_k_xl"
        )
    )
    lm_api_base: str = field(
        default_factory=lambda: os.getenv("LMSTUDIO_BASE_URL", "http://localhost:1234/v1")
    )
    lm_api_key: str = field(
        default_factory=lambda: os.getenv("LMSTUDIO_API_KEY", "lm-studio")
    )
    output_dir: Path = field(
        default_factory=lambda: Path(os.getenv("OUTPUT_DIR", "artifacts"))
    )
    github_token: str | None = field(
        default_factory=lambda: os.getenv("GITHUB_ACCESS_TOKEN")
        or os.getenv("GH_TOKEN")
    )
    enable_ctx: bool = field(default_factory=lambda: _env_flag("ENABLE_CTX", False))
    lm_streaming: bool = field(default_factory=lambda: _env_flag("LMSTUDIO_STREAMING", True))
    lm_auto_unload: bool = field(default_factory=lambda: _env_flag("LMSTUDIO_AUTO_UNLOAD", True))

    def ensure_output_root(self, owner: str, repo: str) -> Path:
        """Return ``<output_root>/<owner>/<repo>`` and create it if missing."""
        repo_root = self.output_dir / owner / repo
        repo_root.mkdir(parents=True, exist_ok=True)
        return repo_root


--- src/lmstudio_llmstxt_generator.egg-info/dependency_links.txt ---



--- src/lmstudio_llmstxt_generator.egg-info/entry_points.txt ---
[console_scripts]
lmstxt = lmstudiotxt_generator.cli:main


--- src/lmstudiotxt_generator/fallback.py ---
from __future__ import annotations

import textwrap
from typing import Dict, List, Tuple

from .analyzer import build_dynamic_buckets, render_llms_markdown
from .schema import LLMS_JSON_SCHEMA


def _summary_from_readme(readme: str) -> str:
    if not readme:
        return "Project overview unavailable."
    lines = [line.strip() for line in readme.splitlines()]
    lines = [line for line in lines if line]
    if not lines:
        return "Project overview unavailable."
    if lines[0].startswith("#"):
        lines = lines[1:]
    excerpt = []
    for line in lines:
        if line.startswith("#"):
            break
        excerpt.append(line)
        if len(" ".join(excerpt)) > 280:
            break
    summary = " ".join(excerpt).strip()
    if not summary:
        return "Project overview unavailable."
    return summary


def _remember_bullets() -> List[str]:
    return [
        "Start with Docs for install & onboarding",
        "Check Tutorials for end-to-end workflows",
        "Review API references before integrating",
    ]


def fallback_llms_payload(
    repo_name: str,
    repo_url: str,
    file_tree: str,
    readme_content: str,
    *,
    default_branch: str | None = None,
) -> Dict[str, object]:
    buckets = build_dynamic_buckets(
        repo_url,
        file_tree,
        default_ref=default_branch,
    )
    summary = _summary_from_readme(readme_content)
    remember = _remember_bullets()
    sections: List[Dict[str, object]] = []
    for title, items in buckets:
        links = [
            {"title": link_title, "url": link_url, "note": note}
            for (link_title, link_url, note) in items
        ]
        sections.append({"title": title, "links": links})
    payload: Dict[str, object] = {
        "schema": LLMS_JSON_SCHEMA,
        "project": {"name": repo_name, "summary": summary},
        "remember": remember,
        "sections": sections,
    }
    return payload


def fallback_markdown_from_payload(repo_name: str, payload: Dict[str, object]) -> str:
    buckets: List[Tuple[str, List[Tuple[str, str, str]]]] = []
    for section in payload["sections"]:
        sec = section  # type: ignore[assignment]
        items = [
            (link["title"], link["url"], link["note"])
            for link in sec["links"]  # type: ignore[index]
        ]
        buckets.append((sec["title"], items))  # type: ignore[arg-type]
    markdown = render_llms_markdown(
        project_name=repo_name,
        project_purpose=payload["project"]["summary"],  # type: ignore[index]
        remember_bullets=payload["remember"],  # type: ignore[index]
        buckets=buckets,
    )
    header = textwrap.dedent(
        """\
        <!-- Generated via fallback path (no LM). -->
        """
    )
    return header + "\n" + markdown


def fallback_llms_markdown(
    repo_name: str,
    repo_url: str,
    file_tree: str,
    readme_content: str,
    *,
    default_branch: str | None = None,
) -> str:
    payload = fallback_llms_payload(
        repo_name=repo_name,
        repo_url=repo_url,
        file_tree=file_tree,
        readme_content=readme_content,
        default_branch=default_branch,
    )
    return fallback_markdown_from_payload(repo_name, payload)


__all__ = [
    "fallback_llms_payload",
    "fallback_llms_markdown",
    "fallback_markdown_from_payload",
]


--- src/lmstudiotxt_generator/full_builder.py ---
from __future__ import annotations

import base64
import os
import re
import textwrap
from dataclasses import dataclass
from typing import Iterable, Optional, Tuple

import requests


@dataclass
class GhRef:
    owner: str
    repo: str
    path: str
    ref: Optional[str] = None


_GH_LINK = re.compile(
    r"https?://(?:raw\.githubusercontent\.com|github\.com)/(?P<owner>[^/]+)/(?P<repo>[^/]+)/"
    r"(?:(?:blob|tree)/)?(?P<ref>[^/]+)/(?P<path>.+)$",
    re.I,
)


def parse_github_link(url: str) -> Optional[GhRef]:
    match = _GH_LINK.match(url)
    if not match:
        return None
    groups = match.groupdict()
    return GhRef(groups["owner"], groups["repo"], groups["path"], groups.get("ref"))


def gh_get_file(
    owner: str,
    repo: str,
    path: str,
    ref: Optional[str] = None,
    token: Optional[str] = None,
) -> Tuple[str, bytes]:
    url = f"https://api.github.com/repos/{owner}/{repo}/contents/{path}"
    params = {"ref": ref} if ref else {}
    headers = {
        "Accept": "application/vnd.github+json",
        "User-Agent": "lmstudio-llmstxt-generator",
    }
    if token:
        headers["Authorization"] = f"Bearer {token}"
    response = requests.get(url, params=params, headers=headers, timeout=30)
    if response.status_code == 404:
        raise FileNotFoundError(f"GitHub 404 for {owner}/{repo}/{path}@{ref or 'default'}")
    response.raise_for_status()
    payload = response.json()
    if payload.get("encoding") == "base64":
        body = base64.b64decode(payload["content"])
    else:
        body = payload.get("content", "").encode("utf-8", "ignore")
    mime_hint = payload.get("type", "file")
    return mime_hint, body


def fetch_raw_file(
    owner: str,
    repo: str,
    path: str,
    ref: str,
) -> bytes:
    url = f"https://raw.githubusercontent.com/{owner}/{repo}/{ref}/{path}"
    response = requests.get(
        url,
        headers={"User-Agent": "lmstudio-llmstxt-generator"},
        timeout=30,
    )
    if response.status_code == 404:
        raise FileNotFoundError(f"Raw GitHub 404 for {owner}/{repo}/{path}@{ref}")  # noqa: EM102
    response.raise_for_status()
    return response.content


_PAGE_LINK = re.compile(r"^\s*-\s*\[(?P<title>.+?)\]\((?P<url>https?://[^\s)]+)\)", re.M)


def iter_llms_links(curated_text: str) -> Iterable[Tuple[str, str]]:
    for match in _PAGE_LINK.finditer(curated_text):
        yield match.group("title").strip(), match.group("url").strip()


def sanitize_path_for_block(title: str, url: str, gh: Optional[GhRef]) -> str:
    if gh:
        path = gh.path
    else:
        path = title.lower().strip().replace(" ", "-")
    return path.lstrip("/")


def build_llms_full_from_repo(
    curated_llms_text: str,
    max_bytes_per_file: int = 800_000,
    max_files: int = 100,
    *,
    prefer_raw: bool = False,
    default_ref: Optional[str] = None,
    token: Optional[str] = None,
) -> str:
    resolved_token = (
        token
        if token is not None
        else os.getenv("GITHUB_ACCESS_TOKEN") or os.getenv("GH_TOKEN")
    )
    blocks = []
    seen = set()
    count = 0

    for title, url in iter_llms_links(curated_llms_text):
        if count >= max_files:
            break
        gh = parse_github_link(url)
        if not gh:
            continue

        key = (gh.owner, gh.repo, gh.path, gh.ref or "")
        if key in seen:
            continue
        seen.add(key)

        resolved_ref = gh.ref or default_ref or "main"
        try:
            if prefer_raw:
                body = fetch_raw_file(gh.owner, gh.repo, gh.path, resolved_ref)
            else:
                _, body = gh_get_file(
                    gh.owner,
                    gh.repo,
                    gh.path,
                    resolved_ref,
                    resolved_token,
                )
        except requests.HTTPError as exc:  # pragma: no cover - diagnostic text only
            message = _format_http_error(gh, resolved_ref, exc, auth_used=not prefer_raw)
            body = message.encode("utf-8")
        except Exception as exc:  # pragma: no cover - diagnostic text only
            message = _format_generic_error(gh, resolved_ref, exc)
            body = message.encode("utf-8")

        if len(body) > max_bytes_per_file:
            body = body[:max_bytes_per_file] + b"\n[truncated]\n"

        block_path = sanitize_path_for_block(title, url, gh)
        blocks.append(f"--- {block_path} ---\n{body.decode('utf-8', 'replace')}\n")
        count += 1

    disclaimer = textwrap.dedent(
        """\
        # llms-full (private-aware)
        > Built by GitHub fetches (raw or authenticated). Large files may be truncated.
        """
    )
    return disclaimer + "\n" + "\n".join(blocks)


def _format_http_error(
    gh: GhRef,
    ref: str,
    exc: requests.HTTPError,
    *,
    auth_used: bool,
) -> str:
    response = exc.response
    status = response.status_code if response is not None else "unknown"
    reason = response.reason if response is not None else str(exc)
    hint = ""
    if auth_used and response is not None and response.status_code == 403:
        hint = (
            " Verify that GITHUB_ACCESS_TOKEN or GH_TOKEN has 'repo' scope and is not expired."
        )
    return (
        f"[fetch-error] {gh.owner}/{gh.repo}/{gh.path}@{ref} :: "
        f"HTTP {status} {reason}.{hint}"
    )


def _format_generic_error(gh: GhRef, ref: str, exc: Exception) -> str:
    return (
        f"[fetch-error] {gh.owner}/{gh.repo}/{gh.path}@{ref} :: {exc}"
    )


--- src/lmstudiotxt_generator/github.py ---
from __future__ import annotations

import base64
import os
import re
from typing import Iterable

import requests

from .models import RepositoryMaterial

_GITHUB_URL = re.compile(
    r"""
    ^
    (?:
        git@github\.com:
        (?P<owner_ssh>[^/]+)/(?P<repo_ssh>[^/]+?)(?:\.git)?
        |
        https?://github\.com/
        (?P<owner_http>[^/]+)/(?P<repo_http>[^/]+?)(?:\.git)?
    )
    (?:/.*)?
    $
    """,
    re.IGNORECASE | re.VERBOSE,
)

_SESSION = requests.Session()


def owner_repo_from_url(repo_url: str) -> tuple[str, str]:
    """Return ``(owner, repo)`` for https or SSH GitHub URLs."""
    m = _GITHUB_URL.match(repo_url.strip())
    if not m:
        raise ValueError(f"Unrecognized GitHub URL: {repo_url!r}")
    owner = m.group("owner_http") or m.group("owner_ssh")
    repo = m.group("repo_http") or m.group("repo_ssh")
    return owner, repo


def _auth_headers(token: str | None) -> dict[str, str]:
    headers = {
        "Accept": "application/vnd.github+json",
        "User-Agent": "lmstudio-llmstxt-generator",
    }
    if token:
        headers["Authorization"] = f"Bearer {token}"
    return headers


def get_repository_metadata(owner: str, repo: str, token: str | None) -> dict[str, object]:
    resp = _SESSION.get(
        f"https://api.github.com/repos/{owner}/{repo}",
        headers=_auth_headers(token),
        timeout=20,
    )
    if resp.status_code == 404:
        raise FileNotFoundError(f"Repository not found: {owner}/{repo}")
    resp.raise_for_status()
    payload = resp.json()
    return {
        "default_branch": payload.get("default_branch", "main"),
        "is_private": bool(payload.get("private", False)),
        "visibility": payload.get("visibility"),
    }


def get_default_branch(owner: str, repo: str, token: str | None) -> str:
    metadata = get_repository_metadata(owner, repo, token)
    return str(metadata.get("default_branch", "main"))


def fetch_file_tree(
    owner: str, repo: str, ref: str, token: str | None
) -> Iterable[str]:
    resp = _SESSION.get(
        f"https://api.github.com/repos/{owner}/{repo}/git/trees/{ref}",
        params={"recursive": 1},
        headers=_auth_headers(token),
        timeout=30,
    )
    resp.raise_for_status()
    payload = resp.json()
    return [
        item["path"]
        for item in payload.get("tree", [])
        if item.get("type") == "blob" and "path" in item
    ]


def fetch_file_content(
    owner: str, repo: str, path: str, ref: str, token: str | None
) -> str | None:
    resp = _SESSION.get(
        f"https://api.github.com/repos/{owner}/{repo}/contents/{path}",
        params={"ref": ref},
        headers=_auth_headers(token),
        timeout=20,
    )
    if resp.status_code == 404:
        return None
    resp.raise_for_status()
    payload = resp.json()
    content = payload.get("content")
    if content and payload.get("encoding") == "base64":
        return base64.b64decode(content).decode("utf-8", "replace")
    if isinstance(content, str):
        return content
    return None


def gather_repository_material(repo_url: str, token: str | None = None) -> RepositoryMaterial:
    owner, repo = owner_repo_from_url(repo_url)
    metadata = get_repository_metadata(owner, repo, token)
    ref = str(metadata.get("default_branch", "main"))

    file_paths = fetch_file_tree(owner, repo, ref, token)
    file_tree = "\n".join(sorted(file_paths))

    readme = fetch_file_content(owner, repo, "README.md", ref, token) or ""

    package_blobs = []
    for candidate in (
        "pyproject.toml",
        "setup.cfg",
        "setup.py",
        "requirements.txt",
        "package.json",
    ):
        content = fetch_file_content(owner, repo, candidate, ref, token)
        if content:
            package_blobs.append(f"=== {candidate} ===\n{content}")

    package_files = "\n\n".join(package_blobs)

    return RepositoryMaterial(
        repo_url=repo_url,
        file_tree=file_tree,
        readme_content=readme,
        package_files=package_files,
        default_branch=ref,
        is_private=bool(metadata.get("is_private", False)),
    )


def construct_raw_url(repo_url: str, path: str, ref: str | None = None) -> str:
    owner, repo = owner_repo_from_url(repo_url)
    if not ref:
        token = os.getenv("GITHUB_ACCESS_TOKEN") or os.getenv("GH_TOKEN")
        try:
            ref = get_default_branch(owner, repo, token)
        except Exception:
            ref = "main"
    return f"https://raw.githubusercontent.com/{owner}/{repo}/{ref}/{path}"


--- src/lmstudiotxt_generator/__init__.py ---
"""LM Studio-powered llms.txt generation toolkit."""

from .analyzer import RepositoryAnalyzer
from .config import AppConfig
from .fallback import (
    fallback_llms_payload,
    fallback_llms_markdown,
)
from .lmstudio import configure_lmstudio_lm, LMStudioConnectivityError
from .models import GenerationArtifacts, RepositoryMaterial
from .schema import LLMS_JSON_SCHEMA

__all__ = [
    "AppConfig",
    "GenerationArtifacts",
    "RepositoryAnalyzer",
    "RepositoryMaterial",
    "configure_lmstudio_lm",
    "LMStudioConnectivityError",
    "fallback_llms_payload",
    "fallback_llms_markdown",
    "LLMS_JSON_SCHEMA",
]


--- src/lmstudiotxt_generator/lmstudio.py ---
from __future__ import annotations

import logging
import subprocess
from typing import Iterable, Optional, Tuple
from urllib.parse import urlparse

import dspy
import requests

from .config import AppConfig

logger = logging.getLogger(__name__)

try:  # Optional dependency recommended for managed unload
    import lmstudio as _LMSTUDIO_SDK  # type: ignore
except Exception:  # pragma: no cover - SDK is optional at runtime
    _LMSTUDIO_SDK = None  # type: ignore[assignment]


class LMStudioConnectivityError(RuntimeError):
    """Raised when LM Studio cannot be reached or does not expose the model."""


_MODEL_ENDPOINTS: tuple[str, ...] = ("/v1/models", "/api/v1/models", "/models")
_LOAD_ENDPOINT_PATTERNS: tuple[str, ...] = (
    "/v1/models/{model}/load",
    "/v1/models/load",
    "/v1/models/{model}",
    "/api/v1/models/{model}/load",
    "/api/v1/models/load",
    "/api/v1/models/{model}",
    "/models/{model}/load",
    "/models/load",
    "/models/{model}",
)
_UNLOAD_ENDPOINT_PATTERNS: tuple[str, ...] = (
    "/v1/models/{model}/unload",
    "/v1/models/unload",
    "/v1/models/{model}",
    "/api/v1/models/{model}/unload",
    "/api/v1/models/unload",
    "/api/v1/models/{model}",
    "/models/{model}/unload",
    "/models/unload",
    "/models/{model}",
)


def _build_lmstudio_url(base: str, endpoint: str) -> str:
    """
    Join ``base`` and ``endpoint`` while avoiding duplicated version prefixes.
    """

    base_trimmed = base.rstrip("/")
    path = endpoint
    for prefix in ("/v1", "/api/v1"):
        if base_trimmed.endswith(prefix) and path.startswith(prefix):
            path = path[len(prefix) :] or ""
            if path and not path.startswith("/"):
                path = "/" + path
            break

    if not path.startswith("/"):
        path = "/" + path if path else ""

    return base_trimmed + path


def _fetch_models(
    base_url: str, headers: dict[str, str]
) -> Tuple[set[str], Optional[str]]:
    """
    Return (models, successful_endpoint) by probing known LM Studio endpoints.

    Recent LM Studio releases mirror OpenAI's `/v1/models` endpoint, while older
    builds exposed `/api/v1/models` or `/models`. We probe the known variants and
    return the first that yields a usable payload.
    """
    last_error: Optional[requests.RequestException] = None
    for endpoint in _MODEL_ENDPOINTS:
        url = _build_lmstudio_url(base_url, endpoint)
        try:
            response = requests.get(url, headers=headers, timeout=5)
            response.raise_for_status()
            payload = response.json()
        except requests.RequestException as exc:
            last_error = exc
            logger.debug("LM Studio GET %s failed: %s", url, exc)
            continue

        models: set[str] = set()
        if isinstance(payload, dict) and "data" in payload:
            for item in payload["data"]:
                if isinstance(item, dict):
                    identifier = item.get("id") or item.get("name")
                    if identifier:
                        models.add(str(identifier))
                elif isinstance(item, str):
                    models.add(item)
        elif isinstance(payload, list):
            for item in payload:
                if isinstance(item, dict):
                    identifier = item.get("id") or item.get("name")
                    if identifier:
                        models.add(str(identifier))
                elif isinstance(item, str):
                    models.add(item)

        logger.debug("LM Studio models from %s: %s", url, models or "<empty>")
        return models, endpoint

    if last_error:
        raise last_error
    return set(), None


def _load_model_http(
    base_url: str,
    headers: dict[str, str],
    model: str,
    endpoint_hint: Optional[str],
) -> bool:
    """
    Attempt to load the requested model via LM Studio's HTTP API.

    Returns True if any request returns a 2xx status code.
    """
    def candidate_paths() -> Iterable[str]:
        if endpoint_hint and endpoint_hint.startswith("/v1"):
            primary = [p for p in _LOAD_ENDPOINT_PATTERNS if p.startswith("/v1")]
            secondary = [p for p in _LOAD_ENDPOINT_PATTERNS if not p.startswith("/v1")]
            yield from primary + secondary
        elif endpoint_hint and endpoint_hint.startswith("/api/v1"):
            primary = [p for p in _LOAD_ENDPOINT_PATTERNS if p.startswith("/api/v1")]
            secondary = [p for p in _LOAD_ENDPOINT_PATTERNS if not p.startswith("/api/v1")]
            yield from primary + secondary
        elif endpoint_hint:
            primary = [p for p in _LOAD_ENDPOINT_PATTERNS if not p.startswith("/api/v1")]
            secondary = [p for p in _LOAD_ENDPOINT_PATTERNS if p.startswith("/api/v1")]
            yield from primary + secondary
        else:
            yield from _LOAD_ENDPOINT_PATTERNS

    for template in candidate_paths():
        url = _build_lmstudio_url(base_url, template.format(model=model))
        body_candidates = (
            None,
            {"model": model},
            {"id": model},
            {"name": model},
        )
        for body in body_candidates:
            try:
                logger.debug("Attempting LM Studio load via %s body=%s", url, body)
                if body is None:
                    response = requests.post(url, headers=headers, timeout=10)
                else:
                    enriched_headers = dict(headers)
                    enriched_headers["Content-Type"] = "application/json"
                    response = requests.post(
                        url,
                        headers=enriched_headers,
                        json=body,
                        timeout=10,
                    )
                if response.status_code < 400:
                    logger.info(
                        "LM Studio accepted load request via %s (status %s)",
                        url,
                        response.status_code,
                    )
                    return True
                logger.debug(
                    "LM Studio rejected load request via %s (status %s: %s)",
                    url,
                    response.status_code,
                    response.text,
                )
            except requests.RequestException as exc:
                logger.debug("LM Studio load request failed via %s: %s", url, exc)
                continue
    return False


def _load_model_cli(model: str) -> bool:
    """
    Attempt to load the model using the `lms` CLI if available.
    """
    try:
        logger.debug("Attempting CLI load for model '%s'", model)
        result = subprocess.run(
            ["lms", "load", model],
            check=False,
            capture_output=True,
            text=True,
            timeout=30,
        )
    except FileNotFoundError:
        logger.debug("LM Studio CLI (lms) not found on PATH; skipping CLI load.")
        return False
    except subprocess.SubprocessError as exc:  # pragma: no cover - defensive
        logger.debug("LM Studio CLI load failed: %s", exc)
        return False

    if result.returncode == 0:
        logger.info("LM Studio CLI reported successful load for '%s'.", model)
        return True

    logger.debug(
        "LM Studio CLI returned %s: %s %s",
        result.returncode,
        result.stdout,
        result.stderr,
    )
    return False


def _host_from_api_base(api_base: str | None) -> Optional[str]:
    if not api_base:
        return None
    parsed = urlparse(str(api_base))
    host = parsed.netloc or parsed.path
    host = host.strip("/") if host else ""
    return host or None


def _configure_sdk_client(config: AppConfig) -> None:
    if _LMSTUDIO_SDK is None:
        return
    host = _host_from_api_base(config.lm_api_base)
    if not host:
        return
    try:
        configure = getattr(_LMSTUDIO_SDK, "configure_default_client", None)
        if callable(configure):
            configure(host)
    except Exception as exc:  # pragma: no cover - diagnostic only
        logger.debug("LM Studio SDK configure_default_client failed: %s", exc)


def _unload_model_sdk(config: AppConfig) -> bool:
    """
    Attempt to unload the configured model via the official LM Studio Python SDK.
    """
    if _LMSTUDIO_SDK is None:
        return False

    _configure_sdk_client(config)

    target_key = (config.lm_model or "").strip()
    handles: list = []
    try:
        handles = list(_LMSTUDIO_SDK.list_loaded_models("llm"))  # type: ignore[attr-defined]
    except AttributeError:
        try:
            client = _LMSTUDIO_SDK.get_default_client()  # type: ignore[attr-defined]
            handles = list(client.llm.list_loaded_models())  # type: ignore[attr-defined]
        except Exception as exc:  # pragma: no cover - diagnostic path
            logger.debug("LM Studio SDK list_loaded_models unavailable: %s", exc)
            handles = []
    except Exception as exc:  # pragma: no cover - diagnostic path
        logger.debug("LM Studio SDK list_loaded_models failed: %s", exc)
        handles = []

    selected = []
    for handle in handles:
        try:
            identifier = getattr(handle, "identifier", None)
            model_key = getattr(handle, "model_key", None) or getattr(handle, "modelKey", None)
        except Exception:  # pragma: no cover - defensive
            identifier = model_key = None
        if target_key and target_key not in {identifier, model_key}:
            continue
        selected.append(handle)
    if not selected:
        selected = handles

    success = False
    for handle in selected:
        try:
            handle.unload()
            success = True
        except Exception as exc:  # pragma: no cover - diagnostic path
            logger.debug("LM Studio SDK failed to unload handle %r: %s", handle, exc)

    if success:
        logger.info("LM Studio SDK unloaded model '%s'.", target_key or selected[0])
        return True

    try:
        if target_key:
            handle = _LMSTUDIO_SDK.llm(target_key)  # type: ignore[attr-defined]
        else:
            handle = _LMSTUDIO_SDK.llm()  # type: ignore[attr-defined]
    except TypeError:
        handle = _LMSTUDIO_SDK.llm()  # type: ignore[attr-defined]
    except Exception as exc:  # pragma: no cover - diagnostic path
        logger.debug("LM Studio SDK llm(%s) failed: %s", target_key or "<default>", exc)
        return False

    try:
        handle.unload()
        logger.info("LM Studio SDK unloaded model '%s'.", target_key or getattr(handle, "model_key", "<default>"))
        return True
    except Exception as exc:  # pragma: no cover - diagnostic path
        logger.debug("LM Studio SDK handle unload failed: %s", exc)
        return False


def _unload_model_http(
    base_url: str,
    headers: dict[str, str],
    model: str,
    endpoint_hint: Optional[str],
) -> bool:
    """
    Attempt to unload the requested model via LM Studio's HTTP API.

    Returns True if any request returns a 2xx status code.
    """

    def candidate_paths() -> Iterable[str]:
        if endpoint_hint and endpoint_hint.startswith("/v1"):
            primary = [p for p in _UNLOAD_ENDPOINT_PATTERNS if p.startswith("/v1")]
            secondary = [p for p in _UNLOAD_ENDPOINT_PATTERNS if not p.startswith("/v1")]
            yield from primary + secondary
        elif endpoint_hint and endpoint_hint.startswith("/api/v1"):
            primary = [p for p in _UNLOAD_ENDPOINT_PATTERNS if p.startswith("/api/v1")]
            secondary = [p for p in _UNLOAD_ENDPOINT_PATTERNS if not p.startswith("/api/v1")]
            yield from primary + secondary
        elif endpoint_hint:
            primary = [p for p in _UNLOAD_ENDPOINT_PATTERNS if not p.startswith("/api/v1")]
            secondary = [p for p in _UNLOAD_ENDPOINT_PATTERNS if p.startswith("/api/v1")]
            yield from primary + secondary
        else:
            yield from _UNLOAD_ENDPOINT_PATTERNS

    for template in candidate_paths():
        url = _build_lmstudio_url(base_url, template.format(model=model))
        body_candidates = (
            None,
            {"model": model},
            {"id": model},
            {"name": model},
        )
        for body in body_candidates:
            try:
                logger.debug("Attempting LM Studio unload via POST %s body=%s", url, body)
                if body is None:
                    response = requests.post(url, headers=headers, timeout=10)
                else:
                    enriched_headers = dict(headers)
                    enriched_headers["Content-Type"] = "application/json"
                    response = requests.post(
                        url,
                        headers=enriched_headers,
                        json=body,
                        timeout=10,
                    )
                if response.status_code < 400:
                    logger.info(
                        "LM Studio accepted unload request via POST %s (status %s)",
                        url,
                        response.status_code,
                    )
                    return True
                logger.debug(
                    "LM Studio rejected unload via POST %s (status %s: %s)",
                    url,
                    response.status_code,
                    response.text,
                )
            except requests.RequestException as exc:
                logger.debug("LM Studio unload request failed via %s: %s", url, exc)
                continue
    return False


def _unload_model_cli(model: str) -> bool:
    """
    Attempt to unload the model using the `lms` CLI if available.
    """
    try:
        logger.debug("Attempting CLI unload for model '%s'", model)
        result = subprocess.run(
            ["lms", "unload", model],
            check=False,
            capture_output=True,
            text=True,
            timeout=30,
        )
    except FileNotFoundError:
        logger.debug("LM Studio CLI (lms) not found on PATH; skipping CLI unload.")
        return False
    except subprocess.SubprocessError as exc:  # pragma: no cover - defensive
        logger.debug("LM Studio CLI unload failed: %s", exc)
        return False

    if result.returncode == 0:
        logger.info("LM Studio CLI reported successful unload for '%s'.", model)
        return True

    logger.debug(
        "LM Studio CLI unload returned %s: %s %s",
        result.returncode,
        result.stdout,
        result.stderr,
    )
    return False


def _ensure_lmstudio_ready(config: AppConfig) -> None:
    """
    Confirm that LM Studio exposes the requested model, attempting to load it if needed.

    Raises
    ------
    LMStudioConnectivityError
        If the LM Studio server cannot be contacted or refuses to expose the model.
    """

    headers = {"Authorization": f"Bearer {config.lm_api_key or ''}"}
    base = config.lm_api_base.rstrip("/")

    try:
        models, endpoint_hint = _fetch_models(base, headers)
    except requests.RequestException as exc:
        raise LMStudioConnectivityError(
            f"Failed to reach LM Studio at {base}: {exc}"
        ) from exc

    if config.lm_model in models:
        logger.debug("LM Studio already has model '%s' loaded.", config.lm_model)
        return

    logger.info(
        "LM Studio does not advertise model '%s'; attempting to load it automatically.",
        config.lm_model,
    )

    loaded = _load_model_http(base, headers, config.lm_model, endpoint_hint)
    if not loaded:
        loaded = _load_model_cli(config.lm_model)

    if not loaded:
        raise LMStudioConnectivityError(
            f"Unable to load model '{config.lm_model}' automatically. "
            "Please load it in the LM Studio UI and retry."
        )

    # Re-query to confirm the model is present.
    try:
        models, _ = _fetch_models(base, headers)
    except requests.RequestException as exc:
        raise LMStudioConnectivityError(
            f"Verified load but subsequent model fetch failed: {exc}"
        ) from exc

    if config.lm_model not in models:
        raise LMStudioConnectivityError(
            f"Model '{config.lm_model}' did not appear in LM Studio after load attempts. "
            "Check the LM Studio logs for more details."
        )

    logger.info("LM Studio model '%s' is ready.", config.lm_model)


def configure_lmstudio_lm(config: AppConfig, *, cache: bool = False) -> dspy.LM:
    """
    Configure DSPy to talk to LM Studio's OpenAI-compatible endpoint.
    """

    _ensure_lmstudio_ready(config)

    lm = dspy.LM(
        f"openai/{config.lm_model}",
        api_base=config.lm_api_base,
        api_key=config.lm_api_key,
        cache=cache,
        streaming=config.lm_streaming,
    )
    dspy.configure(lm=lm)
    return lm


def unload_lmstudio_model(config: AppConfig) -> None:
    """
    Attempt to unload the configured LM Studio model to free resources.
    """

    if _unload_model_sdk(config):
        return

    headers = {"Authorization": f"Bearer {config.lm_api_key or ''}"}
    base = config.lm_api_base.rstrip("/")

    try:
        _, endpoint_hint = _fetch_models(base, headers)
    except requests.RequestException as exc:  # pragma: no cover - informational
        endpoint_hint = None
        logger.debug("Unable to refresh LM Studio endpoint hint before unload: %s", exc)

    if _unload_model_http(base, headers, config.lm_model, endpoint_hint):
        return

    if _unload_model_cli(config.lm_model):
        return

    logger.warning(
        "Failed to unload LM Studio model '%s' via SDK, HTTP, or CLI. The model may remain loaded.",
        config.lm_model,
    )


__all__ = ["configure_lmstudio_lm", "LMStudioConnectivityError", "unload_lmstudio_model"]


--- tests/test_analyzer.py ---
from __future__ import annotations

from lmstudiotxt_generator import analyzer


def test_build_dynamic_buckets_uses_default_branch_and_filters_dead_links(monkeypatch):
    recorded = []

    def fake_construct(repo_url, path, ref=None):
        recorded.append((repo_url, path, ref))
        return f"https://example.com/{ref or 'none'}/{path}"

    monkeypatch.setattr(analyzer, "construct_raw_url", fake_construct)
    monkeypatch.setattr(analyzer, "_url_alive", lambda url: "keep" in url)

    file_tree = "docs/keep.md\nREADME.md\ntrash/missing.md"
    buckets = analyzer.build_dynamic_buckets(
        "https://github.com/example/repo",
        file_tree,
        default_ref="custom-branch",
        validate_urls=True,
    )

    # Only the URL containing 'keep' should remain after validation.
    assert any("keep.md" in url for _, items in buckets for _, url, _ in items)
    assert all("missing.md" not in url for _, items in buckets for _, url, _ in items)
    # construct_raw_url should receive the explicit default branch.
    assert all(ref == "custom-branch" for _, _, ref in recorded if ref is not None)


--- tests/test_full_builder.py ---
from __future__ import annotations

import requests

from lmstudiotxt_generator import full_builder


def _curated_link(ref: str = "main") -> str:
    return f"- [Example](https://github.com/owner/repo/blob/{ref}/dir/file.py)"


def test_build_llms_full_prefers_raw(monkeypatch):
    captured = {}

    def fake_fetch_raw(owner, repo, path, ref):
        captured["call"] = (owner, repo, path, ref)
        return b"print('hello world')\n"

    def explode(*args, **kwargs):
        raise AssertionError("gh_get_file should not be used for public repos")

    monkeypatch.setattr(full_builder, "fetch_raw_file", fake_fetch_raw)
    monkeypatch.setattr(full_builder, "gh_get_file", explode)

    output = full_builder.build_llms_full_from_repo(
        _curated_link(),
        prefer_raw=True,
        default_ref="main",
    )

    assert captured["call"] == ("owner", "repo", "dir/file.py", "main")
    assert "print('hello world')" in output


def test_build_llms_full_private_repo_uses_api(monkeypatch):
    def fake_fetch_raw(*args, **kwargs):
        raise AssertionError("fetch_raw_file should not be used for private repos")

    def fake_gh_get(owner, repo, path, ref, token):
        assert token == "token-123"
        assert ref == "main"
        return "file", b"api-content\n"

    monkeypatch.setattr(full_builder, "fetch_raw_file", fake_fetch_raw)
    monkeypatch.setattr(full_builder, "gh_get_file", fake_gh_get)

    output = full_builder.build_llms_full_from_repo(
        _curated_link(),
        prefer_raw=False,
        default_ref="main",
        token="token-123",
    )

    assert "api-content" in output


def test_build_llms_full_403_hint(monkeypatch):
    def fake_fetch_raw(*args, **kwargs):
        raise AssertionError("fetch_raw_file should not be used when prefer_raw=False")

    def fake_gh_get(owner, repo, path, ref, token):
        response = requests.Response()
        response.status_code = 403
        response.reason = "Forbidden"
        http_error = requests.HTTPError("Forbidden")
        http_error.response = response
        raise http_error

    monkeypatch.setattr(full_builder, "fetch_raw_file", fake_fetch_raw)
    monkeypatch.setattr(full_builder, "gh_get_file", fake_gh_get)

    output = full_builder.build_llms_full_from_repo(
        _curated_link(),
        prefer_raw=False,
        default_ref="main",
        token="token-123",
    )

    assert "HTTP 403 Forbidden" in output
    assert "Verify that GITHUB_ACCESS_TOKEN or GH_TOKEN" in output


--- tests/test_lmstudio.py ---
from __future__ import annotations

from pathlib import Path

import pytest
import requests

from lmstudiotxt_generator.config import AppConfig
from lmstudiotxt_generator import pipeline
import lmstudiotxt_generator.lmstudio as lmstudio
from lmstudiotxt_generator.lmstudio import LMStudioConnectivityError


class _FakeResponse:
    def __init__(self, status_code=200, payload=None, text="OK"):
        self.status_code = status_code
        self._payload = payload or {}
        self.text = text

    def raise_for_status(self):
        if self.status_code >= 400:
            raise requests.HTTPError(self.text)

    def json(self):
        return self._payload


def test_fetch_models_prefers_v1(monkeypatch):
    calls = []

    def fake_get(url, headers=None, timeout=None):
        calls.append(url)
        if url.endswith("/v1/models"):
            return _FakeResponse(
                payload={"data": [{"id": "model-a"}, {"name": "model-b"}]},
            )
        if url.endswith("/models"):
            raise requests.RequestException("legacy endpoint disabled")
        raise AssertionError(f"Unexpected URL {url}")

    monkeypatch.setattr(lmstudio.requests, "get", fake_get)
    config = AppConfig(
        lm_model="model-a",
        lm_api_base="http://localhost:1234/v1",
        lm_api_key="key",
        output_dir=Path("artifacts"),
    )

    lmstudio._ensure_lmstudio_ready(config)

    assert calls[0].endswith("/v1/models")


def test_ensure_ready_auto_load(monkeypatch):
    sequence = [
        _FakeResponse(payload={"data": []}),
        _FakeResponse(payload={"data": [{"id": "target"}]}),
    ]
    posts = []

    def fake_get(url, headers=None, timeout=None):
        return sequence.pop(0)

    def fake_post(url, headers=None, json=None, timeout=None):
        posts.append((url, json))
        return _FakeResponse()

    monkeypatch.setattr(lmstudio.requests, "get", fake_get)
    monkeypatch.setattr(lmstudio.requests, "post", fake_post)

    config = AppConfig(
        lm_model="target",
        lm_api_base="http://localhost:1234/v1",
        lm_api_key="key",
        output_dir=Path("artifacts"),
    )

    lmstudio._ensure_lmstudio_ready(config)

    assert posts  # auto-load attempted


def test_ensure_ready_failure(monkeypatch):
    def fake_get(url, headers=None, timeout=None):
        return _FakeResponse(payload={"data": []})

    def fake_post(url, headers=None, json=None, timeout=None):
        return _FakeResponse(status_code=404, text="missing")

    monkeypatch.setattr(lmstudio.requests, "get", fake_get)
    monkeypatch.setattr(lmstudio.requests, "post", fake_post)
    monkeypatch.setattr(
        lmstudio,
        "_load_model_cli",
        lambda model: False,
    )

    config = AppConfig(
        lm_model="missing-model",
        lm_api_base="http://localhost:1234/v1",
        lm_api_key="key",
        output_dir=Path("artifacts"),
    )

    with pytest.raises(LMStudioConnectivityError):
        lmstudio._ensure_lmstudio_ready(config)


def test_pipeline_fallback(tmp_path, monkeypatch):
    repo_url = "https://github.com/example/repo"
    repo_root = tmp_path / "artifacts"

    def fake_configure(*args, **kwargs):
        raise LMStudioConnectivityError("LM unavailable")

    fake_material = pipeline.RepositoryMaterial(
        repo_url=repo_url,
        file_tree="README.md\nsrc/main.py",
        readme_content="# Title\n\nSummary",
        package_files="",
        default_branch="main",
        is_private=False,
    )

    class FakeAnalyzer:
        def __call__(self, *args, **kwargs):
            raise AssertionError("Should not be invoked because configure fails")

    monkeypatch.setattr(pipeline, "configure_lmstudio_lm", fake_configure)
    monkeypatch.setattr(pipeline, "prepare_repository_material", lambda *a, **k: fake_material)
    monkeypatch.setattr(pipeline, "RepositoryAnalyzer", lambda: FakeAnalyzer())
    config = AppConfig(
        lm_model="missing",
        lm_api_base="http://localhost:1234/v1",
        lm_api_key="key",
        output_dir=repo_root,
    )

    artifacts = pipeline.run_generation(repo_url, config)

    assert artifacts.used_fallback is True
    assert Path(artifacts.llms_txt_path).exists()
    assert Path(artifacts.llms_full_path).exists()
    assert Path(artifacts.json_path).exists()


def test_pipeline_unloads_model(tmp_path, monkeypatch):
    repo_url = "https://github.com/example/repo"
    repo_root = tmp_path / "artifacts"

    fake_material = pipeline.RepositoryMaterial(
        repo_url=repo_url,
        file_tree="README.md\nsrc/main.py",
        readme_content="# Title\n\nSummary",
        package_files="",
        default_branch="main",
        is_private=False,
    )

    class FakeAnalyzer:
        def __call__(self, *args, **kwargs):
            return type("Result", (), {"llms_txt_content": "# Generated\n"})()

    unload_called = {}

    monkeypatch.setattr(pipeline, "prepare_repository_material", lambda *a, **k: fake_material)
    monkeypatch.setattr(pipeline, "RepositoryAnalyzer", lambda: FakeAnalyzer())
    monkeypatch.setattr(pipeline, "configure_lmstudio_lm", lambda *a, **k: None)
    monkeypatch.setattr(
        pipeline,
        "build_llms_full_from_repo",
        lambda content, **_: content + "\n--- full ---\n",
    )
    monkeypatch.setattr(
        pipeline,
        "unload_lmstudio_model",
        lambda cfg: unload_called.setdefault("done", True),
    )

    config = AppConfig(
        lm_model="model",
        lm_api_base="http://localhost:1234/v1",
        lm_api_key="key",
        output_dir=repo_root,
        lm_auto_unload=True,
    )

    artifacts = pipeline.run_generation(repo_url, config)

    assert unload_called.get("done") is True
    assert Path(artifacts.llms_txt_path).exists()
    assert Path(artifacts.llms_full_path).exists()


def test_unload_prefers_sdk(monkeypatch):
    handle_unloaded = {}

    class FakeHandle:
        identifier = "model"
        model_key = "model"

        def unload(self):
            handle_unloaded["done"] = True

    class FakeSDK:
        def __init__(self):
            self.hosts = []

        def configure_default_client(self, host):
            self.hosts.append(host)

        def list_loaded_models(self, kind=None):
            return [FakeHandle()]

    fake_sdk = FakeSDK()
    monkeypatch.setattr(lmstudio, "_LMSTUDIO_SDK", fake_sdk, raising=False)

    def should_not_run(*args, **kwargs):
        raise AssertionError("Fallback path should not execute when SDK succeeds")

    monkeypatch.setattr(lmstudio, "_unload_model_http", should_not_run, raising=False)
    monkeypatch.setattr(lmstudio, "_unload_model_cli", should_not_run, raising=False)

    config = AppConfig(
        lm_model="model",
        lm_api_base="http://localhost:1234/v1",
        lm_api_key=None,
        output_dir=Path("artifacts"),
    )

    lmstudio.unload_lmstudio_model(config)

    assert handle_unloaded.get("done") is True
    assert fake_sdk.hosts == ["localhost:1234"]
