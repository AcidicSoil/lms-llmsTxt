# llms-full (private-aware)
> Built by GitHub fetches (raw or authenticated). Large files may be truncated.

--- example-data/prompt-front-matter/_shared__tm__overview.tm.refactor.md ---
# TaskMaster Overview

## Metadata

- **identifier**: tm-overview  
- **category**: summarization  
- **lifecycle_stage**: inspection  
- **dependencies**: tasks.json  
- **provided_artifacts**: overview bullets, totals table, top pending list, critical path list, issues list  
- **summary**: Summarize TaskMaster tasks.json by status, priority, and dependency health to orient work.

## Inputs

- `tasks.json` path (optional; defaults to repo root)

## Canonical taxonomy (exact strings)

- summarization
- analysis
- reporting

### Stage hints (for inference)

- inspection → summarizing state, reading data
- analysis → detecting cycles, computing paths
- reporting → outputting tables and lists

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).  

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- Body text is not altered.

## Validation

- Identifier matches a normalized id pattern (e.g., kebab-case).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints (inspection, analysis, reporting).
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Input: `/tm-overview`  
  Output:  
    # Overview  
    - Bullet summary of status, priority, dependencies  
    ## Totals  
    | status       | count | percent | notes         |  
    |--------------|-------|---------|---------------|  
    | pending      | 5     | 40%     | high volume   |  
    | in_progress  | 3     | 25%     | active        |  
    | blocked      | 1     | 8%      | dependency    |  
    | done         | 6     | 50%     | completed     |  
    ## Top Pending  
    | id   | title               | priority | unblockers          |  
    |------|---------------------|----------|---------------------|  
    | t-12 | Fix login timeout   | high     | resolve API error   |  
    | t-34 | Deploy frontend     | medium   | wait for backend    |  
    ## Critical Path  
    - t-12 → t-34 → t-56  
    ## Issues  
    - Cycle detected: t-78 → t-90 → t-78  
    - Missing reference: t-11 (no dependencies)  
    - Duplicate entry: t-44 appears twice  

---

# TaskMaster Overview

Trigger: /tm-overview

Purpose: Summarize the current TaskMaster tasks.json by status, priority, dependency health, and critical path to orient work.

Steps:

1. Locate the active tasks.json at repo root or the path supplied in the user message. Do not modify it.
2. Parse fields: id, title, description, status, priority, dependencies, subtasks.
3. Compute counts per status and a table of top pending items by priority.
4. Detect dependency issues: cycles, missing ids, orphans (no deps and not depended on).
5. Approximate a critical path: longest dependency chain among pending→in_progress tasks.

Output format:

- "# Overview" then a bullets summary.
- "## Totals" as a 4-column table: status | count | percent | notes.
- "## Top Pending" table: id | title | priority | unblockers.
- "## Critical Path" as an ordered list of ids with short titles.
- "## Issues" list for cycles, missing references, duplicates.

Examples:

- Input (Codex TUI): /tm-overview
- Output: tables and lists as specified. Keep to <= 200 lines.

Notes:

- Read-only. Assume statuses: pending | in_progress | blocked | done.
- If tasks.json is missing or invalid, output an "## Errors" section with a concise diagnosis.

Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## template_markdown ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.


--- example-data/codex-prompts/tm-overview.refactor.md ---
# TaskMaster Overview

Trigger: $1

Purpose: Summarize the current TaskMaster tasks.json by status, priority, dependency health, and critical path to orient work.

Steps:

1. Locate the active tasks.json at repo root or the path supplied in the user message. Do not modify it.
2. Parse fields: id, title, description, status, priority, dependencies, subtasks.
3. Compute counts per status and a table of top pending items by priority.
4. Detect dependency issues: cycles, missing ids, orphans (no deps and not depended on).
5. Approximate a critical path: longest dependency chain among pending→in_progress tasks.

Output format:

- "# Overview" then a bullets summary.
- "## Totals" as a 4-column table: status | count | percent | notes.
- "## Top Pending" table: id | title | priority | unblockers.
- "## Critical Path" as an ordered list of ids with short titles.
- "## Issues" list for cycles, missing references, duplicates.

Examples:

- Input (Codex TUI): $2
- Output: tables and lists as specified. Keep to <= 200 lines.

Notes:

- Read-only. Assume statuses: pending | in_progress | blocked | done.
- If tasks.json is missing or invalid, output an "## Errors" section with a concise diagnosis.

---

### Affected files
$3

### Root cause
$4

### Proposed fix
$5

### Tests
$6

### Docs gaps
$7

### Open questions
$8


--- example-data/prompt-front-matter/10-scaffold__conventions__version-control-guide.conventions.refactor.md ---
# Version Control Guide

## Metadata

- **Identifier**: version-control-guide
- **Categories**: development practice, workflow guide, code hygiene
- **Stage**: implementation
- **Dependencies**: none
- **Provided Artifacts**: checklist, suggested commands
- **Summary**: Enforce clean incremental commits and clean-room re-implementation to ensure reproducible and safe changes.

## Inputs

- Trigger: /version-control-guide
- Purpose: Enforce clean incremental commits and clean-room re-implementation when finalizing.
- Output format: Checklist plus suggested commands for the current repo state.
- Examples: Convert messy spike into three commits: setup, feature, tests.
- Notes: Never modify remote branches without confirmation.

## Canonical taxonomy (exact strings)

- development practice
- workflow guide
- code hygiene

### Stage hints (for inference)

- implementation
- commit workflow
- development lifecycle

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- Output body unchanged.

## Validation

- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Identifier: version-control-guide  
- Categories: development practice, workflow guide, code hygiene  
- Stage: implementation  
- Dependencies: none  
- Provided Artifacts: checklist, suggested commands  
- Summary: Enforce clean incremental commits and clean-room re-implementation to ensure reproducible and safe changes.

# Version Control Guide

Trigger: /version-control-guide

Purpose: Enforce clean incremental commits and clean-room re-implementation when finalizing.

## Steps

1. Start each feature from a clean branch: `git switch -c <feat>`.
2. Commit in vertical slices with passing tests: `git add -p && git commit`.
3. When solution is proven, recreate a minimal clean diff: stash or copy results, reset, then apply only the final changes.
4. Use `git revert` for bad commits instead of force-pushing shared branches.

## Output format

- Checklist plus suggested commands for the current repo state.

## Examples

- Convert messy spike into three commits: setup, feature, tests.

## Notes

- Never modify remote branches without confirmation.


--- example-data/prompt-front-matter/40-testing__coverage__guide.coverage.refactor.md ---
# Coverage Plan

## Inputs
- Command: `/coverage-guide`
- Input context: none (command runs without arguments)
- Expected output format: concise summary, prioritized recommendations with rationale, coverage gaps and validation steps

## Canonical taxonomy (exact strings)
- testing
- analysis
- prioritization

### Stage hints (for inference)
- analyze → gather data and propose insights
- plan → suggest actionable items
- execute → run tests or apply changes

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.  
   → Identifier: coverage-plan

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).  
   → Categories: testing, analysis, prioritization

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.  
   → Stage: analyze

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).  
   → Dependencies: find . -name 'coverage*', git ls-files

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.  
   → Artifacts: prioritized test recommendations, coverage gap identification, validation steps

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”  
   → Summary: Suggest a plan to raise coverage based on uncovered areas to achieve actionable, high-ROI test additions.

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.  

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤7.
- All values are derived from content or inference using canonical taxonomy and stage hints.

## Validation
- Identifier matches a normalized id pattern → yes (coverage-plan)
- Categories non-empty and drawn from canonical taxonomy (≤3) → yes
- Stage, if present, is one of the allowed stages implied by stage hints → yes (analyze)
- Dependencies, if present, are id-shaped (≤5) → yes
- Summary ≤120 chars; punctuation coherent → 118 characters
- Body text $1 is not altered.

## Output format examples
- Focus on src/auth/login.ts — 0% branch coverage; add error path test.
- Prioritize authentication modules with low branch coverage (e.g., login, token validation).
- Identify missing edge cases in user input handling and validate via unit tests.


--- example-data/prompt-front-matter/50-docs__api-docs__api-docs-local.api-docs.refactor.md ---
# API Docs Local

## Metadata

- **identifier**: api-docs-local
- **categories**: [documentation, retrieval, storage]
- **lifecycle_stage**: configuration
- **dependencies**: []
- **provided_artifacts**: ["docs/apis/ directory", "DOCS.md index file"]
- **summary**: Do fetch API docs and store locally to achieve offline, deterministic reference.

## Inputs

- URLs or package names to retrieve documentation from.

## Canonical taxonomy (exact strings)

- documentation
- retrieval
- storage
- configuration
- generation
- deployment
- validation

### Stage hints (for inference)

- configuration → setup of environment or initial state
- retrieval → fetching data from external sources
- storage → saving content locally
- deployment → making system available to users

## Algorithm

1. Extract signals from $1  
   *Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.*

2. Determine the primary identifier  
   *Prefer explicit input; otherwise infer from main action + object.*  
   *Normalize (lowercase, kebab-case, length-capped, starts with a letter).*  
   *De-duplicate.*

3. Determine categories  
   *Prefer explicit input; otherwise infer from verbs/headings vs $5.*  
   *Validate, sort deterministically, and de-dupe (≤3).*

4. Determine lifecycle/stage (optional)  
   *Prefer explicit input; otherwise map categories via $6.*  
   *Omit if uncertain.*

5. Determine dependencies (optional)  
   *Parse phrases implying order or prerequisites; keep id-shaped items (≤5).*

6. Determine provided artifacts (optional)  
   *Short list (≤3) of unlocked outputs.*

7. Compose summary  
   *One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”*

8. Produce metadata in the requested format  
   *Default to a human-readable serialization; honor any requested alternative.*

9. Reconcile if input already contains metadata  
   *Merge: explicit inputs > existing > inferred.*  
   *Validate lists; move unknowns to an extension field if needed.*  
   *Remove empty keys.*

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.

## Validation

- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Command list and file paths to place docs under `docs/apis/`.
- Example: curl -o docs/apis/github.com/api.json https://api.github.com/docs
- Example: npm view express docs --json > docs/apis/express/README.md

# API Docs Local

Trigger: /api-docs-local

Purpose: Fetch API docs and store locally for offline, deterministic reference.

## Steps

1. Create `docs/apis/` directory.
2. For each provided URL or package, write retrieval commands (curl or `npm view` docs links). Do not fetch automatically without confirmation.
3. Add `DOCS.md` index linking local copies.

## Output format

- Command list and file paths to place docs under `docs/apis/`.


--- example-data/prompt-front-matter/50-docs__api-docs__openapi-generate.api-docs.refactor.md ---
# OpenAPI Generate

## Metadata

- **Identifier**: generate-api  
- **Categories**: code generation, api scaffolding, build  
- **Stage**: build  
- **Dependencies**: none  
- **Provided Artifacts**: 
  - Summary table of generated paths  
  - Scripts to add (e.g., `make generate-api`, `pnpm sdk:gen`)  
  - TODO list for unimplemented handlers  
- **Summary**: Generate server stubs or typed clients from an OpenAPI spec to achieve code scaffolding with validation and CI checks.

## Inputs

- Command: `/openapi-generate <server|client> <lang> <spec-path>`
- Parameters:
  - `<server>`: Generates controllers, routers, validation, and error middleware into `apps/api`
  - `<client>`: Generates a typed SDK into `packages/sdk` with fetch wrapper and retry/backoff
  - `<spec-path>`: Path to OpenAPI spec (e.g., `apis/auth/openapi.yaml`)
- Output format: Summary table of generated paths, scripts to add, and next actions

## Canonical taxonomy (exact strings)

- code generation  
- api scaffolding  
- build  

### Stage hints (for inference)

- generate → build  
- scaffold → build  
- script addition → build  

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.  

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.  

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).  

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.  

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).  

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.  

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”  

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.  

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.  

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.

## Validation

- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- `/openapi-generate client ts apis/auth/openapi.yaml`
- Output: Summary table of generated paths, scripts to add, and next actions
- Notes: Prefer openapi-typescript + zod for TS clients when possible

---

# OpenAPI Generate

Trigger: /openapi-generate <server|client> <lang> <spec-path>

Purpose: Generate server stubs or typed clients from an OpenAPI spec.

**Steps:**

1. Validate `<spec-path>`; fail with actionable errors.
2. For `server`, generate controllers, routers, validation, and error middleware into `apps/api`.
3. For `client`, generate a typed SDK into `packages/sdk` with fetch wrapper and retry/backoff.
4. Add `make generate-api` or `pnpm sdk:gen` scripts and CI step to verify no drift.
5. Produce a diff summary and TODO list for unimplemented handlers.

**Output format:** summary table of generated paths, scripts to add, and next actions.

**Examples:** `/openapi-generate client ts apis/auth/openapi.yaml`.

**Notes:** Prefer openapi-typescript + zod for TS clients when possible.


--- example-data/prompt-front-matter/50-docs__doc-plan__gemini-map.doc-plan.refactor.md ---
# Gemini→Codex Mapper

Task: Given a TOML configuration for a Gemini CLI command, produce a structured Codex prompt file with metadata and example usage. The output must be ready to run via bash.

## Inputs
- TOML input containing `description`, `prompt`, and optional `Expected output` or `Usage`
- Target output format constraints (≤300 words, specific sections)

## Canonical taxonomy (exact strings)
- migration
- prompts
- tooling
- transform
- build
- validate

### Stage hints (for inference)
- "translation" → transform  
- "generates", "writes", "creates" → build  
- "validates" → validate  

## Algorithm
1. Extract signals from TOML:
   - Description and prompt define intent.
   - Expected output defines structure of result.

2. Determine the primary identifier:
   - Prefer explicit input; otherwise infer from main action + object.
   - Normalize to lowercase, kebab-case, length-capped (≤32), starts with letter.
   - Result: `gemini-map`

3. Determine categories:
   - Prefer explicit tags: migration, prompts, tooling
   - Validate and de-dupe → [migration, prompts, tooling]

4. Determine lifecycle/stage:
   - Map from "translation" to "transform"
   - Stage: transform

5. Determine dependencies:
   - No prerequisites mentioned.
   - Dependencies: []

6. Determine provided artifacts:
   - Codex prompt file (structured with role, steps, output, example)
   - Bash snippet for writing the file to `~/.codex/prompts/<filename>.md`

7. Compose summary:
   - "Do translate a Gemini CLI TOML command into a Codex prompt file to achieve structured, reusable prompt generation."

8. Produce metadata in human-readable format:
   - identifier: gemini-map
   - categories: migration, prompts, tooling
   - stage: transform
   - dependencies: []
   - artifacts: codex-prompt-file, bash-write-snippet
   - summary: Do translate a Gemini CLI TOML command into a Codex prompt file to achieve structured, reusable prompt generation.

9. Reconcile if input already contains metadata:
   - No existing metadata; all derived from explicit or inferable signals.

## Assumptions & Constraints
- Output must include metadata block followed by blank line and original body unchanged.
- All identifiers normalized and within constraints.
- Categories strictly from canonical taxonomy.
- Stage inferred via stage hints only if not explicit.
- Artifacts are short-listed (≤3).
- Summary ≤120 characters.

## Validation
- Identifier: `gemini-map` → valid kebab-case, lowercase.
- Categories: [migration, prompts, tooling] → all in taxonomy, non-empty, de-duplicated.
- Stage: transform → valid and implied by translation workflow.
- Dependencies: empty list → valid.
- Artifacts: codex-prompt-file, bash-write-snippet → both valid and ≤3.
- Summary: 108 characters; coherent and punctuated correctly.

## Output format examples
```markdown
# Gemini→Codex Mapper

identifier: gemini-map  
categories: migration, prompts, tooling  
stage: transform  
dependencies: []  
artifacts: codex-prompt-file, bash-write-snippet  
summary: Do translate a Gemini CLI TOML command into a Codex prompt file to achieve structured, reusable prompt generation.

You are a translator that converts a Gemini CLI TOML command into a Codex prompt file.

Steps:

1) Read TOML with `description` and `prompt`.
2) Extract the task, inputs, and outputs implied by the TOML.
3) Write a Codex prompt file ≤ 300 words:

    - Role line `You are ...`
    - Numbered steps
    - Output section
    - Example input and expected output
    - `Usage: /<command>` line
    - YAML-like metadata at top

4) Choose a short, hyphenated filename ≤ 32 chars.
5) Emit a ready-to-run bash snippet:
`cat > ~/.codex/prompts/<filename>.md << 'EOF'` … `EOF`.
6) Do not include destructive commands or secrets.

Example input:

```toml
description = "Draft a PR description"
prompt = "Create sections Summary, Context, Changes from diff stats"
Expected output:

A pr-desc.md file with the structure above and a bash cat > block.

Usage: /gemini-map
```
```


--- example-data/prompt-front-matter/50-docs__doc-plan__owners.doc-plan.refactor.md ---
# Owners

## Inputs
- Path to analyze (e.g., `src/components/Button.tsx`)
- Access to `.github/CODEOWNERS` file
- Git repository with recent commit logs (`git log --pretty='- %an %ae: %s'`)

## Canonical taxonomy (exact strings)
- CLI
- ownership
- review

### Stage hints (for inference)
- discovery
- analysis
- suggestion

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤7.

## Validation
- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: owners  
- Categories: CLI, ownership, review  
- Stage: discovery  
- Dependencies: CODEOWNERS file, git log access  
- Artifacts: @frontend-team (CODEOWNERS), @jane (last 5 commits)  
- Summary: Suggest owners/reviewers for a path using CODEOWNERS and commit history.

---

Trigger: /owners <path>

Purpose: Suggest likely owners or reviewers for the specified path.

You are a CLI assistant focused on helping contributors with the task: Suggest likely owners/reviewers for a path.

1. Gather context by inspecting `.github/CODEOWNERS` for the codeowners (if present); running `git log --pretty='- %an %ae: %s' -- {{args}} | sed -n '1,50p'` for the recent authors for the path.
2. Based on CODEOWNERS and git history, suggest owners.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Suggest likely owners/reviewers for a path.
- Reference evidence from CODEOWNERS or git history for each owner suggestion.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
src/components/Button.tsx

Expected Output:

- Likely reviewers: @frontend-team (CODEOWNERS), @jane (last 5 commits).


--- example-data/prompt-front-matter/50-docs__examples__api-usage.examples.refactor.md ---
# API Usage Analysis

## Metadata

- **identifier**: http-client
- **category**: API Usage Analysis
- **lifecycle_stage**: analysis
- **dependencies**: [rg, grep]
- **provided_artifacts**: 
  - Definition: src/network/httpClient.ts line 42
  - Key usages: services/userService.ts, hooks/useRequest.ts
- **summary**: Do analyze how an internal API is used to achieve clear documentation and visibility into its real-world applications.

## Inputs

- Input symbol: HttpClient
- Tool commands: `rg -n {{args}} . || grep -RIn {{args}} .`

## Canonical taxonomy (exact strings)

- API Usage Analysis
- Code Inspection
- Dependency Mapping
- Documentation Generation

### Stage hints (for inference)

- analysis
- inspection
- gathering
- review
- synthesis

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- All fields must be derived from content or logical inference.

## Validation

- Identifier matches a normalized id pattern (kebab-case, lowercase).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- identifier: http-client  
- category: API Usage Analysis  
- lifecycle_stage: analysis  
- dependencies: [rg, grep]  
- provided_artifacts: 
  - Definition: src/network/httpClient.ts line 42
  - Key usages: services/userService.ts, hooks/useRequest.ts
- summary: Do analyze how an internal API is used to achieve clear documentation and visibility into its real-world applications.


--- example-data/prompt-front-matter/50-docs__examples__reference-implementation.examples.refactor.md ---
# Reference Implementation

## Metadata

- **Identifier**: reference-implementation
- **Categories**: code-generation, api-mapping, diff-generation
- **Lifecycle Stage**: implementation
- **Dependencies**: target-module-path, example-url
- **Provided Artifacts**: side-by-side API table, patch suggestions
- **Summary**: Do map target module's API to reference to achieve consistent structure and naming.

## Steps

1. Accept a path or URL to an example. Extract its public API and patterns.
2. Map target module’s API to the reference.
3. Generate diffs that adopt the same structure and naming.

## Output format

- Side-by-side API table and patch suggestions.


--- example-data/prompt-front-matter/00-ideation__architecture__adr-new.architecture.refactor.md ---
# ADR Drafting Assistant

Task: Given the following prompt, produce a structured **metadata block** and then emit the original body unchanged. The metadata must expose identifiers, categories, optional lifecycle/stage, optional dependencies, optional provided artifacts, and a concise summary. Output = metadata, blank line, then the input text.

## Inputs
- Input prompt: "You are a CLI assistant focused on helping contributors with the task: Draft an Architecture Decision Record with pros/cons."
- Workflow steps: Gather context from `README.md`, draft ADR (Context, Decision, Status, Consequences), synthesize insights.
- Output requirements: Concise summary of goal; workflow triggers/failing jobs/proposed fixes; documented evidence for maintainers' trust.

## Canonical taxonomy (exact strings)
- architecture
- decision-making
- documentation

### Stage hints (for inference)
- ideation → early drafting, context gathering
- planning → structured output design
- implementation → actual code changes
- review → peer feedback or approval

## Algorithm
1. Extract signals from input:
   - Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.
2. Determine the primary identifier:
   - Prefer explicit input; otherwise infer from main action + object.
   - Normalize (lowercase, kebab-case, length-capped, starts with a letter).
   - De-duplicate.
3. Determine categories:
   - Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.
   - Validate, sort deterministically, and de-dupe (≤3).
4. Determine lifecycle/stage (optional):
   - Prefer explicit input; otherwise map categories via stage hints.
   - Omit if uncertain.
5. Determine dependencies (optional):
   - Parse phrases implying order or prerequisites; keep id-shaped items (≤5).
6. Determine provided artifacts (optional):
   - Short list (≤3) of unlocked outputs.
7. Compose summary:
   - One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”
8. Produce metadata in the requested format:
   - Default to a human-readable serialization; honor any requested alternative.
9. Reconcile if input already contains metadata:
   - Merge: explicit inputs > existing > inferred.
   - Validate lists; move unknowns to an extension field if needed.
   - Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then the original body.
- Limit distinct placeholders to ≤7.

## Validation
- Identifier matches a normalized id pattern (e.g., kebab-case, lowercase).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Artifacts are short (≤3) and relevant to output.
- Summary ≤120 chars; punctuation coherent.
- Body text is not altered.

## Output format examples
- Identifier: `adr-draft`
- Categories: architecture, decision-making, documentation
- Lifecycle stage: ideation
- Dependencies: README.md
- Provided artifacts: ADR with pros/cons, evidence summary, workflow insights
- Summary: "Draft an Architecture Decision Record with pros/cons to achieve transparent decision documentation."


--- example-data/prompt-front-matter/00-ideation__architecture__logging-strategy.architecture.refactor.md ---
# Logging Strategy

## Metadata

- identifier: logging-strategy
- categories: [observability, operations, security]
- stage: design
- dependencies: []
- provided_artifacts: ["diff hunks", "short guideline section"]
- summary: Do add or remove diagnostic logs with privacy in mind to achieve structured observability.

## Steps

1. Identify hotspots from recent failures.
2. Insert structured logs with contexts and correlation IDs.
3. Remove noisy or PII-leaking logs.
4. Document log levels and sampling in `OBSERVABILITY.md`.

## Output format

- Diff hunks and a short guideline section.


--- example-data/prompt-front-matter/00-ideation__architecture__modular-architecture.architecture.refactor.md ---
# Modular Architecture

## Metadata

- **identifier**: modular-architecture  
- **categories**: architecture  
- **stage**: design  
- **dependencies**: [module-boundaries-identification]  
- **provided-artifacts**: [module-graph, dependency-diff, contract-test-plan]  
- **summary**: Do modularize services to achieve clear boundaries and testable interfaces.

## Steps

1. Identify services/modules and their public contracts.
2. Flag cross-module imports and circular deps.
3. Propose boundaries, facades, and internal folders.
4. Add "contract tests" for public APIs.

## Output format

- Diagram-ready list of modules and edges, plus diffs.


--- example-data/prompt-front-matter/00-ideation__architecture__stack-evaluation.architecture.refactor.md ---
# Stack Evaluation

## Metadata

- identifier: stack-evaluation
- categories: [evaluation, analysis, recommendation]
- stage: evaluation
- dependencies: []
- provided_artifacts: ["decision memo", "next steps"]
- summary: Evaluate language/framework choices to achieve informed stay-or-switch decisions.

## Steps

1. Detect current stack and conventions.
2. List tradeoffs: maturity, tooling, available examples, hiring, and AI training coverage.
3. Recommend stay-or-switch with migration outline if switching.

## Output format

- Decision memo with pros/cons and next steps.


--- example-data/prompt-front-matter/00-ideation__design__action-diagram.design.refactor.md ---
# Action Diagram Metadata

## Inputs
- Source file path: C:\Users\user\projects\prompts\temp-prompts\00-ideation\design\action-diagram.design.md
- Maximum placeholders allowed: 7

## Canonical taxonomy (exact strings)
- devops
- pipeline
- workflow

### Stage hints (for inference)
- build → development stage
- deploy → production stage
- push → trigger stage

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.

## Validation
- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: build  
- Categories: devops, pipeline, workflow  
- Lifecycle stage: none  
- Dependencies: push  
- Provided artifacts: deployment artifact  
- Summary: Do build to achieve deployment after push

## Metadata
- identifier: build
- categories: ["devops", "pipeline", "workflow"]
- lifecycle_stage: null
- dependencies: ["push"]
- provided_artifacts: ["deployment artifact"]
- summary: Do build to achieve deployment after push

## Nodes
- build
- deploy

## Edges
- push -> build
- build -> deploy


--- example-data/prompt-front-matter/00-ideation__design__api-contract.design.refactor.md ---
# API Contract Design

## Inputs
- Feature or domain string (e.g., "accounts & auth")
- Existing documentation and requirements
- Preference for OpenAPI 3.1 or GraphQL SDL

## Canonical taxonomy (exact strings)
- design
- specification
- contract generation

### Stage hints (for inference)
- design → initial creation of a contract from inputs
- specification → detailed schema definition
- implementation → code generation phase

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- All categories must be from the canonical taxonomy.
- Stage mapping is deterministic and context-aware.

## Validation
- Identifier matches a normalized id pattern (e.g., api-contract).
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Dependencies must be explicit or inferable from input structure.
- Artifacts list ≤3 items; all valid outputs.
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: `api-contract`  
- Categories: design, specification, contract generation  
- Stage: design  
- Dependencies: feature/domain input, existing documentation  
- Artifacts: openapi.yaml, schema.graphql, changelog entry  
- Summary: "Do generate an API contract from requirements to achieve a standardized specification for endpoints."

---

# API Contract

Trigger: /api-contract "<feature or domain>"

Purpose: Author an initial OpenAPI 3.1 or GraphQL SDL contract from requirements.

**Steps:**

1. Parse inputs and existing docs. If REST, prefer OpenAPI 3.1 YAML; if GraphQL, produce SDL.
2. Define resources, operations, request/response schemas, error model, auth, and rate limit headers.
3. Add examples for each endpoint or type. Include pagination and filtering conventions.
4. Save to `apis/<domain>/openapi.yaml` or `apis/<domain>/schema.graphql`.
5. Emit changelog entry `docs/api/CHANGELOG.md` with rationale and breaking-change flags.

**Output format:**

- `Contract Path`, `Design Notes`, and a fenced code block with the spec body.

**Examples:**

- `/api-contract "accounts & auth"` → `apis/auth/openapi.yaml` with OAuth 2.1 flows.

**Notes:**

- Follow JSON:API style for REST unless caller specifies otherwise. Include `429` and `5xx` models.


--- example-data/prompt-front-matter/00-ideation__design__design-assets.design.refactor.md ---
# Design Assets

## Metadata

- **Identifier**: design-assets
- **Categories**: design, brand assets
- **Stage**: generate
- **Dependencies**: brand-colors, brand-name
- **Provided Artifacts**: asset-checklist, generation-commands
- **Summary**: Generate favicons and small design snippets from product brand to achieve consistent visual identity.

## Steps

1. Extract brand colors and name from README or config.
2. Produce favicon set, social preview, and basic UI tokens.
3. Document asset locations and references.

## Output format

- Asset checklist and generation commands.


--- example-data/prompt-front-matter/00-ideation__design__ui-screenshots.design.refactor.md ---
# UI Screenshots

## Metadata

- **Identifier**: ui-screenshots
- **Categories**: analysis, design, code-generation
- **Stage**: design-review
- **Dependencies**: []
- **Provided Artifacts**: issue-list, css-changes, component-updates
- **Summary**: Analyze UI screenshots to identify visual issues and generate actionable CSS or component changes

## Steps

1. Accept screenshot paths or links.
2. Describe visual hierarchy, spacing, contrast, and alignment issues.
3. Output concrete CSS or component changes.

## Output format

- Issue list and code snippets to fix visuals.


--- example-data/prompt-front-matter/00-ideation__requirements__plan-delta.requirements.refactor.md ---
# plan-delta

## Metadata

- **identifier**: plan-delta  
- **categories**: Planning, Task Management, Graph Maintenance  
- **lifecycle_stage**: Mid-Project Adjustment  
- **dependencies**: task graph history, user delta input  
- **provided_artifacts**: 
  - Updated tasks file (valid JSON)  
  - Delta document (Markdown with # Delta, ## Objectives, ## Constraints, ## Impacts, ## Decisions, ## Evidence)  
  - Readiness report (plain text: READY | BLOCKED | DEPRECATED)  
- **summary**: Orchestrate mid-project planning deltas to preserve history and update task graph readiness.

## Inputs

- User-provided delta text with objectives, constraints, findings
- Selection mode: Continue, Hybrid Rebaseline, Full Rebaseline
- Existing tasks file (tasks.json or equivalent)
- Repository context path for task and plan files

## Canonical taxonomy (exact strings)

Planning, Task Management, Graph Maintenance

### Stage hints (for inference)

Mid-project adjustment, delta update, planning revision, graph maintenance, readiness recalculation

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤7.
- All outputs are strictly defined in the output format section.

## Validation

- Identifier matches a normalized id pattern (lowercase, kebab-case).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped or context-based (≤5).
- Provided artifacts match exactly those listed in output format.
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Input →  
  ```
  Mode: Continue
  New objectives: add offline export for tasks
  Constraints: no DB migrations
  Findings: existing export lib supports JSON only
  ```  

  Output →  
  - Updated `tasks.json` with new task `T-342` { title: "Add CSV export", dependencies: ["T-120"], source_doc: "delta-20250921.md", lineage: ["T-120"], supersedes: [] }.  
  - `artifacts/delta-20250921-160500.md` populated with objectives, constraints, impacts, decisions, evidence.  
  - Readiness report lists `T-342` under READY if deps done.

- Input →  
  ```
  Mode: Hybrid Rebaseline
  Changes: ~30% of scope affected by auth provider swap
  ```  

  Output →  
  - Minor-plan version bump recorded in Delta Doc.  
  - New tasks added for provider swap; prior tasks kept with `deprecated` or `blocked` and lineage links.


--- example-data/prompt-front-matter/00-ideation__requirements__planning-process.requirements.refactor.md ---
# Planning Process

## Metadata

- identifier: planning-process
- categories: 
  - planning
  - task-management
  - risk-assessment
  - validation
- stage: planning
- dependencies: 
  - PLAN.md exists or is created
  - Git repository with test runner available
- provided-artifacts: 
  - updated PLAN.md with structured sections and checklist
- summary: Draft, refine, and execute a feature plan with strict scope control and progress tracking.

## Inputs

- Trigger: /planning-process
- Purpose: Draft, refine, and execute a feature plan with strict scope control and progress tracking.
- Output format: Update or create `PLAN.md` with the sections above. Include a checklist for **Tasks**. Keep lines under 100 chars.
- Examples:
  - Input: "Add OAuth login"
  - Output:
    - Goal: Let users sign in with Google.
    - Tasks: [ ] add Google client, [ ] callback route, [ ] session, [ ] E2E test.
    - Won't do: org SSO.
    - Ideas for later: Apple login.
- Notes:
  - Planning only. No code edits.
  - Assume a Git repo with test runner available.

## Canonical taxonomy (exact strings)

- planning
- task-management
- risk-assessment
- validation

### Stage hints (for inference)

- planning → planning
- task management → planning or execution
- validation → validation or testing
- risk assessment → planning or review

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- Do not alter the body text.

## Validation

- Identifier matches a normalized id pattern (kebab-case).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Artifacts ≤3 items.
- Summary ≤120 chars; punctuation coherent.
- Body text is not altered.

## Output format examples

- Metadata block:
  - identifier: planning-process
  - categories: 
    - planning
    - task-management
    - risk-assessment
    - validation
  - stage: planning
  - dependencies: 
    - PLAN.md exists or is created
    - Git repository with test runner available
  - provided-artifacts: 
    - updated PLAN.md with structured sections and checklist
  - summary: Draft, refine, and execute a feature plan with strict scope control and progress tracking.

- Body text (unchanged):
  # Planning Process

  Trigger: /planning-process

  Purpose: Draft, refine, and execute a feature plan with strict scope control and progress tracking.

  ## Steps

  1. If no plan file exists, create `PLAN.md`. If it exists, load it.
  2. Draft sections: **Goal**, **User Story**, **Milestones**, **Tasks**, **Won't do**, **Ideas for later**, **Validation**, **Risks**.
  3. Trim bloat. Convert vague bullets into testable tasks with acceptance criteria.
  4. Tag each task with an owner and estimate. Link to files or paths that will change.
  5. Maintain two backlogs: **Won't do** (explicit non-goals) and **Ideas for later** (deferrable work).
  6. Mark tasks done after tests pass. Append commit SHAs next to completed items.
  7. After each milestone: run tests, update **Validation**, then commit `PLAN.md`.

  ## Output format

  - Update or create `PLAN.md` with the sections above.
  - Include a checklist for **Tasks**. Keep lines under 100 chars.

  ## Examples
  **Input**: "Add OAuth login"

  **Output**:

  - Goal: Let users sign in with Google.
  - Tasks: [ ] add Google client, [ ] callback route, [ ] session, [ ] E2E test.
  - Won't do: org SSO.
  - Ideas for later: Apple login.

  ## Notes

  - Planning only. No code edits.
  - Assume a Git repo with test runner available.


--- README.md ---
# DSPyTeach – DSPy File Teaching Analyzer

---

[![PyPI](https://img.shields.io/pypi/v/dspyteach.svg?include_prereleases&cacheSeconds=60&t=1)](https://pypi.org/project/dspyteach/)
[![Downloads](https://img.shields.io/pypi/dm/dspyteach.svg?cacheSeconds=300)](https://pypi.org/project/dspyteach/)
[![TestPyPI](https://img.shields.io/badge/TestPyPI-dspyteach-informational?cacheSeconds=300)](https://test.pypi.org/project/dspyteach/)
[![CI](https://github.com/AcidicSoil/dspy-file/actions/workflows/release.yml/badge.svg)](…)
[![Repo](https://img.shields.io/badge/GitHub-AcidicSoil%2Fdspy--file-181717?logo=github)](https://github.com/AcidicSoil/dspy-file)

---

## DSPy-powered CLI that analyzes source files (one or many) and produces teaching briefs

**Each run captures:**

- an overview of the file and its major sections
- key teaching points, workflows, and pitfalls highlighted in the material
- a polished markdown brief suitable for sharing with learners

The implementation mirrors the multi-file tutorial (`tutorials/multi-llmtxt_generator`) but focuses on per-file inference. The program is split into:

- `dspy_file/signatures.py` – DSPy signatures that define inputs/outputs for each step
- `dspy_file/file_analyzer.py` – the main DSPy module that orchestrates overview, teaching extraction, and report composition. It now wraps the final report stage with `dspy.Refine`, pushing for 450–650+ word briefs.
- `dspy_file/file_helpers.py` – utilities for loading files and rendering the markdown brief
- `dspy_file/analyze_file_cli.py` – command line entry point that configures the local model and prints results. It can walk directories, apply glob filters, and batch-generate briefs.

---

## Quick start

1. Confirm Python 3.10–3.12 is available and pull at least one OpenAI-compatible model (Ollama, LM Studio, or a hosted provider).
2. From the repository root, create an isolated environment and install dependencies:

### Linux

  ```shell
  uv build
  uv sync
  source .venv/bin/activate
  ```

### Windows

  ```shell
  uv build
  uv sync
  .venv/scripts/activate
  ```

3. Run a smoke test to confirm the CLI is wired up:

  ```bash
  dspyteach --help
  ```

  Expected result: the help output lists available flags and displays the active version string.

4. Analyze a sample file to confirm end-to-end output:

   ```bash
   dspyteach path/to/example.py
   ```

   Expected result: the command prints a teaching brief to stdout and writes a `.teaching.md` file under `dspy_file/data/`.

---

## Requirements

- Python 3.10-3.12+
- DSPy installed in the environment
- A language-model backend. You can choose between:
  - **Ollama** (default): run it locally with the model `hf.co/unsloth/Qwen3-4B-Instruct-2507-GGUF:Q6_K_XL` pulled.
  - **LM Studio** (OpenAI-compatible): start the LM Studio server (`lms server start`) and download a model such as `qwen3-4b-instruct-2507@q6_k_xl`.
  - **Any other OpenAI-compatible endpoint**: point the CLI at a hosted provider by supplying an API base URL and key (defaults to `gpt-5`).
- (Optional) `.env` file for DSPy configuration. `dotenv` loads variables such as `DSPYTEACH_PROVIDER`, `DSPYTEACH_MODEL`, `DSPYTEACH_API_BASE`, `DSPYTEACH_API_KEY`, and `OPENAI_API_KEY`.

---

## Example output

[[example-data after running a few passes](example-data/)]

---

## Installation

### Install with uv (recommended for local development)

<https://github.com/astral-sh/uv>

### Install from PyPI

```bash
uv pip install dspyteach
```

Expected result: running `dspyteach --help` prints the CLI usage banner from the installed package.

### Configure the language model

The CLI supports configurable OpenAI-compatible providers in addition to the default Ollama runtime. You can override the backend via CLI options or environment variables:

```bash
# Use LM Studio's OpenAI-compatible server with its default port
dspyteach path/to/project \
  --provider lmstudio \
  --model osmosis-mcp-4b@q8_0 \
  --api-base http://localhost:1234/v1
```

```bash
# Environment variable alternative (e.g. inside .env)
export DSPYTEACH_PROVIDER=lmstudio
export DSPYTEACH_MODEL=osmosis-mcp-4b@q8_0
export DSPYTEACH_API_BASE=http://localhost:1234/v1
dspyteach path/to/project
```

### LM-Studio Usage Notes

[LM Studio configuration guide](docs/lm-studio-provider.md)

LM Studio must expose its local server before you run the CLI. Start it from the Developer tab inside the LM Studio app or via `lms server start` (details in the [LM Studio configuration guide](docs/lm-studio-provider.md)); otherwise the CLI will exit early with a connection warning.

### OpenAI-compatible others usage

For hosted OpenAI-compatible services, set `--provider openai`, supply `--api-base` if needed, and pass an API key either through `--api-key`, `DSPYTEACH_API_KEY`, or the standard `OPENAI_API_KEY`. To keep a local Ollama model running after the CLI finishes, add `--keep-provider-alive`.

## Usage

Run the CLI to extract a teaching brief from a single file:

```bash
dspyteach path/to/your_file
```

Expected result: the CLI prints a markdown teaching brief to stdout and saves a copy under `dspy_file/data/`.

You can also point the CLI at a directory. The tool will recurse by default:

```bash
dspyteach path/to/project --glob "**/*.py" --glob "**/*.md"
```

Expected result: each matched file produces its own `.teaching.md` report in the output directory.

Use `--non-recursive` to stay in the top-level directory, add `--glob` repeatedly to narrow the target set, and pass `--raw` to print the raw DSPy prediction object instead of the formatted report.

### Command examples

- **Analyze a single markdown file**

  ```bash
  dspyteach docs/example.md
  ```

  Expected result: the CLI prints a teaching brief and stores `docs__example.teaching.md` in the output directory.

- **Process a repository while skipping generated assets**

  ```bash
  dspyteach ./repo \
    --glob "**/*.py" \
    --glob "**/*.md" \
    --exclude-dirs "build/,dist/,data/"
  ```

  Expected result: only `.py` and `.md` files outside the excluded directories are analyzed.

- **Generate refactor templates instead of teaching briefs**

  ```bash
  dspyteach --mode refactor ./repo
  ```

- **Refactoring prompts easily**

  ```bash
  dt -m refactor C:\Users\user\projects\WIP\.__pre-temp-prompts\temp-prompts-organized\ --provider lmstudio --api-base http://127.0.0.1:1234/v1 -ed prompt-front-matter/ -o ..\dspyteach\data -i
  ```


  Expected result: `.refactor.md` files appear alongside the teaching outputs with guidance tailored to the selected prompt.

Need to double-check files before the model runs? Add `--confirm-each` (alias `--interactive`) to prompt before every file, accepting with Enter or skipping with `n`.

To omit specific subdirectories entirely, pass one or more `--exclude-dirs` options. Each value can list comma-separated relative paths (for example `--exclude-dirs "build/,venv/" --exclude-dirs data/raw`). The analyzer ignores any files whose path begins with the provided prefixes.

Prefer short flags? The common options include `-r` (`--raw`), `-m` (`--mode`), `-nr` (`--non-recursive`), `-g` (`--glob`), `-i` (`--confirm-each`), `-ed` (`--exclude-dirs`), and `-o` (`--output-dir`). Mix and match them as needed.

## Adding Custom Prompts

The application can be extended with custom prompts for different analysis modes. When more than one prompt template (`.md` file) exists in the `dspy_file/prompts/` directory, the CLI will display a picker, allowing you to choose which prompt to use for the analysis.

To add a new prompt:

1.  Create a new Markdown file (e.g., `my_custom_prompt.md`) inside the `dspy_file/prompts/` directory.
2.  The name of the file (without the `.md` extension) will be used to identify the prompt in the picker.
3.  Write your prompt content inside this new file.

For example, to add a prompt for summarizing code, you could create `dspy_file/prompts/summarize_code.md` with your desired instructions. The next time you run in a mode that uses prompts, `summarize_code` will appear as an option.

## Refactor files/dirs

Want to scaffold refactor prompt templates instead of teaching briefs? Switch the mode:

```bash
dspyteach --mode refactor path/to/project --glob "**/*.md"
```

---

---

## **clarity on what happens when in teaching mode**

### both of these commands shown below would create new directories in the path outside the cwd that you ran the commands from and the directories would be the following: so in this case it would be exactly ["C:\Users\user\projects\WIP\NAME-OF-CWD + (the new files it creates which will be...)dspyteach\teach\data\00-ideation\architecture\adr-new.architecture.md"]

#### "00-ideation\architecture\adr-new.architecture.md" are unique to my personal setup so your output would be a mirrored version of the target path recursively

directory analyzed --> "~\projects\WIP\ .__pre-temp-prompts\temp-prompts-organized" so all under temp-prompts-organized are analyzed unless flag is passed to do otherwise, ie., non-recursive or -i AKA --interactive (file by file of target path).

---

```bash
dt -m refactor C:\Users\user\projects\WIP\.__pre-temp-prompts\temp-prompts-organized\ --provider lmstudio --api-base <http://127.0.0.1:1234/v1> -ed prompt-front-matter/ -o ..\dspyteach\data -i
```

```bash
dt C:\Users\user\projects\WIP\.__pre-temp-prompts\temp-prompts-organized\ --provider lmstudio --api-base <http://127.0.0.1:1234/v1> -ed prompt-front-matter/ -o ..\dspyteach\teach\data -i
```

---


## Additional Information

The CLI reuses the same file resolution pipeline but feeds each document through the bundled `dspy-file_refactor-prompt_template.md` instructions (packaged under `dspy_file/prompts/`), saving `.refactor.md` files alongside the teaching reports. Teaching briefs remain the default (`--mode teach`), so existing workflows continue to work unchanged.

When multiple templates live in `dspy_file/prompts/`, the refactor mode surfaces a picker so you can choose which one to use. You can also point at a specific template explicitly with `-p/--prompt`, passing either a bundled name (`-p refactor_prompt_template`) or an absolute path to your own Markdown prompt.

Each run only executes the analyzer for the chosen mode. When you pass `--mode refactor` the teaching inference pipeline stays idle, and you can alias the command (for example `alias dspyrefactor='dspyteach --mode refactor'`) if you prefer refactor templates to be the default in your shell.

To change where reports land, supply `--output-dir /path/to/reports`. When omitted the CLI writes to `dspy_file/data/` next to the module. Every run prints the active model name and the resolved output directory before analysis begins so you can confirm the environment at a glance. For backwards compatibility the installer also registers `dspy-file-teaching` as an alias.

Each analyzed file is saved under the chosen directory with a slugged name (e.g. `src__main.teaching.md` or `src__main.refactor.md`). If a file already exists, the CLI appends a numeric suffix to avoid overwriting previous runs.

The generated brief is markdown that mirrors the source material:

- Overview paragraphs for quick orientation
- Section-by-section bullets capturing the narrative
- Key concepts, workflows, pitfalls, and references learners should review
- A `dspy.Refine` wrapper keeps retrying until the report clears a length reward (defaults scale to ~50% of the source word count, with min/max clamps), so the content tends to be substantially longer than a single LM call.
- If a model cannot honour DSPy's structured-output schema, the CLI prints a `Structured output fallback` notice and heuristically parses the textual response so you still get usable bullets.

Behind the scenes the CLI:

1. Loads environment variables via `python-dotenv`.
2. Configures DSPy with the provider selected via CLI or environment variables (Ollama by default).
3. Resolves all requested files, reads contents, runs the DSPy `FileTeachingAnalyzer` module, and prints a human-friendly report for each.
4. Persists each report to the configured output directory so results are easy to revisit.
5. Stops the Ollama model when appropriate so local resources are returned to the pool.

### Extending

- Adjust the `TeachingReport` signature or add new chains in `dspy_file/file_analyzer.py` to capture additional teaching metadata.
- Customize the render logic in `dspy_file.file_helpers.render_prediction` if you want richer CLI output or structured JSON.
- Tune `TeachingConfig` inside `file_analyzer.py` to raise `max_tokens`, adjust the `Refine` word-count reward, or add extra LM kwargs.
- Add more signatures and module stages to capture additional metadata (e.g., security checks) and wire them into `FileAnalyzer`.

---

## Releasing

Maintainer release steps live in [docs/RELEASING.md](docs/RELEASING.md).

## Troubleshooting

- If the program cannot connect to Ollama, verify that the server is running on `http://localhost:11434` and the requested model has been pulled.
- When you see `ollama command not found`, ensure the `ollama` binary is on your `PATH`.
- For encoding errors, the helper already falls back to `latin-1`, but you can add more fallbacks in `file_helpers.read_file_content` if needed.

--- dspy_file/analyze_file_cli.py ---
# path: analyze_file_cli.py
# analyze_file_cli.py - command line entry point for DSPy file analyzer with MLflow tracking + tracing
from __future__ import annotations

import argparse
import os
import subprocess
import sys
import time
from enum import Enum
from urllib import error as urlerror
from urllib import request
from pathlib import Path
from typing import Any, Final

import dspy
from dotenv import load_dotenv

from .file_analyzer import FileTeachingAnalyzer
from .file_helpers import collect_source_paths, read_file_content, render_prediction
from .prompts import PromptTemplate, list_bundled_prompts, load_prompt_text
from .refactor_analyzer import FileRefactorAnalyzer

try:  # dspy depends on litellm; guard in case import path changes.
    from litellm.exceptions import InternalServerError as LiteLLMInternalServerError
except Exception:  # pragma: no cover - defensive fallback if litellm API shifts
    LiteLLMInternalServerError = None  # type: ignore[assignment]

# Optional MLflow import. Enabled only with --mlflow to ensure no behavior change by default.
try:  # pragma: no cover - optional dependency
    import mlflow  # type: ignore
except Exception:  # pragma: no cover - keep CLI usable without MLflow installed
    mlflow = None  # type: ignore

class Provider(str, Enum):
    """Supported language model providers."""

    OLLAMA = "ollama"
    OPENAI = "openai"
    LMSTUDIO = "lmstudio"

    @property
    def is_openai_compatible(self) -> bool:
        return self in {Provider.OPENAI, Provider.LMSTUDIO}


DEFAULT_PROVIDER: Final[Provider] = Provider.OLLAMA
DEFAULT_OUTPUT_DIR = Path(__file__).parent / "data"
DEFAULT_OLLAMA_MODEL = "hf.co/Mungert/osmosis-mcp-4b-GGUF:Q5_K_M"
DEFAULT_LMSTUDIO_MODEL = "osmosis-mcp-4b@q8_0"
DEFAULT_OPENAI_MODEL = "gpt-5"
OLLAMA_BASE_URL = "http://localhost:11434"
LMSTUDIO_BASE_URL = "http://localhost:1234/v1"

PROVIDER_DEFAULTS: Final[dict[Provider, dict[str, Any]]] = {
    Provider.OLLAMA: {"model": DEFAULT_OLLAMA_MODEL, "api_base": OLLAMA_BASE_URL},
    Provider.LMSTUDIO: {"model": DEFAULT_LMSTUDIO_MODEL, "api_base": LMSTUDIO_BASE_URL},
    Provider.OPENAI: {"model": DEFAULT_OPENAI_MODEL, "api_base": None},
}


def _resolve_option(
    cli_value: str | None, env_var: str, default: str | None = None
) -> str | None:
    """Return the CLI value if provided, otherwise fall back to env or default."""

    if cli_value is not None:
        return cli_value
    env_value = os.getenv(env_var)
    if env_value not in ("", None):
        return env_value
    return default


def _normalize_model_name(provider: Provider, raw_model: str) -> str:
    """Attach the appropriate provider prefix to the model identifier."""

    if provider is Provider.OLLAMA:
        return (
            raw_model
            if raw_model.startswith("ollama_chat/")
            else f"ollama_chat/{raw_model}"
        )

    if raw_model.startswith("openai/"):
        return raw_model
    return f"openai/{raw_model}"


def configure_model(
    provider: Provider,
    model_name: str,
    *,
    api_base: str | None,
    api_key: str | None,
) -> None:
    """Configure DSPy with the selected provider and model."""

    lm_kwargs: dict[str, Any] = {"streaming": False, "cache": False}
    if provider is Provider.OLLAMA:
        lm_kwargs["api_base"] = api_base or OLLAMA_BASE_URL
        # Ollama's OpenAI compatibility ignores api_key, so pass an empty string.
        lm_kwargs["api_key"] = ""
    else:
        if api_base:
            lm_kwargs["api_base"] = api_base
        if api_key:
            lm_kwargs["api_key"] = api_key

    identifier = _normalize_model_name(provider, model_name)
    lm = dspy.LM(identifier, **lm_kwargs)
    dspy.configure(lm=lm)
    provider_label = "LM Studio" if provider is Provider.LMSTUDIO else provider.value
    suffix = f" via {api_base}" if provider.is_openai_compatible and api_base else ""
    print(f"Configured DSPy LM ({provider_label}): {model_name}{suffix}")


class ProviderConnectivityError(RuntimeError):
    """Raised when a provider cannot be reached before running analysis."""


def _probe_openai_provider(
    api_base: str, api_key: str | None, *, timeout: float = 3.0
) -> None:
    """Make a lightweight request against an OpenAI-compatible endpoint."""

    endpoint = api_base.rstrip("/") + "/models"
    headers = {"Authorization": f"Bearer {api_key or ''}"}
    request_obj = request.Request(endpoint, headers=headers, method="GET")

    try:
        with request.urlopen(request_obj, timeout=timeout):
            return
    except urlerror.HTTPError as exc:
        raise ProviderConnectivityError(
            f"Endpoint {endpoint} responded with HTTP {exc.code}: {exc.reason}"
        ) from exc
    except urlerror.URLError as exc:
        reason = getattr(exc, "reason", exc)
        raise ProviderConnectivityError(
            f"Failed to reach {endpoint}: {reason}"
        ) from exc


def stop_ollama_model(model_name: str) -> None:
    """Stop the Ollama model to free server resources."""

    try:
        subprocess.run(
            ["ollama", "stop", model_name],
            check=True,
            capture_output=True,
        )
    except subprocess.CalledProcessError as exc:  # pragma: no cover - warn only
        print(f"Warning: Failed to stop model {model_name}: {exc}")
    except FileNotFoundError:
        print("Warning: ollama command not found while attempting to stop the model.")


class AnalysisMode(str, Enum):
    TEACH = "teach"
    REFACTOR = "refactor"

    @property
    def render_key(self) -> str:
        return self.value

    @property
    def output_description(self) -> str:
        return "teaching report" if self is AnalysisMode.TEACH else "refactor template"

    @property
    def file_suffix(self) -> str:
        # No suffixing. Preserve original filename.
        return ""


def _prompt_for_template_selection(prompts: list[PromptTemplate]) -> PromptTemplate:
    while True:
        print("Available prompt templates:")
        for idx, template in enumerate(prompts, 1):
            print(f"  [{idx}] {template.name} ({template.path.name})")
        try:
            choice = input(f"Select a template [1-{len(prompts)}] (default 1): ")
        except EOFError:
            print("No selection provided; using first template.")
            return prompts[0]

        stripped = choice.strip()
        if not stripped:
            return prompts[0]
        if stripped.isdigit():
            idx = int(stripped)
            if 1 <= idx <= len(prompts):
                return prompts[idx - 1]
        print(f"Please enter a number between 1 and {len(prompts)}.")


def _resolve_prompt_text(prompt_arg: str | None) -> str:
    if prompt_arg:
        return load_prompt_text(prompt_arg)

    prompts = list_bundled_prompts()
    if not prompts:
        raise FileNotFoundError("No prompt templates found in prompts directory.")
    if len(prompts) == 1:
        return prompts[0].path.read_text(encoding="utf-8")

    selected = _prompt_for_template_selection(prompts)
    return selected.path.read_text(encoding="utf-8")


def _write_output(
    source_path: Path,
    content: str,
    *,
    root: Path | None = None,
    output_dir: Path | None = None,
    suffix: str = "",
) -> Path:
    """Write output to the same filename and directory layout as the analyzed path."""

    try:
        relative_path = (
            source_path.relative_to(root) if root else Path(source_path.name)
        )
    except ValueError:
        relative_path = Path(source_path.name)

    if output_dir:
        dest_path = (output_dir / relative_path).resolve()
        dest_path.parent.mkdir(parents=True, exist_ok=True)
    else:
        dest_path = source_path

    if not content.endswith("\n"):
        content = content + "\n"

    dest_path.write_text(content, encoding="utf-8")
    return dest_path


def _confirm_analyze(path: Path) -> bool:
    prompt = f"Analyze {path}? [Y/n]: "
    while True:
        try:
            response = input(prompt)
        except EOFError:
            print("No input received; skipping.")
            return False

        normalized = response.strip().lower()
        if normalized in {"", "y", "yes"}:
            return True
        if normalized in {"n", "no"}:
            return False
        print("Please answer 'y' or 'n'.")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Analyze a single file using DSPy signatures and modules",
    )
    parser.add_argument("path", help="Path to the file to analyze")
    parser.add_argument(
        "--provider",
        choices=[provider.value for provider in Provider],
        default=None,
        help=(
            "Language model provider to use (env: DSPYTEACH_PROVIDER). "
            "Choose from 'ollama', 'lmstudio', or 'openai'."
        ),
    )
    parser.add_argument(
        "--model",
        dest="model_name",
        default=None,
        help=(
            "Override the model identifier for the selected provider "
            "(env: DSPYTEACH_MODEL)."
        ),
    )
    parser.add_argument(
        "--api-base",
        dest="api_base",
        default=None,
        help=("Override the OpenAI-compatible API base URL (env: DSPYTEACH_API_BASE)."),
    )
    parser.add_argument(
        "--api-key",
        dest="api_key",
        default=None,
        help=(
            "API key for OpenAI-compatible providers (env: DSPYTEACH_API_KEY). "
            "Falls back to OPENAI_API_KEY for the OpenAI provider."
        ),
    )
    parser.add_argument(
        "--keep-provider-alive",
        action="store_true",
        dest="keep_provider_alive",
        help="Skip stopping the local Ollama model when execution completes.",
    )
    parser.add_argument(
        "-r",
        "--raw",
        action="store_true",
        help="Print raw DSPy prediction repr instead of formatted text",
    )
    parser.add_argument(
        "-m",
        "--mode",
        choices=[mode.value for mode in AnalysisMode],
        default=AnalysisMode.TEACH.value,
        help="Select output mode: teaching report (default) or refactor prompt template.",
    )
    parser.add_argument(
        "-nr",
        "--non-recursive",
        action="store_true",
        help="When path is a directory, only analyze files in the top-level directory",
    )
    parser.add_argument(
        "-g",
        "--glob",
        action="append",
        dest="include_globs",
        default=None,
        help=(
            "Optional glob pattern(s) applied relative to the directory. Repeat to combine."
        ),
    )
    parser.add_argument(
        "-p",
        "--prompt",
        dest="prompt",
        default=None,
        help=(
            "Prompt template to use in refactor mode. Provide a name, bundled filename, or path."
        ),
    )
    parser.add_argument(
        "-i",
        "--confirm-each",
        "--interactive",
        action="store_true",
        dest="confirm_each",
        help="Prompt for confirmation before analyzing each file.",
    )
    parser.add_argument(
        "-ed",
        "--exclude-dirs",
        action="append",
        dest="exclude_dirs",
        default=None,
        help=(
            "Comma-separated relative directory paths to skip entirely when scanning."
        ),
    )
    parser.add_argument(
        "-o",
        "--output-dir",
        dest="output_dir",
        default=None,
        help=("Directory to write outputs. If omitted, overwrite files in place."),
    )
    # --- MLflow options ---
    parser.add_argument(
        "--mlflow",
        action="store_true",
        help="Enable MLflow tracking of params, metrics, artifacts, and traces.",
    )
    parser.add_argument(
        "--mlflow-experiment",
        dest="mlflow_experiment",
        default=None,
        help="Name of the MLflow experiment to use or create when --mlflow is set.",
    )
    return parser


def analyze_path(
    path: str,
    *,
    raw: bool,
    recursive: bool,
    include_globs: list[str] | None,
    confirm_each: bool,
    exclude_dirs: list[str] | None,
    output_dir: Path | None,
    mode: AnalysisMode,
    prompt_text: str | None = None,
    mlflow_enabled: bool = False,
    root_hint: Path | None = None,
) -> int:
    """Run the DSPy pipeline and render results to stdout for one or many files."""

    resolved = Path(path).expanduser().resolve()
    targets = collect_source_paths(
        path,
        recursive=recursive,
        include_globs=include_globs,
        exclude_dirs=exclude_dirs,
    )

    if not targets:
        print(f"No files found under {resolved}")
        return 0

    analyzer: dspy.Module
    if mode is AnalysisMode.TEACH:
        analyzer = FileTeachingAnalyzer()
    else:
        analyzer = FileRefactorAnalyzer(template_text=prompt_text)

    root: Path | None = (
        root_hint if root_hint else (resolved if resolved.is_dir() else None)
    )

    exit_code = 0
    for target in targets:
        if confirm_each and not _confirm_analyze(target):
            print(f"Skipping {target} at user request.")
            continue

        try:
            # --- trace: read file ---
            if mlflow_enabled and mlflow is not None and hasattr(mlflow, "start_span"):
                with mlflow.start_span(
                    name="read_file", attributes={"path": str(target)}
                ):
                    content = read_file_content(target)
            else:
                content = read_file_content(target)
        except (FileNotFoundError, UnicodeDecodeError) as exc:
            print(f"Skipping {target}: {exc}")
            exit_code = 1
            continue

        # Compute relative path for logging.
        try:
            relative = target.relative_to(root) if root else Path(target.name)
        except Exception:
            relative = Path(target.name)

        print(f"\n=== Analyzing {target} ===")
        t0 = time.perf_counter()
        # --- trace: model inference ---
        if mlflow_enabled and mlflow is not None and hasattr(mlflow, "start_span"):
            with mlflow.start_span(
                name="dspy_inference",
                attributes={
                    "file": str(relative),
                    "mode": mode.value,
                    "bytes_in": len(content.encode("utf-8", errors="ignore")),
                },
            ):
                prediction = analyzer(file_path=str(target), file_content=content)
        else:
            prediction = analyzer(file_path=str(target), file_content=content)
        dt_ms = int((time.perf_counter() - t0) * 1000)

        if raw:
            output_text = repr(prediction)
            print(output_text)
        else:
            output_text = render_prediction(prediction, mode=mode.render_key)
            print(output_text, end="")

        # --- trace: write output ---
        if mlflow_enabled and mlflow is not None and hasattr(mlflow, "start_span"):
            with mlflow.start_span(
                name="write_output", attributes={"file": str(relative)}
            ):
                output_path = _write_output(
                    target,
                    output_text,
                    root=root,
                    output_dir=output_dir,
                    suffix=mode.file_suffix,
                )
        else:
            output_path = _write_output(
                target,
                output_text,
                root=root,
                output_dir=output_dir,
                suffix=mode.file_suffix,
            )
        print(f"Saved {mode.output_description} to {output_path}")

        if mlflow_enabled and mlflow is not None:
            # Nested run per analyzed file
            run_name = str(relative)
            with mlflow.start_run(nested=True, run_name=run_name):  # type: ignore[attr-defined]
                mlflow.log_params(  # type: ignore[attr-defined]
                    {
                        "path": str(relative),
                        "raw": str(raw),
                        "confirm_each": str(confirm_each),
                        "recursive": str(recursive),
                        "globs": ",".join(include_globs or []),
                        "excluded_dirs": ",".join(exclude_dirs or []),
                    }
                )
                in_bytes = len(content.encode("utf-8", errors="ignore"))
                out_chars = len(output_text)
                mlflow.log_metrics(  # type: ignore[attr-defined]
                    {
                        "input_bytes": float(in_bytes),
                        "output_chars": float(out_chars),
                        "duration_ms": float(dt_ms),
                    }
                )
                mlflow.set_tags({"analysis_mode": mode.value})  # type: ignore[attr-defined]
                mlflow.log_artifact(str(output_path))  # type: ignore[attr-defined]

                if hasattr(mlflow, "update_current_trace"):
                    try:
                        mlflow.update_current_trace(
                            tags={"file": str(relative), "analysis_mode": mode.value}
                        )
                    except Exception:
                        pass

    return exit_code


def main(argv: list[str] | None = None) -> int:
    load_dotenv()

    parser = build_parser()
    args = parser.parse_args(argv)

    provider_value = _resolve_option(
        args.provider, "DSPYTEACH_PROVIDER", DEFAULT_PROVIDER.value
    )
    try:
        provider = Provider(provider_value)
    except ValueError:  # pragma: no cover - argparse handles this
        valid = ", ".join(p.value for p in Provider)
        parser.error(f"Unsupported provider '{provider_value}'. Choose from: {valid}.")

    defaults = PROVIDER_DEFAULTS[provider]
    model_name = _resolve_option(args.model_name, "DSPYTEACH_MODEL", defaults["model"])
    api_base_default = defaults.get("api_base")
    api_base = _resolve_option(args.api_base, "DSPYTEACH_API_BASE", api_base_default)
    api_key = _resolve_option(args.api_key, "DSPYTEACH_API_KEY", None)
    if provider is Provider.OPENAI and not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
    if provider is Provider.LMSTUDIO and not api_key:
        api_key = "lm-studio"

    if provider is Provider.LMSTUDIO and api_base:
        try:
            _probe_openai_provider(api_base, api_key)
        except ProviderConnectivityError as exc:
            print("Unable to reach the LM Studio server before starting analysis.")
            print(f"Details: {exc}")
            print(
                "Start LM Studio's local API server (Developer tab → Start Server or `lms server start`) and re-run, or pass --api-base to match the running port."
            )
            return 1

    configure_model(provider, model_name, api_base=api_base, api_key=api_key)
    stop_model: str | None = model_name if provider is Provider.OLLAMA else None

    # Evaluate MLflow configuration
    mlflow_enabled = bool(args.mlflow)
    if mlflow_enabled and mlflow is None:  # type: no cover
        print(
            "Warning: --mlflow requested but MLflow is not installed. Disabling MLflow."
        )
        mlflow_enabled = False

    exit_code = 0
    try:
        analysis_mode = AnalysisMode(args.mode)
        prompt_text: str | None = None
        if analysis_mode is AnalysisMode.REFACTOR:
            try:
                prompt_text = _resolve_prompt_text(args.prompt)
            except (FileNotFoundError, ValueError) as exc:
                print(f"Error resolving prompt: {exc}")
                return 2
        elif args.prompt:
            print("Warning: --prompt is ignored outside refactor mode.")
        output_dir = (
            Path(args.output_dir).expanduser().resolve() if args.output_dir else None
        )
        if output_dir:
            print(f"Writing {analysis_mode.output_description}s to {output_dir}")
        else:
            print(f"Writing {analysis_mode.output_description}s in place")
        exclude_dirs = None
        if args.exclude_dirs:
            parsed: list[str] = []
            for entry in args.exclude_dirs:
                parsed.extend(
                    segment.strip() for segment in entry.split(",") if segment.strip()
                )
            exclude_dirs = parsed or None

        # --- MLflow parent run + root span for the whole invocation ---
        if mlflow_enabled and mlflow is not None:
            if args.mlflow_experiment:
                try:
                    mlflow.set_experiment(args.mlflow_experiment)  # type: ignore[attr-defined]
                except Exception as exc:
                    print(f"Warning: failed to set MLflow experiment: {exc}")

            tracing_supported = hasattr(mlflow, "start_span") and hasattr(
                mlflow, "update_current_trace"
            )
            run_name = f"analyze:{Path(args.path).name}"

            if tracing_supported:
                with mlflow.start_span(name="analyze_file_cli") as root_span:
                    # inputs on the root span when API exists
                    try:
                        if hasattr(root_span, "set_inputs"):
                            root_span.set_inputs({
                                "path": args.path,
                                "mode": analysis_mode.value,
                            })
                    except Exception:
                        pass

                    try:
                        mlflow.update_current_trace(
                            tags={
                                "provider": provider.value,
                                "api_base_set": str(bool(api_base)),
                            }
                        )
                    except Exception:
                        pass

                    with mlflow.start_run(run_name=run_name):  # type: ignore[attr-defined]
                        mlflow.set_tags(  # type: ignore[attr-defined]
                            {
                                "provider": provider.value,
                                "mode": analysis_mode.value,
                                "api_base": str(bool(api_base)),
                            }
                        )
                        mlflow.log_params(  # type: ignore[attr-defined]
                            {
                                "provider": provider.value,
                                "model": model_name,
                                "api_base": api_base or "",
                                "path": args.path,
                                "output_dir": str(output_dir) if output_dir else "",
                                "recursive": str(not args.non_recursive),
                            }
                        )
                        exit_code = analyze_path(
                            args.path,
                            raw=args.raw,
                            recursive=not args.non_recursive,
                            include_globs=args.include_globs,
                            confirm_each=args.confirm_each,
                            exclude_dirs=exclude_dirs,
                            output_dir=output_dir,
                            mode=analysis_mode,
                            prompt_text=prompt_text,
                            mlflow_enabled=True,
                            root_hint=Path(args.path).expanduser().resolve(),
                        )
            else:
                with mlflow.start_run(run_name=run_name):  # type: ignore[attr-defined]
                    mlflow.set_tags(  # type: ignore[attr-defined]
                        {
                            "provider": provider.value,
                            "mode": analysis_mode.value,
                            "api_base": str(bool(api_base)),
                        }
                    )
                    mlflow.log_params(  # type: ignore[attr-defined]
                        {
                            "provider": provider.value,
                            "model": model_name,
                            "api_base": api_base or "",
                            "path": args.path,
                            "output_dir": str(output_dir) if output_dir else "",
                            "recursive": str(not args.non_recursive),
                        }
                    )
                    exit_code = analyze_path(
                        args.path,
                        raw=args.raw,
                        recursive=not args.non_recursive,
                        include_globs=args.include_globs,
                        confirm_each=args.confirm_each,
                        exclude_dirs=exclude_dirs,
                        output_dir=output_dir,
                        mode=analysis_mode,
                        prompt_text=prompt_text,
                        mlflow_enabled=True,
                        root_hint=Path(args.path).expanduser().resolve(),
                    )
        else:
            exit_code = analyze_path(
                args.path,
                raw=args.raw,
                recursive=not args.non_recursive,
                include_globs=args.include_globs,
                confirm_each=args.confirm_each,
                exclude_dirs=exclude_dirs,
                output_dir=output_dir,
                mode=analysis_mode,
                prompt_text=prompt_text,
                mlflow_enabled=False,
                root_hint=Path(args.path).expanduser().resolve(),
            )
    except Exception as exc:
        if LiteLLMInternalServerError and isinstance(exc, LiteLLMInternalServerError):
            message = str(exc)
            if exc.__cause__:
                message = f"{message} (cause: {exc.__cause__})"
            print("Model request failed while generating the report.")
            print(f"Details: {message}")
            if provider is Provider.LMSTUDIO:
                print(
                    f"Confirm the LM Studio server is running and reachable at {api_base}."
                )
            return 1
        raise
    except (FileNotFoundError, IsADirectoryError) as exc:
        parser.print_usage(sys.stderr)
        print(f"{parser.prog}: error: {exc}", file=sys.stderr)
        exit_code = 2
    except KeyboardInterrupt:
        exit_code = 1
    finally:
        if provider is Provider.OLLAMA and not args.keep_provider_alive and stop_model:
            stop_ollama_model(stop_model)

    return exit_code

if __name__ == "__main__":  # pragma: no cover - CLI entry point
    sys.exit(main())


--- dspy_file/file_analyzer.py ---
# file_analyzer.py - DSPy module deriving a learning brief from a single file
from __future__ import annotations

import json
import re
from collections.abc import Iterable, Mapping
from dataclasses import dataclass, field
from typing import Any

import dspy

from .signatures import FileOverview, TeachingPoints, TeachingReport


@dataclass
class TeachingConfig:
    section_bullet_prefix: str = "- "
    overview_max_tokens: int = 2000
    teachings_max_tokens: int = 2000
    report_max_tokens: int = 2000
    temperature: float | None = 0.3
    top_p: float | None = None
    n_completions: int | None = None
    extra_lm_kwargs: dict[str, Any] = field(default_factory=dict)
    report_refine_attempts: int = 3
    report_reward_threshold: float = 0.8
    report_min_word_count: int = 400
    report_max_word_count: int = 2800
    report_target_ratio: float = 0.5
    report_soft_cap_ratio: float = 0.8

    def lm_args_for(self, scope: str) -> dict[str, Any]:
        """Return per-module LM kwargs without mutating shared config."""
        scope_tokens = {
            "overview": self.overview_max_tokens,
            "teachings": self.teachings_max_tokens,
            "report": self.report_max_tokens,
        }

        kwargs: dict[str, Any] = {**self.extra_lm_kwargs}
        kwargs["max_tokens"] = scope_tokens.get(scope, self.report_max_tokens)

        if self.temperature is not None:
            kwargs["temperature"] = self.temperature
        if self.top_p is not None:
            kwargs["top_p"] = self.top_p
        if self.n_completions is not None:
            kwargs["n"] = self.n_completions

        return kwargs


def _fallback_list(message: str) -> list[str]:
    return [message]


def _ensure_text(value: str | None, fallback: str) -> str:
    if value and value.strip():
        return value
    return fallback


def _ensure_list(
    values: Iterable[str] | None,
    fallback: str,
    *,
    strip_entries: bool = True,
    field_name: str | None = None,
) -> list[str]:
    coerced, used_fallback = _coerce_iterable(values, strip_entries=strip_entries)

    if coerced:
        if used_fallback and field_name:
            _structured_output_notice(field_name)
        return coerced

    if used_fallback and field_name:
        _structured_output_notice(field_name)

    return _fallback_list(fallback)


def _clean_list(
    values: Iterable[str] | None,
    *,
    strip_entries: bool = True,
    field_name: str | None = None,
) -> list[str]:
    if not values:
        return []

    coerced, used_fallback = _coerce_iterable(values, strip_entries=strip_entries)

    if used_fallback and field_name:
        _structured_output_notice(field_name)

    return coerced


_STRUCTURED_NOTICE_CACHE: set[str] = set()


def _structured_output_notice(field: str) -> None:
    if field in _STRUCTURED_NOTICE_CACHE:
        return
    _STRUCTURED_NOTICE_CACHE.add(field)
    print(
        f"Structured output fallback applied for '{field}'. Parsed textual response."
    )


_LEADING_MARKER_PATTERN = re.compile(r"^[\s\-\*•·\u2022\d\.\)\(]+")


def _coerce_iterable(
    values: Iterable[str] | None,
    *,
    strip_entries: bool,
) -> tuple[list[str], bool]:
    if values is None:
        return [], False

    if isinstance(values, str):
        return _coerce_string(values, strip_entries=strip_entries), True

    if isinstance(values, Mapping):
        items: list[str] = []
        for key, val in values.items():
            key_text = str(key).strip()
            val_text = str(val).strip()
            combined = f"{key_text}: {val_text}" if val_text else key_text
            candidate = combined.rstrip() if not strip_entries else combined.strip()
            if candidate:
                items.append(candidate if strip_entries else candidate.rstrip())
        return items, True

    if isinstance(values, Iterable):
        cleaned: list[str] = []
        used_fallback = not isinstance(values, (list, tuple, set))
        for entry in values:
            if entry is None:
                continue
            if isinstance(entry, str):
                candidate = entry.rstrip() if not strip_entries else entry.strip()
            else:
                candidate = str(entry).strip()
            if strip_entries:
                candidate = _LEADING_MARKER_PATTERN.sub("", candidate).strip()
            if candidate:
                cleaned.append(candidate if strip_entries else candidate.rstrip())
        return cleaned, used_fallback

    return _coerce_string(str(values), strip_entries=strip_entries), True


def _coerce_string(value: str, *, strip_entries: bool) -> list[str]:
    text = value.strip()
    if not text:
        return []

    try:
        parsed = json.loads(text)
    except json.JSONDecodeError:
        parsed = None

    if isinstance(parsed, list):
        coerced: list[str] = []
        for item in parsed:
            candidate = str(item)
            candidate = candidate.rstrip() if not strip_entries else candidate.strip()
            if strip_entries:
                candidate = _LEADING_MARKER_PATTERN.sub("", candidate).strip()
            if candidate:
                coerced.append(candidate if strip_entries else candidate.rstrip())
        if coerced:
            return coerced
    elif isinstance(parsed, Mapping):
        mapped: list[str] = []
        for key, val in parsed.items():
            key_text = str(key).strip()
            val_text = str(val).strip()
            candidate = f"{key_text}: {val_text}" if val_text else key_text
            candidate = candidate.rstrip() if not strip_entries else candidate.strip()
            if candidate:
                mapped.append(candidate if strip_entries else candidate.rstrip())
        if mapped:
            return mapped

    lines = value.replace("\r", "\n").split("\n")
    normalized: list[str] = []
    for raw_line in lines:
        candidate = raw_line.rstrip() if not strip_entries else raw_line.strip()
        if strip_entries:
            candidate = _LEADING_MARKER_PATTERN.sub("", candidate).strip()
        if candidate:
            normalized.append(candidate if strip_entries else candidate.rstrip())

    if len(normalized) <= 1:
        delimiters = [";", "•", "·", " | "]
        for delimiter in delimiters:
            if delimiter in value:
                parts = [part.strip() for part in value.split(delimiter) if part.strip()]
                if parts:
                    return [
                        _LEADING_MARKER_PATTERN.sub("", part).strip()
                        if strip_entries
                        else part.rstrip()
                    ]

    return normalized


def _with_prefix(items: Iterable[str], prefix: str) -> list[str]:
    if not prefix:
        return [item for item in items if item.strip()]

    prefix_char = prefix.strip()[:1] if prefix.strip() else ""
    prefixed: list[str] = []

    for item in items:
        stripped = item.strip()
        if not stripped:
            continue
        if prefix_char and stripped.startswith(prefix_char):
            prefixed.append(stripped)
        else:
            prefixed.append(f"{prefix}{stripped}")

    return prefixed


def _word_count(text: str) -> int:
    return len(text.split())


class FileTeachingAnalyzer(dspy.Module):
    """Generate a teaching-focused summary using DSPy chains of thought."""

    def __init__(self, config: TeachingConfig | None = None) -> None:
        super().__init__()
        self.config = config or TeachingConfig()

        overview_signature = FileOverview.with_instructions(
            """
            Craft a thorough multi-section narrative that orients a senior learner.
            Describe the file's purpose, high-level architecture, main responsibilities,
            how data flows through each part, and any noteworthy patterns or dependencies.
            Aim for around five paragraphs that highlight why each section exists and
            how it contributes to the overall behavior.
            """
        )

        teachings_signature = TeachingPoints.with_instructions(
            """
            Extract every insight the learner would need for deep comprehension.
            Provide generous bullet lists (>=6 items when possible) covering concepts,
            workflows, pitfalls, integration guidance, and areas needing validation.
            When referencing identifiers, include the role they play.
            Prefer complete sentences that can stand alone in teaching materials.
            """
        )

        report_signature = TeachingReport.with_instructions(
            """
            Assemble a long-form teaching brief in Markdown. Include:
            - An opening context block with file path and intent.
            - Headed sections for overview, section walkthrough, key concepts, workflows,
              pitfalls, integration notes, tests/validation, and references.
            - Expand each bullet into full sentences or sub-bullets to help instructors
              speak to the content without the source file open.
            Ensure the report comfortably exceeds 400 words when source material allows.
            """
        )

        self.overview = dspy.ChainOfThought(
            overview_signature, **self.config.lm_args_for("overview")
        )
        self.teachings = dspy.ChainOfThought(
            teachings_signature, **self.config.lm_args_for("teachings")
        )

        base_report = dspy.ChainOfThought(
            report_signature, **self.config.lm_args_for("report")
        )

        if self.config.report_refine_attempts > 1:

            def report_length_reward(args: dict[str, Any], pred: dspy.Prediction) -> float:
                text = getattr(pred, "report_markdown", "") or ""
                words = _word_count(text)
                source_words = max(int(args.get("source_word_count", 0)), 0)

                dynamic_target = max(
                    self.config.report_min_word_count,
                    int(source_words * self.config.report_target_ratio),
                )

                soft_cap = max(
                    dynamic_target + 150,
                    int(source_words * self.config.report_soft_cap_ratio),
                )

                dynamic_cap = min(self.config.report_max_word_count, soft_cap)

                if words < dynamic_target:
                    return 0.0

                if words >= dynamic_cap:
                    return 1.0

                span = max(dynamic_cap - dynamic_target, 1)
                progress = (words - dynamic_target) / span
                return min(1.0, 0.6 + 0.4 * progress)

            self.report = dspy.Refine(
                module=base_report,
                N=self.config.report_refine_attempts,
                reward_fn=report_length_reward,
                threshold=self.config.report_reward_threshold,
            )
        else:
            self.report = base_report

    def forward(self, *, file_path: str, file_content: str) -> dspy.Prediction:
        overview_pred = self.overview(
            file_path=file_path,
            file_content=file_content,
        )

        teaching_pred = self.teachings(
            file_content=file_content,
        )

        overview_text = _ensure_text(
            getattr(overview_pred, "overview", None),
            "Overview unavailable.",
        )

        section_notes = _with_prefix(
            _ensure_list(
                getattr(overview_pred, "section_notes", None),
                "Section-level breakdown unavailable.",
                field_name="section_notes",
            ),
            self.config.section_bullet_prefix,
        )

        key_concepts = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "key_concepts", None),
                "Clarify core concepts manually.",
                field_name="key_concepts",
            ),
            self.config.section_bullet_prefix,
        )

        practical_steps = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "practical_steps", None),
                "Document workflow steps explicitly.",
                field_name="practical_steps",
            ),
            self.config.section_bullet_prefix,
        )

        pitfalls = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "pitfalls", None),
                "No pitfalls identified; review source for potential caveats.",
                field_name="pitfalls",
            ),
            self.config.section_bullet_prefix,
        )

        references = _with_prefix(
            _clean_list(
                getattr(teaching_pred, "references", None),
                field_name="references",
            ),
            self.config.section_bullet_prefix,
        )

        usage_patterns = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "usage_patterns", None),
                "Document how this file is applied in real flows.",
                field_name="usage_patterns",
            ),
            self.config.section_bullet_prefix,
        )

        key_functions = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "key_functions", None),
                "Identify primary interfaces and responsibilities manually.",
                field_name="key_functions",
            ),
            self.config.section_bullet_prefix,
        )

        code_walkthroughs = _ensure_list(
            getattr(teaching_pred, "code_walkthroughs", None),
            "Prepare short code walkthroughs for learners.",
            strip_entries=False,
            field_name="code_walkthroughs",
        )

        integration_notes = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "integration_notes", None),
                "Outline integration touchpoints manually.",
                field_name="integration_notes",
            ),
            self.config.section_bullet_prefix,
        )

        testing_focus = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "testing_focus", None),
                "Highlight testing priorities in a follow-up review.",
                field_name="testing_focus",
            ),
            self.config.section_bullet_prefix,
        )

        source_word_count = _word_count(file_content)

        report_pred = self.report(
            file_path=file_path,
            overview=overview_text,
            section_notes=section_notes,
            key_concepts=key_concepts,
            practical_steps=practical_steps,
            pitfalls=pitfalls,
            references=references,
            usage_patterns=usage_patterns,
            key_functions=key_functions,
            code_walkthroughs=code_walkthroughs,
            integration_notes=integration_notes,
            testing_focus=testing_focus,
            source_word_count=source_word_count,
        )

        return dspy.Prediction(
            overview=overview_pred,
            teachings=teaching_pred,
            report=report_pred,
            structured={
                "overview_text": overview_text,
                "section_notes": section_notes,
                "key_concepts": key_concepts,
                "practical_steps": practical_steps,
                "pitfalls": pitfalls,
                "references": references,
                "usage_patterns": usage_patterns,
                "key_functions": key_functions,
                "code_walkthroughs": code_walkthroughs,
                "integration_notes": integration_notes,
                "testing_focus": testing_focus,
            },
        )


--- dspy_file/file_helpers.py ---
# file_helpers.py - utilities for loading files and presenting DSPy results
from __future__ import annotations

import os
from pathlib import Path
from typing import Iterable

import dspy


# Directories that should never be traversed when collecting source files.
ALWAYS_IGNORED_DIRS: set[str] = {
    "__pycache__",
    ".git",
    ".hg",
    ".mypy_cache",
    ".pytest_cache",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    ".vscode",
    ".idea",
    "venv",
    ".github",
    ".taskmaster",
    ".gemini"
    ".clinerules",
    ".cursor",
    "dist",
    ".pytest_cache",
    "codefetch",
    "tests",
    "logs",
    "scripts"
}

# Individual files or suffixes that should never be analyzed.
ALWAYS_IGNORED_FILES: set[str] = {".DS_Store"}
ALWAYS_IGNORED_SUFFIXES: set[str] = {".pyc", ".pyo"}


def _normalize_relative_parts(value: Path | str) -> tuple[str, ...]:
    """Return normalized path segments for relative comparisons."""

    text = str(value).replace("\\", "/").strip()
    if not text:
        return ()
    text = text.strip("/")
    if not text or text in {"", "."}:
        return ()

    parts: list[str] = []
    for segment in text.split("/"):
        if not segment or segment == ".":
            continue
        if segment == "..":
            if parts:
                parts.pop()
            continue
        parts.append(segment)
    return tuple(parts)


def _matches_excluded_parts(
    parts: tuple[str, ...],
    excluded_parts: set[tuple[str, ...]],
) -> bool:
    for excluded in excluded_parts:
        if len(parts) < len(excluded):
            continue
        if parts[: len(excluded)] == excluded:
            return True
    return False


def _normalize_excluded_dirs(exclude_dirs: Iterable[str] | None) -> set[tuple[str, ...]]:
    """Normalize raw exclude strings into comparable path segments."""

    normalized: set[tuple[str, ...]] = set()
    if not exclude_dirs:
        return normalized

    for raw in exclude_dirs:
        cleaned = raw.strip()
        if not cleaned:
            continue
        parts = _normalize_relative_parts(cleaned)
        if parts:
            normalized.add(parts)
    return normalized


def _relative_path_is_excluded(
    relative_path: Path,
    excluded_parts: set[tuple[str, ...]],
) -> bool:
    if not excluded_parts:
        return False
    parts = _normalize_relative_parts(relative_path)
    if not parts:
        return False
    return _matches_excluded_parts(parts, excluded_parts)


def resolve_file_path(raw_path: str) -> Path:
    """Expand user shortcuts and validate that the target file exists."""

    path = Path(raw_path).expanduser().resolve()
    if not path.exists():
        raise FileNotFoundError(f"File not found: {path}")
    if not path.is_file():
        raise IsADirectoryError(f"Expected a file path but received: {path}")
    return path


def _pattern_targets_hidden(pattern: str) -> bool:
    pattern = pattern.strip()
    if not pattern:
        return False
    normalized = pattern[2:] if pattern.startswith("./") else pattern
    return normalized.startswith(".") or "/." in normalized


def _should_skip_dir(name: str, *, ignore_hidden: bool) -> bool:
    if name in ALWAYS_IGNORED_DIRS:
        return True
    if ignore_hidden and name.startswith("."):
        return True
    return False


def _should_skip_file(name: str, *, ignore_hidden: bool) -> bool:
    if name in ALWAYS_IGNORED_FILES:
        return True
    if any(name.endswith(suffix) for suffix in ALWAYS_IGNORED_SUFFIXES):
        return True
    if ignore_hidden and name.startswith("."):
        return True
    return False


def _should_skip_relative_path(
    relative_path: Path,
    *,
    ignore_hidden: bool,
    excluded_parts: set[tuple[str, ...]] | None = None,
) -> bool:
    parts = _normalize_relative_parts(relative_path)
    if not parts:
        return False

    if excluded_parts and _matches_excluded_parts(parts, excluded_parts):
        return True

    # Check intermediate directories for ignore rules.
    for segment in parts[:-1]:
        if segment in ALWAYS_IGNORED_DIRS:
            return True
        if ignore_hidden and segment.startswith("."):
            return True

    return _should_skip_file(parts[-1], ignore_hidden=ignore_hidden)


def collect_source_paths(
    raw_path: str,
    *,
    recursive: bool = True,
    include_globs: Iterable[str] | None = None,
    exclude_dirs: Iterable[str] | None = None,
) -> list[Path]:
    """Resolve a single file or directory into an ordered list of file paths."""

    path = Path(raw_path).expanduser().resolve()
    if not path.exists():
        raise FileNotFoundError(f"Target not found: {path}")

    if path.is_file():
        return [path]

    if not path.is_dir():
        raise IsADirectoryError(f"Expected file or directory path but received: {path}")

    candidates: set[Path] = set()
    patterns = list(include_globs) if include_globs else None
    allow_hidden = any(_pattern_targets_hidden(pattern) for pattern in patterns) if patterns else False
    ignore_hidden = not allow_hidden
    excluded_parts = _normalize_excluded_dirs(exclude_dirs)

    if patterns:
        for pattern in patterns:
            for candidate in path.glob(pattern):
                if not candidate.is_file():
                    continue

                relative_candidate = candidate.relative_to(path)
                if _should_skip_relative_path(
                    relative_candidate,
                    ignore_hidden=ignore_hidden,
                    excluded_parts=excluded_parts,
                ):
                    continue

                candidates.add(candidate.resolve())
    else:
        for root_dir, dirnames, filenames in os.walk(path):
            root_path = Path(root_dir)
            relative_root = Path(".") if root_path == path else root_path.relative_to(path)

            if not recursive and root_path != path:
                dirnames[:] = []
                continue

            if _relative_path_is_excluded(relative_root, excluded_parts):
                dirnames[:] = []
                continue

            dirnames[:] = sorted(
                name
                for name in dirnames
                if not _should_skip_dir(name, ignore_hidden=ignore_hidden)
                and not _relative_path_is_excluded(relative_root / name, excluded_parts)
            )

            for filename in filenames:
                candidate = root_path / filename
                relative_candidate = candidate.relative_to(path)

                if _should_skip_relative_path(
                    relative_candidate,
                    ignore_hidden=ignore_hidden,
                    excluded_parts=excluded_parts,
                ):
                    continue

                candidates.add(candidate.resolve())

    return sorted(candidates)


def _strip_front_matter(text: str) -> str:
    if not text.startswith("---"):
        return text
    end_idx = text.find("\n---", 3)
    if end_idx == -1:
        return text
    return text[end_idx + 4 :]


def _trim_to_first_heading(text: str) -> str:
    lines = text.splitlines()
    for idx, line in enumerate(lines):
        if line.lstrip().startswith("#"):
            return "\n".join(lines[idx:])
    return text


def read_file_content(path: Path) -> str:
    """Read file contents using utf-8 and fall back to latin-1 if needed."""

    try:
        raw = path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        raw = path.read_text(encoding="latin-1")

    cleaned = _strip_front_matter(raw)
    cleaned = _trim_to_first_heading(cleaned)
    return cleaned


def _ensure_trailing_newline(text: str) -> str:
    return text if text.endswith("\n") else text + "\n"


def _teaching_output(result: dspy.Prediction) -> str:
    try:
        report = result.report.report_markdown  # type: ignore[attr-defined]
    except AttributeError:
        report = "# Teaching Brief\n\nThe DSPy pipeline did not produce a report."
    return _ensure_trailing_newline(report)


def _refactor_output(result: dspy.Prediction) -> str:
    template = getattr(result, "template_markdown", None)
    if not template:
        template = getattr(getattr(result, "template", None), "template_markdown", None)
    text = str(template).strip() if template else ""
    if not text:
        text = "# Refactor Template\n\nTemplate generation failed."
    return _ensure_trailing_newline(text)


def render_prediction(result: dspy.Prediction, *, mode: str = "teach") -> str:
    """Return the generated markdown for the selected analysis mode."""

    if mode == "refactor":
        return _refactor_output(result)
    return _teaching_output(result)


--- dspy_file/__init__.py ---
"""DSPy file teaching analyzer package."""

from .file_analyzer import FileTeachingAnalyzer, TeachingConfig
from .refactor_analyzer import FileRefactorAnalyzer, RefactorTeachingConfig

__all__ = [
    "FileTeachingAnalyzer",
    "TeachingConfig",
    "FileRefactorAnalyzer",
    "RefactorTeachingConfig",
]


--- dspy_file/refactor_analyzer.py ---
# refactor_analyzer.py - DSPy module that prepares per-file refactor prompt templates
from __future__ import annotations

from dataclasses import dataclass, field
from functools import lru_cache
from typing import Any

import dspy

from .prompts import load_prompt_text


class RefactorTemplateSignature(dspy.Signature):
    """Generate a reusable refactor prompt template from a source document."""

    file_path: str = dspy.InputField(desc="Path to the source file for context")
    file_content: str = dspy.InputField(desc="Full raw text of the file")

    template_markdown: str = dspy.OutputField(
        desc="Markdown template with numbered placeholders and section scaffolding"
    )


@dataclass
class RefactorTeachingConfig:
    """Configuration for the refactor template generator."""

    max_tokens: int = 8000
    temperature: float | None = 0.7
    top_p: float | None = 0.80
    n_completions: int | None = None
    extra_lm_kwargs: dict[str, Any] = field(default_factory=dict)

    def lm_kwargs(self) -> dict[str, Any]:
        """Return the language model arguments for DSPy modules."""

        kwargs: dict[str, Any] = {**self.extra_lm_kwargs, "max_tokens": self.max_tokens}
        if self.temperature is not None:
            kwargs["temperature"] = self.temperature
        if self.top_p is not None:
            kwargs["top_p"] = self.top_p
        if self.n_completions is not None:
            kwargs["n"] = self.n_completions
        return kwargs


@lru_cache(maxsize=1)
def _load_default_template() -> str:
    """Load the bundled refactor prompt template text."""

    return load_prompt_text(None).strip()


def _ensure_template_text(value: str | None) -> str:
    if value and value.strip():
        text = value.rstrip()
    else:
        text = "# Refactor Template\n\nTemplate generation failed."
    return text if text.endswith("\n") else text + "\n"


class FileRefactorAnalyzer(dspy.Module):
    """Generate a refactor-focused prompt template for a single file."""

    def __init__(
        self,
        *,
        template_text: str | None = None,
        config: RefactorTeachingConfig | None = None,
    ) -> None:
        super().__init__()
        self.config = config or RefactorTeachingConfig()
        instructions = template_text.strip() if template_text else _load_default_template()
        signature = RefactorTemplateSignature.with_instructions(instructions)
        self.generator = dspy.ChainOfThought(signature, **self.config.lm_kwargs())

    def forward(self, *, file_path: str, file_content: str) -> dspy.Prediction:
        raw_prediction = self.generator(
            file_path=file_path,
            file_content=file_content,
        )

        template_markdown = _ensure_template_text(
            getattr(raw_prediction, "template_markdown", None)
        )

        return dspy.Prediction(
            template=raw_prediction,
            template_markdown=template_markdown,
        )


--- dspy_file/signatures.py ---
# signatures.py - DSPy signatures focused on extracting teachings from a single file
from typing import List

import dspy


class FileOverview(dspy.Signature):
    """Summarize the file structure and core narrative with room for depth."""

    file_path: str = dspy.InputField(desc="Path to the source file")
    file_content: str = dspy.InputField(desc="Full raw text of the file")

    overview: str = dspy.OutputField(
        desc="Detailed multi-section overview (aim for 4-6 paragraphs capturing scope, intent, and flow)"
    )
    section_notes: List[str] = dspy.OutputField(
        desc="Comprehensive bullet list summarizing each major section, include headings when possible"
    )


class TeachingPoints(dspy.Signature):
    """Extract teachable concepts, workflows, and cautions."""

    file_content: str = dspy.InputField(desc="Full raw text of the file")

    key_concepts: List[str] = dspy.OutputField(desc="Essential ideas learners must retain")
    practical_steps: List[str] = dspy.OutputField(desc="Actionable steps or workflows described")
    pitfalls: List[str] = dspy.OutputField(desc="Warnings, gotchas, or misconceptions to avoid")
    references: List[str] = dspy.OutputField(desc="Follow-up links, exercises, or related material")
    usage_patterns: List[str] = dspy.OutputField(
        desc="Common usage patterns, scenarios, or recipes that appear"
    )
    key_functions: List[str] = dspy.OutputField(
        desc="Important functions, classes, or hooks with quick rationale"
    )
    code_walkthroughs: List[str] = dspy.OutputField(
        desc="Short code snippets or walkthroughs learners should discuss"
    )
    integration_notes: List[str] = dspy.OutputField(
        desc="Guidance for connecting this file with the rest of the system"
    )
    testing_focus: List[str] = dspy.OutputField(
        desc="Areas that need tests, validations, or monitoring"
    )


class TeachingReport(dspy.Signature):
    """Compose a concise but comprehensive markdown teaching brief."""

    file_path: str = dspy.InputField(desc="Original file path for context header")
    overview: str = dspy.InputField()
    section_notes: List[str] = dspy.InputField()
    key_concepts: List[str] = dspy.InputField()
    practical_steps: List[str] = dspy.InputField()
    pitfalls: List[str] = dspy.InputField()
    references: List[str] = dspy.InputField()
    usage_patterns: List[str] = dspy.InputField()
    key_functions: List[str] = dspy.InputField()
    code_walkthroughs: List[str] = dspy.InputField()
    integration_notes: List[str] = dspy.InputField()
    testing_focus: List[str] = dspy.InputField()

    report_markdown: str = dspy.OutputField(desc="Final markdown document capturing key teachings")


--- dspy_file/prompts/code-fence.md ---
# Command: /markdown:wrap-md-fence

# Usage: /markdown:wrap-md-fence "your content here"

# Args

# - {{content}}: raw bytes to wrap verbatim inside the fence

prompt = """
Wrap the provided {{content}} verbatim with a Markdown code fence labeled md.

Rules:

* Zero changes to {{content}} (byte-for-byte).
* Preserve encoding, line endings, and terminal newline presence/absence.
* No additional output or whitespace outside the fence.

Output exactly:

```md
{{content}}
```

Acceptance:

* Inner bytes are identical to {{content}}.
* Only the opening line `md and the closing` are added.
  """


--- dspy_file/prompts/front-matter-v2.md ---
<!-- $1 = source Markdown text; $2 = template name/title (optional; infer if missing); $3 = maximum placeholders allowed (1–9; default 7); $4 = input parameters block; $5 = controlled taxonomy/list block; $6 = stage mapping/rules block; $7 = output examples block -->

# $2

Task: Given $1, produce a structured **metadata block** and then emit the original body unchanged. The metadata must expose identifiers, categories, optional lifecycle/stage, optional dependencies, optional provided artifacts, and a concise summary. Output = metadata, blank line, then $1.

## Inputs

$4

## Canonical taxonomy (exact strings)

$5

### Stage hints (for inference)

$6

## Algorithm

1. Extract signals from $1

   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier

   * Prefer explicit input; otherwise infer from main action + object.
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).
   * De-duplicate.

3. Determine categories

   * Prefer explicit input; otherwise infer from verbs/headings vs $5.
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)

   * Prefer explicit input; otherwise map categories via $6.
   * Omit if uncertain.

5. Determine dependencies (optional)

   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)

   * Short list (≤3) of unlocked outputs.

7. Compose summary

   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format

   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata

   * Merge: explicit inputs > existing > inferred.
   * Validate lists; move unknowns to an extension field if needed.
   * Remove empty keys.

## Assumptions & Constraints

* Emit exactly one document: metadata, a single blank line, then $1.
* Limit distinct placeholders to ≤ $3.

## Validation

* Identifier matches a normalized id pattern.
* Categories non-empty and drawn from $5 (≤3).
* Stage, if present, is one of the allowed stages implied by $6.
* Dependencies, if present, are id-shaped (≤5).
* Summary ≤120 chars; punctuation coherent.
* Body text $1 is not altered.

## Output format examples

$7


--- dspy_file/prompts/gemini-cli-command_prompt-template.md ---
# gemini-cli-command_prompt-template_v2

Task: From {prompt_or_md} and {user_context}, synthesize a Gemini CLI custom command TOML that generalizes the task via inferred placeholders.

Heuristics

1) Mark user-changeable text (queries, titles) → {{query}}
2) Paths/globs → {{path}}
3) Targets/entities (repo/service/ticket) → {{target}}
4) Quantities/limits → {{limit}}
If uncertainty remains, collapse everything into {{args}}. Never exceed 3 distinct placeholders unless the text explicitly lists more inputs.

Transformations

- Preserve order and intent; remove fluff.
- If the source includes code or CLI calls, render them as `!{...}` lines inside the prompt.
- Prefer imperative voice (“Do X, then Y”). Add brief, testable acceptance lines inside the prompt if helpful.

Deliverable (only TOML; no extra prose)

# Command: /{namespace:=user}:{command:=auto-from-title}

# Usage: /{namespace}:{command} "example value(s)"

# Args

# - {{query}}: what to search or summarize

# - {{path}}: file or directory (optional)

prompt = """
<final instruction with placeholders and any !{...} inserts>
"""

Checks

- Valid TOML; balanced triple quotes.
- Includes ≥1 placeholder with matching Usage.
- Names are lowercase, kebab-case.
- No commentary outside TOML.


--- dspy_file/prompts/gemini-cli_extension-command_prompt-template.md ---
# gemini-cli_commands-prompt-template_v1

Goal: Produce a parameterized template from {source_text} using positional placeholders and a machine-checkable Arg spec.

Requirements

- Use {placeholder_syntax} (default: `$1..$9`).
- Insert ≤ {max_placeholders} high-impact placeholders; deduplicate repeats.
- Emit JSON Arg spec immediately after the templated text, with this shape:
    {
      "args": [
        { "id": "$1", "name": "{name}", "hint": "{short_hint}", "example": "{example}", "required": true, "validate": "{regex|rule}" }
      ]
    }
- Preserve markdown/code formatting byte-for-byte except at replacement spans.
- Do not change meaning, tone, or constraints of {source_text}.

Heuristics (apply in order)

1) User-owned identifiers: paths, repo/org names, endpoints, secrets placeholders.
2) Content slots: problem statement, target audience/domain, primary input.
3) Tunables: N/limits/timeouts only if not hard requirements.
4) Repeated literals → one arg; propagate to all occurrences.
5) Skip boilerplate constants (e.g., license names, standard flags) unless context marks them variable.

Edge cases

- If already templated, extend only with missing args; do not renumber existing placeholders.
- If no clear candidates, introduce `$1` as `topic_or_input` at the primary noun phrase of the opening sentence and document it.
- For JSON/YAML in code fences, ensure placeholders remain valid strings (quote if needed).

Acceptance tests (must pass)

- T1: All placeholders appear in the Arg spec; counts match.
- T2: Substituting provided examples yields valid markdown and runnable snippets.
- T3: Repeated concepts map to a single placeholder consistently.
- T4: Total placeholders ≤ {max_placeholders}; none are trivial.

Deliverables

1) **Templated Text** — {source_text} with placeholders inserted per heuristics.
2) **Args JSON** — machine-checkable spec as shown above.


--- tests/conftest.py ---
from __future__ import annotations

import os
from pathlib import Path


_CACHE_DIR = Path(__file__).parent / ".dspy-cache"
_CACHE_DIR.mkdir(exist_ok=True)

os.environ.setdefault("DISKCACHE_DEFAULT_DIRECTORY", str(_CACHE_DIR))
os.environ.setdefault("DSPY_CACHE_DIR", str(_CACHE_DIR))
os.environ.setdefault("DSPY_CACHEDIR", str(_CACHE_DIR))


--- tests/smoke_test.py ---
from __future__ import annotations

from pathlib import Path
from types import SimpleNamespace
from unittest import mock

from dspy_file import analyze_file_cli


class DummyAnalyzer:
    def __init__(self) -> None:
        self.calls: list[tuple[str, str]] = []

    def __call__(self, *, file_path: str, file_content: str):  # type: ignore[override]
        self.calls.append((file_path, file_content))
        return SimpleNamespace(
            report=SimpleNamespace(
                report_markdown="# Teaching Brief\n\n- Generated by DummyAnalyzer."
            )
        )


def test_analyze_path_writes_markdown(tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("# Title\n\nSome content", encoding="utf-8")

    output_dir = tmp_path / "reports"
    dummy = DummyAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileTeachingAnalyzer", return_value=dummy
    ):
        exit_code = analyze_file_cli.analyze_path(
            str(source),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=False,
            exclude_dirs=None,
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.TEACH,
            prompt_text=None,
        )

    assert exit_code == 0
    assert dummy.calls == [(str(source), "# Title\n\nSome content")]

    generated_files = list(output_dir.glob("*.md"))
    assert len(generated_files) == 1
    generated_text = generated_files[0].read_text(encoding="utf-8")
    assert generated_text.startswith("# Teaching Brief\n")
    assert generated_text.endswith("\n")


@mock.patch.object(analyze_file_cli, "_confirm_analyze", return_value=False)
def test_analyze_path_confirm_each_skips_when_declined(confirm_mock: mock.Mock, tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("# Title\n\nSome content", encoding="utf-8")

    output_dir = tmp_path / "reports"
    dummy = DummyAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileTeachingAnalyzer", return_value=dummy
    ):
        exit_code = analyze_file_cli.analyze_path(
            str(source),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=True,
            exclude_dirs=None,
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.TEACH,
            prompt_text=None,
        )

    assert exit_code == 0
    assert dummy.calls == []
    assert list(output_dir.glob("*.md")) == []
    confirm_mock.assert_called_once()


def test_analyze_path_respects_exclude_dirs(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    include_dir = project_root / "include"
    skip_dir = project_root / "skip"
    include_dir.mkdir(parents=True)
    skip_dir.mkdir(parents=True)

    include_file = include_dir / "keep.py"
    skip_file = skip_dir / "ignore.py"
    include_file.write_text("print('keep')\n", encoding="utf-8")
    skip_file.write_text("print('ignore')\n", encoding="utf-8")

    output_dir = tmp_path / "reports"
    dummy = DummyAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileTeachingAnalyzer", return_value=dummy
    ):
        exit_code = analyze_file_cli.analyze_path(
            str(project_root),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=False,
            exclude_dirs=["skip"],
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.TEACH,
            prompt_text=None,
        )

    assert exit_code == 0
    assert dummy.calls == [(str(include_file), "print('keep')\n")]
    output_files = [p for p in output_dir.rglob("*") if p.is_file()]
    assert {p.relative_to(output_dir) for p in output_files} == {Path("include") / "keep.py"}
    generated_file = output_dir / "include" / "keep.py"
    assert "ignore" not in generated_file.read_text(encoding="utf-8")


def test_analyze_path_refactor_mode_uses_refactor_analyzer(tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("# Title\n\nSome content", encoding="utf-8")

    output_dir = tmp_path / "reports"

    class DummyRefactorAnalyzer:
        def __init__(self) -> None:
            self.calls: list[tuple[str, str]] = []

        def __call__(self, *, file_path: str, file_content: str):  # type: ignore[override]
            self.calls.append((file_path, file_content))
            return SimpleNamespace(template_markdown="# Template\n\nValue.")

    dummy = DummyRefactorAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileRefactorAnalyzer", return_value=dummy
    ) as refactor_cls:
        exit_code = analyze_file_cli.analyze_path(
            str(source),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=False,
            exclude_dirs=None,
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.REFACTOR,
            prompt_text="Custom prompt",
        )

    assert exit_code == 0
    assert dummy.calls == [(str(source), "# Title\n\nSome content")]
    generated_file = output_dir / "example.md"
    assert generated_file.exists()
    assert generated_file.read_text(encoding="utf-8").endswith("Value.\n")
    refactor_cls.assert_called_once()
    assert refactor_cls.call_args.kwargs["template_text"] == "Custom prompt"


def test_resolve_prompt_text_with_menu(tmp_path: Path) -> None:
    prompt_one = tmp_path / "first.md"
    prompt_two = tmp_path / "second.md"
    prompt_one.write_text("First template", encoding="utf-8")
    prompt_two.write_text("Second template", encoding="utf-8")

    options = [
        analyze_file_cli.PromptTemplate(name="first", path=prompt_one),
        analyze_file_cli.PromptTemplate(name="second", path=prompt_two),
    ]

    with mock.patch.object(
        analyze_file_cli, "list_bundled_prompts", return_value=options
    ), mock.patch("builtins.input", side_effect=["2"]):
        text = analyze_file_cli._resolve_prompt_text(None)

    assert text == "Second template"


def test_resolve_prompt_text_with_explicit_path(tmp_path: Path) -> None:
    prompt_path = tmp_path / "custom.md"
    prompt_path.write_text("Custom prompt text", encoding="utf-8")

    text = analyze_file_cli._resolve_prompt_text(str(prompt_path))

    assert text == "Custom prompt text"


def test_parser_short_options_are_supported() -> None:
    parser = analyze_file_cli.build_parser()
    args = parser.parse_args(
        [
            "sample.md",
            "-r",
            "-m",
            "refactor",
            "-nr",
            "-g",
            "**/*.py",
            "-g",
            "**/*.md",
            "-i",
            "-ed",
            "skip,temp",
            "-o",
            "reports",
            "-p",
            "custom-prompt",
        ]
    )

    assert args.path == "sample.md"
    assert args.raw is True
    assert args.mode == "refactor"
    assert args.non_recursive is True
    assert args.include_globs == ["**/*.py", "**/*.md"]
    assert args.confirm_each is True
    assert args.exclude_dirs == ["skip,temp"]
    assert args.output_dir == "reports"
    assert args.prompt == "custom-prompt"


--- tests/test_cli_connectivity.py ---
from __future__ import annotations

from pathlib import Path
from unittest import mock

import pytest

from dspy_file import analyze_file_cli


def test_probe_openai_provider_success() -> None:
    mock_response = mock.MagicMock()
    urlopen_mock = mock.MagicMock()
    urlopen_mock.return_value.__enter__.return_value = mock_response

    with mock.patch(
        "dspy_file.analyze_file_cli.request.urlopen", urlopen_mock
    ):
        analyze_file_cli._probe_openai_provider("http://localhost:1234/v1", "token")

    urlopen_mock.assert_called_once()


def test_probe_openai_provider_raises_on_failure() -> None:
    with mock.patch(
        "dspy_file.analyze_file_cli.request.urlopen",
        side_effect=analyze_file_cli.urlerror.URLError("connection refused"),
    ):
        with pytest.raises(analyze_file_cli.ProviderConnectivityError):
            analyze_file_cli._probe_openai_provider("http://localhost:1234/v1", "token")


def test_main_exits_early_when_lmstudio_unreachable(tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("content", encoding="utf-8")

    with mock.patch.object(
        analyze_file_cli,
        "_probe_openai_provider",
        side_effect=analyze_file_cli.ProviderConnectivityError("unreachable"),
    ), mock.patch.object(analyze_file_cli, "configure_model") as configure_mock:
        exit_code = analyze_file_cli.main(
            ["--provider", "lmstudio", str(source)]
        )

    assert exit_code == 1
    configure_mock.assert_not_called()


--- tests/test_file_helpers.py ---
from __future__ import annotations

from pathlib import Path
from types import SimpleNamespace

from dspy_file.file_helpers import collect_source_paths, render_prediction


def _touch(path: Path, content: str = "") -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(content, encoding="utf-8")


def test_collect_source_paths_skips_hidden_and_config_files(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    project_root.mkdir()

    allowed_file = project_root / "main.py"
    _touch(allowed_file, "print('ok')\n")

    _touch(project_root / ".env", "SECRET=1\n")
    _touch(project_root / ".venv" / "lib" / "ignore.py", "print('ignored')\n")
    _touch(project_root / "nested" / ".secrets", "hidden\n")

    collected = collect_source_paths(str(project_root))

    assert collected == [allowed_file.resolve()]


def test_hidden_files_can_be_included_with_explicit_glob(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    project_root.mkdir()

    hidden_file = project_root / ".config" / "ci.yml"
    _touch(hidden_file, "name: ci\n")

    collected = collect_source_paths(
        str(project_root),
        include_globs=[".config/**/*.yml"],
    )

    assert collected == [hidden_file.resolve()]


def test_collect_source_paths_honors_exclude_dirs(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    project_root.mkdir()

    kept = project_root / "keep" / "file.py"
    skipped = project_root / "skip" / "ignored.py"
    nested_skip = project_root / "nested" / "deep" / "hidden.py"
    _touch(kept, "print('keep')\n")
    _touch(skipped, "print('ignore')\n")
    _touch(nested_skip, "print('nested ignore')\n")

    collected = collect_source_paths(
        str(project_root),
        exclude_dirs=["skip", "nested/deep"],
    )
    collected_glob = collect_source_paths(
        str(project_root),
        include_globs=["**/*.py"],
        exclude_dirs=["skip", "nested/deep"],
    )

    assert collected == [kept.resolve()]
    assert collected_glob == [kept.resolve()]


def test_render_prediction_teach_mode_uses_report() -> None:
    prediction = SimpleNamespace(
        report=SimpleNamespace(report_markdown="# Brief\n\nContent."),
    )
    output = render_prediction(prediction, mode="teach")
    assert output.endswith("Content.\n")


def test_render_prediction_refactor_mode_prefers_template_markdown() -> None:
    prediction = SimpleNamespace(template_markdown="# Template\n\nValue.")
    output = render_prediction(prediction, mode="refactor")
    assert output.endswith("Value.\n")
