# llms-full (private-aware)
> Built by GitHub fetches (raw or authenticated). Large files may be truncated.

--- example-data/prompt-front-matter/_shared__tm__overview.tm.refactor.md ---
# TaskMaster Overview

## Metadata

- **identifier**: tm-overview  
- **category**: summarization  
- **lifecycle_stage**: inspection  
- **dependencies**: tasks.json  
- **provided_artifacts**: overview bullets, totals table, top pending list, critical path list, issues list  
- **summary**: Summarize TaskMaster tasks.json by status, priority, and dependency health to orient work.

## Inputs

- `tasks.json` path (optional; defaults to repo root)

## Canonical taxonomy (exact strings)

- summarization
- analysis
- reporting

### Stage hints (for inference)

- inspection → summarizing state, reading data
- analysis → detecting cycles, computing paths
- reporting → outputting tables and lists

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).  

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- Body text is not altered.

## Validation

- Identifier matches a normalized id pattern (e.g., kebab-case).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints (inspection, analysis, reporting).
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Input: `/tm-overview`  
  Output:  
    # Overview  
    - Bullet summary of status, priority, dependencies  
    ## Totals  
    | status       | count | percent | notes         |  
    |--------------|-------|---------|---------------|  
    | pending      | 5     | 40%     | high volume   |  
    | in_progress  | 3     | 25%     | active        |  
    | blocked      | 1     | 8%      | dependency    |  
    | done         | 6     | 50%     | completed     |  
    ## Top Pending  
    | id   | title               | priority | unblockers          |  
    |------|---------------------|----------|---------------------|  
    | t-12 | Fix login timeout   | high     | resolve API error   |  
    | t-34 | Deploy frontend     | medium   | wait for backend    |  
    ## Critical Path  
    - t-12 → t-34 → t-56  
    ## Issues  
    - Cycle detected: t-78 → t-90 → t-78  
    - Missing reference: t-11 (no dependencies)  
    - Duplicate entry: t-44 appears twice  

---

# TaskMaster Overview

Trigger: /tm-overview

Purpose: Summarize the current TaskMaster tasks.json by status, priority, dependency health, and critical path to orient work.

Steps:

1. Locate the active tasks.json at repo root or the path supplied in the user message. Do not modify it.
2. Parse fields: id, title, description, status, priority, dependencies, subtasks.
3. Compute counts per status and a table of top pending items by priority.
4. Detect dependency issues: cycles, missing ids, orphans (no deps and not depended on).
5. Approximate a critical path: longest dependency chain among pending→in_progress tasks.

Output format:

- "# Overview" then a bullets summary.
- "## Totals" as a 4-column table: status | count | percent | notes.
- "## Top Pending" table: id | title | priority | unblockers.
- "## Critical Path" as an ordered list of ids with short titles.
- "## Issues" list for cycles, missing references, duplicates.

Examples:

- Input (Codex TUI): /tm-overview
- Output: tables and lists as specified. Keep to <= 200 lines.

Notes:

- Read-only. Assume statuses: pending | in_progress | blocked | done.
- If tasks.json is missing or invalid, output an "## Errors" section with a concise diagnosis.

Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## template_markdown ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.


--- example-data/codex-prompts/tm-overview.refactor.md ---
# TaskMaster Overview

Trigger: $1

Purpose: Summarize the current TaskMaster tasks.json by status, priority, dependency health, and critical path to orient work.

Steps:

1. Locate the active tasks.json at repo root or the path supplied in the user message. Do not modify it.
2. Parse fields: id, title, description, status, priority, dependencies, subtasks.
3. Compute counts per status and a table of top pending items by priority.
4. Detect dependency issues: cycles, missing ids, orphans (no deps and not depended on).
5. Approximate a critical path: longest dependency chain among pending→in_progress tasks.

Output format:

- "# Overview" then a bullets summary.
- "## Totals" as a 4-column table: status | count | percent | notes.
- "## Top Pending" table: id | title | priority | unblockers.
- "## Critical Path" as an ordered list of ids with short titles.
- "## Issues" list for cycles, missing references, duplicates.

Examples:

- Input (Codex TUI): $2
- Output: tables and lists as specified. Keep to <= 200 lines.

Notes:

- Read-only. Assume statuses: pending | in_progress | blocked | done.
- If tasks.json is missing or invalid, output an "## Errors" section with a concise diagnosis.

---

### Affected files
$3

### Root cause
$4

### Proposed fix
$5

### Tests
$6

### Docs gaps
$7

### Open questions
$8


--- example-data/prompt-front-matter/10-scaffold__conventions__version-control-guide.conventions.refactor.md ---
# Version Control Guide

## Metadata

- **Identifier**: version-control-guide
- **Categories**: development practice, workflow guide, code hygiene
- **Stage**: implementation
- **Dependencies**: none
- **Provided Artifacts**: checklist, suggested commands
- **Summary**: Enforce clean incremental commits and clean-room re-implementation to ensure reproducible and safe changes.

## Inputs

- Trigger: /version-control-guide
- Purpose: Enforce clean incremental commits and clean-room re-implementation when finalizing.
- Output format: Checklist plus suggested commands for the current repo state.
- Examples: Convert messy spike into three commits: setup, feature, tests.
- Notes: Never modify remote branches without confirmation.

## Canonical taxonomy (exact strings)

- development practice
- workflow guide
- code hygiene

### Stage hints (for inference)

- implementation
- commit workflow
- development lifecycle

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- Output body unchanged.

## Validation

- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Identifier: version-control-guide  
- Categories: development practice, workflow guide, code hygiene  
- Stage: implementation  
- Dependencies: none  
- Provided Artifacts: checklist, suggested commands  
- Summary: Enforce clean incremental commits and clean-room re-implementation to ensure reproducible and safe changes.

# Version Control Guide

Trigger: /version-control-guide

Purpose: Enforce clean incremental commits and clean-room re-implementation when finalizing.

## Steps

1. Start each feature from a clean branch: `git switch -c <feat>`.
2. Commit in vertical slices with passing tests: `git add -p && git commit`.
3. When solution is proven, recreate a minimal clean diff: stash or copy results, reset, then apply only the final changes.
4. Use `git revert` for bad commits instead of force-pushing shared branches.

## Output format

- Checklist plus suggested commands for the current repo state.

## Examples

- Convert messy spike into three commits: setup, feature, tests.

## Notes

- Never modify remote branches without confirmation.


--- example-data/prompt-front-matter/40-testing__coverage__guide.coverage.refactor.md ---
# Coverage Plan

## Inputs
- Command: `/coverage-guide`
- Input context: none (command runs without arguments)
- Expected output format: concise summary, prioritized recommendations with rationale, coverage gaps and validation steps

## Canonical taxonomy (exact strings)
- testing
- analysis
- prioritization

### Stage hints (for inference)
- analyze → gather data and propose insights
- plan → suggest actionable items
- execute → run tests or apply changes

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.  
   → Identifier: coverage-plan

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).  
   → Categories: testing, analysis, prioritization

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.  
   → Stage: analyze

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).  
   → Dependencies: find . -name 'coverage*', git ls-files

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.  
   → Artifacts: prioritized test recommendations, coverage gap identification, validation steps

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”  
   → Summary: Suggest a plan to raise coverage based on uncovered areas to achieve actionable, high-ROI test additions.

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.  

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤7.
- All values are derived from content or inference using canonical taxonomy and stage hints.

## Validation
- Identifier matches a normalized id pattern → yes (coverage-plan)
- Categories non-empty and drawn from canonical taxonomy (≤3) → yes
- Stage, if present, is one of the allowed stages implied by stage hints → yes (analyze)
- Dependencies, if present, are id-shaped (≤5) → yes
- Summary ≤120 chars; punctuation coherent → 118 characters
- Body text $1 is not altered.

## Output format examples
- Focus on src/auth/login.ts — 0% branch coverage; add error path test.
- Prioritize authentication modules with low branch coverage (e.g., login, token validation).
- Identify missing edge cases in user input handling and validate via unit tests.


--- example-data/prompt-front-matter/50-docs__api-docs__api-docs-local.api-docs.refactor.md ---
# API Docs Local

## Metadata

- **identifier**: api-docs-local
- **categories**: [documentation, retrieval, storage]
- **lifecycle_stage**: configuration
- **dependencies**: []
- **provided_artifacts**: ["docs/apis/ directory", "DOCS.md index file"]
- **summary**: Do fetch API docs and store locally to achieve offline, deterministic reference.

## Inputs

- URLs or package names to retrieve documentation from.

## Canonical taxonomy (exact strings)

- documentation
- retrieval
- storage
- configuration
- generation
- deployment
- validation

### Stage hints (for inference)

- configuration → setup of environment or initial state
- retrieval → fetching data from external sources
- storage → saving content locally
- deployment → making system available to users

## Algorithm

1. Extract signals from $1  
   *Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.*

2. Determine the primary identifier  
   *Prefer explicit input; otherwise infer from main action + object.*  
   *Normalize (lowercase, kebab-case, length-capped, starts with a letter).*  
   *De-duplicate.*

3. Determine categories  
   *Prefer explicit input; otherwise infer from verbs/headings vs $5.*  
   *Validate, sort deterministically, and de-dupe (≤3).*

4. Determine lifecycle/stage (optional)  
   *Prefer explicit input; otherwise map categories via $6.*  
   *Omit if uncertain.*

5. Determine dependencies (optional)  
   *Parse phrases implying order or prerequisites; keep id-shaped items (≤5).*

6. Determine provided artifacts (optional)  
   *Short list (≤3) of unlocked outputs.*

7. Compose summary  
   *One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”*

8. Produce metadata in the requested format  
   *Default to a human-readable serialization; honor any requested alternative.*

9. Reconcile if input already contains metadata  
   *Merge: explicit inputs > existing > inferred.*  
   *Validate lists; move unknowns to an extension field if needed.*  
   *Remove empty keys.*

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.

## Validation

- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Command list and file paths to place docs under `docs/apis/`.
- Example: curl -o docs/apis/github.com/api.json https://api.github.com/docs
- Example: npm view express docs --json > docs/apis/express/README.md

# API Docs Local

Trigger: /api-docs-local

Purpose: Fetch API docs and store locally for offline, deterministic reference.

## Steps

1. Create `docs/apis/` directory.
2. For each provided URL or package, write retrieval commands (curl or `npm view` docs links). Do not fetch automatically without confirmation.
3. Add `DOCS.md` index linking local copies.

## Output format

- Command list and file paths to place docs under `docs/apis/`.


--- example-data/prompt-front-matter/50-docs__api-docs__openapi-generate.api-docs.refactor.md ---
# OpenAPI Generate

## Metadata

- **Identifier**: generate-api  
- **Categories**: code generation, api scaffolding, build  
- **Stage**: build  
- **Dependencies**: none  
- **Provided Artifacts**: 
  - Summary table of generated paths  
  - Scripts to add (e.g., `make generate-api`, `pnpm sdk:gen`)  
  - TODO list for unimplemented handlers  
- **Summary**: Generate server stubs or typed clients from an OpenAPI spec to achieve code scaffolding with validation and CI checks.

## Inputs

- Command: `/openapi-generate <server|client> <lang> <spec-path>`
- Parameters:
  - `<server>`: Generates controllers, routers, validation, and error middleware into `apps/api`
  - `<client>`: Generates a typed SDK into `packages/sdk` with fetch wrapper and retry/backoff
  - `<spec-path>`: Path to OpenAPI spec (e.g., `apis/auth/openapi.yaml`)
- Output format: Summary table of generated paths, scripts to add, and next actions

## Canonical taxonomy (exact strings)

- code generation  
- api scaffolding  
- build  

### Stage hints (for inference)

- generate → build  
- scaffold → build  
- script addition → build  

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.  

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.  

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).  

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.  

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).  

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.  

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”  

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.  

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.  

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.

## Validation

- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- `/openapi-generate client ts apis/auth/openapi.yaml`
- Output: Summary table of generated paths, scripts to add, and next actions
- Notes: Prefer openapi-typescript + zod for TS clients when possible

---

# OpenAPI Generate

Trigger: /openapi-generate <server|client> <lang> <spec-path>

Purpose: Generate server stubs or typed clients from an OpenAPI spec.

**Steps:**

1. Validate `<spec-path>`; fail with actionable errors.
2. For `server`, generate controllers, routers, validation, and error middleware into `apps/api`.
3. For `client`, generate a typed SDK into `packages/sdk` with fetch wrapper and retry/backoff.
4. Add `make generate-api` or `pnpm sdk:gen` scripts and CI step to verify no drift.
5. Produce a diff summary and TODO list for unimplemented handlers.

**Output format:** summary table of generated paths, scripts to add, and next actions.

**Examples:** `/openapi-generate client ts apis/auth/openapi.yaml`.

**Notes:** Prefer openapi-typescript + zod for TS clients when possible.


--- example-data/prompt-front-matter/50-docs__doc-plan__gemini-map.doc-plan.refactor.md ---
# Gemini→Codex Mapper

Task: Given a TOML configuration for a Gemini CLI command, produce a structured Codex prompt file with metadata and example usage. The output must be ready to run via bash.

## Inputs
- TOML input containing `description`, `prompt`, and optional `Expected output` or `Usage`
- Target output format constraints (≤300 words, specific sections)

## Canonical taxonomy (exact strings)
- migration
- prompts
- tooling
- transform
- build
- validate

### Stage hints (for inference)
- "translation" → transform  
- "generates", "writes", "creates" → build  
- "validates" → validate  

## Algorithm
1. Extract signals from TOML:
   - Description and prompt define intent.
   - Expected output defines structure of result.

2. Determine the primary identifier:
   - Prefer explicit input; otherwise infer from main action + object.
   - Normalize to lowercase, kebab-case, length-capped (≤32), starts with letter.
   - Result: `gemini-map`

3. Determine categories:
   - Prefer explicit tags: migration, prompts, tooling
   - Validate and de-dupe → [migration, prompts, tooling]

4. Determine lifecycle/stage:
   - Map from "translation" to "transform"
   - Stage: transform

5. Determine dependencies:
   - No prerequisites mentioned.
   - Dependencies: []

6. Determine provided artifacts:
   - Codex prompt file (structured with role, steps, output, example)
   - Bash snippet for writing the file to `~/.codex/prompts/<filename>.md`

7. Compose summary:
   - "Do translate a Gemini CLI TOML command into a Codex prompt file to achieve structured, reusable prompt generation."

8. Produce metadata in human-readable format:
   - identifier: gemini-map
   - categories: migration, prompts, tooling
   - stage: transform
   - dependencies: []
   - artifacts: codex-prompt-file, bash-write-snippet
   - summary: Do translate a Gemini CLI TOML command into a Codex prompt file to achieve structured, reusable prompt generation.

9. Reconcile if input already contains metadata:
   - No existing metadata; all derived from explicit or inferable signals.

## Assumptions & Constraints
- Output must include metadata block followed by blank line and original body unchanged.
- All identifiers normalized and within constraints.
- Categories strictly from canonical taxonomy.
- Stage inferred via stage hints only if not explicit.
- Artifacts are short-listed (≤3).
- Summary ≤120 characters.

## Validation
- Identifier: `gemini-map` → valid kebab-case, lowercase.
- Categories: [migration, prompts, tooling] → all in taxonomy, non-empty, de-duplicated.
- Stage: transform → valid and implied by translation workflow.
- Dependencies: empty list → valid.
- Artifacts: codex-prompt-file, bash-write-snippet → both valid and ≤3.
- Summary: 108 characters; coherent and punctuated correctly.

## Output format examples
```markdown
# Gemini→Codex Mapper

identifier: gemini-map  
categories: migration, prompts, tooling  
stage: transform  
dependencies: []  
artifacts: codex-prompt-file, bash-write-snippet  
summary: Do translate a Gemini CLI TOML command into a Codex prompt file to achieve structured, reusable prompt generation.

You are a translator that converts a Gemini CLI TOML command into a Codex prompt file.

Steps:

1) Read TOML with `description` and `prompt`.
2) Extract the task, inputs, and outputs implied by the TOML.
3) Write a Codex prompt file ≤ 300 words:

    - Role line `You are ...`
    - Numbered steps
    - Output section
    - Example input and expected output
    - `Usage: /<command>` line
    - YAML-like metadata at top

4) Choose a short, hyphenated filename ≤ 32 chars.
5) Emit a ready-to-run bash snippet:
`cat > ~/.codex/prompts/<filename>.md << 'EOF'` … `EOF`.
6) Do not include destructive commands or secrets.

Example input:

```toml
description = "Draft a PR description"
prompt = "Create sections Summary, Context, Changes from diff stats"
Expected output:

A pr-desc.md file with the structure above and a bash cat > block.

Usage: /gemini-map
```
```


--- example-data/prompt-front-matter/50-docs__doc-plan__owners.doc-plan.refactor.md ---
# Owners

## Inputs
- Path to analyze (e.g., `src/components/Button.tsx`)
- Access to `.github/CODEOWNERS` file
- Git repository with recent commit logs (`git log --pretty='- %an %ae: %s'`)

## Canonical taxonomy (exact strings)
- CLI
- ownership
- review

### Stage hints (for inference)
- discovery
- analysis
- suggestion

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤7.

## Validation
- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: owners  
- Categories: CLI, ownership, review  
- Stage: discovery  
- Dependencies: CODEOWNERS file, git log access  
- Artifacts: @frontend-team (CODEOWNERS), @jane (last 5 commits)  
- Summary: Suggest owners/reviewers for a path using CODEOWNERS and commit history.

---

Trigger: /owners <path>

Purpose: Suggest likely owners or reviewers for the specified path.

You are a CLI assistant focused on helping contributors with the task: Suggest likely owners/reviewers for a path.

1. Gather context by inspecting `.github/CODEOWNERS` for the codeowners (if present); running `git log --pretty='- %an %ae: %s' -- {{args}} | sed -n '1,50p'` for the recent authors for the path.
2. Based on CODEOWNERS and git history, suggest owners.
3. Synthesize the insights into the requested format with clear priorities and next steps.

Output:

- Begin with a concise summary that restates the goal: Suggest likely owners/reviewers for a path.
- Reference evidence from CODEOWNERS or git history for each owner suggestion.
- Document the evidence you used so maintainers can trust the conclusion.

Example Input:
src/components/Button.tsx

Expected Output:

- Likely reviewers: @frontend-team (CODEOWNERS), @jane (last 5 commits).


--- example-data/prompt-front-matter/50-docs__examples__api-usage.examples.refactor.md ---
# API Usage Analysis

## Metadata

- **identifier**: http-client
- **category**: API Usage Analysis
- **lifecycle_stage**: analysis
- **dependencies**: [rg, grep]
- **provided_artifacts**: 
  - Definition: src/network/httpClient.ts line 42
  - Key usages: services/userService.ts, hooks/useRequest.ts
- **summary**: Do analyze how an internal API is used to achieve clear documentation and visibility into its real-world applications.

## Inputs

- Input symbol: HttpClient
- Tool commands: `rg -n {{args}} . || grep -RIn {{args}} .`

## Canonical taxonomy (exact strings)

- API Usage Analysis
- Code Inspection
- Dependency Mapping
- Documentation Generation

### Stage hints (for inference)

- analysis
- inspection
- gathering
- review
- synthesis

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- All fields must be derived from content or logical inference.

## Validation

- Identifier matches a normalized id pattern (kebab-case, lowercase).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- identifier: http-client  
- category: API Usage Analysis  
- lifecycle_stage: analysis  
- dependencies: [rg, grep]  
- provided_artifacts: 
  - Definition: src/network/httpClient.ts line 42
  - Key usages: services/userService.ts, hooks/useRequest.ts
- summary: Do analyze how an internal API is used to achieve clear documentation and visibility into its real-world applications.


--- example-data/prompt-front-matter/50-docs__examples__reference-implementation.examples.refactor.md ---
# Reference Implementation

## Metadata

- **Identifier**: reference-implementation
- **Categories**: code-generation, api-mapping, diff-generation
- **Lifecycle Stage**: implementation
- **Dependencies**: target-module-path, example-url
- **Provided Artifacts**: side-by-side API table, patch suggestions
- **Summary**: Do map target module's API to reference to achieve consistent structure and naming.

## Steps

1. Accept a path or URL to an example. Extract its public API and patterns.
2. Map target module’s API to the reference.
3. Generate diffs that adopt the same structure and naming.

## Output format

- Side-by-side API table and patch suggestions.


--- example-response-format-usage.py ---
# main.py
# Purpose: Automated documentation-powered code generation pipeline using DSPy.
# Now with integrated GitHub repository analysis.

import dspy
import requests
from bs4 import BeautifulSoup
import html2text
from typing import List, Dict, Any
import json
import time
import subprocess

# Import the GitHub helper functions
from repo_helpers import gather_repository_info

MODEL_NAME = "hf.co/bartowski/Qwen_Qwen3-4B-Instruct-2507-GGUF:Q4_K_S"


def stop_ollama_model(model_name: str) -> None:
    """Stop the specified Ollama model to free server resources."""
    try:
        subprocess.run(
            ["ollama", "stop", model_name],
            check=True,
            capture_output=True,
        )
    except subprocess.CalledProcessError as exc:  # pragma: no cover - log warning only
        print(f"Warning: Failed to stop model {model_name}: {exc}")


# --- Data Fetching Classes ---


class DocumentationFetcher:
    """
    Fetches and processes documentation from both standard URLs and GitHub repositories.
    """

    def __init__(self, max_retries=3, delay=1):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
        )
        self.max_retries = max_retries
        self.delay = delay
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = False
        self.html_converter.ignore_images = True

    def fetch_website_url(self, url: str) -> dict:
        """Fetches and cleans content from a standard website URL."""
        for attempt in range(self.max_retries):
            try:
                print(f"📡 Fetching Website: {url} (attempt {attempt + 1})")
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, "html.parser")
                for script in soup(["script", "style", "nav", "footer", "header"]):
                    script.decompose()
                markdown_content = self.html_converter.handle(str(soup))
                return {
                    "url": url,
                    "title": soup.title.string if soup.title else "No title",
                    "content": markdown_content,
                    "success": True,
                }
            except Exception as e:
                print(f"❌ Error fetching {url}: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.delay)
        return {"url": url, "title": "Failed to fetch", "content": "", "success": False}

    def fetch_github_repo(self, url: str) -> dict:
        """Fetches and consolidates content from a GitHub repository."""
        print(f"📦 Fetching GitHub Repo: {url}")
        try:
            # Use the helper function to get README and example files
            combined_content, details = gather_repository_info(url)
            if "Could not access" in combined_content:
                return {
                    "url": url,
                    "title": "Failed to fetch repo",
                    "content": "",
                    "success": False,
                }

            return {
                "url": url,
                "title": f"GitHub Repo: {url.split('/')[-1]}",
                "content": combined_content,
                "success": True,
                "details": details,
            }
        except Exception as e:
            print(f"❌ Error fetching GitHub repo {url}: {e}")
            return {
                "url": url,
                "title": "Failed to fetch repo",
                "content": f"Error: {e}",
                "success": False,
            }

    def fetch_documentation(self, urls: list[str]) -> list[dict]:
        """
        Fetches documentation from a list of URLs, routing to the correct
        fetcher (website or GitHub) based on the URL.
        """
        results = []
        for url in urls:
            if "github.com" in url:
                result = self.fetch_github_repo(url)
            else:
                result = self.fetch_website_url(url)

            results.append(result)
            time.sleep(self.delay)  # Be respectful to servers
        return results


# --- DSPy Signatures and Modules ---


class LibraryAnalyzer(dspy.Signature):
    """Analyze library documentation to understand core concepts, patterns, and examples."""

    library_name: str = dspy.InputField(desc="Name of the library to analyze")
    documentation_content: str = dspy.InputField(
        desc="Combined documentation from websites, READMEs, and code examples"
    )

    core_concepts: list[str] = dspy.OutputField(
        desc="Main concepts, classes, and components of the library."
    )
    common_patterns: list[str] = dspy.OutputField(
        desc="Common usage patterns or workflows."
    )
    key_methods: list[str] = dspy.OutputField(
        desc="List of important methods or functions and their purpose."
    )
    installation_info: str = dspy.OutputField(
        desc="How to install the library (e.g., 'pip install ...')."
    )
    code_examples: list[str] = dspy.OutputField(
        desc="Key code snippets found in the documentation."
    )


class CodeGenerator(dspy.Signature):
    """Generate a complete, working code example for a specific use case."""

    library_info: str = dspy.InputField(
        desc="Summary of the library's concepts, patterns, and methods."
    )
    use_case: str = dspy.InputField(
        desc="The specific task to accomplish with the code."
    )
    requirements: str = dspy.InputField(
        desc="Additional requirements or constraints for the code."
    )

    code_example: str = dspy.OutputField(
        desc="A single, complete, and runnable code block."
    )
    explanation: str = dspy.OutputField(
        desc="A step-by-step explanation of the generated code."
    )
    best_practices: list[str] = dspy.OutputField(
        desc="Tips and best practices for using the library."
    )
    imports_needed: list[str] = dspy.OutputField(
        desc="A list of necessary import statements."
    )


class DocumentationLearningAgent(dspy.Module):
    """Agent that learns from documentation and generates code."""

    def __init__(self):
        super().__init__()
        self.fetcher = DocumentationFetcher()
        self.analyze_docs = dspy.ChainOfThought(LibraryAnalyzer)
        self.generate_code = dspy.ChainOfThought(CodeGenerator)

    def learn_from_urls(self, library_name: str, doc_urls: list[str]) -> Dict:
        """Learns about a library from documentation URLs and GitHub repos."""
        print(f"📚 Learning about {library_name} from {len(doc_urls)} sources...")
        docs = self.fetcher.fetch_documentation(doc_urls)

        combined_content = "\n\n---\n\n".join(
            [
                f"SOURCE: {doc['url']}\n\n{doc['content']}"
                for doc in docs
                if doc["success"]
            ]
        )

        if not combined_content:
            raise ValueError("No documentation could be fetched successfully.")

        analysis = self.analyze_docs(
            library_name=library_name, documentation_content=combined_content
        )

        return {
            "library": library_name,
            "source_urls": [doc["url"] for doc in docs if doc["success"]],
            "core_concepts": analysis.core_concepts,
            "patterns": analysis.common_patterns,
            "methods": analysis.key_methods,
            "installation": analysis.installation_info,
            "examples": analysis.code_examples,
        }

    def generate_example(
        self, library_info: Dict, use_case: str, requirements: str = ""
    ) -> Dict:
        """Generates a code example for a specific use case."""
        info_text = f"""
        Library: {library_info["library"]}
        Core Concepts: {", ".join(library_info["core_concepts"])}
        Common Patterns: {", ".join(library_info["patterns"])}
        Key Methods: {", ".join(library_info["methods"])}
        Installation: {library_info["installation"]}
        """

        code_result = self.generate_code(
            library_info=info_text, use_case=use_case, requirements=requirements
        )

        return {
            "code": code_result.code_example,
            "explanation": code_result.explanation,
            "best_practices": code_result.best_practices,
            "imports": code_result.imports_needed,
        }

def interactive_learning_session():
    """Main interactive session for the library learning system."""
    lm = dspy.LM(
        f"ollama_chat/{MODEL_NAME}",
        api_base="http://localhost:11434",
        api_key="",
        streaming=False,
        cache=False,
        response_format={
        "type": "json_schema",
        "json_schema": {
            "schema": {
                "type": "object",
                "properties": {
                    "project": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string"},
                            "description": {"type": "string"},
                        },
                        "required": ["name", "description"],
                    },
                    "key_concepts": {
                        "type": "array",
                        "items": {"type": "string"},
                        "minItems": 1,
                    },
                    "architecture_overview": {"type": "string"},
                    "important_directories": {
                        "type": "array",
                        "items": {"type": "string"},
                        "minItems": 1,
                    },
                    "entry_points": {
                        "type": "array",
                        "items": {"type": "string"},
                        "minItems": 1,
                    },
                    "development_info": {
                        "type": "object",
                        "properties": {
                            "test_dependencies": {
                                "type": "array",
                                "items": {"type": "string"},
                                "minItems": 1,
                            },
                            "linting_tools": {
                                "type": "array",
                                "items": {"type": "string"},
                                "minItems": 1,
                            },
                            "optional_dependencies": {
                                "type": "array",
                                "items": {"type": "string"},
                            },
                        },
                        "required": ["test_dependencies", "linting_tools"],
                    },
                    "usage_examples": {
                        "type": "array",
                        "items": {"type": "string"},
                        "minItems": 1,
                    },
                },
                "required": [
                    "project",
                    "key_concepts",
                    "architecture_overview",
                    "important_directories",
                    "entry_points",
                    "development_info",
                    "usage_examples",
                ],
            }
        },
    },
    )
    dspy.configure(lm=lm)

    learned_libraries = {}

    try:
        print("🎯 Welcome to the Interactive Library Learning System!")
        print("This system now supports learning from websites AND GitHub repositories.\n")

        agent = DocumentationLearningAgent()

        while True:
            print("\n" + "=" * 60)
            library_name = input(
                "\n📚 Enter the library name (or 'quit' to exit): "
            ).strip()
            if library_name.lower() in ["quit", "exit", "q"]:
                break

            print(
                f"\n🔗 Enter documentation URLs or a GitHub repo URL for {library_name} (one per line, empty line to finish):"
            )
            urls = []
            while True:
                url = input("  URL: ").strip()
                if not url:
                    break
                urls.append(url)

            if not urls:
                continue

            try:
                # Step 1: Learn about the library from the provided sources
                library_info = agent.learn_from_urls(library_name, urls)
                print(f"\n✅ Successfully learned {library_name}!")
                print(f"   - Core Concepts: {library_info.get('core_concepts', 'N/A')}")
                print(f"   - Installation: {library_info.get('installation', 'N/A')}")

                # Step 2: Get all use cases from the user upfront
                print(
                    f"\n🎯 Define use cases for {library_name} (one per line, empty line to finish):"
                )
                use_cases = []
                while True:
                    use_case = input("     Use case: ").strip()
                    if not use_case:
                        break
                    use_cases.append(use_case)

                if not use_cases:
                    print("No use cases provided. Moving on.")
                    continue

                # Step 3: Generate and display all examples
                print(f"\n🔧 Generating {len(use_cases)} examples for {library_name}...")
                all_examples = []
                for i, use_case in enumerate(use_cases, 1):
                    print(f"\n--- EXAMPLE {i}/{len(use_cases)}: {use_case} ---")

                    example = agent.generate_example(library_info, use_case)
                    all_examples.append({"use_case": use_case, **example})

                    print("\n💻 Code Example:")
                    print(f"```python\n{example['code']}\n```")

                    print("\n📦 Required Imports:")
                    print("\n".join([f"  • {imp}" for imp in example["imports"]]))

                    print("\n📝 Explanation:")
                    print(example["explanation"])

                    print("\n✅ Best Practices:")
                    print("\n".join([f"  • {bp}" for bp in example["best_practices"]]))
                    print("--- END OF EXAMPLE ---")

                # Store the complete results
                learned_libraries[library_name] = {
                    "library_info": library_info,
                    "examples": all_examples,
                }

                # Step 4: Offer to save the results to a file
                save_results = (
                    input(f"\n💾 Save learning results for {library_name} to file? (y/n): ")
                    .strip()
                    .lower()
                )
                if save_results in ["y", "yes"]:
                    filename = input(
                        f"   Enter filename (default: {library_name.lower()}_learning.json): "
                    ).strip()
                    if not filename:
                        filename = f"{library_name.lower()}_learning.json"

                    try:
                        with open(filename, "w", encoding="utf-8") as f:
                            json.dump(
                                learned_libraries[library_name], f, indent=2, default=str
                            )
                        print(f"   ✅ Results saved to {filename}")
                    except Exception as e:
                        print(f"   ❌ Error saving file: {e}")

            except Exception as e:
                print(f"❌ An error occurred while learning {library_name}: {e}")

        print("\n👋 Thanks for using the Interactive Library Learning System!")
        if learned_libraries:
            print(f"\n🎉 Session Summary:")
            print(
                f"Successfully learned {len(learned_libraries)} libraries: {list(learned_libraries.keys())}"
            )
    finally:
        stop_ollama_model(MODEL_NAME)


if __name__ == "__main__":
    interactive_learning_session()


--- example-data/prompt-front-matter/00-ideation__architecture__adr-new.architecture.refactor.md ---
# ADR Drafting Assistant

Task: Given the following prompt, produce a structured **metadata block** and then emit the original body unchanged. The metadata must expose identifiers, categories, optional lifecycle/stage, optional dependencies, optional provided artifacts, and a concise summary. Output = metadata, blank line, then the input text.

## Inputs
- Input prompt: "You are a CLI assistant focused on helping contributors with the task: Draft an Architecture Decision Record with pros/cons."
- Workflow steps: Gather context from `README.md`, draft ADR (Context, Decision, Status, Consequences), synthesize insights.
- Output requirements: Concise summary of goal; workflow triggers/failing jobs/proposed fixes; documented evidence for maintainers' trust.

## Canonical taxonomy (exact strings)
- architecture
- decision-making
- documentation

### Stage hints (for inference)
- ideation → early drafting, context gathering
- planning → structured output design
- implementation → actual code changes
- review → peer feedback or approval

## Algorithm
1. Extract signals from input:
   - Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.
2. Determine the primary identifier:
   - Prefer explicit input; otherwise infer from main action + object.
   - Normalize (lowercase, kebab-case, length-capped, starts with a letter).
   - De-duplicate.
3. Determine categories:
   - Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.
   - Validate, sort deterministically, and de-dupe (≤3).
4. Determine lifecycle/stage (optional):
   - Prefer explicit input; otherwise map categories via stage hints.
   - Omit if uncertain.
5. Determine dependencies (optional):
   - Parse phrases implying order or prerequisites; keep id-shaped items (≤5).
6. Determine provided artifacts (optional):
   - Short list (≤3) of unlocked outputs.
7. Compose summary:
   - One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”
8. Produce metadata in the requested format:
   - Default to a human-readable serialization; honor any requested alternative.
9. Reconcile if input already contains metadata:
   - Merge: explicit inputs > existing > inferred.
   - Validate lists; move unknowns to an extension field if needed.
   - Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then the original body.
- Limit distinct placeholders to ≤7.

## Validation
- Identifier matches a normalized id pattern (e.g., kebab-case, lowercase).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped (≤5).
- Artifacts are short (≤3) and relevant to output.
- Summary ≤120 chars; punctuation coherent.
- Body text is not altered.

## Output format examples
- Identifier: `adr-draft`
- Categories: architecture, decision-making, documentation
- Lifecycle stage: ideation
- Dependencies: README.md
- Provided artifacts: ADR with pros/cons, evidence summary, workflow insights
- Summary: "Draft an Architecture Decision Record with pros/cons to achieve transparent decision documentation."


--- example-data/prompt-front-matter/00-ideation__architecture__logging-strategy.architecture.refactor.md ---
# Logging Strategy

## Metadata

- identifier: logging-strategy
- categories: [observability, operations, security]
- stage: design
- dependencies: []
- provided_artifacts: ["diff hunks", "short guideline section"]
- summary: Do add or remove diagnostic logs with privacy in mind to achieve structured observability.

## Steps

1. Identify hotspots from recent failures.
2. Insert structured logs with contexts and correlation IDs.
3. Remove noisy or PII-leaking logs.
4. Document log levels and sampling in `OBSERVABILITY.md`.

## Output format

- Diff hunks and a short guideline section.


--- example-data/prompt-front-matter/00-ideation__architecture__modular-architecture.architecture.refactor.md ---
# Modular Architecture

## Metadata

- **identifier**: modular-architecture  
- **categories**: architecture  
- **stage**: design  
- **dependencies**: [module-boundaries-identification]  
- **provided-artifacts**: [module-graph, dependency-diff, contract-test-plan]  
- **summary**: Do modularize services to achieve clear boundaries and testable interfaces.

## Steps

1. Identify services/modules and their public contracts.
2. Flag cross-module imports and circular deps.
3. Propose boundaries, facades, and internal folders.
4. Add "contract tests" for public APIs.

## Output format

- Diagram-ready list of modules and edges, plus diffs.


--- example-data/prompt-front-matter/00-ideation__architecture__stack-evaluation.architecture.refactor.md ---
# Stack Evaluation

## Metadata

- identifier: stack-evaluation
- categories: [evaluation, analysis, recommendation]
- stage: evaluation
- dependencies: []
- provided_artifacts: ["decision memo", "next steps"]
- summary: Evaluate language/framework choices to achieve informed stay-or-switch decisions.

## Steps

1. Detect current stack and conventions.
2. List tradeoffs: maturity, tooling, available examples, hiring, and AI training coverage.
3. Recommend stay-or-switch with migration outline if switching.

## Output format

- Decision memo with pros/cons and next steps.


--- example-data/prompt-front-matter/00-ideation__design__action-diagram.design.refactor.md ---
# Action Diagram Metadata

## Inputs
- Source file path: C:\Users\user\projects\prompts\temp-prompts\00-ideation\design\action-diagram.design.md
- Maximum placeholders allowed: 7

## Canonical taxonomy (exact strings)
- devops
- pipeline
- workflow

### Stage hints (for inference)
- build → development stage
- deploy → production stage
- push → trigger stage

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.

## Validation
- Identifier matches a normalized id pattern.
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: build  
- Categories: devops, pipeline, workflow  
- Lifecycle stage: none  
- Dependencies: push  
- Provided artifacts: deployment artifact  
- Summary: Do build to achieve deployment after push

## Metadata
- identifier: build
- categories: ["devops", "pipeline", "workflow"]
- lifecycle_stage: null
- dependencies: ["push"]
- provided_artifacts: ["deployment artifact"]
- summary: Do build to achieve deployment after push

## Nodes
- build
- deploy

## Edges
- push -> build
- build -> deploy


--- example-data/prompt-front-matter/00-ideation__design__api-contract.design.refactor.md ---
# API Contract Design

## Inputs
- Feature or domain string (e.g., "accounts & auth")
- Existing documentation and requirements
- Preference for OpenAPI 3.1 or GraphQL SDL

## Canonical taxonomy (exact strings)
- design
- specification
- contract generation

### Stage hints (for inference)
- design → initial creation of a contract from inputs
- specification → detailed schema definition
- implementation → code generation phase

## Algorithm
1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs $5.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via $6.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints
- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤ 7.
- All categories must be from the canonical taxonomy.
- Stage mapping is deterministic and context-aware.

## Validation
- Identifier matches a normalized id pattern (e.g., api-contract).
- Categories non-empty and drawn from $5 (≤3).
- Stage, if present, is one of the allowed stages implied by $6.
- Dependencies, if present, are id-shaped (≤5).
- Dependencies must be explicit or inferable from input structure.
- Artifacts list ≤3 items; all valid outputs.
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples
- Identifier: `api-contract`  
- Categories: design, specification, contract generation  
- Stage: design  
- Dependencies: feature/domain input, existing documentation  
- Artifacts: openapi.yaml, schema.graphql, changelog entry  
- Summary: "Do generate an API contract from requirements to achieve a standardized specification for endpoints."

---

# API Contract

Trigger: /api-contract "<feature or domain>"

Purpose: Author an initial OpenAPI 3.1 or GraphQL SDL contract from requirements.

**Steps:**

1. Parse inputs and existing docs. If REST, prefer OpenAPI 3.1 YAML; if GraphQL, produce SDL.
2. Define resources, operations, request/response schemas, error model, auth, and rate limit headers.
3. Add examples for each endpoint or type. Include pagination and filtering conventions.
4. Save to `apis/<domain>/openapi.yaml` or `apis/<domain>/schema.graphql`.
5. Emit changelog entry `docs/api/CHANGELOG.md` with rationale and breaking-change flags.

**Output format:**

- `Contract Path`, `Design Notes`, and a fenced code block with the spec body.

**Examples:**

- `/api-contract "accounts & auth"` → `apis/auth/openapi.yaml` with OAuth 2.1 flows.

**Notes:**

- Follow JSON:API style for REST unless caller specifies otherwise. Include `429` and `5xx` models.


--- example-data/prompt-front-matter/00-ideation__design__design-assets.design.refactor.md ---
# Design Assets

## Metadata

- **Identifier**: design-assets
- **Categories**: design, brand assets
- **Stage**: generate
- **Dependencies**: brand-colors, brand-name
- **Provided Artifacts**: asset-checklist, generation-commands
- **Summary**: Generate favicons and small design snippets from product brand to achieve consistent visual identity.

## Steps

1. Extract brand colors and name from README or config.
2. Produce favicon set, social preview, and basic UI tokens.
3. Document asset locations and references.

## Output format

- Asset checklist and generation commands.


--- example-data/prompt-front-matter/00-ideation__design__ui-screenshots.design.refactor.md ---
# UI Screenshots

## Metadata

- **Identifier**: ui-screenshots
- **Categories**: analysis, design, code-generation
- **Stage**: design-review
- **Dependencies**: []
- **Provided Artifacts**: issue-list, css-changes, component-updates
- **Summary**: Analyze UI screenshots to identify visual issues and generate actionable CSS or component changes

## Steps

1. Accept screenshot paths or links.
2. Describe visual hierarchy, spacing, contrast, and alignment issues.
3. Output concrete CSS or component changes.

## Output format

- Issue list and code snippets to fix visuals.


--- example-data/prompt-front-matter/00-ideation__requirements__plan-delta.requirements.refactor.md ---
# plan-delta

## Metadata

- **identifier**: plan-delta  
- **categories**: Planning, Task Management, Graph Maintenance  
- **lifecycle_stage**: Mid-Project Adjustment  
- **dependencies**: task graph history, user delta input  
- **provided_artifacts**: 
  - Updated tasks file (valid JSON)  
  - Delta document (Markdown with # Delta, ## Objectives, ## Constraints, ## Impacts, ## Decisions, ## Evidence)  
  - Readiness report (plain text: READY | BLOCKED | DEPRECATED)  
- **summary**: Orchestrate mid-project planning deltas to preserve history and update task graph readiness.

## Inputs

- User-provided delta text with objectives, constraints, findings
- Selection mode: Continue, Hybrid Rebaseline, Full Rebaseline
- Existing tasks file (tasks.json or equivalent)
- Repository context path for task and plan files

## Canonical taxonomy (exact strings)

Planning, Task Management, Graph Maintenance

### Stage hints (for inference)

Mid-project adjustment, delta update, planning revision, graph maintenance, readiness recalculation

## Algorithm

1. Extract signals from $1  
   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier  
   * Prefer explicit input; otherwise infer from main action + object.  
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).  
   * De-duplicate.

3. Determine categories  
   * Prefer explicit input; otherwise infer from verbs/headings vs canonical taxonomy.  
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)  
   * Prefer explicit input; otherwise map categories via stage hints.  
   * Omit if uncertain.

5. Determine dependencies (optional)  
   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)  
   * Short list (≤3) of unlocked outputs.

7. Compose summary  
   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format  
   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata  
   * Merge: explicit inputs > existing > inferred.  
   * Validate lists; move unknowns to an extension field if needed.  
   * Remove empty keys.

## Assumptions & Constraints

- Emit exactly one document: metadata, a single blank line, then $1.
- Limit distinct placeholders to ≤7.
- All outputs are strictly defined in the output format section.

## Validation

- Identifier matches a normalized id pattern (lowercase, kebab-case).
- Categories non-empty and drawn from canonical taxonomy (≤3).
- Stage, if present, is one of the allowed stages implied by stage hints.
- Dependencies, if present, are id-shaped or context-based (≤5).
- Provided artifacts match exactly those listed in output format.
- Summary ≤120 chars; punctuation coherent.
- Body text $1 is not altered.

## Output format examples

- Input →  
  ```
  Mode: Continue
  New objectives: add offline export for tasks
  Constraints: no DB migrations
  Findings: existing export lib supports JSON only
  ```  

  Output →  
  - Updated `tasks.json` with new task `T-342` { title: "Add CSV export", dependencies: ["T-120"], source_doc: "delta-20250921.md", lineage: ["T-120"], supersedes: [] }.  
  - `artifacts/delta-20250921-160500.md` populated with objectives, constraints, impacts, decisions, evidence.  
  - Readiness report lists `T-342` under READY if deps done.

- Input →  
  ```
  Mode: Hybrid Rebaseline
  Changes: ~30% of scope affected by auth provider swap
  ```  

  Output →  
  - Minor-plan version bump recorded in Delta Doc.  
  - New tasks added for provider swap; prior tasks kept with `deprecated` or `blocked` and lineage links.


--- .taskmaster/scripts/bump.md ---
# bump version and commit workflow

uv build
uv version --bump patch --bump beta
git add .
git commit -m "Release: 0.1.4b1"
git tag -a v0.1.4b1 -m "Release 0.1.4b1"
git push origin HEAD --tags

{ yes "" | dt -m refactor ~/.gemini/commands \
    --provider lmstudio \
    --api-base <http://10.0.0.81:1234/v1> --confirm-each; } \
  |& tee "dspyteach.all.$(date +%Y%m%d-%H%M%S).log"

{ dt generate-sequence \
  --goal "Cut a release" ~/.codex/prompts \
    --provider lmstudio \
    --api-base http://10.0.0.81:1234/v1; }

dspyteach rank-prompts "How do I commit my changes?" ~/.codex/prompts 0.6

dt generate-sequence \
  --goal "Cut a release"


{ dspyteach generate-sequence "Cut a release" -P lmstudio -B 10.0.0.81:1234/v1 ;} |& tee "dspyteach.all.$(date +%Y%m%d-%H%M%S).log"


dspyteach -P lmstudio -B 10.0.0.81:1234/v1 generate-sequence "Cut a release"


 dspyteach generate-sequence "Cut a release"
  dspyteach -P lmstudio -B <http://localhost:1234/v1> generate-sequence
  --goal "Cut a release"


--- AGENTS.md ---
<!-- path: ~/projects/dspy-file/AGENTS.md -->

# AGENTS.md — Tool Selection (Python)

## DSPy Framework Playbook (All‑Purpose)

**When to apply:** Use these rules whenever creating or editing DSPy **Signatures**, **Modules**, **Programs**, **Optimizers**, **Metrics**, or **RAG** components in any repository.

**Canonical docs**

- Signatures: <https://dspy.ai/learn/programming/signatures/>
- Modules (Predict / ChainOfThought / ReAct): <https://dspy.ai/learn/programming/modules/>
- Optimizers overview: <https://dspy.ai/learn/optimization/optimizers/>
- MIPROv2: <https://dspy.ai/api/optimizers/MIPROv2/>
- BootstrapFewShot: <https://dspy.ai/api/optimizers/BootstrapFewShot/>
- Assertions: <https://dspy.ai/learn/programming/7-assertions/>
- Metrics: <https://dspy.ai/learn/evaluation/metrics/>
- RAG tutorial: <https://dspy.ai/tutorials/rag/>

### Quick posture

- **Program, don’t prompt.** Encode task instructions in **Signature docstrings**; compose behavior from Modules. Avoid ad‑hoc long prompts.
- **Provider‑agnostic.** Support OpenAI/Ollama/Anthropic/etc. by configuring the LM once via `dspy.settings.configure(lm=...)` before building modules.
- **Composable by default.** Prefer several small Modules over a single monolith; pass data via fields, not globals.
- **Repro first.** Pin DSPy and core deps in `pyproject.toml`; commit seeds and compile artifacts for deterministic rebuilds.

### Order of operations (universal)

1) **Define Signature(s)** — Put task instructions in the **docstring**; declare inputs with `dspy.InputField`, outputs with `dspy.OutputField`. (Docs: Signatures)
2) **Compose Modules** — Start with `dspy.Predict`; add `dspy.ChainOfThought` when rationale is useful; use `dspy.ReAct` for tools/agents. (Docs: Modules)
3) **Choose a Metric** — Make it concrete: e.g., `accuracy`/`F1` (classification), `nDCG@k` (ranking), exact/EM (QA). (Docs: Metrics)
4) **Add Assertions** — Specify schema+constraints and enable automatic retries on violation. (Docs: Assertions)
5) **(Optional) Retrieval** — Add a retriever and route evidence to the program for RAG tasks. (Docs: RAG tutorial)
6) **Optimize** — Run `BootstrapFewShot` to assemble demos → then `MIPROv2` to jointly tune instructions+demos against your metric. (Docs: Optimizers)

### Minimum rules (Codex must enforce)

- **Instruction location:** All task guidance lives in Signature docstrings; no hidden prompts in module code.
- **LM config timing:** Call `dspy.settings.configure(lm=...)` **before** constructing Modules/Programs so compiles/optimizers see the right LM.
- **Schema guard (example):** For ranked outputs, emit a 3‑column Markdown table: `prompt | score | rationale`, where `score` ∈ `[0,1]`.
- **Metrics defaults:** Classification → `accuracy` or `F1`; Ranking → `nDCG@k`; QA → exact match / token‑F1. Document any custom metric in one line.
- **Tracing & artifacts:** Enable tracing during iteration; persist optimized programs/artifacts after `compile()` for reproducibility.
- **Small pieces, loosely joined:** Prefer many tiny units; inject dependencies; avoid singletons and global state.

### Environment & compatibility

- **Versioning:** Prefer `dspy>=2` (or latest stable); if upgrading major versions, re‑run optimizers and refresh assertions.
- **Providers:** Keep provider/model/api‑base in env (e.g., `DSPY_OPENAI_API_KEY`, `OPENAI_API_BASE`, etc.). Do not hardcode keys.
- **Eval data:** Keep small, labeled eval sets under `eval/` to make metrics and compilation meaningful.

### Long‑form guide (for humans)

See **docs/pre-work-dspy.md** for the expanded checklist and examples
-------

When you need to call tools from the shell, use this rubric:

File & Text

-------

- Find files by file name: `fd`

- Find files with path name: `fd -p <file-path>`

- List files in a directory: `fd . <directory>`

- Find files with extension and pattern: `fd -e <extension> <pattern>`

- Find Text: `rg` (ripgrep)

- Find Code Structure: `ast-grep`

  - Common languages:

    - Python → `ast-grep --lang python -p '<pattern>'`

    - TypeScript → `ast-grep --lang ts -p '<pattern>'`

    - Bash → `ast-grep --lang bash -p '<pattern>'`

    - TSX (React) → `ast-grep --lang tsx -p '<pattern>'`

    - JavaScript → `ast-grep --lang js -p '<pattern>'`

    - Rust → `ast-grep --lang rust -p '<pattern>'`

    - JSON → `ast-grep --lang json -p '<pattern>'`

  - Prefer `ast-grep` over ripgrep/grep unless a plain-text search is explicitly requested.

- Select among matches: pipe to `fzf`

Data
----

- JSON: `jq`

- YAML/XML: `yq`

Python Tooling
--------------

- Package Management & Virtual Envs: `uv`
    (fast replacement for pip/pip-tools/virtualenv; use `uv pip install ...`, `uv run ...`)

- Linting & Formatting: `ruff`
    (linter + formatter; use `ruff check .`, `ruff format .`)

- Static Typing: `mypy`
    (type checking; use `mypy .`)

- Security: `bandit`
    (Python security linter; use `bandit -r .`)

- Testing: `pytest`
    (test runner; use `pytest -q`, `pytest -k <pattern>` to filter tests)

- Logging: `loguru`
    (runtime logging utility; import in code:)

        from loguru import logger
        logger.info("message")

Notes
-----

- Prefer `uv` for Python dependency and environment management instead of pip/venv/poetry/pip-tools.

MCP\_SERVERS
------------

- Use the `dspy_Docs` MCP server to get latest docs for DSPy usage.

- Use the `lmstudio_docs` MCP server to get latest docs for LM Studio API usage.

-------

## Proactive TODO/FIXME Annotations

Add TODO/FIXME notes as you work—don’t wait for a cleanup pass. Use them to mark: missing tests, unclear contracts, temporary workarounds, performance/security concerns, or places where design choices need follow-up.

**Format (single line):**

    TODO(scope|owner): short, imperative next step — why it matters [evidence: <source|cmd|ticket>]
    FIXME(scope|owner): what is broken — minimal repro or constraint [evidence: <source|cmd|ticket>]

- `scope|owner` is optional but encouraged (e.g., `ui`, `backend`, `deps`, or a handle like `@alice`).

- Keep it ≤120 chars when possible; link to issues for details.

**Examples (per language comment style):**

    # TODO(domain|@alice): replace naive parse with streaming parser — OOM on large inputs [evidence: profile.txt]
    # FIXME(api): 500 on empty payload — add validation + test [evidence: pytest -k empty_payload]


    // TODO(ui): debounce search — noisy network on fast typing [evidence: trace.log]
    // FIXME(auth|@bob): refresh token race — guard with mutex [evidence: unit test 'refresh-concurrency']


    # TODO(devex): switch to uv task for one-liners [evidence: uv run --help]

### Workflow (aligned with `todos.md`)

1. Gather evidence with the command you used during investigation and reference it in the note’s `[evidence: ...]`.

2. Add the TODO/FIXME in the code at the closest actionable location.

3. Commit with a concise message (e.g., `chore(todos): mark debounce + auth race with evidence`).

4. Before opening a PR, **find and group** all annotations as described in `todos.md` so maintainers can review them together.

### Discover & verify (standard commands)

- Plain-text sweep:

```bash
        rg -n "TODO|FIXME" | fzf
```

- Syntax-aware matches (prefer this when patterns are noisy):

```bash
        ast-grep --lang python -p "// TODO(_) (_) : (_)"
        ast-grep --lang ts -p "// FIXME(_) : (_)"
```

- File targeting:

```bash
        fd -e py -e ts | xargs rg -n "TODO|FIXME"
```

### PR checklist (copy into your PR template)

- Added TODO/FIXME where follow-ups are needed, with `[evidence: ...]`.

- Ran `rg`/`ast-grep` to list all annotations and grouped them per `todos.md` for reviewers.

- Linked or opened issues for any TODO expected to live >2 sprints.

### Retirement policy

- Convert TODO → issue if it will outlive the current PR.

- Remove the annotation when addressed; reference the commit/issue that resolves it.

-------

Rules for Best-Practice

-------

<file\_length\_and\_structure>

- Prefer maintainability signals over fixed line caps.

- Split when cognitive complexity > 15, cohesion drops, or fan-in/out spikes.

- Group by feature. Keep a file to one capability plus its close helpers.

- Use clear folder names and consistent naming.

</file\_length\_and\_structure>

<paradigm\_and\_style>

- Use OOP, functional, or data-oriented styles as idiomatic for the language.

- Favor composition. In OOP, model behavior behind small interfaces or protocols.

- Prefer pure functions and algebraic data types where natural.

</paradigm\_and\_style>

<single\_responsibility\_principle>

- Aim for one capability and its close helpers. Avoid micro-files.

- Enforce through module boundaries and public APIs, not line counts.

</single\_responsibility\_principle>

<modular\_design>

- Design modules to be interchangeable, testable, and isolated.

- Keep public surfaces small. Inject dependencies. Avoid tight coupling.

- Optimize for replaceability and test seams over premature reuse.

</modular\_design>

<roles\_by\_platform>

- UI stacks: ViewModel for UI logic, Manager for business logic, Coordinator for navigation and state flow.

- Backend and CLI: Service, Handler, Repository, Job, Workflow.

- Do not mix view code with business logic.

</roles\_by\_platform>

<function\_and\_class\_size>

- Size by behavior, not lines.

- Functions ≤ 20–30 cognitive steps.

- Split a class when it owns more than one lifecycle or more than one external dependency graph.

</function\_and\_class\_size>

<naming\_and\_readability>

- Use intention revealing names.

- Allow domain terms with qualifiers, for example `UserData`, `BillingInfo`.

- Forbid empty suffixes like `Helper` or `Utils` unless tightly scoped.

</naming\_and\_readability>

<scalability\_mindset>

- Build for extension points from day one, such as interfaces, protocols, and constructor injection.

- Prefer local duplication over unstable abstractions.

- Document contracts at module seams.

</scalability\_mindset>

<avoid\_god\_classes>

- Do not centralize everything in one file or class.

- Split into UI, State, Handlers, Networking, and other focused parts.

</avoid\_god\_classes>

<dependency\_injection>

- Backends: prefer constructor injection. Keep containers optional.

- Swift, Kotlin, TypeScript: use protocols or interfaces. Inject by initializer or factory.

- Limit global singletons. Provide test doubles at seams.

</dependency\_injection>

<testing>

- Require deterministic seams.

- Add contract tests for modules and layers.

- Use snapshot or golden tests for UI and renderers.

</testing>

<architecture\_boundaries>

- Feature oriented packaging with clear dependency direction: UI → app → domain → infra.

- Stabilize domain modules. Keep infra replaceable.

- Enforce imports with rules or module maps.

</architecture\_boundaries>

-------


--- GEMINI.md ---
<!-- path: ~/projects/dspy-file/AGENTS.md -->

# AGENTS.md — Tool Selection (Python)

## DSPy Framework Playbook (All‑Purpose)

**When to apply:** Use these rules whenever creating or editing DSPy **Signatures**, **Modules**, **Programs**, **Optimizers**, **Metrics**, or **RAG** components in any repository.

**Canonical docs**

- Signatures: <https://dspy.ai/learn/programming/signatures/>
- Modules (Predict / ChainOfThought / ReAct): <https://dspy.ai/learn/programming/modules/>
- Optimizers overview: <https://dspy.ai/learn/optimization/optimizers/>
- MIPROv2: <https://dspy.ai/api/optimizers/MIPROv2/>
- BootstrapFewShot: <https://dspy.ai/api/optimizers/BootstrapFewShot/>
- Assertions: <https://dspy.ai/learn/programming/7-assertions/>
- Metrics: <https://dspy.ai/learn/evaluation/metrics/>
- RAG tutorial: <https://dspy.ai/tutorials/rag/>

### Quick posture

- **Program, don’t prompt.** Encode task instructions in **Signature docstrings**; compose behavior from Modules. Avoid ad‑hoc long prompts.
- **Provider‑agnostic.** Support OpenAI/Ollama/Anthropic/etc. by configuring the LM once via `dspy.settings.configure(lm=...)` before building modules.
- **Composable by default.** Prefer several small Modules over a single monolith; pass data via fields, not globals.
- **Repro first.** Pin DSPy and core deps in `pyproject.toml`; commit seeds and compile artifacts for deterministic rebuilds.

### Order of operations (universal)

1) **Define Signature(s)** — Put task instructions in the **docstring**; declare inputs with `dspy.InputField`, outputs with `dspy.OutputField`. (Docs: Signatures)
2) **Compose Modules** — Start with `dspy.Predict`; add `dspy.ChainOfThought` when rationale is useful; use `dspy.ReAct` for tools/agents. (Docs: Modules)
3) **Choose a Metric** — Make it concrete: e.g., `accuracy`/`F1` (classification), `nDCG@k` (ranking), exact/EM (QA). (Docs: Metrics)
4) **Add Assertions** — Specify schema+constraints and enable automatic retries on violation. (Docs: Assertions)
5) **(Optional) Retrieval** — Add a retriever and route evidence to the program for RAG tasks. (Docs: RAG tutorial)
6) **Optimize** — Run `BootstrapFewShot` to assemble demos → then `MIPROv2` to jointly tune instructions+demos against your metric. (Docs: Optimizers)

### Minimum rules (Codex must enforce)

- **Instruction location:** All task guidance lives in Signature docstrings; no hidden prompts in module code.
- **LM config timing:** Call `dspy.settings.configure(lm=...)` **before** constructing Modules/Programs so compiles/optimizers see the right LM.
- **Schema guard (example):** For ranked outputs, emit a 3‑column Markdown table: `prompt | score | rationale`, where `score` ∈ `[0,1]`.
- **Metrics defaults:** Classification → `accuracy` or `F1`; Ranking → `nDCG@k`; QA → exact match / token‑F1. Document any custom metric in one line.
- **Tracing & artifacts:** Enable tracing during iteration; persist optimized programs/artifacts after `compile()` for reproducibility.
- **Small pieces, loosely joined:** Prefer many tiny units; inject dependencies; avoid singletons and global state.

### Environment & compatibility

- **Versioning:** Prefer `dspy>=2` (or latest stable); if upgrading major versions, re‑run optimizers and refresh assertions.
- **Providers:** Keep provider/model/api‑base in env (e.g., `DSPY_OPENAI_API_KEY`, `OPENAI_API_BASE`, etc.). Do not hardcode keys.
- **Eval data:** Keep small, labeled eval sets under `eval/` to make metrics and compilation meaningful.

### Long‑form guide (for humans)

See **docs/pre-work-dspy.md** for the expanded checklist and examples
-------

When you need to call tools from the shell, use this rubric:

File & Text

-------

- Find files by file name: `fd`

- Find files with path name: `fd -p <file-path>`

- List files in a directory: `fd . <directory>`

- Find files with extension and pattern: `fd -e <extension> <pattern>`

- Find Text: `rg` (ripgrep)

- Find Code Structure: `ast-grep`

  - Common languages:

    - Python → `ast-grep --lang python -p '<pattern>'`

    - TypeScript → `ast-grep --lang ts -p '<pattern>'`

    - Bash → `ast-grep --lang bash -p '<pattern>'`

    - TSX (React) → `ast-grep --lang tsx -p '<pattern>'`

    - JavaScript → `ast-grep --lang js -p '<pattern>'`

    - Rust → `ast-grep --lang rust -p '<pattern>'`

    - JSON → `ast-grep --lang json -p '<pattern>'`

  - Prefer `ast-grep` over ripgrep/grep unless a plain-text search is explicitly requested.

- Select among matches: pipe to `fzf`

Data
----

- JSON: `jq`

- YAML/XML: `yq`

Python Tooling
--------------

- Package Management & Virtual Envs: `uv`
    (fast replacement for pip/pip-tools/virtualenv; use `uv pip install ...`, `uv run ...`)

- Linting & Formatting: `ruff`
    (linter + formatter; use `ruff check .`, `ruff format .`)

- Static Typing: `mypy`
    (type checking; use `mypy .`)

- Security: `bandit`
    (Python security linter; use `bandit -r .`)

- Testing: `pytest`
    (test runner; use `pytest -q`, `pytest -k <pattern>` to filter tests)

- Logging: `loguru`
    (runtime logging utility; import in code:)

        from loguru import logger
        logger.info("message")

Notes
-----

- Prefer `uv` for Python dependency and environment management instead of pip/venv/poetry/pip-tools.

MCP\_SERVERS
------------

- Use the `dspy_Docs` MCP server to get latest docs for DSPy usage.

- Use the `lmstudio_docs` MCP server to get latest docs for LM Studio API usage.

-------

## Proactive TODO/FIXME Annotations

Add TODO/FIXME notes as you work—don’t wait for a cleanup pass. Use them to mark: missing tests, unclear contracts, temporary workarounds, performance/security concerns, or places where design choices need follow-up.

**Format (single line):**

    TODO(scope|owner): short, imperative next step — why it matters [evidence: <source|cmd|ticket>]
    FIXME(scope|owner): what is broken — minimal repro or constraint [evidence: <source|cmd|ticket>]

- `scope|owner` is optional but encouraged (e.g., `ui`, `backend`, `deps`, or a handle like `@alice`).

- Keep it ≤120 chars when possible; link to issues for details.

**Examples (per language comment style):**

    # TODO(domain|@alice): replace naive parse with streaming parser — OOM on large inputs [evidence: profile.txt]
    # FIXME(api): 500 on empty payload — add validation + test [evidence: pytest -k empty_payload]


    // TODO(ui): debounce search — noisy network on fast typing [evidence: trace.log]
    // FIXME(auth|@bob): refresh token race — guard with mutex [evidence: unit test 'refresh-concurrency']


    # TODO(devex): switch to uv task for one-liners [evidence: uv run --help]

### Workflow (aligned with `todos.md`)

1. Gather evidence with the command you used during investigation and reference it in the note’s `[evidence: ...]`.

2. Add the TODO/FIXME in the code at the closest actionable location.

3. Commit with a concise message (e.g., `chore(todos): mark debounce + auth race with evidence`).

4. Before opening a PR, **find and group** all annotations as described in `todos.md` so maintainers can review them together.

### Discover & verify (standard commands)

- Plain-text sweep:

```bash
        rg -n "TODO|FIXME" | fzf
```

- Syntax-aware matches (prefer this when patterns are noisy):

```bash
        ast-grep --lang python -p "// TODO(_) (_) : (_)"
        ast-grep --lang ts -p "// FIXME(_) : (_)"
```

- File targeting:

```bash
        fd -e py -e ts | xargs rg -n "TODO|FIXME"
```

### PR checklist (copy into your PR template)

- Added TODO/FIXME where follow-ups are needed, with `[evidence: ...]`.

- Ran `rg`/`ast-grep` to list all annotations and grouped them per `todos.md` for reviewers.

- Linked or opened issues for any TODO expected to live >2 sprints.

### Retirement policy

- Convert TODO → issue if it will outlive the current PR.

- Remove the annotation when addressed; reference the commit/issue that resolves it.

-------

Rules for Best-Practice

-------

<file\_length\_and\_structure>

- Prefer maintainability signals over fixed line caps.

- Split when cognitive complexity > 15, cohesion drops, or fan-in/out spikes.

- Group by feature. Keep a file to one capability plus its close helpers.

- Use clear folder names and consistent naming.

</file\_length\_and\_structure>

<paradigm\_and\_style>

- Use OOP, functional, or data-oriented styles as idiomatic for the language.

- Favor composition. In OOP, model behavior behind small interfaces or protocols.

- Prefer pure functions and algebraic data types where natural.

</paradigm\_and\_style>

<single\_responsibility\_principle>

- Aim for one capability and its close helpers. Avoid micro-files.

- Enforce through module boundaries and public APIs, not line counts.

</single\_responsibility\_principle>

<modular\_design>

- Design modules to be interchangeable, testable, and isolated.

- Keep public surfaces small. Inject dependencies. Avoid tight coupling.

- Optimize for replaceability and test seams over premature reuse.

</modular\_design>

<roles\_by\_platform>

- UI stacks: ViewModel for UI logic, Manager for business logic, Coordinator for navigation and state flow.

- Backend and CLI: Service, Handler, Repository, Job, Workflow.

- Do not mix view code with business logic.

</roles\_by\_platform>

<function\_and\_class\_size>

- Size by behavior, not lines.

- Functions ≤ 20–30 cognitive steps.

- Split a class when it owns more than one lifecycle or more than one external dependency graph.

</function\_and\_class\_size>

<naming\_and\_readability>

- Use intention revealing names.

- Allow domain terms with qualifiers, for example `UserData`, `BillingInfo`.

- Forbid empty suffixes like `Helper` or `Utils` unless tightly scoped.

</naming\_and\_readability>

<scalability\_mindset>

- Build for extension points from day one, such as interfaces, protocols, and constructor injection.

- Prefer local duplication over unstable abstractions.

- Document contracts at module seams.

</scalability\_mindset>

<avoid\_god\_classes>

- Do not centralize everything in one file or class.

- Split into UI, State, Handlers, Networking, and other focused parts.

</avoid\_god\_classes>

<dependency\_injection>

- Backends: prefer constructor injection. Keep containers optional.

- Swift, Kotlin, TypeScript: use protocols or interfaces. Inject by initializer or factory.

- Limit global singletons. Provide test doubles at seams.

</dependency\_injection>

<testing>

- Require deterministic seams.

- Add contract tests for modules and layers.

- Use snapshot or golden tests for UI and renderers.

</testing>

<architecture\_boundaries>

- Feature oriented packaging with clear dependency direction: UI → app → domain → infra.

- Stabilize domain modules. Keep infra replaceable.

- Enforce imports with rules or module maps.

</architecture\_boundaries>

-------


--- README.md ---
# dspyteach – DSPy File Teaching Analyzer

---

[![PyPI](https://img.shields.io/pypi/v/dspyteach.svg?include_prereleases&cacheSeconds=60&t=1)](https://pypi.org/project/dspyteach/)
[![Downloads](https://img.shields.io/pypi/dm/dspyteach.svg?cacheSeconds=300)](https://pypi.org/project/dspyteach/)
[![TestPyPI](https://img.shields.io/badge/TestPyPI-dspyteach-informational?cacheSeconds=300)](https://test.pypi.org/project/dspyteach/)
[![CI](https://github.com/AcidicSoil/dspy-file/actions/workflows/release.yml/badge.svg)](…)
[![Repo](https://img.shields.io/badge/GitHub-AcidicSoil%2Fdspy--file-181717?logo=github)](https://github.com/AcidicSoil/dspy-file)

---

## DSPy-powered CLI that analyzes source files (one or many) and produces teaching briefs

**Each run captures:**

- an overview of the file and its major sections
- key teaching points, workflows, and pitfalls highlighted in the material
- a polished markdown brief suitable for sharing with learners

The implementation mirrors the multi-file tutorial (`tutorials/multi-llmtxt_generator`) but focuses on per-file inference. The program is split into:

- `dspy_file/signatures.py` – DSPy signatures that define inputs/outputs for each step
- `dspy_file/file_analyzer.py` – the main DSPy module that orchestrates overview, teaching extraction, and report composition. It now wraps the final report stage with `dspy.Refine`, pushing for 450–650+ word briefs.
- `dspy_file/file_helpers.py` – utilities for loading files and rendering the markdown brief
- `dspy_file/analyze_file_cli.py` – command line entry point that configures the local model and prints results. It can walk directories, apply glob filters, and batch-generate briefs.

---

## Quick start

1. Confirm Python 3.10–3.12 is available and pull at least one OpenAI-compatible model (Ollama, LM Studio, or a hosted provider).
2. From the repository root, create an isolated environment and install dependencies:

   ```bash
   uv venv -p 3.12
   source .venv/bin/activate
   uv sync
   ```

3. Run a smoke test to confirm the CLI is wired up:

   ```bash
   dspyteach --help
   ```

   Expected result: the help output lists available flags and displays the active version string.

4. Analyze a sample file to confirm end-to-end output:

   ```bash
   dspyteach path/to/example.py
   ```

   Expected result: the command prints a teaching brief to stdout and writes a `.teaching.md` file under `dspy_file/data/`.

---

## Requirements

- Python 3.10-3.12+
- DSPy installed in the environment
- A language-model backend. You can choose between:
  - **Ollama** (default): run it locally with the model `hf.co/unsloth/Qwen3-4B-Instruct-2507-GGUF:Q6_K_XL` pulled.
  - **LM Studio** (OpenAI-compatible): start the LM Studio server (`lms server start`) and download a model such as `qwen3-4b-instruct-2507@q6_k_xl`.
  - **Any other OpenAI-compatible endpoint**: point the CLI at a hosted provider by supplying an API base URL and key (defaults to `gpt-5`).
- (Optional) `.env` file for DSPy configuration. `dotenv` loads variables such as `DSPYTEACH_PROVIDER`, `DSPYTEACH_MODEL`, `DSPYTEACH_API_BASE`, `DSPYTEACH_API_KEY`, and `OPENAI_API_KEY`.

---

## Example output

[[example-data after running a few passes](example-data/)]

---

## Installation

### Install with uv (recommended for local development)

```bash
uv venv -p 3.12
source .venv/bin/activate
uv sync
```

Expected result: the virtual environment contains the project dependencies and `dspyteach --version` reports the local build.

### Install from PyPI

```bash
pip install dspyteach
```

Expected result: running `dspyteach --help` prints the CLI usage banner from the installed package.

### Configure the language model

The CLI now supports configurable OpenAI-compatible providers in addition to the default Ollama runtime. You can override the backend via CLI options or environment variables:

```bash
# Use LM Studio's OpenAI-compatible server with its default port
dspyteach path/to/project \
  --provider lmstudio \
  --model osmosis-mcp-4b@q8_0 \
  --api-base http://localhost:1234/v1
```

```bash
# Environment variable alternative (e.g. inside .env)
export DSPYTEACH_PROVIDER=lmstudio
export DSPYTEACH_MODEL=osmosis-mcp-4b@q8_0
export DSPYTEACH_API_BASE=http://localhost:1234/v1
dspyteach path/to/project
```

### LM-Studio Usage Notes

[LM Studio configuration guide](docs/lm-studio-provider.md)

LM Studio must expose its local server before you run the CLI. Start it from the Developer tab inside the LM Studio app or via `lms server start` (details in the [LM Studio configuration guide](docs/lm-studio-provider.md)); otherwise the CLI will exit early with a connection warning.

### OpenAI-compatible others usage

For hosted OpenAI-compatible services, set `--provider openai`, supply `--api-base` if needed, and pass an API key either through `--api-key`, `DSPYTEACH_API_KEY`, or the standard `OPENAI_API_KEY`. To keep a local Ollama model running after the CLI finishes, add `--keep-provider-alive`.

## Usage

Run the CLI to extract a teaching brief from a single file:

```bash
dspyteach path/to/your_file
```

Expected result: the CLI prints a markdown teaching brief to stdout and saves a copy under `dspy_file/data/`.

You can also point the CLI at a directory. The tool will recurse by default:

```bash
dspyteach path/to/project --glob "**/*.py" --glob "**/*.md"
```

Expected result: each matched file produces its own `.teaching.md` report in the output directory.

Use `--non-recursive` to stay in the top-level directory, add `--glob` repeatedly to narrow the target set, and pass `--raw` to print the raw DSPy prediction object instead of the formatted report.

### Command examples

- **Analyze a single markdown file**

  ```bash
  dspyteach docs/example.md
  ```

  Expected result: the CLI prints a teaching brief and stores `docs__example.teaching.md` in the output directory.

- **Process a repository while skipping generated assets**

  ```bash
  dspyteach ./repo \
    --glob "**/*.py" \
    --glob "**/*.md" \
    --exclude-dirs "build/,dist/,data/"
  ```

  Expected result: only `.py` and `.md` files outside the excluded directories are analyzed.

- **Generate refactor templates instead of teaching briefs**

  ```bash
  dspyteach ./repo --mode refactor --prompt refactor_prompt_template
  ```

  Expected result: `.refactor.md` files appear alongside the teaching outputs with guidance tailored to the selected prompt.

Need to double-check files before the model runs? Add `--confirm-each` (alias `--interactive`) to prompt before every file, accepting with Enter or skipping with `n`.

To omit specific subdirectories entirely, pass one or more `--exclude-dirs` options. Each value can list comma-separated relative paths (for example `--exclude-dirs "build/,venv/" --exclude-dirs data/raw`). The analyzer ignores any files whose path begins with the provided prefixes.

Prefer short flags? The common options include `-r` (`--raw`), `-m` (`--mode`), `-nr` (`--non-recursive`), `-g` (`--glob`), `-i` (`--confirm-each`), `-ed` (`--exclude-dirs`), and `-o` (`--output-dir`). Mix and match them as needed.

## Refactor files/dirs

Want to scaffold refactor prompt templates instead of teaching briefs? Switch the mode:

```bash
dspyteach path/to/project --mode refactor --glob "**/*.md"
```

---

---

## **clarity on what happens when in teaching mode**

### both of these commands shown below would create new directories in the path outside the cwd that you ran the commands from and the directories would be the following: so in this case it would be exactly ["C:\Users\user\projects\WIP\NAME-OF-CWD + (the new files it creates which will be...)dspyteach\teach\data\00-ideation\architecture\adr-new.architecture.md]"

#### "00-ideation\architecture\adr-new.architecture.md" are unique to my personal setup so your output would be a mirrored version of the target path recursively

directory analyzed --> "~\projects\WIP\ .__pre-temp-prompts\temp-prompts-organized" so all under temp-prompts-organized are analyzed unless flag is passed to do otherwise, ie., non-recursive or -i AKA --interactive (file by file of target path).

---

```bash
dt -m refactor C:\Users\user\projects\WIP\.__pre-temp-prompts\temp-prompts-organized\ --provider lmstudio --api-base <http://127.0.0.1:1234/v1> -ed prompt-front-matter/ -o ..\dspyteach\data -i
```

```bash
dt C:\Users\user\projects\WIP\.__pre-temp-prompts\temp-prompts-organized\ --provider lmstudio --api-base <http://127.0.0.1:1234/v1> -ed prompt-front-matter/ -o ..\dspyteach\teach\data -i
```

---


## Additional Information

The CLI reuses the same file resolution pipeline but feeds each document through the bundled `dspy-file_refactor-prompt_template.md` instructions (packaged under `dspy_file/prompts/`), saving `.refactor.md` files alongside the teaching reports. Teaching briefs remain the default (`--mode teach`), so existing workflows continue to work unchanged.

When multiple templates live in `dspy_file/prompts/`, the refactor mode surfaces a picker so you can choose which one to use. You can also point at a specific template explicitly with `-p/--prompt`, passing either a bundled name (`-p refactor_prompt_template`) or an absolute path to your own Markdown prompt.

Each run only executes the analyzer for the chosen mode. When you pass `--mode refactor` the teaching inference pipeline stays idle, and you can alias the command (for example `alias dspyrefactor='dspyteach --mode refactor'`) if you prefer refactor templates to be the default in your shell.

To change where reports land, supply `--output-dir /path/to/reports`. When omitted the CLI writes to `dspy_file/data/` next to the module. Every run prints the active model name and the resolved output directory before analysis begins so you can confirm the environment at a glance. For backwards compatibility the installer also registers `dspy-file-teaching` as an alias.

Each analyzed file is saved under the chosen directory with a slugged name (e.g. `src__main.teaching.md` or `src__main.refactor.md`). If a file already exists, the CLI appends a numeric suffix to avoid overwriting previous runs.

The generated brief is markdown that mirrors the source material:

- Overview paragraphs for quick orientation
- Section-by-section bullets capturing the narrative
- Key concepts, workflows, pitfalls, and references learners should review
- A `dspy.Refine` wrapper keeps retrying until the report clears a length reward (defaults scale to ~50% of the source word count, with min/max clamps), so the content tends to be substantially longer than a single LM call.
- If a model cannot honour DSPy's structured-output schema, the CLI prints a `Structured output fallback` notice and heuristically parses the textual response so you still get usable bullets.

Behind the scenes the CLI:

1. Loads environment variables via `python-dotenv`.
2. Configures DSPy with the provider selected via CLI or environment variables (Ollama by default).
3. Resolves all requested files, reads contents, runs the DSPy `FileTeachingAnalyzer` module, and prints a human-friendly report for each.
4. Persists each report to the configured output directory so results are easy to revisit.
5. Stops the Ollama model when appropriate so local resources are returned to the pool.

### Extending

- Adjust the `TeachingReport` signature or add new chains in `dspy_file/file_analyzer.py` to capture additional teaching metadata.
- Customize the render logic in `dspy_file.file_helpers.render_prediction` if you want richer CLI output or structured JSON.
- Tune `TeachingConfig` inside `file_analyzer.py` to raise `max_tokens`, adjust the `Refine` word-count reward, or add extra LM kwargs.
- Add more signatures and module stages to capture additional metadata (e.g., security checks) and wire them into `FileAnalyzer`.

---

## Releasing

Maintainer release steps live in [docs/RELEASING.md](docs/RELEASING.md).

## Troubleshooting

- If the program cannot connect to Ollama, verify that the server is running on `http://localhost:11434` and the requested model has been pulled.
- When you see `ollama command not found`, ensure the `ollama` binary is on your `PATH`.
- For encoding errors, the helper already falls back to `latin-1`, but you can add more fallbacks in `file_helpers.read_file_content` if needed.


--- json_response_integration_report.md ---
# Integration Report: JSON Response Formatting

**Date:** October 17, 2025
**Status:** Proposed

## 1. Objective

This document outlines a proposal to integrate a feature into the `dspy-file` command-line tool (`analyze_file_cli.py`) to enforce structured JSON output from language models. This is based on the `response_format` parameter within the **DSPy framework**, which instructs DSPy to request JSON-formatted output from the underlying model provider.

## 2. DSPy Framework Abstraction

It is important to note that `response_format` is a feature of the `dspy.LM` abstraction layer, not necessarily a native parameter of the end-provider's API. The DSPy framework is responsible for translating this parameter into the correct, provider-specific API call.

- When `response_format` is passed to `dspy.LM` for an **Ollama** model, DSPy's Ollama client handler converts this into the native `format: 'json'` parameter in the request sent to the Ollama server.
- Similarly, for **OpenAI-compatible** endpoints, DSPy would use the provider's required format, such as `response_format={"type": "json_object"}`.

The code proposed in this document correctly interacts with the DSPy abstraction layer.

## 3. Initial Proposal (Ollama-Specific)

The initial goal was to create an optional, provider-specific feature for users of Ollama.

### 3.1. Key Mechanisms

1. **CLI Flag:** A new `--json-response` command-line flag was proposed to allow users to explicitly enable this functionality.
2. **Provider Gating:** The core logic is enclosed in `if provider is Provider.OLLAMA:` checks to ensure it only runs when Ollama is the selected provider.
3. **External Schemas:** The proposal involves loading the required JSON schema from external files (e.g., `teach.schema.json`, `refactor.schema.json`) based on the selected analysis mode.

## 4. Generalized Proposal (Provider-Agnostic)

To extend this feature to other providers, the Ollama-specific gating can be removed.

### 4.1. Key Mechanisms

1. **CLI Flag:** The `--json-response` flag remains the sole controller for the feature.
2. **Generalized Logic:** The `if provider is Provider.OLLAMA` checks are removed, allowing the `response_format` to be passed to any configured provider via the `dspy.LM` constructor.

### 4.2. Considerations & Caveats

- **Provider Capability:** The success of this feature depends on both the DSPy provider implementation and the underlying model's ability to generate valid JSON. While the `response_format` parameter instructs the model, it does not guarantee schema conformance, which still relies on the model's instruction-following capabilities.

## 5. Final Proposed Code (Provider-Agnostic)

The following represents the final proposed code structure for a generalized implementation.

```python
# In dspy_file/analyze_file_cli.py

# 1. In build_parser()
parser.add_argument(
    "--json-response",
    action="store_true",
    dest="json_response",
    help="Request a JSON response using a schema (if supported by the provider's DSPy handler).",
)

# 2. In configure_model()
def configure_model(
    # ...,
    response_format: dict[str, Any] | None = None,
):
    lm_kwargs: dict[str, Any] = {"streaming": False, "cache": False}

    if response_format:
        lm_kwargs["response_format"] = response_format

    if provider is Provider.OLLAMA:
        # ...
    else:
        # ...

    lm = dspy.LM(identifier, **lm_kwargs)
    dspy.configure(lm=lm)

# 3. In main()
# ...
analysis_mode = AnalysisMode(args.mode)
response_format = None
if args.json_response:
    schema_filename = f"{analysis_mode.value}.schema.json"
    schema_path = Path(__file__).parent / "prompts" / schema_filename
    try:
        with schema_path.open("r", encoding="utf-8") as f:
            schema = json.load(f)
        response_format = {
            "type": "json_schema",
            "json_schema": {"schema": schema},
        }
    except Exception as e:
        print(f"Warning: Could not load schema from {schema_path}. Error: {e}")

configure_model(..., response_format=response_format)
# ...
```


--- dspy_file/analyze_file_cli.py ---
# path: analyze_file_cli.py
# analyze_file_cli.py - command line entry point for DSPy file analyzer
from __future__ import annotations

import argparse
import os
import subprocess
import sys
from enum import Enum
from urllib import error as urlerror
from urllib import request
from pathlib import Path
from typing import Any, Final

import dspy
from dotenv import load_dotenv

from .file_analyzer import FileTeachingAnalyzer
from .file_helpers import collect_source_paths, read_file_content, render_prediction
from .prompts import PromptTemplate, list_bundled_prompts, load_prompt_text
from .refactor_analyzer import FileRefactorAnalyzer

try:  # dspy depends on litellm; guard in case import path changes.
    from litellm.exceptions import InternalServerError as LiteLLMInternalServerError
except Exception:  # pragma: no cover - defensive fallback if litellm API shifts
    LiteLLMInternalServerError = None  # type: ignore[assignment]


class Provider(str, Enum):
    """Supported language model providers."""

    OLLAMA = "ollama"
    OPENAI = "openai"
    LMSTUDIO = "lmstudio"

    @property
    def is_openai_compatible(self) -> bool:
        return self in {Provider.OPENAI, Provider.LMSTUDIO}


DEFAULT_PROVIDER: Final[Provider] = Provider.OLLAMA
DEFAULT_OUTPUT_DIR = Path(__file__).parent / "data"
DEFAULT_OLLAMA_MODEL = "hf.co/Mungert/osmosis-mcp-4b-GGUF:Q5_K_M"
DEFAULT_LMSTUDIO_MODEL = "osmosis-mcp-4b@q8_0"
DEFAULT_OPENAI_MODEL = "gpt-5"
OLLAMA_BASE_URL = "http://localhost:11434"
LMSTUDIO_BASE_URL = "http://localhost:1234/v1"

PROVIDER_DEFAULTS: Final[dict[Provider, dict[str, Any]]] = {
    Provider.OLLAMA: {"model": DEFAULT_OLLAMA_MODEL, "api_base": OLLAMA_BASE_URL},
    Provider.LMSTUDIO: {"model": DEFAULT_LMSTUDIO_MODEL, "api_base": LMSTUDIO_BASE_URL},
    Provider.OPENAI: {"model": DEFAULT_OPENAI_MODEL, "api_base": None},
}


def _resolve_option(
    cli_value: str | None, env_var: str, default: str | None = None
) -> str | None:
    """Return the CLI value if provided, otherwise fall back to env or default."""

    if cli_value is not None:
        return cli_value
    env_value = os.getenv(env_var)
    if env_value not in ("", None):
        return env_value
    return default


def _normalize_model_name(provider: Provider, raw_model: str) -> str:
    """Attach the appropriate provider prefix to the model identifier."""

    if provider is Provider.OLLAMA:
        return (
            raw_model
            if raw_model.startswith("ollama_chat/")
            else f"ollama_chat/{raw_model}"
        )

    if raw_model.startswith("openai/"):
        return raw_model
    return f"openai/{raw_model}"


def configure_model(
    provider: Provider,
    model_name: str,
    *,
    api_base: str | None,
    api_key: str | None,
) -> None:
    """Configure DSPy with the selected provider and model."""

    lm_kwargs: dict[str, Any] = {"streaming": False, "cache": False}
    if provider is Provider.OLLAMA:
        lm_kwargs["api_base"] = api_base or OLLAMA_BASE_URL
        # Ollama's OpenAI compatibility ignores api_key, so pass an empty string.
        lm_kwargs["api_key"] = ""
    else:
        if api_base:
            lm_kwargs["api_base"] = api_base
        if api_key:
            lm_kwargs["api_key"] = api_key

    identifier = _normalize_model_name(provider, model_name)
    lm = dspy.LM(identifier, **lm_kwargs)
    dspy.configure(lm=lm)
    provider_label = "LM Studio" if provider is Provider.LMSTUDIO else provider.value
    suffix = f" via {api_base}" if provider.is_openai_compatible and api_base else ""
    print(f"Configured DSPy LM ({provider_label}): {model_name}{suffix}")


class ProviderConnectivityError(RuntimeError):
    """Raised when a provider cannot be reached before running analysis."""


def _probe_openai_provider(
    api_base: str, api_key: str | None, *, timeout: float = 3.0
) -> None:
    """Make a lightweight request against an OpenAI-compatible endpoint."""

    endpoint = api_base.rstrip("/") + "/models"
    headers = {"Authorization": f"Bearer {api_key or ''}"}
    request_obj = request.Request(endpoint, headers=headers, method="GET")

    try:
        with request.urlopen(request_obj, timeout=timeout):
            return
    except urlerror.HTTPError as exc:
        raise ProviderConnectivityError(
            f"Endpoint {endpoint} responded with HTTP {exc.code}: {exc.reason}"
        ) from exc
    except urlerror.URLError as exc:
        reason = getattr(exc, "reason", exc)
        raise ProviderConnectivityError(
            f"Failed to reach {endpoint}: {reason}"
        ) from exc


def stop_ollama_model(model_name: str) -> None:
    """Stop the Ollama model to free server resources."""

    try:
        subprocess.run(
            ["ollama", "stop", model_name],
            check=True,
            capture_output=True,
        )
    except subprocess.CalledProcessError as exc:  # pragma: no cover - warn only
        print(f"Warning: Failed to stop model {model_name}: {exc}")
    except FileNotFoundError:
        print("Warning: ollama command not found while attempting to stop the model.")


class AnalysisMode(str, Enum):
    TEACH = "teach"
    REFACTOR = "refactor"

    @property
    def render_key(self) -> str:
        return self.value

    @property
    def output_description(self) -> str:
        return "teaching report" if self is AnalysisMode.TEACH else "refactor template"

    @property
    def file_suffix(self) -> str:
        # No suffixing. Preserve original filename.
        return ""


def _prompt_for_template_selection(prompts: list[PromptTemplate]) -> PromptTemplate:
    while True:
        print("Available prompt templates:")
        for idx, template in enumerate(prompts, 1):
            print(f"  [{idx}] {template.name} ({template.path.name})")
        try:
            choice = input(f"Select a template [1-{len(prompts)}] (default 1): ")
        except EOFError:
            print("No selection provided; using first template.")
            return prompts[0]

        stripped = choice.strip()
        if not stripped:
            return prompts[0]
        if stripped.isdigit():
            idx = int(stripped)
            if 1 <= idx <= len(prompts):
                return prompts[idx - 1]
        print(f"Please enter a number between 1 and {len(prompts)}.")


def _resolve_prompt_text(prompt_arg: str | None) -> str:
    if prompt_arg:
        return load_prompt_text(prompt_arg)

    prompts = list_bundled_prompts()
    if not prompts:
        raise FileNotFoundError("No prompt templates found in prompts directory.")
    if len(prompts) == 1:
        return prompts[0].path.read_text(encoding="utf-8")

    selected = _prompt_for_template_selection(prompts)
    return selected.path.read_text(encoding="utf-8")


def _write_output(
    source_path: Path,
    content: str,
    *,
    root: Path | None = None,
    output_dir: Path | None = None,
    suffix: str = "",
) -> Path:
    """Write output to the same filename and directory layout as the analyzed path.

    If output_dir is provided, mirror the relative directory structure inside it and
    keep the original filename. Otherwise overwrite the source file in place.
    """

    try:
        relative_path = (
            source_path.relative_to(root) if root else Path(source_path.name)
        )
    except ValueError:
        relative_path = Path(source_path.name)

    if output_dir:
        dest_path = (output_dir / relative_path).resolve()
        dest_path.parent.mkdir(parents=True, exist_ok=True)
    else:
        dest_path = source_path

    if not content.endswith("\n"):
        content = content + "\n"

    dest_path.write_text(content, encoding="utf-8")
    return dest_path


def _confirm_analyze(path: Path) -> bool:
    """Prompt the user for confirmation before analyzing a file."""

    prompt = f"Analyze {path}? [Y/n]: "
    while True:
        try:
            response = input(prompt)
        except EOFError:
            print("No input received; skipping.")
            return False

        normalized = response.strip().lower()
        if normalized in {"", "y", "yes"}:
            return True
        if normalized in {"n", "no"}:
            return False
        print("Please answer 'y' or 'n'.")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Analyze a single file using DSPy signatures and modules",
    )
    parser.add_argument("path", help="Path to the file to analyze")
    parser.add_argument(
        "--provider",
        choices=[provider.value for provider in Provider],
        default=None,
        help=(
            "Language model provider to use (env: DSPYTEACH_PROVIDER). "
            "Choose from 'ollama', 'lmstudio', or 'openai'."
        ),
    )
    parser.add_argument(
        "--model",
        dest="model_name",
        default=None,
        help=(
            "Override the model identifier for the selected provider "
            "(env: DSPYTEACH_MODEL)."
        ),
    )
    parser.add_argument(
        "--api-base",
        dest="api_base",
        default=None,
        help=("Override the OpenAI-compatible API base URL (env: DSPYTEACH_API_BASE)."),
    )
    parser.add_argument(
        "--api-key",
        dest="api_key",
        default=None,
        help=(
            "API key for OpenAI-compatible providers (env: DSPYTEACH_API_KEY). "
            "Falls back to OPENAI_API_KEY for the OpenAI provider."
        ),
    )
    parser.add_argument(
        "--keep-provider-alive",
        action="store_true",
        dest="keep_provider_alive",
        help="Skip stopping the local Ollama model when execution completes.",
    )
    parser.add_argument(
        "-r",
        "--raw",
        action="store_true",
        help="Print raw DSPy prediction repr instead of formatted text",
    )
    parser.add_argument(
        "-m",
        "--mode",
        choices=[mode.value for mode in AnalysisMode],
        default=AnalysisMode.TEACH.value,
        help="Select output mode: teaching report (default) or refactor prompt template.",
    )
    parser.add_argument(
        "-nr",
        "--non-recursive",
        action="store_true",
        help="When path is a directory, only analyze files in the top-level directory",
    )
    parser.add_argument(
        "-g",
        "--glob",
        action="append",
        dest="include_globs",
        default=None,
        help=(
            "Optional glob pattern(s) applied relative to the directory. Repeat to combine."
        ),
    )
    parser.add_argument(
        "-p",
        "--prompt",
        dest="prompt",
        default=None,
        help=(
            "Prompt template to use in refactor mode. Provide a name, bundled filename, or path."
        ),
    )
    parser.add_argument(
        "-i",
        "--confirm-each",
        "--interactive",
        action="store_true",
        dest="confirm_each",
        help="Prompt for confirmation before analyzing each file.",
    )
    parser.add_argument(
        "-ed",
        "--exclude-dirs",
        action="append",
        dest="exclude_dirs",
        default=None,
        help=(
            "Comma-separated relative directory paths to skip entirely when scanning."
        ),
    )
    parser.add_argument(
        "-o",
        "--output-dir",
        dest="output_dir",
        default=None,
        help=("Directory to write outputs. If omitted, overwrite files in place."),
    )
    return parser


def analyze_path(
    path: str,
    *,
    raw: bool,
    recursive: bool,
    include_globs: list[str] | None,
    confirm_each: bool,
    exclude_dirs: list[str] | None,
    output_dir: Path | None,
    mode: AnalysisMode,
    prompt_text: str | None = None,
) -> int:
    """Run the DSPy pipeline and render results to stdout for one or many files."""

    resolved = Path(path).expanduser().resolve()
    targets = collect_source_paths(
        path,
        recursive=recursive,
        include_globs=include_globs,
        exclude_dirs=exclude_dirs,
    )

    if not targets:
        print(f"No files found under {resolved}")
        return 0

    analyzer: dspy.Module
    if mode is AnalysisMode.TEACH:
        analyzer = FileTeachingAnalyzer()
    else:
        analyzer = FileRefactorAnalyzer(template_text=prompt_text)
    root: Path | None = resolved if resolved.is_dir() else None

    exit_code = 0
    for target in targets:
        if confirm_each and not _confirm_analyze(target):
            print(f"Skipping {target} at user request.")
            continue

        try:
            content = read_file_content(target)
        except (FileNotFoundError, UnicodeDecodeError) as exc:
            print(f"Skipping {target}: {exc}")
            exit_code = 1
            continue

        print(f"\n=== Analyzing {target} ===")
        prediction = analyzer(file_path=str(target), file_content=content)

        if raw:
            output_text = repr(prediction)
            print(output_text)
        else:
            output_text = render_prediction(prediction, mode=mode.render_key)
            print(output_text, end="")

        output_path = _write_output(
            target,
            output_text,
            root=root,
            output_dir=output_dir,
            suffix=mode.file_suffix,
        )
        print(f"Saved {mode.output_description} to {output_path}")

    return exit_code


def main(argv: list[str] | None = None) -> int:
    load_dotenv()

    parser = build_parser()
    args = parser.parse_args(argv)

    provider_value = _resolve_option(
        args.provider, "DSPYTEACH_PROVIDER", DEFAULT_PROVIDER.value
    )
    try:
        provider = Provider(provider_value)
    except ValueError:  # pragma: no cover - argparse handles this
        valid = ", ".join(p.value for p in Provider)
        parser.error(f"Unsupported provider '{provider_value}'. Choose from: {valid}.")

    defaults = PROVIDER_DEFAULTS[provider]
    model_name = _resolve_option(args.model_name, "DSPYTEACH_MODEL", defaults["model"])
    api_base_default = defaults.get("api_base")
    api_base = _resolve_option(args.api_base, "DSPYTEACH_API_BASE", api_base_default)
    api_key = _resolve_option(args.api_key, "DSPYTEACH_API_KEY", None)
    if provider is Provider.OPENAI and not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
    if provider is Provider.LMSTUDIO and not api_key:
        api_key = "lm-studio"

    if provider is Provider.LMSTUDIO and api_base:
        try:
            _probe_openai_provider(api_base, api_key)
        except ProviderConnectivityError as exc:
            print("Unable to reach the LM Studio server before starting analysis.")
            print(f"Details: {exc}")
            print(
                "Start LM Studio's local API server (Developer tab → Start Server or "
                "`lms server start`) and re-run, or pass --api-base to match the running port."
            )
            return 1

    configure_model(provider, model_name, api_base=api_base, api_key=api_key)
    stop_model: str | None = model_name if provider is Provider.OLLAMA else None

    exit_code = 0
    try:
        analysis_mode = AnalysisMode(args.mode)
        prompt_text: str | None = None
        if analysis_mode is AnalysisMode.REFACTOR:
            try:
                prompt_text = _resolve_prompt_text(args.prompt)
            except (FileNotFoundError, ValueError) as exc:
                print(f"Error resolving prompt: {exc}")
                return 2
        elif args.prompt:
            print("Warning: --prompt is ignored outside refactor mode.")
        output_dir = (
            Path(args.output_dir).expanduser().resolve() if args.output_dir else None
        )
        if output_dir:
            print(f"Writing {analysis_mode.output_description}s to {output_dir}")
        else:
            print(f"Writing {analysis_mode.output_description}s in place")
        exclude_dirs = None
        if args.exclude_dirs:
            parsed: list[str] = []
            for entry in args.exclude_dirs:
                parsed.extend(
                    segment.strip() for segment in entry.split(",") if segment.strip()
                )
            exclude_dirs = parsed or None
        try:
            exit_code = analyze_path(
                args.path,
                raw=args.raw,
                recursive=not args.non_recursive,
                include_globs=args.include_globs,
                confirm_each=args.confirm_each,
                exclude_dirs=exclude_dirs,
                output_dir=output_dir,
                mode=analysis_mode,
                prompt_text=prompt_text,
            )
        except Exception as exc:
            if LiteLLMInternalServerError and isinstance(
                exc, LiteLLMInternalServerError
            ):
                message = str(exc)
                if exc.__cause__:
                    message = f"{message} (cause: {exc.__cause__})"
                print("Model request failed while generating the report.")
                print(f"Details: {message}")
                if provider is Provider.LMSTUDIO:
                    print(
                        "Confirm the LM Studio server is running and reachable at "
                        f"{api_base}."
                    )
                return 1
            raise
    except (FileNotFoundError, IsADirectoryError) as exc:
        parser.print_usage(sys.stderr)
        print(f"{parser.prog}: error: {exc}", file=sys.stderr)
        exit_code = 2
    except KeyboardInterrupt:
        exit_code = 1
    finally:
        if provider is Provider.OLLAMA and not args.keep_provider_alive and stop_model:
            stop_ollama_model(stop_model)

    return exit_code


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    sys.exit(main())


--- dspy_file/file_analyzer.py ---
# file_analyzer.py - DSPy module deriving a learning brief from a single file
from __future__ import annotations

import json
import re
from collections.abc import Iterable, Mapping
from dataclasses import dataclass, field
from typing import Any

import dspy

from .signatures import FileOverview, TeachingPoints, TeachingReport


@dataclass
class TeachingConfig:
    section_bullet_prefix: str = "- "
    overview_max_tokens: int = 2000
    teachings_max_tokens: int = 2000
    report_max_tokens: int = 2000
    temperature: float | None = 0.3
    top_p: float | None = None
    n_completions: int | None = None
    extra_lm_kwargs: dict[str, Any] = field(default_factory=dict)
    report_refine_attempts: int = 3
    report_reward_threshold: float = 0.8
    report_min_word_count: int = 400
    report_max_word_count: int = 2800
    report_target_ratio: float = 0.5
    report_soft_cap_ratio: float = 0.8

    def lm_args_for(self, scope: str) -> dict[str, Any]:
        """Return per-module LM kwargs without mutating shared config."""
        scope_tokens = {
            "overview": self.overview_max_tokens,
            "teachings": self.teachings_max_tokens,
            "report": self.report_max_tokens,
        }

        kwargs: dict[str, Any] = {**self.extra_lm_kwargs}
        kwargs["max_tokens"] = scope_tokens.get(scope, self.report_max_tokens)

        if self.temperature is not None:
            kwargs["temperature"] = self.temperature
        if self.top_p is not None:
            kwargs["top_p"] = self.top_p
        if self.n_completions is not None:
            kwargs["n"] = self.n_completions

        return kwargs


def _fallback_list(message: str) -> list[str]:
    return [message]


def _ensure_text(value: str | None, fallback: str) -> str:
    if value and value.strip():
        return value
    return fallback


def _ensure_list(
    values: Iterable[str] | None,
    fallback: str,
    *,
    strip_entries: bool = True,
    field_name: str | None = None,
) -> list[str]:
    coerced, used_fallback = _coerce_iterable(values, strip_entries=strip_entries)

    if coerced:
        if used_fallback and field_name:
            _structured_output_notice(field_name)
        return coerced

    if used_fallback and field_name:
        _structured_output_notice(field_name)

    return _fallback_list(fallback)


def _clean_list(
    values: Iterable[str] | None,
    *,
    strip_entries: bool = True,
    field_name: str | None = None,
) -> list[str]:
    if not values:
        return []

    coerced, used_fallback = _coerce_iterable(values, strip_entries=strip_entries)

    if used_fallback and field_name:
        _structured_output_notice(field_name)

    return coerced


_STRUCTURED_NOTICE_CACHE: set[str] = set()


def _structured_output_notice(field: str) -> None:
    if field in _STRUCTURED_NOTICE_CACHE:
        return
    _STRUCTURED_NOTICE_CACHE.add(field)
    print(
        f"Structured output fallback applied for '{field}'. Parsed textual response."
    )


_LEADING_MARKER_PATTERN = re.compile(r"^[\s\-\*•·\u2022\d\.\)\(]+")


def _coerce_iterable(
    values: Iterable[str] | None,
    *,
    strip_entries: bool,
) -> tuple[list[str], bool]:
    if values is None:
        return [], False

    if isinstance(values, str):
        return _coerce_string(values, strip_entries=strip_entries), True

    if isinstance(values, Mapping):
        items: list[str] = []
        for key, val in values.items():
            key_text = str(key).strip()
            val_text = str(val).strip()
            combined = f"{key_text}: {val_text}" if val_text else key_text
            candidate = combined.rstrip() if not strip_entries else combined.strip()
            if candidate:
                items.append(candidate if strip_entries else candidate.rstrip())
        return items, True

    if isinstance(values, Iterable):
        cleaned: list[str] = []
        used_fallback = not isinstance(values, (list, tuple, set))
        for entry in values:
            if entry is None:
                continue
            if isinstance(entry, str):
                candidate = entry.rstrip() if not strip_entries else entry.strip()
            else:
                candidate = str(entry).strip()
            if strip_entries:
                candidate = _LEADING_MARKER_PATTERN.sub("", candidate).strip()
            if candidate:
                cleaned.append(candidate if strip_entries else candidate.rstrip())
        return cleaned, used_fallback

    return _coerce_string(str(values), strip_entries=strip_entries), True


def _coerce_string(value: str, *, strip_entries: bool) -> list[str]:
    text = value.strip()
    if not text:
        return []

    try:
        parsed = json.loads(text)
    except json.JSONDecodeError:
        parsed = None

    if isinstance(parsed, list):
        coerced: list[str] = []
        for item in parsed:
            candidate = str(item)
            candidate = candidate.rstrip() if not strip_entries else candidate.strip()
            if strip_entries:
                candidate = _LEADING_MARKER_PATTERN.sub("", candidate).strip()
            if candidate:
                coerced.append(candidate if strip_entries else candidate.rstrip())
        if coerced:
            return coerced
    elif isinstance(parsed, Mapping):
        mapped: list[str] = []
        for key, val in parsed.items():
            key_text = str(key).strip()
            val_text = str(val).strip()
            candidate = f"{key_text}: {val_text}" if val_text else key_text
            candidate = candidate.rstrip() if not strip_entries else candidate.strip()
            if candidate:
                mapped.append(candidate if strip_entries else candidate.rstrip())
        if mapped:
            return mapped

    lines = value.replace("\r", "\n").split("\n")
    normalized: list[str] = []
    for raw_line in lines:
        candidate = raw_line.rstrip() if not strip_entries else raw_line.strip()
        if strip_entries:
            candidate = _LEADING_MARKER_PATTERN.sub("", candidate).strip()
        if candidate:
            normalized.append(candidate if strip_entries else candidate.rstrip())

    if len(normalized) <= 1:
        delimiters = [";", "•", "·", " | "]
        for delimiter in delimiters:
            if delimiter in value:
                parts = [part.strip() for part in value.split(delimiter) if part.strip()]
                if parts:
                    return [
                        _LEADING_MARKER_PATTERN.sub("", part).strip()
                        if strip_entries
                        else part.rstrip()
                    ]

    return normalized


def _with_prefix(items: Iterable[str], prefix: str) -> list[str]:
    if not prefix:
        return [item for item in items if item.strip()]

    prefix_char = prefix.strip()[:1] if prefix.strip() else ""
    prefixed: list[str] = []

    for item in items:
        stripped = item.strip()
        if not stripped:
            continue
        if prefix_char and stripped.startswith(prefix_char):
            prefixed.append(stripped)
        else:
            prefixed.append(f"{prefix}{stripped}")

    return prefixed


def _word_count(text: str) -> int:
    return len(text.split())


class FileTeachingAnalyzer(dspy.Module):
    """Generate a teaching-focused summary using DSPy chains of thought."""

    def __init__(self, config: TeachingConfig | None = None) -> None:
        super().__init__()
        self.config = config or TeachingConfig()

        overview_signature = FileOverview.with_instructions(
            """
            Craft a thorough multi-section narrative that orients a senior learner.
            Describe the file's purpose, high-level architecture, main responsibilities,
            how data flows through each part, and any noteworthy patterns or dependencies.
            Aim for around five paragraphs that highlight why each section exists and
            how it contributes to the overall behavior.
            """
        )

        teachings_signature = TeachingPoints.with_instructions(
            """
            Extract every insight the learner would need for deep comprehension.
            Provide generous bullet lists (>=6 items when possible) covering concepts,
            workflows, pitfalls, integration guidance, and areas needing validation.
            When referencing identifiers, include the role they play.
            Prefer complete sentences that can stand alone in teaching materials.
            """
        )

        report_signature = TeachingReport.with_instructions(
            """
            Assemble a long-form teaching brief in Markdown. Include:
            - An opening context block with file path and intent.
            - Headed sections for overview, section walkthrough, key concepts, workflows,
              pitfalls, integration notes, tests/validation, and references.
            - Expand each bullet into full sentences or sub-bullets to help instructors
              speak to the content without the source file open.
            Ensure the report comfortably exceeds 400 words when source material allows.
            """
        )

        self.overview = dspy.ChainOfThought(
            overview_signature, **self.config.lm_args_for("overview")
        )
        self.teachings = dspy.ChainOfThought(
            teachings_signature, **self.config.lm_args_for("teachings")
        )

        base_report = dspy.ChainOfThought(
            report_signature, **self.config.lm_args_for("report")
        )

        if self.config.report_refine_attempts > 1:

            def report_length_reward(args: dict[str, Any], pred: dspy.Prediction) -> float:
                text = getattr(pred, "report_markdown", "") or ""
                words = _word_count(text)
                source_words = max(int(args.get("source_word_count", 0)), 0)

                dynamic_target = max(
                    self.config.report_min_word_count,
                    int(source_words * self.config.report_target_ratio),
                )

                soft_cap = max(
                    dynamic_target + 150,
                    int(source_words * self.config.report_soft_cap_ratio),
                )

                dynamic_cap = min(self.config.report_max_word_count, soft_cap)

                if words < dynamic_target:
                    return 0.0

                if words >= dynamic_cap:
                    return 1.0

                span = max(dynamic_cap - dynamic_target, 1)
                progress = (words - dynamic_target) / span
                return min(1.0, 0.6 + 0.4 * progress)

            self.report = dspy.Refine(
                module=base_report,
                N=self.config.report_refine_attempts,
                reward_fn=report_length_reward,
                threshold=self.config.report_reward_threshold,
            )
        else:
            self.report = base_report

    def forward(self, *, file_path: str, file_content: str) -> dspy.Prediction:
        overview_pred = self.overview(
            file_path=file_path,
            file_content=file_content,
        )

        teaching_pred = self.teachings(
            file_content=file_content,
        )

        overview_text = _ensure_text(
            getattr(overview_pred, "overview", None),
            "Overview unavailable.",
        )

        section_notes = _with_prefix(
            _ensure_list(
                getattr(overview_pred, "section_notes", None),
                "Section-level breakdown unavailable.",
                field_name="section_notes",
            ),
            self.config.section_bullet_prefix,
        )

        key_concepts = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "key_concepts", None),
                "Clarify core concepts manually.",
                field_name="key_concepts",
            ),
            self.config.section_bullet_prefix,
        )

        practical_steps = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "practical_steps", None),
                "Document workflow steps explicitly.",
                field_name="practical_steps",
            ),
            self.config.section_bullet_prefix,
        )

        pitfalls = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "pitfalls", None),
                "No pitfalls identified; review source for potential caveats.",
                field_name="pitfalls",
            ),
            self.config.section_bullet_prefix,
        )

        references = _with_prefix(
            _clean_list(
                getattr(teaching_pred, "references", None),
                field_name="references",
            ),
            self.config.section_bullet_prefix,
        )

        usage_patterns = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "usage_patterns", None),
                "Document how this file is applied in real flows.",
                field_name="usage_patterns",
            ),
            self.config.section_bullet_prefix,
        )

        key_functions = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "key_functions", None),
                "Identify primary interfaces and responsibilities manually.",
                field_name="key_functions",
            ),
            self.config.section_bullet_prefix,
        )

        code_walkthroughs = _ensure_list(
            getattr(teaching_pred, "code_walkthroughs", None),
            "Prepare short code walkthroughs for learners.",
            strip_entries=False,
            field_name="code_walkthroughs",
        )

        integration_notes = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "integration_notes", None),
                "Outline integration touchpoints manually.",
                field_name="integration_notes",
            ),
            self.config.section_bullet_prefix,
        )

        testing_focus = _with_prefix(
            _ensure_list(
                getattr(teaching_pred, "testing_focus", None),
                "Highlight testing priorities in a follow-up review.",
                field_name="testing_focus",
            ),
            self.config.section_bullet_prefix,
        )

        source_word_count = _word_count(file_content)

        report_pred = self.report(
            file_path=file_path,
            overview=overview_text,
            section_notes=section_notes,
            key_concepts=key_concepts,
            practical_steps=practical_steps,
            pitfalls=pitfalls,
            references=references,
            usage_patterns=usage_patterns,
            key_functions=key_functions,
            code_walkthroughs=code_walkthroughs,
            integration_notes=integration_notes,
            testing_focus=testing_focus,
            source_word_count=source_word_count,
        )

        return dspy.Prediction(
            overview=overview_pred,
            teachings=teaching_pred,
            report=report_pred,
            structured={
                "overview_text": overview_text,
                "section_notes": section_notes,
                "key_concepts": key_concepts,
                "practical_steps": practical_steps,
                "pitfalls": pitfalls,
                "references": references,
                "usage_patterns": usage_patterns,
                "key_functions": key_functions,
                "code_walkthroughs": code_walkthroughs,
                "integration_notes": integration_notes,
                "testing_focus": testing_focus,
            },
        )


--- dspy_file/file_helpers.py ---
# file_helpers.py - utilities for loading files and presenting DSPy results
from __future__ import annotations

import os
from pathlib import Path
from typing import Iterable

import dspy


# Directories that should never be traversed when collecting source files.
ALWAYS_IGNORED_DIRS: set[str] = {
    "__pycache__",
    ".git",
    ".hg",
    ".mypy_cache",
    ".pytest_cache",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    ".vscode",
    ".idea",
    "venv",
}

# Individual files or suffixes that should never be analyzed.
ALWAYS_IGNORED_FILES: set[str] = {".DS_Store"}
ALWAYS_IGNORED_SUFFIXES: set[str] = {".pyc", ".pyo"}


def _normalize_relative_parts(value: Path | str) -> tuple[str, ...]:
    """Return normalized path segments for relative comparisons."""

    text = str(value).replace("\\", "/").strip()
    if not text:
        return ()
    text = text.strip("/")
    if not text or text in {"", "."}:
        return ()

    parts: list[str] = []
    for segment in text.split("/"):
        if not segment or segment == ".":
            continue
        if segment == "..":
            if parts:
                parts.pop()
            continue
        parts.append(segment)
    return tuple(parts)


def _matches_excluded_parts(
    parts: tuple[str, ...],
    excluded_parts: set[tuple[str, ...]],
) -> bool:
    for excluded in excluded_parts:
        if len(parts) < len(excluded):
            continue
        if parts[: len(excluded)] == excluded:
            return True
    return False


def _normalize_excluded_dirs(exclude_dirs: Iterable[str] | None) -> set[tuple[str, ...]]:
    """Normalize raw exclude strings into comparable path segments."""

    normalized: set[tuple[str, ...]] = set()
    if not exclude_dirs:
        return normalized

    for raw in exclude_dirs:
        cleaned = raw.strip()
        if not cleaned:
            continue
        parts = _normalize_relative_parts(cleaned)
        if parts:
            normalized.add(parts)
    return normalized


def _relative_path_is_excluded(
    relative_path: Path,
    excluded_parts: set[tuple[str, ...]],
) -> bool:
    if not excluded_parts:
        return False
    parts = _normalize_relative_parts(relative_path)
    if not parts:
        return False
    return _matches_excluded_parts(parts, excluded_parts)


def resolve_file_path(raw_path: str) -> Path:
    """Expand user shortcuts and validate that the target file exists."""

    path = Path(raw_path).expanduser().resolve()
    if not path.exists():
        raise FileNotFoundError(f"File not found: {path}")
    if not path.is_file():
        raise IsADirectoryError(f"Expected a file path but received: {path}")
    return path


def _pattern_targets_hidden(pattern: str) -> bool:
    pattern = pattern.strip()
    if not pattern:
        return False
    normalized = pattern[2:] if pattern.startswith("./") else pattern
    return normalized.startswith(".") or "/." in normalized


def _should_skip_dir(name: str, *, ignore_hidden: bool) -> bool:
    if name in ALWAYS_IGNORED_DIRS:
        return True
    if ignore_hidden and name.startswith("."):
        return True
    return False


def _should_skip_file(name: str, *, ignore_hidden: bool) -> bool:
    if name in ALWAYS_IGNORED_FILES:
        return True
    if any(name.endswith(suffix) for suffix in ALWAYS_IGNORED_SUFFIXES):
        return True
    if ignore_hidden and name.startswith("."):
        return True
    return False


def _should_skip_relative_path(
    relative_path: Path,
    *,
    ignore_hidden: bool,
    excluded_parts: set[tuple[str, ...]] | None = None,
) -> bool:
    parts = _normalize_relative_parts(relative_path)
    if not parts:
        return False

    if excluded_parts and _matches_excluded_parts(parts, excluded_parts):
        return True

    # Check intermediate directories for ignore rules.
    for segment in parts[:-1]:
        if segment in ALWAYS_IGNORED_DIRS:
            return True
        if ignore_hidden and segment.startswith("."):
            return True

    return _should_skip_file(parts[-1], ignore_hidden=ignore_hidden)


def collect_source_paths(
    raw_path: str,
    *,
    recursive: bool = True,
    include_globs: Iterable[str] | None = None,
    exclude_dirs: Iterable[str] | None = None,
) -> list[Path]:
    """Resolve a single file or directory into an ordered list of file paths."""

    path = Path(raw_path).expanduser().resolve()
    if not path.exists():
        raise FileNotFoundError(f"Target not found: {path}")

    if path.is_file():
        return [path]

    if not path.is_dir():
        raise IsADirectoryError(f"Expected file or directory path but received: {path}")

    candidates: set[Path] = set()
    patterns = list(include_globs) if include_globs else None
    allow_hidden = any(_pattern_targets_hidden(pattern) for pattern in patterns) if patterns else False
    ignore_hidden = not allow_hidden
    excluded_parts = _normalize_excluded_dirs(exclude_dirs)

    if patterns:
        for pattern in patterns:
            for candidate in path.glob(pattern):
                if not candidate.is_file():
                    continue

                relative_candidate = candidate.relative_to(path)
                if _should_skip_relative_path(
                    relative_candidate,
                    ignore_hidden=ignore_hidden,
                    excluded_parts=excluded_parts,
                ):
                    continue

                candidates.add(candidate.resolve())
    else:
        for root_dir, dirnames, filenames in os.walk(path):
            root_path = Path(root_dir)
            relative_root = Path(".") if root_path == path else root_path.relative_to(path)

            if not recursive and root_path != path:
                dirnames[:] = []
                continue

            if _relative_path_is_excluded(relative_root, excluded_parts):
                dirnames[:] = []
                continue

            dirnames[:] = sorted(
                name
                for name in dirnames
                if not _should_skip_dir(name, ignore_hidden=ignore_hidden)
                and not _relative_path_is_excluded(relative_root / name, excluded_parts)
            )

            for filename in filenames:
                candidate = root_path / filename
                relative_candidate = candidate.relative_to(path)

                if _should_skip_relative_path(
                    relative_candidate,
                    ignore_hidden=ignore_hidden,
                    excluded_parts=excluded_parts,
                ):
                    continue

                candidates.add(candidate.resolve())

    return sorted(candidates)


def _strip_front_matter(text: str) -> str:
    if not text.startswith("---"):
        return text
    end_idx = text.find("\n---", 3)
    if end_idx == -1:
        return text
    return text[end_idx + 4 :]


def _trim_to_first_heading(text: str) -> str:
    lines = text.splitlines()
    for idx, line in enumerate(lines):
        if line.lstrip().startswith("#"):
            return "\n".join(lines[idx:])
    return text


def read_file_content(path: Path) -> str:
    """Read file contents using utf-8 and fall back to latin-1 if needed."""

    try:
        raw = path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        raw = path.read_text(encoding="latin-1")

    cleaned = _strip_front_matter(raw)
    cleaned = _trim_to_first_heading(cleaned)
    return cleaned


def _ensure_trailing_newline(text: str) -> str:
    return text if text.endswith("\n") else text + "\n"


def _teaching_output(result: dspy.Prediction) -> str:
    try:
        report = result.report.report_markdown  # type: ignore[attr-defined]
    except AttributeError:
        report = "# Teaching Brief\n\nThe DSPy pipeline did not produce a report."
    return _ensure_trailing_newline(report)


def _refactor_output(result: dspy.Prediction) -> str:
    template = getattr(result, "template_markdown", None)
    if not template:
        template = getattr(getattr(result, "template", None), "template_markdown", None)
    text = str(template).strip() if template else ""
    if not text:
        text = "# Refactor Template\n\nTemplate generation failed."
    return _ensure_trailing_newline(text)


def render_prediction(result: dspy.Prediction, *, mode: str = "teach") -> str:
    """Return the generated markdown for the selected analysis mode."""

    if mode == "refactor":
        return _refactor_output(result)
    return _teaching_output(result)


--- dspy_file/__init__.py ---
"""DSPy file teaching analyzer package."""

from .file_analyzer import FileTeachingAnalyzer, TeachingConfig
from .refactor_analyzer import FileRefactorAnalyzer, RefactorTeachingConfig

__all__ = [
    "FileTeachingAnalyzer",
    "TeachingConfig",
    "FileRefactorAnalyzer",
    "RefactorTeachingConfig",
]


--- dspy_file/refactor_analyzer.py ---
# refactor_analyzer.py - DSPy module that prepares per-file refactor prompt templates
from __future__ import annotations

from dataclasses import dataclass, field
from functools import lru_cache
from typing import Any

import dspy

from .prompts import load_prompt_text


class RefactorTemplateSignature(dspy.Signature):
    """Generate a reusable refactor prompt template from a source document."""

    file_path: str = dspy.InputField(desc="Path to the source file for context")
    file_content: str = dspy.InputField(desc="Full raw text of the file")

    template_markdown: str = dspy.OutputField(
        desc="Markdown template with numbered placeholders and section scaffolding"
    )


@dataclass
class RefactorTeachingConfig:
    """Configuration for the refactor template generator."""

    max_tokens: int = 8000
    temperature: float | None = 0.7
    top_p: float | None = 0.80
    n_completions: int | None = None
    extra_lm_kwargs: dict[str, Any] = field(default_factory=dict)

    def lm_kwargs(self) -> dict[str, Any]:
        """Return the language model arguments for DSPy modules."""

        kwargs: dict[str, Any] = {**self.extra_lm_kwargs, "max_tokens": self.max_tokens}
        if self.temperature is not None:
            kwargs["temperature"] = self.temperature
        if self.top_p is not None:
            kwargs["top_p"] = self.top_p
        if self.n_completions is not None:
            kwargs["n"] = self.n_completions
        return kwargs


@lru_cache(maxsize=1)
def _load_default_template() -> str:
    """Load the bundled refactor prompt template text."""

    return load_prompt_text(None).strip()


def _ensure_template_text(value: str | None) -> str:
    if value and value.strip():
        text = value.rstrip()
    else:
        text = "# Refactor Template\n\nTemplate generation failed."
    return text if text.endswith("\n") else text + "\n"


class FileRefactorAnalyzer(dspy.Module):
    """Generate a refactor-focused prompt template for a single file."""

    def __init__(
        self,
        *,
        template_text: str | None = None,
        config: RefactorTeachingConfig | None = None,
    ) -> None:
        super().__init__()
        self.config = config or RefactorTeachingConfig()
        instructions = template_text.strip() if template_text else _load_default_template()
        signature = RefactorTemplateSignature.with_instructions(instructions)
        self.generator = dspy.ChainOfThought(signature, **self.config.lm_kwargs())

    def forward(self, *, file_path: str, file_content: str) -> dspy.Prediction:
        raw_prediction = self.generator(
            file_path=file_path,
            file_content=file_content,
        )

        template_markdown = _ensure_template_text(
            getattr(raw_prediction, "template_markdown", None)
        )

        return dspy.Prediction(
            template=raw_prediction,
            template_markdown=template_markdown,
        )


--- dspy_file/signatures.py ---
# signatures.py - DSPy signatures focused on extracting teachings from a single file
from typing import List

import dspy


class FileOverview(dspy.Signature):
    """Summarize the file structure and core narrative with room for depth."""

    file_path: str = dspy.InputField(desc="Path to the source file")
    file_content: str = dspy.InputField(desc="Full raw text of the file")

    overview: str = dspy.OutputField(
        desc="Detailed multi-section overview (aim for 4-6 paragraphs capturing scope, intent, and flow)"
    )
    section_notes: List[str] = dspy.OutputField(
        desc="Comprehensive bullet list summarizing each major section, include headings when possible"
    )


class TeachingPoints(dspy.Signature):
    """Extract teachable concepts, workflows, and cautions."""

    file_content: str = dspy.InputField(desc="Full raw text of the file")

    key_concepts: List[str] = dspy.OutputField(desc="Essential ideas learners must retain")
    practical_steps: List[str] = dspy.OutputField(desc="Actionable steps or workflows described")
    pitfalls: List[str] = dspy.OutputField(desc="Warnings, gotchas, or misconceptions to avoid")
    references: List[str] = dspy.OutputField(desc="Follow-up links, exercises, or related material")
    usage_patterns: List[str] = dspy.OutputField(
        desc="Common usage patterns, scenarios, or recipes that appear"
    )
    key_functions: List[str] = dspy.OutputField(
        desc="Important functions, classes, or hooks with quick rationale"
    )
    code_walkthroughs: List[str] = dspy.OutputField(
        desc="Short code snippets or walkthroughs learners should discuss"
    )
    integration_notes: List[str] = dspy.OutputField(
        desc="Guidance for connecting this file with the rest of the system"
    )
    testing_focus: List[str] = dspy.OutputField(
        desc="Areas that need tests, validations, or monitoring"
    )


class TeachingReport(dspy.Signature):
    """Compose a concise but comprehensive markdown teaching brief."""

    file_path: str = dspy.InputField(desc="Original file path for context header")
    overview: str = dspy.InputField()
    section_notes: List[str] = dspy.InputField()
    key_concepts: List[str] = dspy.InputField()
    practical_steps: List[str] = dspy.InputField()
    pitfalls: List[str] = dspy.InputField()
    references: List[str] = dspy.InputField()
    usage_patterns: List[str] = dspy.InputField()
    key_functions: List[str] = dspy.InputField()
    code_walkthroughs: List[str] = dspy.InputField()
    integration_notes: List[str] = dspy.InputField()
    testing_focus: List[str] = dspy.InputField()

    report_markdown: str = dspy.OutputField(desc="Final markdown document capturing key teachings")


--- dspy_file/prompts/code-fence.md ---
# Command: /markdown:wrap-md-fence

# Usage: /markdown:wrap-md-fence "your content here"

# Args

# - {{content}}: raw bytes to wrap verbatim inside the fence

prompt = """
Wrap the provided {{content}} verbatim with a Markdown code fence labeled md.

Rules:

* Zero changes to {{content}} (byte-for-byte).
* Preserve encoding, line endings, and terminal newline presence/absence.
* No additional output or whitespace outside the fence.

Output exactly:

```md
{{content}}
```

Acceptance:

* Inner bytes are identical to {{content}}.
* Only the opening line `md and the closing` are added.
  """


--- dspy_file/prompts/front-matter-v2.md ---
<!-- $1 = source Markdown text; $2 = template name/title (optional; infer if missing); $3 = maximum placeholders allowed (1–9; default 7); $4 = input parameters block; $5 = controlled taxonomy/list block; $6 = stage mapping/rules block; $7 = output examples block -->

# $2

Task: Given $1, produce a structured **metadata block** and then emit the original body unchanged. The metadata must expose identifiers, categories, optional lifecycle/stage, optional dependencies, optional provided artifacts, and a concise summary. Output = metadata, blank line, then $1.

## Inputs

$4

## Canonical taxonomy (exact strings)

$5

### Stage hints (for inference)

$6

## Algorithm

1. Extract signals from $1

   * Titles/headings, imperative verbs, intent sentences, explicit tags, and dependency phrasing.

2. Determine the primary identifier

   * Prefer explicit input; otherwise infer from main action + object.
   * Normalize (lowercase, kebab-case, length-capped, starts with a letter).
   * De-duplicate.

3. Determine categories

   * Prefer explicit input; otherwise infer from verbs/headings vs $5.
   * Validate, sort deterministically, and de-dupe (≤3).

4. Determine lifecycle/stage (optional)

   * Prefer explicit input; otherwise map categories via $6.
   * Omit if uncertain.

5. Determine dependencies (optional)

   * Parse phrases implying order or prerequisites; keep id-shaped items (≤5).

6. Determine provided artifacts (optional)

   * Short list (≤3) of unlocked outputs.

7. Compose summary

   * One sentence (≤120 chars): “Do <verb> <object> to achieve <outcome>.”

8. Produce metadata in the requested format

   * Default to a human-readable serialization; honor any requested alternative.

9. Reconcile if input already contains metadata

   * Merge: explicit inputs > existing > inferred.
   * Validate lists; move unknowns to an extension field if needed.
   * Remove empty keys.

## Assumptions & Constraints

* Emit exactly one document: metadata, a single blank line, then $1.
* Limit distinct placeholders to ≤ $3.

## Validation

* Identifier matches a normalized id pattern.
* Categories non-empty and drawn from $5 (≤3).
* Stage, if present, is one of the allowed stages implied by $6.
* Dependencies, if present, are id-shaped (≤5).
* Summary ≤120 chars; punctuation coherent.
* Body text $1 is not altered.

## Output format examples

$7


--- dspy_file/prompts/gemini-cli-command_prompt-template.md ---
# gemini-cli-command_prompt-template_v2

Task: From {prompt_or_md} and {user_context}, synthesize a Gemini CLI custom command TOML that generalizes the task via inferred placeholders.

Heuristics

1) Mark user-changeable text (queries, titles) → {{query}}
2) Paths/globs → {{path}}
3) Targets/entities (repo/service/ticket) → {{target}}
4) Quantities/limits → {{limit}}
If uncertainty remains, collapse everything into {{args}}. Never exceed 3 distinct placeholders unless the text explicitly lists more inputs.

Transformations

- Preserve order and intent; remove fluff.
- If the source includes code or CLI calls, render them as `!{...}` lines inside the prompt.
- Prefer imperative voice (“Do X, then Y”). Add brief, testable acceptance lines inside the prompt if helpful.

Deliverable (only TOML; no extra prose)

# Command: /{namespace:=user}:{command:=auto-from-title}

# Usage: /{namespace}:{command} "example value(s)"

# Args

# - {{query}}: what to search or summarize

# - {{path}}: file or directory (optional)

prompt = """
<final instruction with placeholders and any !{...} inserts>
"""

Checks

- Valid TOML; balanced triple quotes.
- Includes ≥1 placeholder with matching Usage.
- Names are lowercase, kebab-case.
- No commentary outside TOML.


--- dspy_file/prompts/gemini-cli_extension-command_prompt-template.md ---
# gemini-cli_commands-prompt-template_v1

Goal: Produce a parameterized template from {source_text} using positional placeholders and a machine-checkable Arg spec.

Requirements

- Use {placeholder_syntax} (default: `$1..$9`).
- Insert ≤ {max_placeholders} high-impact placeholders; deduplicate repeats.
- Emit JSON Arg spec immediately after the templated text, with this shape:
    {
      "args": [
        { "id": "$1", "name": "{name}", "hint": "{short_hint}", "example": "{example}", "required": true, "validate": "{regex|rule}" }
      ]
    }
- Preserve markdown/code formatting byte-for-byte except at replacement spans.
- Do not change meaning, tone, or constraints of {source_text}.

Heuristics (apply in order)

1) User-owned identifiers: paths, repo/org names, endpoints, secrets placeholders.
2) Content slots: problem statement, target audience/domain, primary input.
3) Tunables: N/limits/timeouts only if not hard requirements.
4) Repeated literals → one arg; propagate to all occurrences.
5) Skip boilerplate constants (e.g., license names, standard flags) unless context marks them variable.

Edge cases

- If already templated, extend only with missing args; do not renumber existing placeholders.
- If no clear candidates, introduce `$1` as `topic_or_input` at the primary noun phrase of the opening sentence and document it.
- For JSON/YAML in code fences, ensure placeholders remain valid strings (quote if needed).

Acceptance tests (must pass)

- T1: All placeholders appear in the Arg spec; counts match.
- T2: Substituting provided examples yields valid markdown and runnable snippets.
- T3: Repeated concepts map to a single placeholder consistently.
- T4: Total placeholders ≤ {max_placeholders}; none are trivial.

Deliverables

1) **Templated Text** — {source_text} with placeholders inserted per heuristics.
2) **Args JSON** — machine-checkable spec as shown above.


--- tests/conftest.py ---
from __future__ import annotations

import os
from pathlib import Path


_CACHE_DIR = Path(__file__).parent / ".dspy-cache"
_CACHE_DIR.mkdir(exist_ok=True)

os.environ.setdefault("DISKCACHE_DEFAULT_DIRECTORY", str(_CACHE_DIR))
os.environ.setdefault("DSPY_CACHE_DIR", str(_CACHE_DIR))
os.environ.setdefault("DSPY_CACHEDIR", str(_CACHE_DIR))


--- tests/smoke_test.py ---
from __future__ import annotations

from pathlib import Path
from types import SimpleNamespace
from unittest import mock

from dspy_file import analyze_file_cli


class DummyAnalyzer:
    def __init__(self) -> None:
        self.calls: list[tuple[str, str]] = []

    def __call__(self, *, file_path: str, file_content: str):  # type: ignore[override]
        self.calls.append((file_path, file_content))
        return SimpleNamespace(
            report=SimpleNamespace(
                report_markdown="# Teaching Brief\n\n- Generated by DummyAnalyzer."
            )
        )


def test_analyze_path_writes_markdown(tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("# Title\n\nSome content", encoding="utf-8")

    output_dir = tmp_path / "reports"
    dummy = DummyAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileTeachingAnalyzer", return_value=dummy
    ):
        exit_code = analyze_file_cli.analyze_path(
            str(source),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=False,
            exclude_dirs=None,
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.TEACH,
            prompt_text=None,
        )

    assert exit_code == 0
    assert dummy.calls == [(str(source), "# Title\n\nSome content")]

    generated_files = list(output_dir.glob("*.md"))
    assert len(generated_files) == 1
    generated_text = generated_files[0].read_text(encoding="utf-8")
    assert generated_text.startswith("# Teaching Brief\n")
    assert generated_text.endswith("\n")


@mock.patch.object(analyze_file_cli, "_confirm_analyze", return_value=False)
def test_analyze_path_confirm_each_skips_when_declined(confirm_mock: mock.Mock, tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("# Title\n\nSome content", encoding="utf-8")

    output_dir = tmp_path / "reports"
    dummy = DummyAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileTeachingAnalyzer", return_value=dummy
    ):
        exit_code = analyze_file_cli.analyze_path(
            str(source),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=True,
            exclude_dirs=None,
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.TEACH,
            prompt_text=None,
        )

    assert exit_code == 0
    assert dummy.calls == []
    assert list(output_dir.glob("*.md")) == []
    confirm_mock.assert_called_once()


def test_analyze_path_respects_exclude_dirs(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    include_dir = project_root / "include"
    skip_dir = project_root / "skip"
    include_dir.mkdir(parents=True)
    skip_dir.mkdir(parents=True)

    include_file = include_dir / "keep.py"
    skip_file = skip_dir / "ignore.py"
    include_file.write_text("print('keep')\n", encoding="utf-8")
    skip_file.write_text("print('ignore')\n", encoding="utf-8")

    output_dir = tmp_path / "reports"
    dummy = DummyAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileTeachingAnalyzer", return_value=dummy
    ):
        exit_code = analyze_file_cli.analyze_path(
            str(project_root),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=False,
            exclude_dirs=["skip"],
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.TEACH,
            prompt_text=None,
        )

    assert exit_code == 0
    assert dummy.calls == [(str(include_file), "print('keep')\n")]
    generated = list(output_dir.glob("*.md"))
    assert len(generated) == 1
    assert "ignore" not in generated[0].read_text(encoding="utf-8")


def test_analyze_path_refactor_mode_uses_refactor_analyzer(tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("# Title\n\nSome content", encoding="utf-8")

    output_dir = tmp_path / "reports"

    class DummyRefactorAnalyzer:
        def __init__(self) -> None:
            self.calls: list[tuple[str, str]] = []

        def __call__(self, *, file_path: str, file_content: str):  # type: ignore[override]
            self.calls.append((file_path, file_content))
            return SimpleNamespace(template_markdown="# Template\n\nValue.")

    dummy = DummyRefactorAnalyzer()

    with mock.patch.object(
        analyze_file_cli, "FileRefactorAnalyzer", return_value=dummy
    ) as refactor_cls:
        exit_code = analyze_file_cli.analyze_path(
            str(source),
            raw=False,
            recursive=True,
            include_globs=None,
            confirm_each=False,
            exclude_dirs=None,
            output_dir=output_dir,
            mode=analyze_file_cli.AnalysisMode.REFACTOR,
            prompt_text="Custom prompt",
        )

    assert exit_code == 0
    assert dummy.calls == [(str(source), "# Title\n\nSome content")]
    generated_files = list(output_dir.glob("*.refactor.md"))
    assert len(generated_files) == 1
    assert generated_files[0].read_text(encoding="utf-8").endswith("Value.\n")
    refactor_cls.assert_called_once()
    assert refactor_cls.call_args.kwargs["template_text"] == "Custom prompt"


def test_resolve_prompt_text_with_menu(tmp_path: Path) -> None:
    prompt_one = tmp_path / "first.md"
    prompt_two = tmp_path / "second.md"
    prompt_one.write_text("First template", encoding="utf-8")
    prompt_two.write_text("Second template", encoding="utf-8")

    options = [
        analyze_file_cli.PromptTemplate(name="first", path=prompt_one),
        analyze_file_cli.PromptTemplate(name="second", path=prompt_two),
    ]

    with mock.patch.object(
        analyze_file_cli, "list_bundled_prompts", return_value=options
    ), mock.patch("builtins.input", side_effect=["2"]):
        text = analyze_file_cli._resolve_prompt_text(None)

    assert text == "Second template"


def test_resolve_prompt_text_with_explicit_path(tmp_path: Path) -> None:
    prompt_path = tmp_path / "custom.md"
    prompt_path.write_text("Custom prompt text", encoding="utf-8")

    text = analyze_file_cli._resolve_prompt_text(str(prompt_path))

    assert text == "Custom prompt text"


def test_parser_short_options_are_supported() -> None:
    parser = analyze_file_cli.build_parser()
    args = parser.parse_args(
        [
            "sample.md",
            "-r",
            "-m",
            "refactor",
            "-nr",
            "-g",
            "**/*.py",
            "-g",
            "**/*.md",
            "-i",
            "-ed",
            "skip,temp",
            "-o",
            "reports",
            "-p",
            "custom-prompt",
        ]
    )

    assert args.path == "sample.md"
    assert args.raw is True
    assert args.mode == "refactor"
    assert args.non_recursive is True
    assert args.include_globs == ["**/*.py", "**/*.md"]
    assert args.confirm_each is True
    assert args.exclude_dirs == ["skip,temp"]
    assert args.output_dir == "reports"
    assert args.prompt == "custom-prompt"


--- tests/test_cli_connectivity.py ---
from __future__ import annotations

from pathlib import Path
from unittest import mock

import pytest

from dspy_file import analyze_file_cli


def test_probe_openai_provider_success() -> None:
    mock_response = mock.MagicMock()
    urlopen_mock = mock.MagicMock()
    urlopen_mock.return_value.__enter__.return_value = mock_response

    with mock.patch(
        "dspy_file.analyze_file_cli.request.urlopen", urlopen_mock
    ):
        analyze_file_cli._probe_openai_provider("http://localhost:1234/v1", "token")

    urlopen_mock.assert_called_once()


def test_probe_openai_provider_raises_on_failure() -> None:
    with mock.patch(
        "dspy_file.analyze_file_cli.request.urlopen",
        side_effect=analyze_file_cli.urlerror.URLError("connection refused"),
    ):
        with pytest.raises(analyze_file_cli.ProviderConnectivityError):
            analyze_file_cli._probe_openai_provider("http://localhost:1234/v1", "token")


def test_main_exits_early_when_lmstudio_unreachable(tmp_path: Path) -> None:
    source = tmp_path / "example.md"
    source.write_text("content", encoding="utf-8")

    with mock.patch.object(
        analyze_file_cli,
        "_probe_openai_provider",
        side_effect=analyze_file_cli.ProviderConnectivityError("unreachable"),
    ), mock.patch.object(analyze_file_cli, "configure_model") as configure_mock:
        exit_code = analyze_file_cli.main(
            ["--provider", "lmstudio", str(source)]
        )

    assert exit_code == 1
    configure_mock.assert_not_called()


--- tests/test_file_helpers.py ---
from __future__ import annotations

from pathlib import Path
from types import SimpleNamespace

from dspy_file.file_helpers import collect_source_paths, render_prediction


def _touch(path: Path, content: str = "") -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(content, encoding="utf-8")


def test_collect_source_paths_skips_hidden_and_config_files(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    project_root.mkdir()

    allowed_file = project_root / "main.py"
    _touch(allowed_file, "print('ok')\n")

    _touch(project_root / ".env", "SECRET=1\n")
    _touch(project_root / ".venv" / "lib" / "ignore.py", "print('ignored')\n")
    _touch(project_root / "nested" / ".secrets", "hidden\n")

    collected = collect_source_paths(str(project_root))

    assert collected == [allowed_file.resolve()]


def test_hidden_files_can_be_included_with_explicit_glob(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    project_root.mkdir()

    hidden_file = project_root / ".config" / "ci.yml"
    _touch(hidden_file, "name: ci\n")

    collected = collect_source_paths(
        str(project_root),
        include_globs=[".config/**/*.yml"],
    )

    assert collected == [hidden_file.resolve()]


def test_collect_source_paths_honors_exclude_dirs(tmp_path: Path) -> None:
    project_root = tmp_path / "project"
    project_root.mkdir()

    kept = project_root / "keep" / "file.py"
    skipped = project_root / "skip" / "ignored.py"
    nested_skip = project_root / "nested" / "deep" / "hidden.py"
    _touch(kept, "print('keep')\n")
    _touch(skipped, "print('ignore')\n")
    _touch(nested_skip, "print('nested ignore')\n")

    collected = collect_source_paths(
        str(project_root),
        exclude_dirs=["skip", "nested/deep"],
    )
    collected_glob = collect_source_paths(
        str(project_root),
        include_globs=["**/*.py"],
        exclude_dirs=["skip", "nested/deep"],
    )

    assert collected == [kept.resolve()]
    assert collected_glob == [kept.resolve()]


def test_render_prediction_teach_mode_uses_report() -> None:
    prediction = SimpleNamespace(
        report=SimpleNamespace(report_markdown="# Brief\n\nContent."),
    )
    output = render_prediction(prediction, mode="teach")
    assert output.endswith("Content.\n")


def test_render_prediction_refactor_mode_prefers_template_markdown() -> None:
    prediction = SimpleNamespace(template_markdown="# Template\n\nValue.")
    output = render_prediction(prediction, mode="refactor")
    assert output.endswith("Value.\n")
