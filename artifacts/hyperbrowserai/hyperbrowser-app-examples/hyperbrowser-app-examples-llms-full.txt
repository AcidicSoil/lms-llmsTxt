# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- scrape-to-api/README.md ---
# Turn Any Webpage into REST API ðŸš€

Transform any webpage into a fully-functional REST API with just a few clicks using [Hyperbrowser](https://hyperbrowser.ai). No coding required - just point, click, and get your API endpoint!

## âœ¨ Features

- ðŸŽ¯ **Visual Selection**: Point and click to select any elements from any webpage
- ðŸ”„ **Instant API Generation**: Get a REST API endpoint instantly with your selected data
- ðŸ“š **Complete Documentation**: Auto-generated OpenAPI specs, TypeScript SDK, and Postman collection
- ðŸŽ¨ **Multiple Data Types**: Extract text, links, images, and HTML content
- ðŸ“¦ **Structured Data**: Get clean, structured JSON responses
- âš¡ **Real-time Preview**: See your API response as you select elements

## ðŸš€ Getting Started

1. **Get Your API Key**
   - Visit [hyperbrowser.ai](https://hyperbrowser.ai)
   - Sign up for a free account
   - Get your API key from the dashboard

2. **Setup Environment**
   ```bash
   # Clone this repository
   git clone https://github.com/yourusername/webpage-to-api
   
   # Install dependencies
   npm install
   
   # Create .env.local file and add your API key
   echo "HYPERBROWSER_API_KEY=your_api_key_here" > .env.local
   
   # Start the development server
   npm run dev
   ```

3. **Start Converting!**
   - Open `http://localhost:3000` in your browser
   - Enter any webpage URL
   - Select elements you want in your API
   - Click "Generate API" and get your endpoint!

## ðŸŽ® How It Works

1. **Enter a URL**: Start by entering any public webpage URL
2. **Select Elements**: Use the visual selector to pick elements you want in your API
3. **Configure Fields**: Name your fields and choose between single or multiple selections
4. **Generate API**: Click generate and get your API endpoint instantly!

## ðŸ“¦ What You Get

- ðŸ”— **REST API Endpoint**: Ready-to-use API endpoint
- ðŸ“˜ **OpenAPI Spec**: Standard OpenAPI/Swagger documentation
- ðŸ’» **TypeScript SDK**: Type-safe SDK for your API
- ðŸ§ª **Postman Collection**: Pre-configured API testing collection
- ðŸ“¦ **Complete Bundle**: All files bundled together for easy download

## ðŸ› ï¸ Built With

- [Hyperbrowser](https://hyperbrowser.ai) - The powerful browser automation platform
- Next.js 14 - React Framework
- TailwindCSS - Styling
- TypeScript - Type Safety

## ðŸ”‘ API Key Security

Your Hyperbrowser API key is sensitive! Make sure to:
- Never commit your `.env.local` file
- Keep your API key secret
- Use environment variables in production

## ðŸ“ License

MIT License - feel free to use this in your projects!

## ðŸ™‹â€â™‚ï¸ Support

Need help? Check out:
- [Hyperbrowser Documentation](https://hyperbrowser.ai/docs)
- [Discord Community](https://discord.gg/hyperbrowser)
- [GitHub Issues](https://github.com/yourusername/webpage-to-api/issues)

---

Built with â¤ï¸ using [Hyperbrowser](https://hyperbrowser.ai)


## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)
- [hyperbrowser.ai](https://hyperbrowser.ai)
- [Hyperbrowser Documentation](https://hyperbrowser.ai/docs)
- [Discord Community](https://discord.gg/hyperbrowser)
- [GitHub Issues](https://github.com/yourusername/webpage-to-api/issues)

--- scrape-to-api/next.config.js ---
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    serverComponentsExternalPackages: ['@hyperbrowser/sdk', 'puppeteer-core']
  },
  webpack: (config, { isServer }) => {
    if (!isServer) {
      // Don't bundle server-only modules for client
      config.resolve.fallback = {
        ...config.resolve.fallback,
        fs: false,
        net: false,
        tls: false,
        dns: false,
        'node:assert': false,
        'node:child_process': false,
        'node:fs/promises': false,
        'node:path': false,
        'node:url': false,
        'node:util': false,
        'node:stream': false,
        'node:buffer': false,
        'node:crypto': false,
        'node:os': false,
        'node:events': false,
        'node:querystring': false,
        'node:http': false,
        'node:https': false,
        'node:zlib': false,
        'child_process': false,
        'assert': false,
        'path': false,
        'url': false,
        'util': false,
        'stream': false,
        'buffer': false,
        'crypto': false,
        'os': false,
        'events': false,
        'querystring': false,
        'http': false,
        'https': false,
        'zlib': false,
      };
      
      // Exclude server-only packages from client bundle
      config.externals = config.externals || [];
      config.externals.push(
        '@hyperbrowser/sdk',
        'puppeteer-core',
        'cheerio',
        'jsdom',
        'archiver'
      );
    }
    return config;
  },
};

module.exports = nextConfig;


--- scrape-to-api/tailwind.config.js ---
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './pages/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    './app/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {
      colors: {
        accent: '#F0FF26',
        terminal: '#000000',
      },
      fontFamily: {
        sans: ['system-ui', '-apple-system', 'sans-serif'],
        mono: ['ui-monospace', 'SF Mono', 'Monaco', 'monospace'],
      },
      fontWeight: {
        normal: '500',
        medium: '500',
        semibold: '600',
        bold: '700',
      },
      letterSpacing: {
        tight4: '-0.04em',
      },
      animation: {
        'fade-in': 'fadeIn 0.5s ease-out',
      },
      keyframes: {
        fadeIn: {
          '0%': { opacity: '0', transform: 'translateY(10px)' },
          '100%': { opacity: '1', transform: 'translateY(0)' },
        },
      },
      backdropBlur: {
        md: '12px',
      },
    },
  },
  plugins: [],
} 

--- openai-source-forge/lib/classifier.ts ---
import OpenAI from 'openai'

export type QuestionType = 'technical' | 'research' | 'medical'

export interface QuestionClassification {
  type: QuestionType
  confidence: number
  reasoning: string
}

function getOpenAIClient(): OpenAI {
  if (!process.env.OPENAI_API_KEY) {
    throw new Error('OpenAI API key not configured')
  }
  
  return new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  })
}

export async function classifyQuestion(query: string): Promise<QuestionClassification> {
  const openai = getOpenAIClient()
  
  const systemPrompt = `You are an expert question classifier. Your task is to classify questions into one of three categories:

1. "technical" - Programming, software development, API usage, coding problems, web development, system architecture, etc.
2. "research" - Academic research, scientific studies, literature reviews, theoretical concepts, scholarly topics, etc.
3. "medical" - Medical conditions, treatments, healthcare, pharmaceuticals, clinical studies, medical procedures, etc.

Respond with a JSON object containing:
- type: one of "technical", "research", or "medical"
- confidence: a number between 0 and 1 representing your confidence
- reasoning: a brief explanation of your classification

Examples:
- "How to implement OAuth2 in Node.js?" â†’ technical
- "What are the latest findings on Alzheimer's disease?" â†’ medical
- "Impact of climate change on marine ecosystems" â†’ research
- "Best practices for React performance optimization" â†’ technical
- "Effectiveness of meditation on anxiety disorders" â†’ medical
- "Historical analysis of Roman Empire expansion" â†’ research`

  const userPrompt = `Classify this question: "${query}"`

  try {
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini', // Using the cheaper model for classification
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature: 0.3,
      max_tokens: 200,
    })

    const response = completion.choices[0]?.message?.content || ''
    const classification = JSON.parse(response) as QuestionClassification
    
    // Validate the response
    if (!['technical', 'research', 'medical'].includes(classification.type)) {
      throw new Error('Invalid classification type')
    }
    
    if (classification.confidence < 0 || classification.confidence > 1) {
      throw new Error('Invalid confidence score')
    }
    
    return classification
    
  } catch (error) {
    console.error('Classification error:', error)
    // Default to technical if classification fails
    return {
      type: 'technical',
      confidence: 0.5,
      reasoning: 'Classification failed, defaulting to technical'
    }
  }
} 

--- scrape-to-api/lib/client-utils.ts ---
// Client-safe utility functions (no server-only dependencies)

interface ValidationResult {
  valid: boolean;
  count: number;
  error?: string;
}

// Validate CSS selector against HTML content
export function validateSelector(selector: string, html: string): ValidationResult {
  try {
    // Create a temporary DOM element to test the selector
    const parser = new DOMParser();
    const doc = parser.parseFromString(html, 'text/html');
    
    // Try to query the selector
    const elements = doc.querySelectorAll(selector);
    
    return {
      valid: elements.length > 0,
      count: elements.length
    };
  } catch (error) {
    return {
      valid: false,
      count: 0,
      error: error instanceof Error ? error.message : 'Invalid selector'
    };
  }
}

// Generate CSS selector for a clicked element (simple client-safe version)
export function generateSelectorForElement(element: HTMLElement): string {
  try {
    // Priority: id > unique class > tag with nth-child
    
    // Check for ID
    if (element.id) {
      return `#${element.id}`;
    }
    
    // Check for unique class
    const classes = element.className.split(' ').filter(cls => cls.trim());
    if (classes.length > 0) {
      for (const cls of classes) {
        const selector = `.${cls}`;
        const doc = element.ownerDocument;
        if (doc && doc.querySelectorAll(selector).length === 1) {
          return selector;
        }
      }
    }
    
    // Build a path from the element to the root
    const path = [];
    let current: HTMLElement | null = element;
    
    while (current && current.nodeType === Node.ELEMENT_NODE) {
      let selector = current.tagName.toLowerCase();
      
      // Add index if there are siblings with the same tag
      if (current.parentNode) {
        const siblings = Array.from(current.parentNode.children);
        const sameTagSiblings = siblings.filter(sibling => 
          sibling.tagName.toLowerCase() === selector
        );
        
        if (sameTagSiblings.length > 1) {
          const index = sameTagSiblings.indexOf(current) + 1;
          selector += `:nth-child(${index})`;
        }
      }
      
      path.unshift(selector);
      current = current.parentElement;
      
      // Don't go beyond body
      if (current && current.tagName.toLowerCase() === 'body') {
        break;
      }
    }
    
    return path.join(' > ');
  } catch (error) {
    console.error('Error generating selector:', error);
    // Fallback to a simple selector
    return element.tagName.toLowerCase() + (element.id ? `#${element.id}` : '');
  }
}

// Clean selector name for API field (client-safe version)
export function cleanSelectorName(text: string): string {
  return text
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, '_')
    .replace(/^_+|_+$/g, '')
    .substring(0, 50) || 'field';
}

// Get suggested name for a selector based on element content (client-safe version)
export function suggestSelectorName(selector: string, html: string): string {
  try {
    const parser = new DOMParser();
    const doc = parser.parseFromString(html, 'text/html');
    const element = doc.querySelector(selector);
    
    if (!element) return 'field';
    
    // Try to get a meaningful name from various attributes
    const id = element.getAttribute('id');
    const className = element.getAttribute('class');
    const text = element.textContent?.trim();
    const tagName = element.tagName?.toLowerCase();
    
    // Priority: id > meaningful class > text content > tag name
    if (id && id.length < 30) {
      return cleanSelectorName(id);
    }
    
    if (className) {
      const classes = className.split(' ').filter(cls => 
        cls.length > 2 && cls.length < 20 && !cls.match(/^(btn|button|text|item|content|wrapper|container)$/i)
      );
      if (classes.length > 0) {
        return cleanSelectorName(classes[0]);
      }
    }
    
    if (text && text.length > 0 && text.length < 50) {
      return cleanSelectorName(text);
    }
    
    return tagName || 'field';
  } catch (error) {
    return 'field';
  }
}

// Clean HTML for preview (preserve styles and scripts while making them safe)
export function cleanHtmlForPreview(html: string): string {
  let cleanedHtml = html;
  
  // Only remove inline event handlers for security
  cleanedHtml = cleanedHtml.replace(/\s*on\w+\s*=\s*"[^"]*"/gi, '');
  cleanedHtml = cleanedHtml.replace(/\s*on\w+\s*=\s*'[^']*'/gi, '');
  
  // Make all form submissions safe
  cleanedHtml = cleanedHtml.replace(/action\s*=\s*"[^"]*"/gi, 'action="#"');
  cleanedHtml = cleanedHtml.replace(/method\s*=\s*"[^"]*"/gi, 'method="get"');
  
  // Make all links safe but preserve their original href in a data attribute
  cleanedHtml = cleanedHtml.replace(/href\s*=\s*"([^"]*)"/gi, 'href="#" data-original-href="$1"');
  
  // Add sandbox attributes to iframes
  cleanedHtml = cleanedHtml.replace(/<iframe/gi, '<iframe sandbox="allow-same-origin"');
  
  // Add our safety styles while preserving original styles
  const safetyStyles = `
    <style>
      /* Base safety styles */
      * { box-sizing: border-box; }
      
      /* Make sure images don't overflow */
      img { max-width: 100%; height: auto; }
      
      /* Ensure video elements are contained */
      video { max-width: 100%; }
      
      /* Style for highlighted elements */
      .element-highlight {
        outline: 2px solid #F0FF26 !important;
        outline-offset: 2px !important;
      }
      
      /* Hover effect for selectable elements */
      *:hover {
        cursor: pointer;
      }
    </style>
  `;
  
  // Insert safety styles after opening head tag or create head if it doesn't exist
  if (cleanedHtml.includes('<head>')) {
    cleanedHtml = cleanedHtml.replace('<head>', '<head>' + safetyStyles);
  } else if (cleanedHtml.includes('<body>')) {
    cleanedHtml = cleanedHtml.replace('<body>', '<head>' + safetyStyles + '</head><body>');
  } else {
    cleanedHtml = '<head>' + safetyStyles + '</head>' + cleanedHtml;
  }
  
  return cleanedHtml;
} 

--- scrape-to-api/lib/codegen.ts ---
import { SelectorConfig } from './selectors';

export interface CodegenOptions {
  slug: string;
  url: string;
  title: string;
  selectors: SelectorConfig[];
  baseUrl: string;
  sampleData: any;
}

export interface CodegenResult {
  openapi: string;
  sdk: string;
  postman: string;
}

export function generateOpenAPISpec(options: CodegenOptions): string {
  const { slug, url, title, selectors, baseUrl, sampleData } = options;
  
  // Generate schema from selectors and sample data
  const properties: any = {};
  const requiredFields: string[] = [];
  
  selectors.forEach(selector => {
    const sampleValue = sampleData[selector.name];
    let type = 'string';
    let format: string | undefined;
    
    if (selector.multiple) {
      properties[selector.name] = {
        type: 'array',
        items: { type: 'string' },
        description: `Data extracted from: ${selector.selector}`
      };
    } else {
      // Infer type from sample data
      if (sampleValue !== null && sampleValue !== undefined) {
        if (typeof sampleValue === 'number') {
          type = 'number';
        } else if (typeof sampleValue === 'boolean') {
          type = 'boolean';
        } else if (selector.attribute === 'href' || selector.attribute === 'src') {
          type = 'string';
          format = 'uri';
        }
      }
      
      properties[selector.name] = {
        type,
        ...(format && { format }),
        description: `Data extracted from: ${selector.selector}`,
        example: sampleValue
      };
    }
    
    if (sampleValue !== null && sampleValue !== undefined) {
      requiredFields.push(selector.name);
    }
  });
  
  const spec = {
    openapi: '3.0.0',
    info: {
      title: `${title} API`,
      version: '1.0.0',
      description: `Auto-generated API for scraping data from ${url}`,
      contact: {
        name: 'Scrape2API',
        url: 'https://github.com/hyperbrowser/scrape2api'
      }
    },
    servers: [
      {
        url: baseUrl,
        description: 'Production server'
      }
    ],
    paths: {
      [`/api/data/${slug}`]: {
        get: {
          summary: `Get data from ${title}`,
          description: `Retrieve scraped data from ${url}`,
          operationId: `getData${slug.replace(/[^a-zA-Z0-9]/g, '')}`,
          tags: ['Data'],
          responses: {
            '200': {
              description: 'Successful response',
              content: {
                'application/json': {
                  schema: {
                    type: 'object',
                    properties: {
                      data: {
                        type: 'object',
                        properties,
                        required: requiredFields
                      },
                      meta: {
                        type: 'object',
                        properties: {
                          url: { type: 'string', format: 'uri' },
                          lastUpdated: { type: 'string', format: 'date-time' },
                          slug: { type: 'string' }
                        }
                      }
                    }
                  }
                }
              }
            },
            '404': {
              description: 'Data not found'
            }
          }
        }
      }
    },
    components: {
      schemas: {
        Error: {
          type: 'object',
          properties: {
            error: { type: 'string' },
            message: { type: 'string' }
          }
        }
      }
    }
  };
  
  return JSON.stringify(spec, null, 2);
}

export function generateTypeScriptSDK(options: CodegenOptions): string {
  const { slug, url, title, selectors, baseUrl, sampleData } = options;
  
  // Generate TypeScript interface
  const interfaceFields = selectors.map(selector => {
    const sampleValue = sampleData[selector.name];
    let type = 'string';
    
    if (selector.multiple) {
      type = 'string[]';
    } else if (sampleValue !== null && sampleValue !== undefined) {
      if (typeof sampleValue === 'number') {
        type = 'number';
      } else if (typeof sampleValue === 'boolean') {
        type = 'boolean';
      }
    }
    
    const optional = sampleValue === null || sampleValue === undefined ? '?' : '';
    return `  ${selector.name}${optional}: ${type};`;
  }).join('\n');
  
  const className = `${slug.replace(/[^a-zA-Z0-9]/g, '')}Client`;
  
  return `// Auto-generated TypeScript SDK for ${title}
// Source: ${url}
// Generated: ${new Date().toISOString()}

export interface ${className}Data {
${interfaceFields}
}

export interface ${className}Response {
  data: ${className}Data;
  meta: {
    url: string;
    lastUpdated: string;
    slug: string;
  };
}

export class ${className} {
  private baseUrl: string;
  
  constructor(baseUrl: string = '${baseUrl}') {
    this.baseUrl = baseUrl.replace(/\\/$/, '');
  }
  
  async getData(): Promise<${className}Response> {
    const response = await fetch(\`\${this.baseUrl}/api/data/${slug}\`);
    
    if (!response.ok) {
      throw new Error(\`HTTP error! status: \${response.status}\`);
    }
    
    return response.json();
  }
}

// Usage example:
// const client = new ${className}();
// const data = await client.getData();
// console.log(data.data.${selectors[0]?.name || 'field'});
`;
}

export function generatePostmanCollection(options: CodegenOptions): string {
  const { slug, url, title, baseUrl } = options;
  
  const collection = {
    info: {
      name: `${title} API`,
      description: `Auto-generated Postman collection for scraping ${url}`,
      schema: 'https://schema.getpostman.com/json/collection/v2.1.0/collection.json'
    },
    item: [
      {
        name: 'Get Data',
        request: {
          method: 'GET',
          header: [
            {
              key: 'Accept',
              value: 'application/json'
            }
          ],
          url: {
            raw: `${baseUrl}/api/data/${slug}`,
            host: [baseUrl.replace(/^https?:\/\//, '').split('/')[0]],
            path: ['api', 'data', slug]
          },
          description: `Retrieve scraped data from ${url}`
        },
        response: []
      }
    ],
    variable: [
      {
        key: 'baseUrl',
        value: baseUrl,
        type: 'string'
      }
    ]
  };
  
  return JSON.stringify(collection, null, 2);
}

export function generateCodeFiles(options: CodegenOptions): CodegenResult {
  return {
    openapi: generateOpenAPISpec(options),
    sdk: generateTypeScriptSDK(options),
    postman: generatePostmanCollection(options)
  };
} 

--- scrape-to-api/lib/crawl.ts ---
import { Hyperbrowser } from '@hyperbrowser/sdk';

export interface CrawlOptions {
  url: string;
  apiKey: string;
  onProgress?: (message: string) => void;
}

export interface CrawlResult {
  html: string;
  url: string;
  title: string;
  success: boolean;
  error?: string;
}

export async function crawlPage(options: CrawlOptions): Promise<CrawlResult> {
  const { url, apiKey, onProgress } = options;
  let session: any = null;
  let browser: any = null;
  
  try {
    onProgress?.('ðŸš€ Launching browser session...');
    
    // Initialize Hyperbrowser with official SDK pattern
    const hb = new Hyperbrowser({
      apiKey,
    });

    // Create browser session using official SDK
    session = await hb.sessions.create({
      useStealth: true,
      useProxy: false  // Disable proxy to avoid tunnel errors
    });

    onProgress?.('ðŸŒ Connecting to browser...');
    
    // Connect with Puppeteer using official pattern
    const { connect } = await import('puppeteer-core');
    browser = await connect({
      browserWSEndpoint: session.wsEndpoint,
      defaultViewport: null,
    });

    const [page] = await browser.pages();

    onProgress?.('ðŸ“„ Navigating to target URL...');
    
    // Navigate to the URL with retry logic
    let retries = 2;
    while (retries > 0) {
      try {
        await page.goto(url, {
          waitUntil: 'networkidle0', // Wait for network to be idle
          timeout: 15000
        });
        break; // Success, exit retry loop
      } catch (navError) {
        retries--;
        if (retries === 0) {
          throw navError; // Re-throw if no retries left
        }
        onProgress?.(`âš ï¸ Navigation failed, retrying... (${retries} attempts left)`);
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    }

    onProgress?.('â³ Waiting for page to load...');
    
    // Wait for page to fully load
    await new Promise(resolve => setTimeout(resolve, 2000));

    // Get the fully rendered HTML including styles
    const html = await page.evaluate((pageUrl: string) => {
      // Function to convert relative URLs to absolute
      function toAbsoluteUrl(relativeUrl: string) {
        try {
          return new URL(relativeUrl, pageUrl).href;
        } catch {
          return relativeUrl;
        }
      }

      // Clone the document to modify it safely
      const clone = document.documentElement.cloneNode(true) as HTMLElement;
      
      // Convert all resource URLs to absolute
      // Handle <link> tags (CSS)
      clone.querySelectorAll('link[href]').forEach(link => {
        const href = link.getAttribute('href');
        if (href) link.setAttribute('href', toAbsoluteUrl(href));
      });

      // Handle <script> tags
      clone.querySelectorAll('script[src]').forEach(script => {
        const src = script.getAttribute('src');
        if (src) script.setAttribute('src', toAbsoluteUrl(src));
      });

      // Handle <img> tags
      clone.querySelectorAll('img[src]').forEach(img => {
        const src = img.getAttribute('src');
        if (src) img.setAttribute('src', toAbsoluteUrl(src));
      });

      // Handle <a> tags
      clone.querySelectorAll('a[href]').forEach(a => {
        const href = a.getAttribute('href');
        if (href) a.setAttribute('href', toAbsoluteUrl(href));
      });

      // Handle inline CSS with url()
      const styleSheets = document.styleSheets;
      const inlineStyles = document.createElement('style');
      Array.from(styleSheets).forEach(sheet => {
        try {
          Array.from(sheet.cssRules).forEach(rule => {
            let cssText = rule.cssText;
            // Convert url() paths to absolute
            cssText = cssText.replace(/url\(['"]?([^'")]+)['"]?\)/g, (match, p1) => {
              return `url("${toAbsoluteUrl(p1)}")`;
            });
            inlineStyles.appendChild(document.createTextNode(cssText + "\n"));
          });
        } catch (e) {
          // Handle CORS errors for external stylesheets
          if (sheet.href) {
            inlineStyles.appendChild(document.createTextNode(`@import url("${sheet.href}");\n`));
          }
        }
      });
      
      // Add the collected styles to the head
      clone.querySelector('head')?.appendChild(inlineStyles);

      // Capture computed styles for all elements
      document.querySelectorAll('*').forEach((el, index) => {
        const computed = window.getComputedStyle(el);
        const styles = document.createElement('style');
        styles.textContent = `[data-styled="${index}"] { ${Array.from(computed).map(prop => 
          `${prop}: ${computed.getPropertyValue(prop)};`
        ).join(' ')} }`;
        clone.querySelector('head')?.appendChild(styles);
        (clone as any).querySelector(el.tagName.toLowerCase())?.setAttribute('data-styled', index.toString());
      });
      
      return clone.outerHTML;
    }, url);
    
    // Extract page title using Puppeteer
    const title = await page.title();
    
    onProgress?.('âœ… Page scraped successfully!');
    
    return {
      html,
      url,
      title,
      success: true
    };
    
  } catch (error) {
    console.error('Crawl error:', error);
    return {
      html: '',
      url,
      title: '',
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error'
    };
  } finally {
    try {
      if (browser) await browser.close();
      if (session?.destroy) await session.destroy();
    } catch (error) {
      console.error('Error cleaning up:', error);
    }
  }
}

// Clean HTML for preview (remove scripts, etc.)
export function cleanHtmlForPreview(html: string): string {
  // Remove script tags to prevent execution
  let cleanedHtml = html.replace(/<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>/gi, '');
  
  // Remove event handlers
  cleanedHtml = cleanedHtml.replace(/\s*on\w+\s*=\s*"[^"]*"/gi, '');
  cleanedHtml = cleanedHtml.replace(/\s*on\w+\s*=\s*'[^']*'/gi, '');
  
  // Remove form actions and change method to prevent submission
  cleanedHtml = cleanedHtml.replace(/action\s*=\s*"[^"]*"/gi, 'action="#"');
  cleanedHtml = cleanedHtml.replace(/method\s*=\s*"[^"]*"/gi, 'method="get"');
  
  // Add base styles to prevent layout issues
  const baseStyles = `
    <style>
      * { box-sizing: border-box; }
      body { margin: 0; padding: 16px; }
      img { max-width: 100%; height: auto; }
      iframe { display: none; }
      video { max-width: 100%; }
      a { color: inherit; text-decoration: none; }
      .selector-highlight { 
        outline: 2px solid #F0FF26 !important; 
        cursor: pointer !important;
      }
      .selector-selected {
        outline: 2px solid #00ff00 !important;
      }
    </style>
  `;
  
  // Insert styles after opening head tag or body tag
  if (cleanedHtml.includes('<head>')) {
    cleanedHtml = cleanedHtml.replace('<head>', '<head>' + baseStyles);
  } else if (cleanedHtml.includes('<body>')) {
    cleanedHtml = cleanedHtml.replace('<body>', '<body>' + baseStyles);
  } else {
    cleanedHtml = baseStyles + cleanedHtml;
  }
  
  return cleanedHtml;
}


--- scrape-to-api/lib/kv.ts ---
// Simple in-memory KV store for caching API responses
export interface CachedData {
  json: any;
  lastUpdated: number;
  url: string;
  slug: string;
}

class InMemoryKV {
  private store: Map<string, CachedData> = new Map();
  private readonly CACHE_FILE = '.cache.json';
  
  constructor() {
    // Try to load cached data from file system
    try {
      const fs = require('fs');
      if (fs.existsSync(this.CACHE_FILE)) {
        const data = JSON.parse(fs.readFileSync(this.CACHE_FILE, 'utf8'));
        this.store = new Map(Object.entries(data));
      }
    } catch (e) {
      console.warn('Failed to load cache file:', e);
    }
  }

  private persistToFile(): void {
    try {
      const fs = require('fs');
      const data = Object.fromEntries(this.store);
      fs.writeFileSync(this.CACHE_FILE, JSON.stringify(data, null, 2));
    } catch (e) {
      console.warn('Failed to persist cache to file:', e);
    }
  }
  
  set(slug: string, data: any, url: string): void {
    this.store.set(slug, {
      json: data,
      lastUpdated: Date.now(),
      url,
      slug
    });
    this.persistToFile();
  }
  
  get(slug: string): CachedData | null {
    const data = this.store.get(slug);
    if (!data) return null;

    // Check if data is too old (7 days)
    const now = Date.now();
    const maxAge = 7 * 24 * 60 * 60 * 1000; // 7 days
    if (now - data.lastUpdated > maxAge) {
      this.delete(slug);
      return null;
    }

    return data;
  }
  
  has(slug: string): boolean {
    return this.store.has(slug);
  }
  
  delete(slug: string): boolean {
    const result = this.store.delete(slug);
    this.persistToFile();
    return result;
  }
  
  clear(): void {
    this.store.clear();
    this.persistToFile();
  }
  
  list(): CachedData[] {
    return Array.from(this.store.values());
  }
  
  size(): number {
    return this.store.size;
  }
  
  // Clean up old entries (older than 7 days)
  cleanup(): number {
    const now = Date.now();
    const maxAge = 7 * 24 * 60 * 60 * 1000; // 7 days
    let cleaned = 0;
    
    for (const [slug, data] of this.store.entries()) {
      if (now - data.lastUpdated > maxAge) {
        this.store.delete(slug);
        cleaned++;
      }
    }
    
    if (cleaned > 0) {
      this.persistToFile();
    }
    
    return cleaned;
  }
}

// Export singleton instance
export const kv = new InMemoryKV();

// Generate a unique slug for URLs
export function generateSlug(url: string): string {
  const cleanUrl = url.replace(/^https?:\/\//, '').replace(/[^a-zA-Z0-9]/g, '-');
  const timestamp = Date.now().toString(36);
  const random = Math.random().toString(36).substring(2, 6);
  return `${cleanUrl.substring(0, 30)}-${timestamp}-${random}`;
}

// Auto-cleanup on module load
setInterval(() => {
  kv.cleanup();
}, 24 * 60 * 60 * 1000); // Run cleanup every 24 hours 

--- scrape-to-api/lib/selectors.ts ---
import * as cheerio from 'cheerio';
import { getCssSelector } from 'css-selector-generator';

export interface SelectorConfig {
  id: string;
  selector: string;
  name: string;
  attribute?: string; // 'text', 'href', 'src', or any attribute name
  multiple?: boolean; // if true, returns array
}

export interface ExtractedData {
  [key: string]: any;
}

// Extract data from HTML using selectors
export function extractDataFromHtml(html: string, selectors: SelectorConfig[]): ExtractedData {
  const $ = cheerio.load(html);
  const result: ExtractedData = {};
  
  for (const config of selectors) {
    try {
      const elements = $(config.selector);
      
      if (elements.length === 0) {
        result[config.name] = config.multiple ? [] : null;
        continue;
      }
      
      const values: any[] = [];
      
      elements.each((_, element) => {
        const $el = $(element);
        let value: any;
        
        if (!config.attribute || config.attribute === 'text') {
          value = $el.text().trim();
        } else if (config.attribute === 'html') {
          value = $el.html();
        } else {
          value = $el.attr(config.attribute);
        }
        
        if (value !== undefined && value !== null && value !== '') {
          values.push(value);
        }
      });
      
      if (config.multiple) {
        result[config.name] = values;
      } else {
        result[config.name] = values.length > 0 ? values[0] : null;
      }
      
    } catch (error) {
      console.error(`Error extracting data for selector ${config.selector}:`, error);
      result[config.name] = config.multiple ? [] : null;
    }
  }
  
  return result;
}

// Generate CSS selector for a clicked element
export function generateSelectorForElement(element: HTMLElement, html: string): string {
  try {
    // Use css-selector-generator to create a unique selector
    const selector = getCssSelector(element, {
      selectors: ['id', 'class', 'tag', 'attribute'],
      includeTag: true,
      whitelist: [],
      blacklist: [],
      combineWithinSelector: true,
      combineBetweenSelectors: true,
      root: null,
      maxCombinations: 50,
      maxCandidates: 100
    });
    
    return selector;
  } catch (error) {
    console.error('Error generating selector:', error);
    // Fallback to a simple selector
    return element.tagName.toLowerCase() + (element.id ? `#${element.id}` : '');
  }
}

// Validate selector by testing it against HTML
export function validateSelector(selector: string, html: string): { valid: boolean; count: number; sample?: string } {
  try {
    const $ = cheerio.load(html);
    const elements = $(selector);
    
    return {
      valid: elements.length > 0,
      count: elements.length,
      sample: elements.length > 0 ? elements.first().text().trim().substring(0, 100) : undefined
    };
  } catch (error) {
    return {
      valid: false,
      count: 0
    };
  }
}

// Clean selector name for API field
export function cleanSelectorName(text: string): string {
  return text
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, '_')
    .replace(/^_+|_+$/g, '')
    .substring(0, 50) || 'field';
}

// Get suggested name for a selector based on element content
export function suggestSelectorName(selector: string, html: string): string {
  try {
    const $ = cheerio.load(html);
    const element = $(selector).first();
    
    if (element.length === 0) return 'field';
    
    // Try to get a meaningful name from various attributes
    const id = element.attr('id');
    const className = element.attr('class');
    const text = element.text().trim();
    const tagName = element.prop('tagName')?.toLowerCase();
    
    // Priority: id > meaningful class > text content > tag name
    if (id && id.length < 30) {
      return cleanSelectorName(id);
    }
    
    if (className) {
      const classes = className.split(' ').filter(cls => 
        cls.length > 2 && cls.length < 20 && !cls.match(/^(btn|button|text|item|content|wrapper|container)$/i)
      );
      if (classes.length > 0) {
        return cleanSelectorName(classes[0]);
      }
    }
    
    if (text && text.length > 0 && text.length < 50) {
      return cleanSelectorName(text);
    }
    
    return tagName || 'field';
  } catch (error) {
    return 'field';
  }
} 

--- scrape-to-api/lib/zip.ts ---
import archiver from 'archiver';
import { CodegenResult } from './codegen';

export interface ZipOptions {
  slug: string;
  title: string;
  codegenResult: CodegenResult;
  sampleData: any;
}

export function createZipBuffer(options: ZipOptions): Promise<Buffer> {
  return new Promise((resolve, reject) => {
    const { slug, title, codegenResult, sampleData } = options;
    
    const archive = archiver('zip', {
      zlib: { level: 9 } // Sets the compression level
    });
    
    const buffers: Buffer[] = [];
    
    archive.on('data', (chunk) => {
      buffers.push(chunk);
    });
    
    archive.on('end', () => {
      const finalBuffer = Buffer.concat(buffers);
      resolve(finalBuffer);
    });
    
    archive.on('error', (err) => {
      reject(err);
    });
    
    // Add files to archive
    archive.append(codegenResult.openapi, { name: 'openapi.yaml' });
    archive.append(codegenResult.sdk, { name: 'sdk.ts' });
    archive.append(codegenResult.postman, { name: 'postman.json' });
    
    // Add sample data
    archive.append(JSON.stringify(sampleData, null, 2), { name: 'sample-data.json' });
    
    // Add README
    const readme = generateReadme(slug, title);
    archive.append(readme, { name: 'README.md' });
    
    // Add usage examples
    const examples = generateExamples(slug, title);
    archive.append(examples, { name: 'examples.md' });
    
    archive.finalize();
  });
}

function generateReadme(slug: string, title: string): string {
  return `# ${title} API

Auto-generated API client for scraping data from ${title}.

## Files Included

- \`openapi.yaml\` - OpenAPI 3.0 specification
- \`sdk.ts\` - TypeScript SDK for easy integration
- \`postman.json\` - Postman collection for testing
- \`sample-data.json\` - Sample response data
- \`examples.md\` - Usage examples

## Quick Start

### Using the TypeScript SDK

\`\`\`typescript
import { ${slug.replace(/[^a-zA-Z0-9]/g, '')}Client } from './sdk';

const client = new ${slug.replace(/[^a-zA-Z0-9]/g, '')}Client();
const data = await client.getData();
console.log(data);
\`\`\`

### Using fetch directly

\`\`\`javascript
const response = await fetch('/api/data/${slug}');
const data = await response.json();
console.log(data);
\`\`\`

## API Endpoint

\`GET /api/data/${slug}\`

Returns JSON data scraped from the source website.

## Response Format

\`\`\`json
{
  "data": {
    // Your scraped data fields
  },
  "meta": {
    "url": "source-url",
    "lastUpdated": "2024-01-01T00:00:00.000Z",
    "slug": "${slug}"
  }
}
\`\`\`

Generated by [Scrape2API](https://github.com/hyperbrowser/scrape2api)
`;
}

function generateExamples(slug: string, title: string): string {
  return `# Usage Examples

## JavaScript/TypeScript

### Basic Usage
\`\`\`javascript
// Using fetch
const response = await fetch('/api/data/${slug}');
const result = await response.json();
console.log(result.data);
\`\`\`

### With Error Handling
\`\`\`javascript
try {
  const response = await fetch('/api/data/${slug}');
  if (!response.ok) {
    throw new Error(\`HTTP error! status: \${response.status}\`);
  }
  const result = await response.json();
  console.log(result.data);
} catch (error) {
  console.error('Error fetching data:', error);
}
\`\`\`

## Python

### Using requests
\`\`\`python
import requests

response = requests.get('/api/data/${slug}')
response.raise_for_status()
data = response.json()
print(data['data'])
\`\`\`

## curl

### Basic request
\`\`\`bash
curl -X GET '/api/data/${slug}' \\
  -H 'Accept: application/json'
\`\`\`

### With pretty printing
\`\`\`bash
curl -X GET '/api/data/${slug}' \\
  -H 'Accept: application/json' | jq .
\`\`\`

## Node.js

### Using node-fetch
\`\`\`javascript
const fetch = require('node-fetch');

async function getData() {
  const response = await fetch('/api/data/${slug}');
  const data = await response.json();
  return data;
}

getData().then(console.log).catch(console.error);
\`\`\`

### Using axios
\`\`\`javascript
const axios = require('axios');

axios.get('/api/data/${slug}')
  .then(response => console.log(response.data))
  .catch(error => console.error(error));
\`\`\`
`;
} 

## Links discovered
- [Scrape2API](https://github.com/hyperbrowser/scrape2api)

--- README.md ---
# Hyperbrowser Example Web Apps

Welcome to the **Hyperbrowser Example Web Apps** repository! ðŸš€

This repository contains a collection of complete, production-ready web applications built using [Hyperbrowser](https://hyperbrowser.ai) - the powerful browser automation and web scraping platform.

## What is Hyperbrowser?

Hyperbrowser is a cutting-edge platform that enables developers to build sophisticated web automation, scraping, and browser-based applications with ease. These examples showcase the full potential of what you can build with Hyperbrowser's capabilities.

## What's Inside

This repository features full-stack web applications that demonstrate:

- ðŸŒ **Complete Web Applications** - End-to-end examples ready for production use
- ðŸ¤– **Browser Automation** - Real-world automation workflows and patterns
- ðŸ“Š **Data Extraction** - Advanced web scraping and data processing examples
- ðŸŽ¨ **Modern UI/UX** - Beautiful, responsive interfaces built with best practices
- ðŸ”§ **Production Patterns** - Scalable architectures and deployment strategies

## Getting Started

### Prerequisites

Before running these examples, you'll need to:

1. **Get Your API Keys** ðŸ”‘
   
   Visit [**Hyperbrowser.ai**](https://hyperbrowser.ai) to sign up and obtain your API keys. These are required to run the applications in this repository.

2. **Set Up Your Environment**
   
   Each example includes its own setup instructions, but generally you'll need:
   - Node.js (v16 or higher)
   - Your Hyperbrowser API credentials
   - Any additional dependencies listed in each project's README

### Quick Start

1. Clone this repository:
   ```bash
   git clone https://github.com/your-org/hyperbrowser-app-examples.git
   cd hyperbrowser-app-examples
   ```

2. Choose an example application and follow its specific setup instructions

3. Configure your Hyperbrowser API keys in the application's environment variables

4. Run the application and start exploring!

## Examples Overview

Each directory in this repository contains a complete application with:

- ðŸ“ **Source Code** - Full application code with comments and documentation
- ðŸ“– **README** - Detailed setup and usage instructions
- ðŸ”§ **Configuration** - Environment setup and deployment guides
- ðŸ§ª **Tests** - Example tests and quality assurance patterns

## Support & Resources

- ðŸ“š **Documentation**: [Hyperbrowser Docs](https://hyperbrowser.ai/docs)
- ðŸ”‘ **Get API Keys**: [Hyperbrowser.ai](https://hyperbrowser.ai)
- ðŸ’¬ **Community**: Join our community for support and discussions
- ðŸ› **Issues**: Report bugs or request features in this repository

## Contributing

We welcome contributions! If you've built something amazing with Hyperbrowser and want to share it:

1. Fork this repository
2. Create a new directory for your example
3. Include comprehensive documentation
4. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**Ready to build something amazing?** Get your API keys at [**Hyperbrowser.ai**](https://hyperbrowser.ai) and start exploring these examples! ðŸŽ‰


## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)
- [**Hyperbrowser.ai**](https://hyperbrowser.ai)
- [Hyperbrowser Docs](https://hyperbrowser.ai/docs)
- [Hyperbrowser.ai](https://hyperbrowser.ai)

--- assets-optimizer/README.md ---
# Assets Optimizer

A powerful Next.js 14 app that optimizes web assets (images, CSS, JavaScript, fonts) from any website using the Hyperbrowser SDK. Built with modern UI components inspired by deep-crawler.

## âœ¨ Features

- **Real Asset Extraction**: Uses Hyperbrowser to scrape and extract assets from any website
- **Smart Optimization**: 
  - Images â†’ AVIF format with 50% quality for maximum compression
  - Fonts â†’ Subset to only used characters and convert to WOFF2
  - Video â†’ Extract poster frames in JPEG format
- **Live Progress Tracking**: Real-time terminal sidebar showing extraction progress
- **Modern UI**: Clean, responsive interface with glassmorphism design
- **Download Options**: Get optimized assets as ZIP file plus detailed JSON report

## ðŸš€ Getting Started

### Prerequisites

- Node.js 18+ 
- A [Hyperbrowser API key](https://hyperbrowser.ai) (required for web scraping)

### Installation

1. **Get your API key**
   - Visit [Hyperbrowser.ai](https://hyperbrowser.ai)
   - Sign up for an account
   - Get your API key from the dashboard

2. **Set up environment variables**
   ```bash
   # Create .env.local file
   cp .env.example .env.local
   
   # Add your API key to .env.local
   HYPERBROWSER_API_KEY=your_hyperbrowser_api_key_here
   ```

3. **Install dependencies**
   ```bash
   npm install
   ```

4. **Run the development server**
   ```bash
   npm run dev
   ```

5. **Open your browser**
   Navigate to [http://localhost:3000](http://localhost:3000)

## ðŸ“– Usage

1. **Enter a URL**: Paste any website URL (e.g., `https://hyperbrowser.ai`)
2. **Start Optimization**: Click "Start Optimization" to begin asset extraction
3. **Monitor Progress**: Watch real-time logs in the terminal sidebar
4. **Download Results**: Get optimized assets as ZIP file and detailed JSON report

## ðŸ› ï¸ How It Works

1. **Web Scraping**: Uses Hyperbrowser SDK to extract HTML content from the target website
2. **Asset Discovery**: Parses HTML to find images, CSS files, JavaScript, and fonts
3. **Smart Download**: Downloads all discovered assets with proper error handling
4. **Optimization Pipeline**:
   - **Images**: Convert to AVIF format for 50%+ size reduction
   - **Fonts**: Subset to only used characters and convert to WOFF2
   - **Videos**: Extract poster frames as optimized JPEG
   - **Other files**: Keep as-is for compatibility
5. **Bundle Creation**: Package optimized assets with updated HTML into downloadable ZIP

## ðŸ”§ Tech Stack

- **Frontend**: Next.js 14, React, TypeScript, Tailwind CSS
- **Web Scraping**: Hyperbrowser SDK
- **Asset Processing**: Sharp (images), subset-font (fonts), FFmpeg (videos)
- **UI Components**: Custom components with glassmorphism design
- **Icons**: Lucide React

## ðŸ“Š Optimization Results

Typical results from optimizing a modern website:
- **Images**: 50-80% size reduction (JPEG/PNG â†’ AVIF)
- **Fonts**: 70-90% size reduction (subset + WOFF2)
- **Overall**: 40-60% total asset size reduction

## ðŸ”‘ Environment Variables

Create a `.env.local` file with:

```env
# Required: Your Hyperbrowser API key
HYPERBROWSER_API_KEY=your_hyperbrowser_api_key_here
```

## ðŸ“ API Endpoints

- `POST /api/optimize` - Main optimization endpoint that streams progress via Server-Sent Events

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ðŸ“„ License

MIT License - see LICENSE file for details

## ðŸ™‹â€â™‚ï¸ Support

- [Hyperbrowser Documentation](https://hyperbrowser.ai/docs)
- [GitHub Issues](https://github.com/yourusername/assets-optimizer/issues)

---

Built with â¤ï¸ using [Hyperbrowser](https://hyperbrowser.ai)


## Links discovered
- [Hyperbrowser API key](https://hyperbrowser.ai)
- [Hyperbrowser.ai](https://hyperbrowser.ai)
- [http://localhost:3000](http://localhost:3000)
- [Hyperbrowser Documentation](https://hyperbrowser.ai/docs)
- [GitHub Issues](https://github.com/yourusername/assets-optimizer/issues)
- [Hyperbrowser](https://hyperbrowser.ai)

--- assets-optimizer/next.config.js ---
/** @type {import('next').NextConfig} */
const nextConfig = {
  webpack: (config, { isServer }) => {
    config.experiments = {
      ...config.experiments,
      asyncWebAssembly: true,
    };
    return config;
  },
};

module.exports = nextConfig; 

--- assets-optimizer/tailwind.config.js ---
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './pages/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    './app/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {
      colors: {
        accent: '#F0FF26',
        terminal: '#000000',
        bg: '#060606',
        neon: '#2AF5FF',
      },
      fontFamily: {
        sans: ['Inter', 'system-ui', 'sans-serif'],
        mono: ['ui-monospace', 'SF Mono', 'Monaco', 'monospace'],
      },
      fontWeight: {
        normal: '500',
        medium: '500',
        semibold: '600',
        bold: '700',
      },
      letterSpacing: {
        tight4: '-0.04em',
      },
      animation: {
        'fade-in': 'fadeIn 0.5s ease-out',
      },
      keyframes: {
        fadeIn: {
          '0%': { opacity: '0', transform: 'translateY(10px)' },
          '100%': { opacity: '1', transform: 'translateY(0)' },
        },
      },
      backdropBlur: {
        md: '12px',
      },
    },
  },
  plugins: [],
}; 

--- assets-optimizer/lib/hyper.ts ---
import { Hyperbrowser } from '@hyperbrowser/sdk';
import * as cheerio from 'cheerio';

if (!process.env.HYPERBROWSER_API_KEY) {
  throw new Error('HYPERBROWSER_API_KEY environment variable is required. Get your API key from https://hyperbrowser.ai and add it to your .env.local file.');
}

export async function scrape(url: string) {
  const hb = new Hyperbrowser({
    apiKey: process.env.HYPERBROWSER_API_KEY,
  });

  try {
    // Scrape the webpage to get HTML content
    const scrapeResult = await hb.scrape.startAndWait({
      url,
      scrapeOptions: {
        formats: ['html'],
        onlyMainContent: false, // We need the full page to get all assets
        timeout: 30000,
        waitFor: 3000, // Wait for assets to load
      },
      sessionOptions: {
        useStealth: true,
        acceptCookies: true,
      }
    });

    if (!scrapeResult.data?.html) {
      throw new Error('Failed to extract HTML content from the webpage');
    }

    const html = scrapeResult.data.html;
    const $ = cheerio.load(html);

    // Extract CSS content
    let css = '';
    $('style').each((_, el) => {
      css += $(el).html() + '\n';
    });

    // Extract assets from the HTML
    const assets = await extractAssets($, url);

    return {
      html,
      css,
      assets
    };

  } catch (error) {
    console.error('Error scraping with Hyperbrowser:', error);
    throw error;
  }
}

async function extractAssets($: cheerio.Root, baseUrl: string) {
  const assets: Array<{
    url: string;
    type: string;
    buffer: Buffer;
    size: number;
  }> = [];

  // Helper function to resolve relative URLs
  const resolveUrl = (url: string) => {
    try {
      return new URL(url, baseUrl).href;
    } catch {
      return url;
    }
  };

  // Extract images
  const imageSelectors = ['img[src]', 'source[srcset]', 'link[rel="icon"]', 'link[rel="apple-touch-icon"]'];
  for (const selector of imageSelectors) {
    $(selector).each((_, el) => {
      const src = $(el).attr('src') || $(el).attr('href');
      if (src && !src.startsWith('data:') && !src.startsWith('blob:')) {
        const fullUrl = resolveUrl(src);
        if (fullUrl.match(/\.(jpg|jpeg|png|gif|svg|webp|ico)$/i)) {
          assets.push({
            url: fullUrl,
            type: getImageMimeType(fullUrl),
            buffer: Buffer.alloc(0), // Will be filled by downloadAsset
            size: 0
          });
        }
      }
    });
  }

  // Extract CSS files
  $('link[rel="stylesheet"]').each((_, el) => {
    const href = $(el).attr('href');
    if (href && !href.startsWith('data:')) {
      const fullUrl = resolveUrl(href);
      assets.push({
        url: fullUrl,
        type: 'text/css',
        buffer: Buffer.alloc(0),
        size: 0
      });
    }
  });

  // Extract JavaScript files
  $('script[src]').each((_, el) => {
    const src = $(el).attr('src');
    if (src && !src.startsWith('data:') && !src.startsWith('blob:')) {
      const fullUrl = resolveUrl(src);
      assets.push({
        url: fullUrl,
        type: 'application/javascript',
        buffer: Buffer.alloc(0),
        size: 0
      });
    }
  });

  // Extract fonts
  $('link[rel="preload"][as="font"], link[href*=".woff"], link[href*=".ttf"], link[href*=".otf"]').each((_, el) => {
    const href = $(el).attr('href');
    if (href && !href.startsWith('data:')) {
      const fullUrl = resolveUrl(href);
      assets.push({
        url: fullUrl,
        type: getFontMimeType(fullUrl),
        buffer: Buffer.alloc(0),
        size: 0
      });
    }
  });

  // Download all assets
  const downloadedAssets = await Promise.all(
    assets.map(async (asset) => {
      try {
        const downloaded = await downloadAsset(asset.url);
        return {
          ...asset,
          buffer: downloaded.buffer,
          size: downloaded.size
        };
      } catch (error) {
        console.error(`Failed to download ${asset.url}:`, error);
        return null;
      }
    })
  );

  // Filter out failed downloads and duplicates
  const validAssets = downloadedAssets
    .filter((asset): asset is NonNullable<typeof asset> => asset !== null)
    .filter((asset, index, self) => 
      self.findIndex(a => a.url === asset.url) === index
    );

  return validAssets;
}

async function downloadAsset(url: string): Promise<{ buffer: Buffer; size: number }> {
  try {
    const response = await fetch(url, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
      }
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }

    const buffer = Buffer.from(await response.arrayBuffer());
    return {
      buffer,
      size: buffer.length
    };
  } catch (error) {
    console.error(`Failed to download ${url}:`, error);
    throw error;
  }
}

function getImageMimeType(url: string): string {
  const ext = url.split('.').pop()?.toLowerCase();
  switch (ext) {
    case 'jpg':
    case 'jpeg':
      return 'image/jpeg';
    case 'png':
      return 'image/png';
    case 'gif':
      return 'image/gif';
    case 'svg':
      return 'image/svg+xml';
    case 'webp':
      return 'image/webp';
    case 'ico':
      return 'image/x-icon';
    default:
      return 'image/jpeg';
  }
}

function getFontMimeType(url: string): string {
  const ext = url.split('.').pop()?.toLowerCase();
  switch (ext) {
    case 'woff':
      return 'font/woff';
    case 'woff2':
      return 'font/woff2';
    case 'ttf':
      return 'font/ttf';
    case 'otf':
      return 'font/otf';
    default:
      return 'font/woff2';
  }
} 

--- assets-optimizer/lib/report.ts ---
export function generateReport(totalOrig: number, totalOpt: number, table: any[]) {
  const percent = ((totalOrig - totalOpt) / totalOrig * 100).toFixed(2);
  return {
    summary: { totalOrig, totalOpt, percent },
    table,
  };
} 

--- churnhunter/README.md ---
# ChurnHunter ðŸŽ¯

CLI tool to analyze signup/demo flows for churn risk using HyperBrowser automation and OpenAI analysis.

## Quick Setup

1. **Install dependencies:**
   ```bash
   npm install
   ```

2. **Set up environment variables:**
   ```bash
   cp env.example .env
   # Edit .env with your API keys
   # Or export them directly:
   export HYPERBROWSER_API_KEY=your_hyperbrowser_api_key_here
   export OPENAI_API_KEY=your_openai_api_key_here
   ```

3. **Get API Keys:**
   - **HyperBrowser API Key**: Get from [https://hyperbrowser.ai](https://hyperbrowser.ai)
   - **OpenAI API Key**: Get from [https://platform.openai.com](https://platform.openai.com/)

## Usage

```bash
# Run with URL prompt
npx ts-node churnhunter.ts

# Run with direct URL
npx ts-node churnhunter.ts --url https://example.com

# Get JSON output
npx ts-node churnhunter.ts --json --url https://example.com

# Show help
npx ts-node churnhunter.ts --help
```

## Exit Codes

- `0` - Low churn risk (score < 70)
- `1` - High churn risk (score â‰¥ 70) 
- `2` - Error occurred

## What it does

1. **Automated Crawling**: Uses HyperBrowser to simulate a real user going through your signup/demo flow
2. **AI Analysis**: Sends the captured events to GPT-4o for UX analysis
3. **Risk Scoring**: Gets a 0-100 churn risk score with specific friction points
4. **Actionable Report**: Shows colored CLI report with recommendations

## Sample Output

```
ðŸŽ¯ ChurnHunter Analysis Report
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
URL: https://example.com
Churn Risk Score: 45/100
Risk Level: MODERATE

âš ï¸  Friction Points:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Issue                                â”‚ Recommendation                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Signup form has too many fields      â”‚ Reduce required fields to email and password only         â”‚
â”‚ No clear value proposition shown     â”‚ Add benefit statements above the signup form              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Events captured: 23
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
``` 

## Links discovered
- [https://hyperbrowser.ai](https://hyperbrowser.ai)
- [https://platform.openai.com](https://platform.openai.com/)

--- churnhunter/churnhunter.ts ---
import { Hyperbrowser } from '@hyperbrowser/sdk';
import { OpenAI } from 'openai';
import { config } from 'dotenv';

config();
const chalk = require('chalk');
const yargs = require('yargs');
import { hideBin } from 'yargs/helpers';
const Table = require('cli-table3');
import * as readline from 'readline';

interface ChurnAnalysis {
  score: number;
  frictions: Array<{
    point: string;
    hint: string;
  }>;
}

interface StepEvent {
  timestamp: string;
  action: string;
  selector?: string;
  text?: string;
  url?: string;
  error?: string;
  screenshot?: string;
  [key: string]: any;
}

class ChurnHunter {
  private hbClient: Hyperbrowser;
  private openai: OpenAI;
  private events: StepEvent[] = [];

  constructor() {
    const hyperApiKey = process.env.HYPERBROWSER_API_KEY;
    const openaiApiKey = process.env.OPENAI_API_KEY;

    if (!hyperApiKey) {
      console.error(chalk.red('âŒ HYPERBROWSER_API_KEY environment variable is required'));
      process.exit(2);
    }

    if (!openaiApiKey) {
      console.error(chalk.red('âŒ OPENAI_API_KEY environment variable is required'));
      process.exit(2);
    }

    this.hbClient = new Hyperbrowser({ apiKey: hyperApiKey });
    this.openai = new OpenAI({ apiKey: openaiApiKey });
  }

  private async promptForUrl(): Promise<string> {
    const rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
    });

    return new Promise((resolve, reject) => {
      rl.question(chalk.cyan('ðŸŒ Enter the URL to analyze: '), (answer) => {
        rl.close();
        resolve(answer.trim());
      });
      
      rl.on('error', (error) => {
        rl.close();
        reject(error);
      });
    });
  }

  private async performSignupCrawl(url: string): Promise<void> {
    if (!process.argv.includes('--json')) {
      console.log(chalk.blue('ðŸš€ Starting HyperAgent session...'));
    }

    const task = `Navigate to ${url} and complete a typical user signup or demo flow. 
    Take actions like a new user would: look for signup buttons, fill forms, 
    navigate through onboarding steps, and explore key features. 
    Pay attention to any friction points, confusing UI elements, 
    slow loading times, or steps that might cause user drop-off.`;

    try {
      const result = await this.hbClient.agents.hyperAgent.startAndWait({
        task: task,
        maxSteps: 20,
        sessionOptions: {
          acceptCookies: true,
        }
      });

             // Convert the result to our events format for analysis
       this.events = [{
         timestamp: new Date().toISOString(),
         action: 'task_completed',
         url: url,
         result: result.data?.finalResult || 'Task completed'
       }];

       if (!process.argv.includes('--json')) {
         console.log(chalk.green(`âœ… Task completed successfully`));
       }

    } catch (error) {
      console.error(chalk.red('âŒ Error during HyperAgent execution:'), error);
      
      // Add error event for analysis
      this.events = [{
        timestamp: new Date().toISOString(),
        action: 'task_error',
        url: url,
        error: error instanceof Error ? error.message : 'Unknown error occurred'
      }];
    }
  }

  private truncateEventsForGPT(events: StepEvent[]): string {
    const eventsJson = JSON.stringify(events, null, 2);
    const maxLength = 15000; // Keep under 16k chars for GPT

    if (eventsJson.length <= maxLength) {
      return eventsJson;
    }

    // Truncate from the middle, keep first and last events
    const firstEvents = events.slice(0, Math.floor(events.length * 0.3));
    const lastEvents = events.slice(-Math.floor(events.length * 0.3));
    const truncatedEvents = [...firstEvents, 
      { timestamp: new Date().toISOString(), action: '[events_truncated]', note: 'Middle events removed to fit GPT context' }, 
      ...lastEvents
    ];

    return JSON.stringify(truncatedEvents, null, 2);
  }

  private async analyzeWithGPT(events: StepEvent[]): Promise<ChurnAnalysis> {
    if (!process.argv.includes('--json')) {
      console.log(chalk.blue('ðŸ§  Analyzing with GPT-4o...'));
    }

    const eventsJson = this.truncateEventsForGPT(events);

    const prompt = `You are a UX expert analyzing a user's signup/demo flow for churn risk. 

Based on the following browser automation events, provide a churn risk analysis:

${eventsJson}

Analyze the flow for:
- Friction points (slow loading, confusing UI, complex forms)
- User experience issues (too many steps, unclear navigation)
- Technical problems (errors, timeouts, broken features)
- Onboarding quality (guidance, value demonstration)

Respond with JSON in this exact format:
{
  "score": <0-100 integer, where 100 = highest churn risk>,
  "frictions": [
    {
      "point": "Brief description of friction point",
      "hint": "Actionable suggestion to fix it"
    }
  ]
}

Score guidelines:
- 0-30: Excellent UX, low churn risk
- 31-50: Good UX with minor issues
- 51-70: Moderate issues, some churn risk
- 71-100: High churn risk, significant problems`;

    try {
      const response = await this.openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: prompt }],
        response_format: { type: 'json_object' },
        temperature: 0.1,
      });

      const content = response.choices[0]?.message?.content;
      if (!content) {
        throw new Error('No response from OpenAI');
      }

      return JSON.parse(content) as ChurnAnalysis;
    } catch (error) {
      console.error(chalk.red('âŒ Error analyzing with GPT:'), error);
      throw error;
    }
  }

  private printColoredReport(analysis: ChurnAnalysis, url: string): void {
    console.log('\n' + chalk.bold.blue('ðŸŽ¯ ChurnHunter Analysis Report'));
    console.log(chalk.gray('â•'.repeat(50)));
    
    console.log(`${chalk.bold('URL:')} ${chalk.underline(url)}`);
    
    // Score with color coding
    const scoreColor = analysis.score >= 70 ? 'red' : 
                      analysis.score >= 50 ? 'yellow' : 'green';
    console.log(`${chalk.bold('Churn Risk Score:')} ${chalk[scoreColor](analysis.score)}/100`);
    
    // Risk level
    const riskLevel = analysis.score >= 70 ? 'HIGH' : 
                     analysis.score >= 50 ? 'MODERATE' : 'LOW';
    const riskColor = analysis.score >= 70 ? 'red' : 
                     analysis.score >= 50 ? 'yellow' : 'green';
    console.log(`${chalk.bold('Risk Level:')} ${chalk[riskColor](riskLevel)}\n`);

    // Frictions table
    if (analysis.frictions.length > 0) {
      console.log(chalk.bold.yellow('âš ï¸  Friction Points:'));
      
      const table = new Table({
        head: [chalk.bold('Issue'), chalk.bold('Recommendation')],
        colWidths: [40, 60],
        wordWrap: true,
        style: {
          head: ['cyan'],
          border: ['gray']
        }
      });

      analysis.frictions.forEach(friction => {
        table.push([friction.point, friction.hint]);
      });

      console.log(table.toString());
    } else {
      console.log(chalk.green('âœ… No significant friction points detected!'));
    }

    console.log(`\n${chalk.gray('Events captured:')} ${this.events.length}`);
    console.log(chalk.gray('â•'.repeat(50)));
  }

  public async run(): Promise<void> {
    try {
      const argv = await yargs(hideBin(process.argv))
        .option('url', {
          alias: 'u',
          type: 'string',
          description: 'Target URL to analyze',
        })
        .option('json', {
          alias: 'j',
          type: 'boolean',
          description: 'Output results as JSON',
          default: false,
        })
        .help()
        .alias('help', 'h')
        .parse();

      let url = argv.url;
      if (!url) {
        url = await this.promptForUrl();
      }

      if (!url) {
        console.error(chalk.red('âŒ URL is required'));
        process.exit(2);
      }

      // Validate URL
      try {
        new URL(url);
      } catch {
        console.error(chalk.red('âŒ Invalid URL provided'));
        process.exit(2);
      }

      if (!argv.json) {
        console.log(chalk.bold.green('ðŸŽ¯ ChurnHunter - Signup Flow Analyzer'));
        console.log(chalk.gray(`Analyzing: ${url}\n`));
      }

      // Perform the crawl
      await this.performSignupCrawl(url);

      if (this.events.length === 0) {
        console.error(chalk.red('âŒ No events captured during crawl'));
        process.exit(2);
      }

      // Analyze with GPT
      const analysis = await this.analyzeWithGPT(this.events);

      // Output results
      if (argv.json) {
        console.log(JSON.stringify({
          url,
          events_count: this.events.length,
          analysis
        }, null, 2));
      } else {
        this.printColoredReport(analysis, url);
        
        // Final message
        if (analysis.score >= 70) {
          console.log(chalk.red('\nâš ï¸  HIGH CHURN RISK DETECTED! Immediate action recommended.'));
        } else if (analysis.score >= 50) {
          console.log(chalk.yellow('\nðŸ’¡ Moderate friction detected. Consider optimizing the user flow.'));
        } else {
          console.log(chalk.green('\nâœ¨ Great user experience! Low churn risk.'));
        }
      }

      // Exit with appropriate code
      process.exit(analysis.score >= 70 ? 1 : 0);

    } catch (error) {
      console.error(chalk.red('âŒ Error:'), error);
      process.exit(2);
    }
  }
}

// Main execution
if (require.main === module) {
  const churnHunter = new ChurnHunter();
  churnHunter.run().catch(error => {
    console.error(chalk.red('âŒ Fatal error:'), error);
    process.exit(2);
  });
} 

## Links discovered
- [scoreColor](https://github.com/hyperbrowserai/hyperbrowser-app-examples/blob/main/churnhunter/analysis.score)
- [riskColor](https://github.com/hyperbrowserai/hyperbrowser-app-examples/blob/main/churnhunter/riskLevel.md)

--- competitor-tracker/README.md ---
# Competitor Tracker ðŸš€

> Monitor competitor websites for changes and get AI-powered summaries with accurate change detection.

## Features

âœ¨ **Smart Change Detection**
- Track competitor websites for real changes
- Accurate pattern-based summaries (no AI hallucinations)
- Detect UUID changes, time updates, content modifications
- Visual diff highlighting

ðŸŽ¯ **Flexible Monitoring**
- Schedule hourly, 3-hourly, or daily crawls
- CSS selector targeting for specific page elements
- Manual crawl triggers for instant checks

ðŸ”§ **Developer Friendly**
- Built with Next.js 14, TypeScript, and Tailwind CSS
- Clean JSON-based data storage
- Dark theme UI with neon accents

## Quick Start

### 1. Get API Keys

- **Hyperbrowser API Key**: Get yours at [hyperbrowser.ai](https://hyperbrowser.ai)
- **OpenAI API Key**: Get from [OpenAI Platform](https://platform.openai.com/api-keys)

### 2. Install & Setup

```bash
# Clone and install dependencies
npm install

# Create environment file
cp .env.example .env.local
```

### 3. Configure Environment

Add your API keys to `.env.local`:

```env
HYPERBROWSER_API_KEY=your_hyperbrowser_key_here
OPENAI_API_KEY=your_openai_key_here
```

### 4. Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) ðŸ•¶ï¸

## Testing URLs

Perfect for testing your competitor tracker:

- `https://httpbin.org/uuid` - Returns different UUID each time
- `https://worldtimeapi.org/api/timezone/America/New_York` - Current time updates
- `https://httpbin.org/cache` - Timestamp and cache headers
- `https://api.github.com/zen` - Random motivational quotes
- `https://httpbin.org/headers` - Request headers with timestamps

## Environment Variables

| Key | Description | Required |
| --- | --- | --- |
| `HYPERBROWSER_API_KEY` | Get from [hyperbrowser.ai](https://hyperbrowser.ai) | âœ… |
| `OPENAI_API_KEY` | Get from [OpenAI Platform](https://platform.openai.com/api-keys) | âœ… |
| `SLACK_WEBHOOK_URL` | Slack incoming webhook (optional) | âŒ |
| `DISCORD_WEBHOOK_URL` | Discord webhook (optional) | âŒ |

## How It Works

1. **Add URLs**: Track competitor websites with configurable frequency
2. **Smart Crawling**: Uses Hyperbrowser SDK for reliable web scraping
3. **Change Detection**: Compares HTML content using diff algorithms
4. **Pattern Recognition**: Identifies specific changes (UUIDs, times, numbers, text)
5. **Accurate Summaries**: No AI hallucinations - shows exactly what changed

## License

MIT


## Links discovered
- [hyperbrowser.ai](https://hyperbrowser.ai)
- [OpenAI Platform](https://platform.openai.com/api-keys)
- [http://localhost:3000](http://localhost:3000)

--- competitor-tracker/next.config.ts ---
import type { NextConfig } from 'next'

const nextConfig: NextConfig = {
  serverExternalPackages: [
    '@hyperbrowser/sdk',
    'diff',
    'pixelmatch',
    '@supabase/supabase-js',
  ],
  webpack: (config) => {
    config.resolve.fallback = {
      ...config.resolve.fallback,
      fs: false,
      net: false,
      tls: false,
    }
    return config
  },
}

export default nextConfig;


--- competitor-tracker/tailwind.config.js ---
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './app/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    './lib/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {
      colors: {
        accent: '#F0FF26',
        background: '#060606',
      },
      fontFamily: {
        sans: ['Inter', 'system-ui', 'sans-serif'],
        mono: ['ui-monospace', 'SF Mono', 'monospace'],
      },
      fontWeight: {
        normal: '500',
        medium: '500',
        semibold: '600',
        bold: '700',
      },
      letterSpacing: {
        tight4: '-0.04em',
      },
      animation: {
        'fade-in': 'fadeIn 0.5s ease-out',
      },
      keyframes: {
        fadeIn: {
          '0%': { opacity: '0', transform: 'translateY(10px)' },
          '100%': { opacity: '1', transform: 'translateY(0)' },
        },
      },
      backdropBlur: {
        md: '12px',
      },
    },
  },
  plugins: [],
} 

--- competitor-tracker/types.ts ---
export type Frequency = 'hourly' | '3hourly' | 'daily'

export interface TrackedURL {
  id: string
  url: string
  frequency: Frequency
  selectors?: string
  created_at: string
}

export interface Snapshot {
  id: string
  tracked_url_id: string
  html: string
  created_at: string
}

export interface Change {
  id: string
  tracked_url_id: string
  snapshot_id: string
  diff_html: string
  pixel_change: number // Keep for backward compatibility, always 0
  summary: string
  impact: 'low' | 'medium' | 'high'
  created_at: string
}

export interface Database {
  tracked_urls: TrackedURL[]
  snapshots: Snapshot[]
  changes: Change[]
} 

--- competitor-tracker/lib/crawl.ts ---
import { Hyperbrowser } from '@hyperbrowser/sdk'

if (!process.env.HYPERBROWSER_API_KEY) {
  throw new Error('HYPERBROWSER_API_KEY env var is required')
}

const hb = new Hyperbrowser({ apiKey: process.env.HYPERBROWSER_API_KEY })

export interface CrawlResult {
  html: string
}

export async function crawl(url: string): Promise<CrawlResult> {
  const scrape = await hb.scrape.startAndWait({
    url,
    scrapeOptions: {
      formats: ['html'],
      timeout: 60000,
      waitFor: 5000,
    },
    sessionOptions: {
      useStealth: true,
      adblock: true,
    },
  })

  if (!scrape.data?.html) {
    throw new Error('Failed to crawl page - missing HTML data')
  }

  return {
    html: scrape.data.html,
  }
} 

--- competitor-tracker/lib/db.ts ---
import { promises as fs } from 'fs'
import path from 'path'
import { TrackedURL, Snapshot, Change } from '../types'

const DATA_DIR = path.join(process.cwd(), 'data')
const DB_PATH = path.join(DATA_DIR, 'db.json')

interface DB {
  tracked_urls: TrackedURL[]
  snapshots: Snapshot[]
  changes: Change[]
}

async function ensureFile() {
  await fs.mkdir(DATA_DIR, { recursive: true })
  try {
    await fs.access(DB_PATH)
  } catch {
    const initial: DB = { tracked_urls: [], snapshots: [], changes: [] }
    await fs.writeFile(DB_PATH, JSON.stringify(initial, null, 2))
  }
}

export async function readDb(): Promise<DB> {
  await ensureFile()
  const raw = await fs.readFile(DB_PATH, 'utf-8')
  return JSON.parse(raw)
}

export async function writeDb(db: DB) {
  await fs.writeFile(DB_PATH, JSON.stringify(db, null, 2))
} 

--- competitor-tracker/lib/diff-html.ts ---
import { diffChars, Change } from 'diff'

export function diffHtml(oldHtml: string, newHtml: string): Change[] {
  return diffChars(oldHtml, newHtml)
} 

--- competitor-tracker/lib/notify.ts ---
import { TrackedURL } from '../types'

export async function notify(
  trackedUrl: TrackedURL, 
  summary: { summary: string; impact: 'low' | 'medium' | 'high' }, 
  hasChanges: number
) {
  const webhookUrls = [
    process.env.SLACK_WEBHOOK_URL, 
    process.env.DISCORD_WEBHOOK_URL
  ].filter(Boolean) as string[]

  if (webhookUrls.length === 0) {
    console.log('No webhook URLs configured, skipping notification')
    return
  }

  const payload = {
    username: 'Competitor Tracker',
    embeds: [
      {
        title: `Change detected on ${trackedUrl.url}`,
        description: summary.summary,
        color: summary.impact === 'high' ? 0xff4444 : summary.impact === 'medium' ? 0xffaa44 : 0x44ff44,
        fields: [
          { name: 'Impact', value: summary.impact.toUpperCase(), inline: true },
          { name: 'Type', value: 'Content Changes', inline: true },
          { name: 'URL', value: trackedUrl.url, inline: false },
        ],
        timestamp: new Date().toISOString(),
      },
    ],
  }

  try {
    await Promise.all(
      webhookUrls.map((url) =>
        fetch(url, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(payload),
        }),
      ),
    )
    console.log(`Notification sent for ${trackedUrl.url}`)
  } catch (error) {
    console.error('Failed to send notification:', error)
  }
} 

--- competitor-tracker/lib/summarize.ts ---
import OpenAI from 'openai'
import type { Change } from 'diff'

if (!process.env.OPENAI_API_KEY) {
  throw new Error('OPENAI_API_KEY env var is required')
}

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })

export interface SummaryResult {
  summary: string
  impact: 'low' | 'medium' | 'high'
}

export async function summarize(diffBlocks: Change[]): Promise<SummaryResult> {
  // Extract meaningful changes from the diff
  const addedContent = diffBlocks
    .filter(b => b.added)
    .map(b => b.value)
    .join('')

  const removedContent = diffBlocks
    .filter(b => b.removed)
    .map(b => b.value)
    .join('')

  // Get full content to check for patterns
  const fullContent = diffBlocks.map(b => b.value).join('')

  // If no meaningful changes, return a simple summary
  if (!addedContent && !removedContent) {
    return {
      summary: "Minor content updates detected with no significant visible changes.",
      impact: 'low'
    }
  }

  // Check for specific patterns and return direct descriptions
  const uuidPattern = /[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}/i
  const timePattern = /\d{1,2}:\d{2}:\d{2}|\d{1,2}:\d{2}/
  const datePattern = /\d{4}-\d{2}-\d{2}|\d{1,2}\/\d{1,2}\/\d{4}/
  const numberPattern = /\b\d+\b/g

  // Check if this is a UUID API response (httpbin.org/uuid style)
  if (fullContent.includes('"uuid":') || fullContent.includes('uuid')) {
    // Extract UUIDs from added and removed content
    const addedUuids = addedContent.match(/[0-9a-f-]{20,}/gi) || []
    const removedUuids = removedContent.match(/[0-9a-f-]{20,}/gi) || []
    
    if (addedUuids.length > 0 || removedUuids.length > 0) {
      const oldUuid = removedUuids[0] || 'unknown'
      const newUuid = addedUuids[0] || 'unknown'
      return {
        summary: `UUID updated: ${oldUuid.slice(0, 8)}... â†’ ${newUuid.slice(0, 8)}...`,
        impact: 'low'
      }
    }
  }

  // Check if it's a complete UUID change
  if (uuidPattern.test(addedContent) || uuidPattern.test(removedContent)) {
    const oldUuid = removedContent.match(uuidPattern)?.[0] || 'unknown'
    const newUuid = addedContent.match(uuidPattern)?.[0] || 'unknown'
    return {
      summary: `UUID value changed from ${oldUuid.slice(0, 8)}... to ${newUuid.slice(0, 8)}...`,
      impact: 'low'
    }
  }

  // Check if it's time changes
  if (timePattern.test(addedContent) || timePattern.test(removedContent)) {
    return {
      summary: "Time display updated with current timestamp.",
      impact: 'low'
    }
  }

  // Check if it's date changes
  if (datePattern.test(addedContent) || datePattern.test(removedContent)) {
    return {
      summary: "Date values updated to current date.",
      impact: 'low'
    }
  }

  // Check if it's mostly numbers changing
  const addedNumbers = addedContent.match(numberPattern) || []
  const removedNumbers = removedContent.match(numberPattern) || []
  if (addedNumbers.length > 0 || removedNumbers.length > 0) {
    return {
      summary: `Numeric content updated: ${removedNumbers.slice(0, 3).join(', ')} â†’ ${addedNumbers.slice(0, 3).join(', ')}`,
      impact: 'low'
    }
  }

  // Check for HTML structure changes
  const hasHtmlTags = /<[^>]+>/.test(addedContent + removedContent)
  if (hasHtmlTags) {
    return {
      summary: "HTML structure modified - page layout or elements updated.",
      impact: 'medium'
    }
  }

  // For text content, show actual text changes (limited)
  const cleanAdded = addedContent.replace(/<[^>]*>/g, '').replace(/\s+/g, ' ').trim().slice(0, 50)
  const cleanRemoved = removedContent.replace(/<[^>]*>/g, '').replace(/\s+/g, ' ').trim().slice(0, 50)

  if (cleanAdded || cleanRemoved) {
    let summary = "Text content changed: "
    if (cleanRemoved) summary += `removed "${cleanRemoved}"`
    if (cleanAdded && cleanRemoved) summary += ", "
    if (cleanAdded) summary += `added "${cleanAdded}"`
    
    return {
      summary: summary,
      impact: 'low'
    }
  }

  // Fallback
  return {
    summary: "Content changes detected on the website.",
    impact: 'low'
  }
} 

--- competitor-tracker/supabase/edge-functions/crawl.ts ---
import { serve } from 'https://deno.land/std@0.177.0/http/server.ts'

const SITE_URL = Deno.env.get('SITE_URL') ?? ''

serve(async (req) => {
  const res = await fetch(`${SITE_URL}/api/crawl`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({}),
  })

  return new Response(await res.text(), { status: res.status })
}) 

--- deep-crawler-bot/README.md ---

https://github.com/user-attachments/assets/c81d962e-c99e-463f-ae45-e8472cf4739c


# DeepCrawler ðŸ•·ï¸

> Uncover every hidden API endpoint on any website in 60 seconds.

DeepCrawler is a powerful web application that uses AI-powered browser automation to discover API endpoints on any website. Built with Next.js 14 and powered by the Hyperbrowser SDK.

## âœ¨ Features

- **ðŸš€ Fast Discovery**: Find API endpoints in under 60 seconds
- **ðŸ“Š Real-time Progress**: Live terminal showing crawl progress
- **ðŸ“ Export Options**: Download as Postman collection or copy as JSON
- **ðŸŽ¨ Modern UI**: Glassmorphism design with smooth animations
- **ðŸ”’ Rate Limited**: Built-in protection against abuse
- **ðŸ“± Responsive**: Works on desktop and mobile devices

## ðŸ› ï¸ Tech Stack

- **Framework**: Next.js 14 (App Router)
- **Language**: TypeScript
- **Styling**: Tailwind CSS
- **Automation**: Hyperbrowser SDK
- **Icons**: Lucide React
- **Deployment**: Vercel-ready

## ðŸš€ Quick Start

### Prerequisites

- Node.js 18+ 
- A [Hyperbrowser API key](https://hyperbrowser.ai)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd deep-crawler-bot
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env.local
   ```
   
   Add your Hyperbrowser API key to `.env.local`:
   ```env
   HYPERBROWSER_API_KEY=your_api_key_here
   ```

4. **Run the development server**
   ```bash
   npm run dev
   ```

5. **Open your browser**
   Navigate to [http://localhost:3000](http://localhost:3000)

## ðŸ“– Usage

1. **Enter a URL**: Paste any website URL (e.g., `news.ycombinator.com`)
2. **Start Crawling**: Click "Start Crawl" to begin discovery
3. **Watch Progress**: Monitor real-time logs in the terminal sidebar
4. **Export Results**: Download as Postman collection or copy JSON

### Example Results

Crawling `https://hyperbrowser.ai` typically discovers:
- `/api/stories` - Story listings
- `/api/user/{id}` - User profiles  
- `/api/item/{id}` - Individual items
- And many more...

## ðŸ”§ Configuration

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `HYPERBROWSER_API_KEY` | Your Hyperbrowser API key | âœ… |

### Customization

The app can be customized by modifying:
- **Colors**: Update `tailwind.config.js` 
- **Timeout**: Adjust crawl timeout in `app/api/crawl/route.ts`
- **Rate Limits**: Modify rate limiting in the API route
- **UI Components**: Edit components in `/components`

## ðŸ—ï¸ Project Structure

```
deep-crawler-bot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/crawl/route.ts      # Main crawling endpoint
â”‚   â”œâ”€â”€ layout.tsx              # Root layout
â”‚   â”œâ”€â”€ page.tsx                # Home page
â”‚   â””â”€â”€ globals.css             # Global styles
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ UrlForm.tsx             # URL input form
â”‚   â”œâ”€â”€ ProgressBar.tsx         # Progress indicator
â”‚   â”œâ”€â”€ ResultCard.tsx          # Results display
â”‚   â””â”€â”€ TerminalSidebar.tsx     # Live log sidebar
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ hyper.ts                # Hyperbrowser wrapper
â”‚   â””â”€â”€ utils.ts                # Helper functions
â””â”€â”€ public/                     # Static assets
```

## ðŸ”¬ How It Works

1. **Input Validation**: URL is validated and normalized
2. **Rate Limiting**: IP-based rate limiting (1 request/hour)
3. **Browser Launch**: Stealth browser with residential proxy
4. **Network Monitoring**: Intercepts all network requests
5. **API Detection**: Filters for API-like endpoints
6. **Real-time Streaming**: Server-Sent Events for live updates
7. **Data Processing**: Deduplication and Postman generation
8. **Export**: Multiple export formats available

## ðŸ“Š Performance

- **TTFB**: < 200ms on Vercel
- **Build Time**: ~30 seconds
- **Bundle Size**: Optimized for production
- **Crawl Speed**: Most sites complete in 30-60 seconds

## ðŸš¢ Deployment

### Vercel (Recommended)

1. **Connect your repository** to Vercel
2. **Add environment variables** in project settings
3. **Deploy** - that's it!

### Other Platforms

The app works on any Node.js hosting platform:
- Railway
- Render  
- Heroku
- DigitalOcean App Platform

## ðŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Development

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ðŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ðŸ†˜ Support

- **Documentation**: [Hyperbrowser Docs](https://docs.hyperbrowser.ai)
- **API Issues**: Contact Hyperbrowser support
- **App Issues**: Open a GitHub issue

## ðŸ™ Acknowledgments

- [Hyperbrowser](https://hyperbrowser.ai) for the powerful browser automation SDK

---

<div align="center">
  <p>Built with â¤ï¸ using Hyperbrowser SDK</p>
  <p>
    <a href="https://hyperbrowser.ai">Get your API key</a> â€¢
    <a href="https://docs.hyperbrowser.ai">Documentation</a>
  </p>
</div>


## Links discovered
- [Hyperbrowser API key](https://hyperbrowser.ai)
- [http://localhost:3000](http://localhost:3000)
- [LICENSE](https://github.com/hyperbrowserai/hyperbrowser-app-examples/blob/main/deep-crawler-bot/LICENSE.md)
- [Hyperbrowser Docs](https://docs.hyperbrowser.ai)
- [Hyperbrowser](https://hyperbrowser.ai)
- [Get your API key](https://hyperbrowser.ai)
- [Documentation](https://docs.hyperbrowser.ai)

--- deep-crawler-bot/next.config.js ---
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    serverComponentsExternalPackages: ['@hyperbrowser/sdk']
  },
  webpack: (config) => {
    config.resolve.fallback = {
      ...config.resolve.fallback,
      fs: false,
      net: false,
      tls: false,
    };
    return config;
  },
};

module.exports = nextConfig;


--- deep-crawler-bot/tailwind.config.js ---
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './pages/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    './app/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {
      colors: {
        accent: '#F0FF26',
        terminal: '#000000',
      },
      fontFamily: {
        sans: ['Inter', 'system-ui', 'sans-serif'],
        mono: ['ui-monospace', 'SF Mono', 'Monaco', 'monospace'],
      },
      fontWeight: {
        normal: '500',
        medium: '500',
        semibold: '600',
        bold: '700',
      },
      letterSpacing: {
        tight4: '-0.04em',
      },
      animation: {
        'fade-in': 'fadeIn 0.5s ease-out',
      },
      keyframes: {
        fadeIn: {
          '0%': { opacity: '0', transform: 'translateY(10px)' },
          '100%': { opacity: '1', transform: 'translateY(0)' },
        },
      },
      backdropBlur: {
        md: '12px',
      },
    },
  },
  plugins: [],
} 

--- deep-crawler-bot/lib/hyper.ts ---
import { Hyperbrowser } from '@hyperbrowser/sdk'

if (!process.env.HYPERBROWSER_API_KEY) {
  throw new Error('HYPERBROWSER_API_KEY environment variable is required')
}

export const hb = new Hyperbrowser({
  apiKey: process.env.HYPERBROWSER_API_KEY,
})

export interface CrawlOptions {
  url: string
  timeout?: number
  stealth?: boolean
  proxy?: 'residential' | 'datacenter' | false
}

export interface ApiEndpoint {
  method: string
  url: string
  status: number
  size: number
  timestamp?: number
}

export const defaultCrawlOptions: Partial<CrawlOptions> = {
  timeout: 60000,
  stealth: true,
  proxy: 'residential'
} 

--- deep-crawler-bot/lib/utils.ts ---
interface ApiEndpoint {
  method: string
  url: string
  status: number
  size: number
}

export function dedupeEndpoints(endpoints: ApiEndpoint[]): ApiEndpoint[] {
  const seen = new Set<string>()
  const unique: ApiEndpoint[] = []

  for (const endpoint of endpoints) {
    const key = `${endpoint.method}:${endpoint.url}`
    if (!seen.has(key)) {
      seen.add(key)
      unique.push(endpoint)
    }
  }

  return unique.sort((a, b) => {
    // Sort by method first, then by URL
    if (a.method !== b.method) {
      return a.method.localeCompare(b.method)
    }
    return a.url.localeCompare(b.url)
  })
}

export function generatePostmanCollection(originalUrl: string, endpoints: ApiEndpoint[]) {
  const hostname = new URL(originalUrl).hostname
  const collectionName = `DeepCrawler - ${hostname}`

  const collection = {
    info: {
      name: collectionName,
      description: `API endpoints discovered by DeepCrawler from ${originalUrl}`,
      schema: 'https://schema.getpostman.com/json/collection/v2.1.0/collection.json'
    },
    item: endpoints.map((endpoint, index) => {
      const url = new URL(endpoint.url)
      const pathSegments = url.pathname.split('/').filter(Boolean)
      const name = pathSegments.length > 0 
        ? pathSegments[pathSegments.length - 1] || 'root'
        : 'root'

      return {
        name: `${endpoint.method} ${name}`,
        request: {
          method: endpoint.method,
          header: [
            {
              key: 'Accept',
              value: 'application/json',
              type: 'text'
            },
            {
              key: 'User-Agent',
              value: 'DeepCrawler/1.0',
              type: 'text'
            }
          ],
          url: {
            raw: endpoint.url,
            protocol: url.protocol.slice(0, -1),
            host: url.hostname.split('.'),
            port: url.port || (url.protocol === 'https:' ? '443' : '80'),
            path: pathSegments,
            query: url.search ? url.search.slice(1).split('&').map(param => {
              const [key, value = ''] = param.split('=')
              return { key: decodeURIComponent(key), value: decodeURIComponent(value) }
            }) : []
          },
          description: `Status: ${endpoint.status}, Size: ${endpoint.size} bytes`
        },
        response: []
      }
    }),
    variable: []
  }

  return collection
}

export function extractHostname(url: string): string {
  try {
    return new URL(url).hostname
  } catch {
    return 'unknown-host'
  }
}

export function formatBytes(bytes: number): string {
  if (bytes === 0) return '0 B'
  const k = 1024
  const sizes = ['B', 'KB', 'MB', 'GB']
  const i = Math.floor(Math.log(bytes) / Math.log(k))
  return parseFloat((bytes / Math.pow(k, i)).toFixed(1)) + ' ' + sizes[i]
}

export function isApiUrl(url: string): boolean {
  const apiPatterns = [
    /\/api\//i,
    /\/v\d+\//i,
    /\.json(\?|$)/i,
    /\/graphql/i,
    /\/rest\//i,
    /\/endpoints?\//i
  ]
  
  return apiPatterns.some(pattern => pattern.test(url))
} 

--- deep-job-researcher/README.md ---
# Deep Job Researcher

**Built with [Hyperbrowser](https://hyperbrowser.ai)**

A production-ready Next.js app that matches your resume or portfolio with live job postings using AI-powered web scraping and intelligent matching.

## Features

- **Resume Analysis**: Upload PDF resumes and extract candidate profiles
- **Portfolio Scraping**: Live scraping of portfolio websites using Hyperbrowser
- **Real-time Job Crawling**: Scrapes actual job boards for current openings
- **AI-Powered Matching**: Uses OpenAI to score and rank job matches
- **Live Console**: Real-time progress tracking during crawling and analysis
- **Export Options**: Download results as JSON or CSV
- **Dark Theme**: Professional black and green interface

## Setup

1. **Get an API key** at [hyperbrowser.ai](https://hyperbrowser.ai)

2. **Clone and install**:
   ```bash
   git clone <repository-url>
   cd deep-job-researcher
   npm install
   ```

3. **Environment setup**:
   ```bash
   cp .env.example .env.local
   # Add your API keys to .env.local
   ```

4. **Quick start**:
   ```bash
   npm run dev
   ```

## Usage

1. Toggle between Resume or Portfolio mode
2. Upload a PDF resume or enter a portfolio URL
3. Watch the live console as the app crawls job boards
4. Review AI-matched job opportunities with scores and tailored pitches
5. Export results or copy outreach emails

## Growth Use Case

Perfect for job seekers who want to automate the discovery and matching process with real-time job market data, demonstrating Hyperbrowser's ability to turn any website into structured data.

---

Follow @hyperbrowser_ai for updates.


## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)
- [hyperbrowser.ai](https://hyperbrowser.ai)

--- deep-job-researcher/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;


--- deep-job-researcher/tailwind.config.js ---
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './pages/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    './app/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {
      colors: {
        background: '#0B0B0B',
        foreground: '#E5E5E5',
        accent: '#00FF88',
        'accent-hover': '#00DD77',
        console: '#1A1A1A',
        border: '#333333',
        gray: {
          900: '#0B0B0B',
          800: '#1A1A1A',
          700: '#333333',
          600: '#666666',
          500: '#999999',
          400: '#CCCCCC',
          300: '#DDDDDD',
          200: '#E5E5E5',
          100: '#F5F5F5',
        }
      },
      fontFamily: {
        mono: ['ui-monospace', 'SFMono-Regular', 'Monaco', 'Consolas', 'Liberation Mono', 'Menlo', 'monospace'],
      },
    },
  },
  plugins: [],
} 

--- deep-job-researcher/lib/candidate.ts ---
import OpenAI from 'openai';

export interface CandidateProfile {
  name?: string;
  headline?: string;
  skills: string[];
  yearsExperience: number;
  topProjects: string[];
  gaps: string[];
  suggestions: string[];
}

function getOpenAIClient() {
  if (!process.env.OPENAI_API_KEY) {
    throw new Error('OPENAI_API_KEY environment variable is required');
  }
  return new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });
}

export async function buildCandidateProfile(text: string): Promise<CandidateProfile> {
  try {
    const openai = getOpenAIClient();
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'system',
          content: `Extract candidate profile information from the provided text (resume or portfolio). Return a JSON object with the following structure:
          {
            "name": "string (if present)",
            "headline": "string (brief professional title/summary)",
            "skills": ["array of technical skills, tools, frameworks, languages"],
            "yearsExperience": "number (estimated total years of professional experience)",
            "topProjects": ["array of notable projects or achievements"],
            "gaps": ["array of potential skill gaps or areas for improvement"],
            "suggestions": ["array of suggested improvements or focus areas"]
          }
          
          Be thorough in extracting skills and technologies mentioned. For years of experience, make a reasonable estimate based on career progression, education, and project timelines.`
        },
        {
          role: 'user',
          content: text
        }
      ],
      response_format: { type: 'json_object' },
      temperature: 0.3,
    });

    const content = completion.choices[0]?.message?.content;
    if (!content) {
      throw new Error('No response from OpenAI');
    }

    const profile = JSON.parse(content) as CandidateProfile;
    
    // Validate and provide defaults
    return {
      name: profile.name || undefined,
      headline: profile.headline || 'Professional',
      skills: Array.isArray(profile.skills) ? profile.skills : [],
      yearsExperience: typeof profile.yearsExperience === 'number' ? profile.yearsExperience : 0,
      topProjects: Array.isArray(profile.topProjects) ? profile.topProjects : [],
      gaps: Array.isArray(profile.gaps) ? profile.gaps : [],
      suggestions: Array.isArray(profile.suggestions) ? profile.suggestions : [],
    };
  } catch (error) {
    console.error('Error building candidate profile:', error);
    throw new Error('Failed to analyze candidate profile');
  }
} 

--- deep-job-researcher/lib/hb.ts ---
import { Hyperbrowser } from '@hyperbrowser/sdk';

export function getClient(): Hyperbrowser {
  const apiKey = process.env.HYPERBROWSER_API_KEY;
  if (!apiKey) {
    throw new Error('HYPERBROWSER_API_KEY environment variable is required');
  }
  return new Hyperbrowser({ apiKey });
}

export async function withClient<T>(
  operation: (client: Hyperbrowser) => Promise<T>
): Promise<T> {
  const client = getClient();
  return await operation(client);
} 

--- deep-job-researcher/lib/jobs.ts ---
import { withClient } from './hb';

export interface JobListing {
  title: string;
  company: string;
  location?: string;
  remote?: boolean;
  url: string;
  description: string;
  source: string;
}

export interface JobSource {
  name: string;
  url: string;
  depth: number;
  selector?: string;
}

const JOB_SOURCES: JobSource[] = [
  {
    name: 'Work at a Startup',
    url: 'https://www.workatastartup.com/jobs',
    depth: 2,
  },
];

export async function crawlJobSources(
  onProgress?: (message: string) => void
): Promise<JobListing[]> {
  const allJobs: JobListing[] = [];

  for (const source of JOB_SOURCES) {
    try {
      onProgress?.(`[CRAWL] Starting ${source.name}...`);
      
      const jobs = await withClient(async (client) => {
        const result = await client.crawl.startAndWait({
          url: source.url,
          maxPages: 10,
          scrapeOptions: {
            formats: ['markdown', 'html'],
          },
        });

        onProgress?.(`[CRAWL] ${source.name} - ${result.status || 'completed'} - ${result.data?.length || 0} pages`);
        
        return parseJobsFromCrawlResult(result, source);
      });

      allJobs.push(...jobs);
      onProgress?.(`[PARSE] Extracted ${jobs.length} jobs from ${source.name}`);
      
    } catch (error) {
      console.error(`Error crawling ${source.name}:`, error);
      onProgress?.(`[ERROR] Failed to crawl ${source.name}: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  onProgress?.(`[COMPLETE] Total jobs found: ${allJobs.length}`);
  return allJobs;
}

function parseJobsFromCrawlResult(crawlResult: any, source: JobSource): JobListing[] {
  const jobs: JobListing[] = [];
  
  if (!crawlResult.data) {
    return jobs;
  }

  for (const page of crawlResult.data) {
    const content = page.markdown || page.html;
    if (!content) continue;

    const pageJobs = extractJobsFromHTML(content, page.metadata?.url || source.url, source);
    jobs.push(...pageJobs);
  }

  return jobs;
}

function extractJobsFromHTML(content: string, pageUrl: string, source: JobSource): JobListing[] {
  const jobs: JobListing[] = [];
  
  // Check if content is markdown or HTML
  const isMarkdown = !content.includes('<html') && !content.includes('<!DOCTYPE');
  
  if (isMarkdown) {
    // Parse markdown content (preferred for Work at a Startup)
    const lines = content.split('\n');
    let currentJob: Partial<JobListing> = {};
    
    for (let i = 0; i < lines.length; i++) {
      const line = lines[i].trim();
      
      // Look for job titles (markdown headers or links)
      const titleMatch = line.match(/^#{1,6}\s*(.+)$/) || line.match(/^\[([^\]]+)\]/);
      const linkMatch = line.match(/\[([^\]]+)\]\(([^)]+)\)/); // Extract [title](url) format
      
      if (titleMatch && isJobTitle(titleMatch[1])) {
        // Save previous job if complete
        if (currentJob.title && currentJob.company) {
          jobs.push({
            title: currentJob.title,
            company: currentJob.company,
            url: currentJob.url || pageUrl,
            description: currentJob.description || 'No description available',
            source: source.name,
            remote: currentJob.remote,
            location: currentJob.location,
          });
        }
        
        // Start new job
        let jobUrl = pageUrl;
        let jobTitle = titleMatch[1].trim();
        
        // If it's a markdown link, extract the URL
        if (linkMatch) {
          jobTitle = linkMatch[1].trim();
          jobUrl = linkMatch[2].trim();
          
          // Make sure URL is absolute
          if (jobUrl.startsWith('/')) {
            const baseUrl = new URL(pageUrl).origin;
            jobUrl = baseUrl + jobUrl;
          }
        }
        
        currentJob = {
          title: jobTitle,
          url: jobUrl,
        };
        
        // Look for company info in next few lines
        for (let j = i + 1; j < Math.min(i + 5, lines.length); j++) {
          const nextLine = lines[j].trim();
          if (nextLine && !nextLine.startsWith('#') && !nextLine.startsWith('*')) {
            // Extract company name (remove markdown formatting)
            const company = nextLine.replace(/[\*_\[\]()]/g, '').trim();
            if (company.length > 1 && company.length < 50) {
              currentJob.company = company;
              break;
            }
          }
        }
        
        // Extract description from surrounding context
        const descLines = lines.slice(i + 1, Math.min(i + 10, lines.length));
        currentJob.description = descLines
          .filter(l => l.trim() && !l.startsWith('#'))
          .join(' ')
          .replace(/[\*_\[\]]/g, '')
          .substring(0, 200);
          
        currentJob.remote = /remote|anywhere|distributed/i.test(content);
        currentJob.location = extractLocationFromText(content);
      }
    }
    
    // Don't forget the last job
    if (currentJob.title && currentJob.company) {
      jobs.push({
        title: currentJob.title,
        company: currentJob.company,
        url: currentJob.url || pageUrl,
        description: currentJob.description || 'No description available',
        source: source.name,
        remote: currentJob.remote,
        location: currentJob.location,
      });
    }
  } else {
    // Parse HTML content (fallback)
    const jobPatterns = [
      // Work at a Startup patterns with links
      /<a[^>]*href=["']([^"']+)["'][^>]*>[\s\S]*?<h[1-6][^>]*>([^<]+)<\/h[1-6]>[\s\S]*?<\/a>/gi,
      /<div[^>]*job[^>]*>[\s\S]*?<h[1-6][^>]*>([^<]+)<\/h[1-6]>[\s\S]*?company[^>]*>([^<]+)<[\s\S]*?<\/div>/gi,
      /<article[^>]*>[\s\S]*?<h[1-6][^>]*>([^<]+)<\/h[1-6]>[\s\S]*?<span[^>]*>([^<]+)<\/span>[\s\S]*?<\/article>/gi,
      // Generic patterns
      /<div[^>]*role[^>]*>[\s\S]*?<h[1-6][^>]*>([^<]+)<\/h[1-6]>[\s\S]*?<span[^>]*>([^<]+)<\/span>[\s\S]*?<\/div>/gi,
    ];

    for (const pattern of jobPatterns) {
      let match;
      while ((match = pattern.exec(content)) !== null) {
        let title, company, jobUrl = pageUrl;
        
        // Check if first capture group is URL (link pattern)
        if (match[1] && match[1].includes('/')) {
          jobUrl = match[1];
          title = match[2]?.trim();
          company = extractCompanyFromJobContent(content, title);
          
          // Make URL absolute if relative
          if (jobUrl.startsWith('/')) {
            const baseUrl = new URL(pageUrl).origin;
            jobUrl = baseUrl + jobUrl;
          }
        } else {
          // Regular pattern
          title = match[1]?.trim();
          company = match[2]?.trim();
        }
        
        if (title && title.length > 3 && isJobTitle(title)) {
          jobs.push({
            title,
            company: company || source.name,
            url: jobUrl,
            description: extractJobDescription(content, title),
            source: source.name,
            remote: /remote|anywhere|distributed/i.test(content),
            location: extractLocation(content),
          });
        }
      }
    }

    // If no jobs found with patterns, try a more generic approach
    if (jobs.length === 0) {
      const titleMatches = content.match(/<h[1-6][^>]*>([^<]{10,100})<\/h[1-6]>/gi);
      if (titleMatches) {
        titleMatches.slice(0, 10).forEach((match, index) => {
          const title = match.replace(/<[^>]*>/g, '').trim();
          if (title && isJobTitle(title)) {
            jobs.push({
              title,
              company: source.name,
              url: pageUrl,
              description: `Job posting from ${source.name}`,
              source: source.name,
            });
          }
        });
      }
    }
  }

  return jobs.slice(0, 20); // Limit to 20 jobs per source
}

function extractJobDescription(html: string, title: string): string {
  // Try to find description near the title
  const titleIndex = html.toLowerCase().indexOf(title.toLowerCase());
  if (titleIndex === -1) return 'No description available';
  
  const surrounding = html.substring(titleIndex, titleIndex + 500);
  const cleanText = surrounding.replace(/<[^>]*>/g, ' ').replace(/\s+/g, ' ').trim();
  
  return cleanText.substring(0, 200) + (cleanText.length > 200 ? '...' : '');
}

function extractLocation(html: string): string | undefined {
  const locationPattern = /(?:location|based|office)[^>]*>([^<]+)</gi;
  const match = locationPattern.exec(html);
  return match?.[1]?.trim();
}

function extractLocationFromText(text: string): string | undefined {
  const locationPatterns = [
    /(?:location|based|office)[\s:]*([^\n\r\|]{1,50})/gi,
    /(?:remote|san francisco|new york|london|berlin|toronto|seattle|austin|boston|chicago|los angeles|miami|denver)/gi,
  ];
  
  for (const pattern of locationPatterns) {
    const match = pattern.exec(text);
    if (match) {
      return match[1]?.trim() || match[0]?.trim();
    }
  }
  
  return undefined;
}

function extractCompanyFromJobContent(content: string, jobTitle: string): string {
  // Look for company name near the job title
  const titleIndex = content.toLowerCase().indexOf(jobTitle.toLowerCase());
  if (titleIndex === -1) return 'Unknown Company';
  
  // Search in surrounding text
  const surrounding = content.substring(
    Math.max(0, titleIndex - 200), 
    titleIndex + jobTitle.length + 200
  );
  
  // Common company name patterns
  const companyPatterns = [
    /company:\s*([^\n\r]{1,50})/gi,
    /at\s+([A-Z][a-zA-Z\s&\.]{2,30})/g,
    /\b([A-Z][a-zA-Z]{2,}(?:\s+[A-Z][a-zA-Z]*)*)\s+(?:is|seeks|hiring)/g,
  ];
  
  for (const pattern of companyPatterns) {
    const match = pattern.exec(surrounding);
    if (match && match[1]) {
      const company = match[1].trim();
      if (company.length > 2 && company.length < 40) {
        return company;
      }
    }
  }
  
  return 'Unknown Company';
}

function isJobTitle(text: string): boolean {
  const jobKeywords = [
    'engineer', 'developer', 'designer', 'manager', 'analyst', 'specialist',
    'consultant', 'architect', 'lead', 'senior', 'junior', 'intern',
    'frontend', 'backend', 'fullstack', 'devops', 'qa', 'data',
    'product', 'marketing', 'sales', 'support', 'operations'
  ];
  
  const lowerText = text.toLowerCase();
  return jobKeywords.some(keyword => lowerText.includes(keyword));
} 

## Links discovered
- [title](https://github.com/hyperbrowserai/hyperbrowser-app-examples/blob/main/deep-job-researcher/lib/url.md)
- ["'](https://github.com/hyperbrowserai/hyperbrowser-app-examples/blob/main/deep-job-researcher/lib/[^"']+.md)

--- deep-job-researcher/lib/match.ts ---
import OpenAI from 'openai';
import { CandidateProfile } from './candidate';
import { JobListing } from './jobs';

export interface JobMatch {
  title: string;
  company: string;
  url: string;
  score: number;
  missingSkills: string[];
  pitch: string;
  location?: string;
  remote?: boolean;
  source: string;
}

function getOpenAIClient() {
  if (!process.env.OPENAI_API_KEY) {
    throw new Error('OPENAI_API_KEY environment variable is required');
  }
  return new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });
}

export async function llmMatch(
  candidate: CandidateProfile,
  jobs: JobListing[],
  onProgress?: (message: string) => void
): Promise<JobMatch[]> {
  try {
    onProgress?.('[LLM] Starting job matching analysis...');

    const openai = getOpenAIClient();
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'system',
          content: `You are a professional recruiter analyzing job matches for a candidate. Given a candidate profile and a list of job opportunities, provide a detailed analysis of how well each job matches the candidate.

          Return a JSON object with this exact structure:
          {
            "matches": [
              {
                "title": "exact job title from the job listing",
                "company": "exact company name from the job listing", 
                "url": "exact URL from the job listing",
                "score": "number from 0-100 indicating match quality",
                "missingSkills": ["array of skills candidate lacks for this role"],
                "pitch": "one sentence tailored pitch for why this candidate fits this role"
              }
            ]
          }

          Scoring criteria:
          - 90-100: Perfect match, candidate has all required skills and experience
          - 70-89: Strong match, candidate has most skills with minor gaps
          - 50-69: Good match, candidate has core skills but missing some important ones
          - 30-49: Potential match, candidate has some relevant skills but significant gaps
          - 0-29: Poor match, candidate lacks most required skills

          For missingSkills, identify specific technical skills, tools, or experience gaps.
          For pitch, craft a compelling one-liner that highlights the candidate's best attributes for that specific role.
          
          Only return jobs with scores above 30. Sort by score descending.`
        },
        {
          role: 'user',
          content: `Candidate Profile:
          Name: ${candidate.name || 'Anonymous'}
          Headline: ${candidate.headline}
          Years Experience: ${candidate.yearsExperience}
          Skills: ${candidate.skills.join(', ')}
          Top Projects: ${candidate.topProjects.join(', ')}
          
          Job Opportunities:
          ${jobs.map((job, index) => `
          ${index + 1}. ${job.title} at ${job.company}
          URL: ${job.url}
          Description: ${job.description}
          Location: ${job.location || 'Not specified'}
          Remote: ${job.remote ? 'Yes' : 'No'}
          Source: ${job.source}
          `).join('\n')}`
        }
      ],
      response_format: { type: 'json_object' },
      temperature: 0.3,
    });

    const content = completion.choices[0]?.message?.content;
    if (!content) {
      throw new Error('No response from OpenAI');
    }

    onProgress?.('[LLM] Processing match results...');

    const response = JSON.parse(content);
    const matches = response.matches || [];

    // Validate and enhance matches with original job data
    const validMatches = matches
      .filter((match: any) => match.score >= 30)
      .map((match: any) => {
        const originalJob = jobs.find(j => 
          j.title === match.title && j.company === match.company
        );
        
        return {
          title: match.title,
          company: match.company,
          url: match.url,
          score: Math.min(100, Math.max(0, match.score)),
          missingSkills: Array.isArray(match.missingSkills) ? match.missingSkills : [],
          pitch: match.pitch || 'Great fit for this role',
          location: originalJob?.location,
          remote: originalJob?.remote,
          source: originalJob?.source || 'Unknown',
        };
      })
      .sort((a: JobMatch, b: JobMatch) => b.score - a.score);

    onProgress?.(`[LLM] Found ${validMatches.length} quality matches`);
    return validMatches;
    
  } catch (error) {
    console.error('Error in LLM matching:', error);
    onProgress?.('[ERROR] Failed to analyze job matches');
    throw new Error('Failed to analyze job matches');
  }
}

export function formatMatchesForExport(matches: JobMatch[], candidate: CandidateProfile) {
  return {
    candidate: {
      name: candidate.name,
      headline: candidate.headline,
      yearsExperience: candidate.yearsExperience,
      skills: candidate.skills,
    },
    matches: matches.map(match => ({
      ...match,
      exportDate: new Date().toISOString(),
    })),
    summary: {
      totalMatches: matches.length,
      averageScore: matches.length > 0 
        ? Math.round(matches.reduce((sum, match) => sum + match.score, 0) / matches.length)
        : 0,
      topScore: matches.length > 0 ? Math.max(...matches.map(m => m.score)) : 0,
    }
  };
}

export function generateOutreachEmail(match: JobMatch, candidate: CandidateProfile): string {
  const subject = `Application for ${match.title} at ${match.company}`;
  
  const body = `Subject: ${subject}

Dear Hiring Manager,

I hope this email finds you well. I am writing to express my strong interest in the ${match.title} position at ${match.company}.

${match.pitch} With ${candidate.yearsExperience} years of experience and expertise in ${candidate.skills.slice(0, 5).join(', ')}, I believe I would be a valuable addition to your team.

${candidate.topProjects.length > 0 ? `Some of my notable achievements include: ${candidate.topProjects.slice(0, 2).join(', ')}.` : ''}

I would welcome the opportunity to discuss how my background and skills align with your team's needs. Thank you for your consideration.

Best regards,
${candidate.name || 'Your Name'}

---
Application generated with Hyperbrowser Deep Job Researcher`;

  return body;
} 

--- deep-job-researcher/lib/pdf.ts ---
export async function extractTextFromPDF(buffer: Buffer): Promise<string> {
  try {
    console.log('PDF extraction - buffer size:', buffer.length);
    
    if (buffer.length === 0) {
      throw new Error('PDF buffer is empty');
    }

    // Check if it's actually a PDF by looking at the header
    const header = buffer.toString('utf8', 0, 4);
    if (!header.startsWith('%PDF')) {
      throw new Error('File does not appear to be a valid PDF (missing PDF header)');
    }

    // Try traditional PDF text extraction first
    try {
      console.log('Trying pdf-parse-new...');
      const pdfParse = await import('pdf-parse-new');
      const data = await pdfParse.default(buffer);
      console.log('pdf-parse-new result:', { pages: data.numpages, textLength: data.text?.length || 0 });
      
      if (data.text && data.text.trim().length > 50) {
        console.log('PDF text extraction successful, length:', data.text.length);
        return data.text;
      }
      
      console.log('PDF text extraction returned insufficient content, trying GPT-4o...');
    } catch (error) {
      console.error('PDF text extraction failed:', error);
      console.log('Falling back to GPT-4o for complex/image-based PDF...');
    }

    // If traditional extraction failed, provide helpful error message
    throw new Error('Could not extract text from PDF. This might be an image-based or complex PDF. Try using the Portfolio URL option instead, or use a text-based PDF.');

  } catch (error) {
    console.error('PDF extraction failed:', error);
    throw new Error(`PDF extraction failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

export function validatePDFFile(file: File): { valid: boolean; error?: string } {
  if (!file) {
    return { valid: false, error: 'No file provided' };
  }

  if (file.type !== 'application/pdf') {
    return { valid: false, error: 'File must be a PDF' };
  }

  const maxSize = 5 * 1024 * 1024; // 5MB
  if (file.size > maxSize) {
    return { valid: false, error: 'File size must be less than 5MB' };
  }

  return { valid: true };
} 

--- deep-reddit-researcher/README.md ---
# Deep Reddit Researcher

**Built with [Hyperbrowser](https://hyperbrowser.ai)**

A powerful Next.js application that performs deep Reddit research using AI-powered web automation. Search any topic and get real-time insights from Reddit discussions with live screenshots and intelligent Q&A capabilities.

## âœ¨ Features

- **Real-time Reddit Research** - Automatically searches and explores Reddit threads
- **Live Screenshots** - Watch as the app browses through Reddit pages in real-time
- **AI-Powered Q&A** - Ask questions about your research findings using GPT-4o-mini
- **Anti-Detection Browsing** - Uses Hyperbrowser's stealth technology to bypass Reddit's bot protection
- **Professional UI** - Clean, modern interface with live progress tracking
- **Data Export** - Download research results as JSON for further analysis

## ðŸš€ Quick Start

### Prerequisites

- Node.js 18+ installed
- A Hyperbrowser API key
- An OpenAI API key

### 1. Get Your API Keys

Get your Hyperbrowser API key at **[https://hyperbrowser.ai](https://hyperbrowser.ai)**

### 2. Clone and Install

```bash
git clone <your-repo-url>
cd deep-reddit-researcher
npm install
```

### 3. Environment Setup

Create a `.env.local` file in the root directory:

```env
HYPERBROWSER_API_KEY=your_hyperbrowser_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
```

### 4. Run the Application

```bash
# Start development server
npm run dev

# Build for production
npm run build

# Start production server
npm start
```

Open [http://localhost:3000](http://localhost:3000) to use the application.

## ðŸ“– How to Use

1. **Start Research** - Enter any topic you want to research on Reddit
2. **Watch Live Progress** - See real-time screenshots as the app browses Reddit
3. **Review Results** - Screenshots appear in the left panel as pages are visited
4. **Ask Questions** - Once research is complete, ask AI questions about the findings
5. **Download Data** - Export your research results as JSON

## ðŸ”§ How It Works

### Research Process
1. **Search Reddit** - Uses multiple Reddit URL formats for maximum success
2. **Stealth Browsing** - Employs Hyperbrowser's anti-detection technology
3. **Content Extraction** - Scrapes thread titles, content, and comments
4. **Screenshot Capture** - Takes 1280x720 screenshots of each page visited
5. **AI Analysis** - Processes content for intelligent Q&A responses

### Anti-Bot Protection
- Uses proxy rotation and stealth browsing
- Handles CAPTCHA solving automatically
- Bypasses Reddit's rate limiting and bot detection
- Falls back to mock data if all attempts are blocked

## ðŸ—ï¸ Technical Stack

- **Framework**: Next.js 15 with App Router
- **Styling**: Tailwind CSS v4
- **Animations**: Framer Motion
- **Web Automation**: Hyperbrowser SDK
- **AI**: OpenAI GPT-4o-mini
- **Data Processing**: Cheerio for HTML parsing
- **TypeScript**: Full type safety

## ðŸ“ Project Structure

```
deep-reddit-researcher/
â”œâ”€â”€ app/                    # Next.js app directory
â”‚   â”œâ”€â”€ api/search/        # API route for research & Q&A
â”‚   â”œâ”€â”€ globals.css        # Global styles
â”‚   â”œâ”€â”€ layout.tsx         # Root layout
â”‚   â””â”€â”€ page.tsx           # Main application page
â”œâ”€â”€ components/            # React components
â”‚   â”œâ”€â”€ DownloadBtn.tsx    # Data export functionality
â”‚   â”œâ”€â”€ LiveConsole.tsx    # Activity log display
â”‚   â”œâ”€â”€ Progress.tsx       # Progress bar component
â”‚   â”œâ”€â”€ QueryForm.tsx      # Search input form
â”‚   â””â”€â”€ ShotCarousel.tsx   # Screenshot carousel
â”œâ”€â”€ lib/                   # Utility libraries
â”‚   â”œâ”€â”€ hb.ts             # Hyperbrowser client setup
â”‚   â”œâ”€â”€ json.ts           # JSON utilities
â”‚   â”œâ”€â”€ openai.ts         # OpenAI integration
â”‚   â””â”€â”€ reddit.ts         # Reddit scraping logic
â””â”€â”€ public/shots/         # Screenshot storage
```

## âš¡ Performance Notes

- Initial searches may take 60-120 seconds due to Reddit's anti-bot measures
- Subsequent requests are typically faster (30-60 seconds)
- Screenshots are optimized and cached locally
- Uses Hyperbrowser's global proxy network for reliability

## ðŸ”’ Security & Privacy

- API keys are stored securely in environment variables
- All web requests go through Hyperbrowser's secure infrastructure
- No user data is stored or logged
- Screenshots are saved locally and can be deleted anytime

## ðŸ› Troubleshooting

### Common Issues

**Slow initial loading**: This is normal due to Reddit's anti-bot protection. First searches can take 1-2 minutes.

**500 errors**: Usually indicates API key issues or network timeouts. Check your environment variables.

**No screenshots**: Ensure the `public/shots/` directory exists and is writable.

**Q&A not working**: Verify your OpenAI API key is correct and has sufficient credits.

### Debug Mode

Check the browser console and terminal logs for detailed debugging information during research.

## ðŸ“„ License

MIT License - feel free to use this project for your own research needs.

---

**Get started with Hyperbrowser today: [https://hyperbrowser.ai](https://hyperbrowser.ai)**

Follow @hyperbrowser_ai for updates.

## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)
- [https://hyperbrowser.ai](https://hyperbrowser.ai)
- [http://localhost:3000](http://localhost:3000)

--- deep-reddit-researcher/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  images: {
    remotePatterns: [
      {
        protocol: 'https',
        hostname: 'www.reddit.com',
      },
    ],
  },
};

export default nextConfig;


--- deep-reddit-researcher/lib/hb.ts ---
import { Hyperbrowser } from "@hyperbrowser/sdk";

export function getHB() {
  const key = process.env.HYPERBROWSER_API_KEY;
  console.log('API Key available:', !!key);
  if (!key) throw new Error("Missing HYPERBROWSER_API_KEY");
  return new Hyperbrowser({ apiKey: key });
}

--- deep-reddit-researcher/lib/json.ts ---
export function threadsToJSON(arr: any[]) {
    return JSON.stringify(arr, null, 2);
  }

--- deep-reddit-researcher/lib/openai.ts ---
import OpenAI from 'openai';

export function getOpenAI() {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) throw new Error("Missing OPENAI_API_KEY");
  return new OpenAI({ apiKey });
}

export async function askQuestion(question: string, context: string): Promise<string> {
  const openai = getOpenAI();
  
  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
      {
        role: "system",
        content: `You are a Reddit research assistant. You have access to research data from Reddit threads. Answer questions based on the provided context. Be conversational, insightful, and reference specific posts/comments when relevant. If the context doesn't contain enough information to answer the question, say so honestly.`
      },
      {
        role: "user",
        content: `Context from Reddit research:
${context}

Question: ${question}`
      }
    ],
    max_tokens: 800,
    temperature: 0.7
  });

  return response.choices[0]?.message?.content || "I couldn't generate a response.";
}

--- deep-reddit-researcher/lib/reddit.ts ---
import { getHB } from "./hb";
import * as cheerio from "cheerio";
import fs from "fs";
import path from "path";

export interface Thread {
  title: string;
  url: string;
  shot: string;
  snippet: string;
  content: string;
}

export interface ResearchData {
  threads: Thread[];
  logs: string[];
  fullContent: string;
}

function createVisiblePlaceholder(text: string, color: string): string {
  // Create an SVG file instead of PNG - browsers display SVG much more reliably
  const svg = `<svg width="800" height="600" xmlns="http://www.w3.org/2000/svg">
    <rect width="100%" height="100%" fill="${color}" stroke="#ffffff" stroke-width="4"/>
    <text x="50%" y="40%" text-anchor="middle" dominant-baseline="middle" 
          fill="white" font-family="Arial, sans-serif" font-size="32" font-weight="bold">
      ${text}
    </text>
    <text x="50%" y="60%" text-anchor="middle" dominant-baseline="middle" 
          fill="white" font-family="Arial, sans-serif" font-size="18">
      Demo Screenshot
    </text>
    <circle cx="100" cy="100" r="30" fill="white" opacity="0.3"/>
    <circle cx="700" cy="100" r="30" fill="white" opacity="0.3"/>
    <circle cx="100" cy="500" r="30" fill="white" opacity="0.3"/>
    <circle cx="700" cy="500" r="30" fill="white" opacity="0.3"/>
  </svg>`;
  return svg;
}

function generateMockThreadContent(url: string, query: string): string {
  const queryLower = query.toLowerCase();
  
  // Extract thread type from URL
  const isGTA = queryLower.includes('gta') || queryLower.includes('grand theft auto');
  const isAutomation = queryLower.includes('automation') || queryLower.includes('web');
  
  if (url.includes('abc123')) {
    if (isGTA) {
      return `<html><body><h1>PS5 GTA Online crashes after update</h1>
        <div class="post-content">Anyone else experiencing frequent crashes on PS5 after the latest GTA Online update? 
        My game keeps crashing every 30-45 minutes during missions. Already tried restarting console and reinstalling.</div>
        <div class="comment">Same issue here! Rockstar really needs to fix this.</div>
        <div class="comment">Try clearing cache and rebuilding database. Worked for me.</div>
        <div class="comment">This has been happening since the last update. Very frustrating.</div>
        </body></html>`;
    } else if (isAutomation) {
      return `<html><body><h1>Web automation struggles with Selenium</h1>
        <div class="post-content">Been trying to automate a complex web form but keeps failing on dynamic elements. 
        Anyone have experience with handling AJAX-loaded content and shadow DOM elements?</div>
        <div class="comment">Try using explicit waits instead of implicit ones.</div>
        <div class="comment">Playwright handles shadow DOM better than Selenium.</div>
        </body></html>`;
    }
  } else if (url.includes('def456')) {
    if (isGTA) {
      return `<html><body><h1>GTA 5 PS5 loading issues solutions</h1>
        <div class="post-content">Compiled list of solutions for common GTA V PS5 loading problems and performance issues.</div>
        <div class="comment">Switching to SSD helped reduce loading times significantly.</div>
        <div class="comment">Make sure you're running the PS5 version not PS4 compatibility mode.</div>
        </body></html>`;
    }
  } else if (url.includes('ghi789')) {
    if (isGTA) {
      return `<html><body><h1>GTA V PS5 version problems megathread</h1>
        <div class="post-content">Central discussion for all GTA V Enhanced Edition issues on PlayStation 5.</div>
        <div class="comment">Ray tracing causes frame drops in certain areas.</div>
        <div class="comment">Online mode is more stable than single player for some reason.</div>
        </body></html>`;
    }
  }
  
  // Generic fallback
  return `<html><body><h1>${query} discussion</h1>
    <div class="post-content">Community discussion about ${query} with various user experiences and solutions.</div>
    <div class="comment">Thanks for bringing this up, very relevant topic.</div>
    <div class="comment">I've had similar experiences with this issue.</div>
    </body></html>`;
}

export async function searchThreads(
  query: string, 
  onProgress?: (log: string, screenshot?: string) => void
): Promise<ResearchData> {
  const logs: string[] = [];
  const hb = getHB();

  const pushLog = (log: string, screenshot?: string) => {
    logs.push(log);
    onProgress?.(log, screenshot);
  };

  // Try multiple Reddit URL formats
  const searchUrls = [
    `https://old.reddit.com/search?q=${encodeURIComponent(query)}&sort=relevance&t=all`,
    `https://www.reddit.com/search?q=${encodeURIComponent(query)}`,
    `https://www.reddit.com/r/all/search?q=${encodeURIComponent(query)}`
  ];
  
  let searchResult: any = null;
  let usedUrl = "";

  pushLog(`[SEARCH] Searching Reddit for: ${query}`);

  try {
    // Try different Reddit URLs until we get proper HTML content
    for (const searchUrl of searchUrls) {
    try {
      pushLog(`[TRYING] ${searchUrl}`);
      
              const result = await hb.scrape.startAndWait({
          url: searchUrl,
          scrapeOptions: {
            formats: ["html"]
          },
          sessionOptions: {
            useProxy: true,
            solveCaptchas: true,
            acceptCookies: true,
            adblock: true
          }
        });

      if (result?.data?.html && 
          !result.data.html.includes('data:image/png;base64') &&
          !result.data.html.includes('whoa there, pardner') &&
          !result.data.html.includes('blocked due to a network policy') &&
          !result.data.html.includes('<title>Blocked</title>')) {
        searchResult = result;
        usedUrl = searchUrl;
        pushLog(`[SUCCESS] Got proper HTML from: ${searchUrl}`);
        break;
      } else {
        if (result?.data?.html?.includes('whoa there, pardner')) {
          pushLog(`[BLOCKED] ${searchUrl} returned Reddit block page - "whoa there, pardner!"`);
        } else if (result?.data?.html?.includes('data:image/png;base64')) {
          pushLog(`[BLOCKED] ${searchUrl} returned base64 image content`);
        } else {
          pushLog(`[SKIP] ${searchUrl} returned unusable content`);
        }
      }
    } catch (error) {
      pushLog(`[ERROR] Failed to scrape ${searchUrl}: ${error}`);
      continue;
    }
  }

  if (!searchResult?.data?.html) {
    // If all URLs fail, create realistic mock data based on the query
    pushLog(`[FALLBACK] All Reddit URLs failed (blocked by anti-bot protection), using realistic demo data`);
    
    // Generate relevant mock data based on query keywords
    const queryLower = query.toLowerCase();
    let mockThreads = [];
    
    if (queryLower.includes('gta') || queryLower.includes('grand theft auto')) {
      mockThreads = [
        '/r/gtaonline/comments/abc123/ps5_gta_online_crashes_after_update/',
        '/r/GrandTheftAutoV/comments/def456/gta_5_ps5_loading_issues_solutions/',
        '/r/playstation/comments/ghi789/gta_v_ps5_version_problems_megathread/'
      ];
    } else if (queryLower.includes('automation') || queryLower.includes('web')) {
      mockThreads = [
        '/r/webdev/comments/abc123/web_automation_struggles_selenium/',
        '/r/QualityAssurance/comments/def456/automation_testing_pain_points/',
        '/r/selenium/comments/ghi789/common_automation_challenges/'
      ];
    } else {
      // Generic threads based on query
      mockThreads = [
        `/r/discussion/comments/abc123/${query.replace(/\s+/g, '_').toLowerCase()}_discussion/`,
        `/r/help/comments/def456/${query.replace(/\s+/g, '_').toLowerCase()}_solutions/`,
        `/r/community/comments/ghi789/${query.replace(/\s+/g, '_').toLowerCase()}_experiences/`
      ];
    }
    
    const mockHtml = `
      <html><body>
        ${mockThreads.map(thread => `<a href="${thread}">${thread.split('/').pop()?.replace(/_/g, ' ') || 'Discussion'}</a>`).join('\n        ')}
      </body></html>
    `;
    searchResult = { data: { html: mockHtml } };
    usedUrl = "demo-fallback";
  }

  const $ = cheerio.load(searchResult.data.html);
    
    // Debug: Log a sample of the HTML to understand the structure
    pushLog(`[DEBUG] Sample HTML: ${searchResult.data.html.slice(0, 500)}...`);
    pushLog(`[DEBUG] Total links found: ${$("a").length}`);
    pushLog(`[DEBUG] Links with /r/: ${$("a[href*='/r/']").length}`);
    pushLog(`[DEBUG] Links with /comments/: ${$("a[href*='/comments/']").length}`);
    
    // Look for Reddit thread links - broader selectors for modern Reddit
    const links = $("a")
      .filter((_, el) => {
        const href = $(el).attr("href");
        if (!href) return false;
        
        return (
          href.includes("/comments/") || 
          (href.includes("/r/") && href.includes("/comments/")) ||
          (href.match(/\/r\/\w+\/comments\/\w+/) !== null) ||
          (href.startsWith("/r/") && href.split("/").length >= 5)
        );
      })
      .slice(0, 5)
      .map((_, el) => {
        const href = $(el).attr("href");
        const fullUrl = href?.startsWith("http") ? href : `https://www.reddit.com${href}`;
        return fullUrl;
      })
      .get()
      .filter((url, index, arr) => arr.indexOf(url) === index && url); // Remove duplicates and empty URLs

    pushLog(`[FOUND] Found ${links.length} Reddit threads to explore`);

    const threads: Thread[] = [];
    let allContent = `Research Query: ${query}\n\n`;

    const outDir = path.resolve("public/shots");
    if (!fs.existsSync(outDir)) fs.mkdirSync(outDir, { recursive: true });

    // Take real screenshot of search page
    let searchShotRel = "/shots/search.svg";
    try {
      const searchScreenshotResult = await hb.scrape.startAndWait({
        url: usedUrl,
        scrapeOptions: {
          formats: ["html", "screenshot"]
        },
        sessionOptions: {
          useProxy: true,
          solveCaptchas: true,
          acceptCookies: true,
          adblock: true
        }
      });

      pushLog(`[DEBUG] Screenshot result keys: ${Object.keys(searchScreenshotResult?.data || {}).join(', ')}`);
      pushLog(`[DEBUG] Has screenshot: ${!!searchScreenshotResult?.data?.screenshot}`);


      
      const screenshot = searchScreenshotResult?.data?.screenshot;
      
      if (screenshot) {
        pushLog(`[DEBUG] Screenshot type: ${typeof screenshot}`);
        pushLog(`[DEBUG] Screenshot length: ${screenshot?.length || 'N/A'}`);
        pushLog(`[DEBUG] Screenshot preview: ${typeof screenshot === 'string' ? screenshot.slice(0, 100) + '...' : 'Binary data'}`);
        
        // Save real search screenshot
        const searchShotPath = path.join(process.cwd(), "public/shots/search.png");
        
        try {
          if (typeof screenshot === 'string') {
            if (screenshot.startsWith('http')) {
              // Screenshot is a URL - download it
              pushLog(`[DEBUG] Downloading screenshot from URL: ${screenshot}`);
              const response = await fetch(screenshot);
              if (response.ok) {
                const buffer = Buffer.from(await response.arrayBuffer());
                fs.writeFileSync(searchShotPath, buffer);
                pushLog(`[DEBUG] Downloaded and saved screenshot, size: ${buffer.length} bytes`);
              } else {
                throw new Error(`Failed to download screenshot: ${response.status}`);
              }
            } else if (screenshot.startsWith('data:image')) {
              // Base64 data URL
              const base64Data = screenshot.replace(/^data:image\/[^;]+;base64,/, '');
              fs.writeFileSync(searchShotPath, Buffer.from(base64Data, 'base64'));
              pushLog(`[DEBUG] Saved as data URL, base64 length: ${base64Data.length}`);
            } else {
              // Plain base64
              fs.writeFileSync(searchShotPath, Buffer.from(screenshot, 'base64'));
              pushLog(`[DEBUG] Saved as plain base64, length: ${screenshot.length}`);
            }
          } else {
            // Binary data
            fs.writeFileSync(searchShotPath, screenshot);
            pushLog(`[DEBUG] Saved as binary data`);
          }
          
          searchShotRel = "/shots/search.png";
          pushLog(`[SHOT] Real search screenshot captured`, searchShotRel);
        } catch (error) {
          pushLog(`[ERROR] Failed to save screenshot: ${error}`);
        }
      } else {
        // Fallback to placeholder
        const searchShotPath = path.join(process.cwd(), "public/shots/search.svg");
        const searchPlaceholder = createVisiblePlaceholder("Reddit Search", "#2563eb");
        fs.writeFileSync(searchShotPath, searchPlaceholder, 'utf8');
        pushLog(`[SHOT] Placeholder search screenshot saved`, searchShotRel);
      }
    } catch (error) {
      // Fallback to placeholder
      const searchShotPath = path.join(process.cwd(), "public/shots/search.svg");
      const searchPlaceholder = createVisiblePlaceholder("Reddit Search", "#2563eb");
      fs.writeFileSync(searchShotPath, searchPlaceholder, 'utf8');
      pushLog(`[SHOT] Fallback search placeholder (screenshot failed)`, searchShotRel);
    }

    for (let i = 0; i < links.length; i++) {
      const url = links[i];
      pushLog(`[VISIT ${i + 1}/${links.length}] Exploring: ${url}`);

      try {
        // Take real screenshot using Hyperbrowser
        let shotRel = `/shots/thread-${i}.svg`;
        try {
          const screenshotResult = await hb.scrape.startAndWait({
            url: url,
            scrapeOptions: {
              formats: ["html", "screenshot"]
            },
            sessionOptions: {
              useProxy: true,
              solveCaptchas: true,
              acceptCookies: true,
              adblock: true
            }
          });

          pushLog(`[DEBUG] Thread ${i + 1} screenshot result keys: ${Object.keys(screenshotResult?.data || {}).join(', ')}`);
          pushLog(`[DEBUG] Thread ${i + 1} has screenshot: ${!!screenshotResult?.data?.screenshot}`);
                    
          const screenshot = screenshotResult?.data?.screenshot;
          
          if (screenshot) {
            pushLog(`[DEBUG] Thread ${i + 1} screenshot type: ${typeof screenshot}`);
            pushLog(`[DEBUG] Thread ${i + 1} screenshot length: ${screenshot?.length || 'N/A'}`);
            pushLog(`[DEBUG] Thread ${i + 1} screenshot preview: ${typeof screenshot === 'string' ? screenshot.slice(0, 100) + '...' : 'Binary data'}`);
            
            // Save real screenshot
            const shotPath = path.join(process.cwd(), `public/shots/thread-${i}.png`);
            
            try {
              if (typeof screenshot === 'string') {
                if (screenshot.startsWith('http')) {
                  // Screenshot is a URL - download it
                  pushLog(`[DEBUG] Thread ${i + 1} downloading screenshot from URL: ${screenshot}`);
                  const response = await fetch(screenshot);
                  if (response.ok) {
                    const buffer = Buffer.from(await response.arrayBuffer());
                    fs.writeFileSync(shotPath, buffer);
                    pushLog(`[DEBUG] Thread ${i + 1} downloaded and saved screenshot, size: ${buffer.length} bytes`);
                  } else {
                    throw new Error(`Failed to download screenshot: ${response.status}`);
                  }
                } else if (screenshot.startsWith('data:image')) {
                  // Base64 data URL
                  const base64Data = screenshot.replace(/^data:image\/[^;]+;base64,/, '');
                  fs.writeFileSync(shotPath, Buffer.from(base64Data, 'base64'));
                  pushLog(`[DEBUG] Thread ${i + 1} saved as data URL, base64 length: ${base64Data.length}`);
                } else {
                  // Plain base64
                  fs.writeFileSync(shotPath, Buffer.from(screenshot, 'base64'));
                  pushLog(`[DEBUG] Thread ${i + 1} saved as plain base64, length: ${screenshot.length}`);
                }
              } else {
                // Binary data
                fs.writeFileSync(shotPath, screenshot);
                pushLog(`[DEBUG] Thread ${i + 1} saved as binary data`);
              }
              
              shotRel = `/shots/thread-${i}.png`;
              pushLog(`[SHOT] Real screenshot captured for thread ${i + 1}`, shotRel);
            } catch (error) {
              pushLog(`[ERROR] Thread ${i + 1} failed to save screenshot: ${error}`);
            }
          } else {
            // Fallback to placeholder if screenshot fails
            const shotPath = path.join(process.cwd(), `public/shots/thread-${i}.svg`);
            const threadPlaceholder = createVisiblePlaceholder(`Thread ${i + 1}`, "#059669");
            fs.writeFileSync(shotPath, threadPlaceholder, 'utf8');
            pushLog(`[SHOT] Placeholder screenshot saved for thread ${i + 1}`, shotRel);
          }
        } catch (error) {
          // Fallback to placeholder if screenshot fails
          const shotPath = path.join(process.cwd(), `public/shots/thread-${i}.svg`);
          const threadPlaceholder = createVisiblePlaceholder(`Thread ${i + 1}`, "#059669");
          fs.writeFileSync(shotPath, threadPlaceholder, 'utf8');
          pushLog(`[SHOT] Fallback placeholder for thread ${i + 1} (screenshot failed)`, shotRel);
        }

        // Handle both real URLs and mock URLs
        let pageResult: any;
        let isMockData = url.includes('comments/abc123') || url.includes('comments/def456') || url.includes('comments/ghi789');
        
        if (isMockData) {
          // Generate realistic mock content for demo purposes
          pushLog(`[MOCK] Generating demo content for: ${url}`);
          pageResult = { data: { html: generateMockThreadContent(url, query) } };
        } else {
          // Scrape real thread
                          pageResult = await hb.scrape.startAndWait({
                  url: url,
                  scrapeOptions: {
                    formats: ["html"]
                  },
                  sessionOptions: {
                    useProxy: true,
                    solveCaptchas: true,
                    acceptCookies: true,
                    adblock: true
                  }
                });
        }

        // Extract content
        if (pageResult?.data?.html) {
          const $$ = cheerio.load(pageResult.data.html);
          
          // Extract title and content - try multiple selectors for Reddit's different layouts + mock content
          const title = $$("h1").first().text().trim() || 
                       $$('[data-testid="post-content"] h3').first().text().trim() ||
                       $$('[slot="title"]').first().text().trim() ||
                       $$('shreddit-title').first().text().trim() ||
                       $$('.Post-title').first().text().trim() ||
                       `Thread ${i + 1}`;
          
          // Get post content - try multiple selectors for Reddit's different layouts + mock content
          const postContent = $$('[data-testid="post-content"]').text().trim() ||
                             $$('[slot="text-body"]').text().trim() ||
                             $$('.Post-body').text().trim() ||
                             $$('div[data-click-id="text"]').text().trim() ||
                             $$('.RichTextJSON-root').text().trim() ||
                             $$('.post-content').text().trim(); // For mock content
          
          // Get comments - try multiple selectors including mock content
          const comments = $$('[data-testid="comment"], .Comment, [data-testid="comment-content"], shreddit-comment, .comment')
            .slice(0, 5)
            .map((_, el) => $$(el).text().trim())
            .get()
            .filter(text => text.length > 10) // Filter out very short comments
            .join("\n\n");
          
          const fullThreadContent = `${postContent}${comments ? `\n\nTop Comments:\n${comments}` : ''}`;
          const snippet = fullThreadContent.slice(0, 200) + (fullThreadContent.length > 200 ? '...' : '');

          threads.push({
            title,
            url,
            shot: shotRel,
            snippet,
            content: fullThreadContent
          });

          allContent += `\n--- Thread ${i + 1}: ${title} ---\n`;
          allContent += `URL: ${url}\n`;
          allContent += `Content: ${fullThreadContent}\n\n`;

          pushLog(`[EXTRACTED] Content from: ${title}`);
        }
      } catch (error) {
        pushLog(`[ERROR] Failed to process thread ${i + 1}: ${error}`);
      }
    }

    pushLog(`[COMPLETE] Research complete! Explored ${threads.length} threads`);
    
                // Ensure search screenshot is always included
            if (!threads.some(t => t.shot === searchShotRel)) {
              threads.unshift({
                title: "Reddit Search Results",
                url: usedUrl,
                shot: searchShotRel,
                snippet: `Search completed for: ${query}`,
                content: `Search query: ${query}\nURL used: ${usedUrl}`
              });
            }
    
    return {
      threads,
      logs,
      fullContent: allContent
    };

  } catch (error) {
    pushLog(`[ERROR] Research failed: ${error}`);
    throw error;
  }
}

## Links discovered
- [${thread.split('/').pop()?.replace(/_/g, ' ') || 'Discussion'}](https://github.com/hyperbrowserai/hyperbrowser-app-examples/blob/main/deep-reddit-researcher/lib/${thread}.md)

--- documentation-buddy/README.md ---
# Documentation Buddy

Turn any documentation into an intelligent AI-powered chatbot! Documentation Buddy crawls documentation websites using Hyperbrowser and creates an AI assistant that can answer questions about the content.

![Documentation Buddy](https://img.shields.io/badge/Built%20with-Next.js-000000?style=flat&logo=next.js)
![Hyperbrowser](https://img.shields.io/badge/Powered%20by-Hyperbrowser-blue)
![OpenAI](https://img.shields.io/badge/AI-OpenAI-green)

## Features

- ðŸ•¸ï¸ **Smart Web Crawling**: Automatically discovers and crawls documentation pages
- ðŸ¤– **AI-Powered Chat**: Chat with an AI that knows your documentation inside out
- ðŸŽ¯ **Intelligent Context**: Finds relevant documentation sections for each question
- âš¡ **Fast & Reliable**: Built with Hyperbrowser for robust web scraping
- ðŸŽ¨ **Beautiful UI**: Clean, modern interface built with Tailwind CSS
- ðŸ“± **Responsive**: Works perfectly on desktop and mobile

## Demo

1. **Enter a documentation URL** (e.g., `https://nextjs.org/docs`)
2. **Wait for crawling** - The app discovers and processes all documentation pages
3. **Start chatting** - Ask questions about the documentation and get AI-powered answers

## Quick Start

### Prerequisites

- Node.js 18+ and npm/yarn/pnpm
- [Hyperbrowser API Key](https://hyperbrowser.ai)
- [OpenAI API Key](https://platform.openai.com/api-keys)

### Installation

1. **Clone and install dependencies:**
   ```bash
   git clone <your-repo>
   cd documentation-buddy
   npm install
   ```

2. **Set up environment variables:**
   ```bash
   cp .env.example .env.local
   ```
   
   Then edit `.env.local` and add your API keys:
   ```env
   HYPERBROWSER_API_KEY=your_hyperbrowser_api_key_here
   OPENAI_API_KEY=your_openai_api_key_here
   ```

3. **Run the development server:**
   ```bash
   npm run dev
   ```

4. **Open [http://localhost:3000](http://localhost:3000)** in your browser

## How It Works

### 1. Web Crawling with Hyperbrowser
- Uses Hyperbrowser's `/crawl` endpoint to discover documentation pages
- Intelligently filters for documentation paths (`/docs`, `/api`, `/guide`, etc.)
- Extracts clean markdown content from each page
- Handles authentication, JavaScript, and complex site structures

### 2. AI-Powered Q&A
- Processes user questions to find relevant documentation sections
- Uses OpenAI GPT-4 to generate contextual answers
- References specific documentation pages in responses
- Maintains conversation context for follow-up questions

### 3. Smart Content Processing
- Chunks large documentation into manageable pieces
- Scores and ranks content relevance for each question
- Limits context size to stay within AI model limits
- Preserves document structure and formatting

## Supported Documentation Sites

Documentation Buddy works with most documentation websites, including:

- âœ… **Next.js** - `https://nextjs.org/docs`
- âœ… **React** - `https://react.dev/learn`
- âœ… **Vercel AI SDK** - `https://sdk.vercel.ai/docs`
- âœ… **Tailwind CSS** - `https://tailwindcss.com/docs`
- âœ… **Most Gitbook, Docusaurus, and custom documentation sites**

## Configuration

### Crawling Options

You can customize the crawling behavior by modifying `src/app/api/crawl/route.ts`:

```typescript
const crawlResult = await client.crawl.startAndWait({
  url: url,
  maxPages: maxPages, 
  includePatterns: [
    "/docs/*",
    "/api/*",
   
  ],
  scrapeOptions: {
    formats: ["markdown"],
    onlyMainContent: true,
    excludeTags: ["nav", "footer", "aside"],
    waitFor: 2000, 
  },
});
```

### AI Model Configuration

Customize the AI behavior in `src/app/api/chat/route.ts`:

```typescript
const result = streamText({
  model: openai('gpt-4o-mini'), 
  temperature: 0.7, 
  maxTokens: 1000, 
});
```

## Project Structure

```
documentation-buddy/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ crawl/          # Hyperbrowser crawling endpoint
â”‚   â”‚   â”‚   â””â”€â”€ chat/           # OpenAI chat endpoint
â”‚   â”‚   â”œâ”€â”€ layout.tsx          # Root layout
â”‚   â”‚   â””â”€â”€ page.tsx            # Main page
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx   # Chat UI component
â”‚   â”‚   â”œâ”€â”€ LoadingState.tsx    # Loading animation
â”‚   â”‚   â””â”€â”€ UrlInput.tsx        # URL input form
â”‚   â”œâ”€â”€ contexts/
â”‚   â”‚   â””â”€â”€ DocumentationContext.tsx # State management
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ types.ts            # TypeScript types
â”‚       â””â”€â”€ utils.ts            # Utility functions
â”œâ”€â”€ .env.example                # Environment variables template
â””â”€â”€ README.md
```

## API Endpoints

### `POST /api/crawl`
Crawls a documentation website and returns structured content.

**Request:**
```json
{
  "url": "https://nextjs.org/docs",
  "maxPages": 50
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "url": "https://nextjs.org/docs",
    "pages": [...],
    "totalPages": 42,
    "crawledAt": "2024-01-01T00:00:00.000Z"
  }
}
```

### `POST /api/chat`
AI-powered chat endpoint with documentation context.

**Request:**
```json
{
  "messages": [...],
  "documentationContext": [...]
}
```

## Deployment

### Vercel (Recommended)

1. **Push to GitHub**
2. **Connect to Vercel**
3. **Add environment variables** in Vercel dashboard
4. **Deploy**

### Docker

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm", "start"]
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## Troubleshooting

### Common Issues

**"Hyperbrowser API key not configured"**
- Make sure you've added `HYPERBROWSER_API_KEY` to `.env.local`
- Get your API key from [Hyperbrowser Dashboard](https://app.hyperbrowser.ai/dashboard)

**"Failed to crawl documentation"**
- Check if the URL is accessible and contains documentation
- Some sites may require authentication or have anti-bot measures
- Try adjusting the `maxPages` limit

**"OpenAI API key not configured"**
- Add `OPENAI_API_KEY` to `.env.local`
- Get your API key from [OpenAI Platform](https://platform.openai.com/api-keys)

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Built With

- [Next.js](https://nextjs.org/) - React framework
- [Hyperbrowser](https://hyperbrowser.ai/) - Web crawling and browser automation
- [OpenAI](https://openai.com/) - AI language model
- [Vercel AI SDK](https://sdk.vercel.ai/) - AI chat interface
- [Tailwind CSS](https://tailwindcss.com/) - Styling
- [Lucide React](https://lucide.dev/) - Icons

---

**Made with â¤ï¸ and powered by Hyperbrowser & OpenAI**


## Links discovered
- [Documentation Buddy](https://img.shields.io/badge/Built%20with-Next.js-000000?style=flat&logo=next.js)
- [Hyperbrowser](https://img.shields.io/badge/Powered%20by-Hyperbrowser-blue)
- [OpenAI](https://img.shields.io/badge/AI-OpenAI-green)
- [Hyperbrowser API Key](https://hyperbrowser.ai)
- [OpenAI API Key](https://platform.openai.com/api-keys)
- [http://localhost:3000](http://localhost:3000)
- [Hyperbrowser Dashboard](https://app.hyperbrowser.ai/dashboard)
- [OpenAI Platform](https://platform.openai.com/api-keys)
- [LICENSE](https://github.com/hyperbrowserai/hyperbrowser-app-examples/blob/main/documentation-buddy/LICENSE.md)
- [Next.js](https://nextjs.org/)
- [Hyperbrowser](https://hyperbrowser.ai/)
- [OpenAI](https://openai.com/)
- [Vercel AI SDK](https://sdk.vercel.ai/)
- [Tailwind CSS](https://tailwindcss.com/)
- [Lucide React](https://lucide.dev/)

--- documentation-buddy/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;


--- documentation-buddy/tailwind.config.ts ---
import type { Config } from "tailwindcss";

export default {
  content: [
    "./src/pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/components/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        background: "var(--background)",
        foreground: "var(--foreground)",
        accent: "#F0FF26",
      },
      fontFamily: {
        sans: ["var(--font-manrope)", "ui-sans-serif", "system-ui"],
        mono: ["var(--font-dm-mono)", "ui-monospace", "monospace"],
      },
      letterSpacing: {
        tight02: "-0.02em",
      },
    },
  },
  plugins: [
    require('@tailwindcss/typography'),
  ],
} satisfies Config; 

--- documentation-buddy/src/lib/types.ts ---
export interface CrawledPage {
  url: string;
  title?: string;
  content: string;
  status: 'completed' | 'failed';
  error?: string;
}

export interface DocumentationData {
  id: string;
  url: string;
  originalUrl?: string;
  pages: CrawledPage[];
  crawledAt: Date;
  totalPages: number;
}

export interface ChatMessage {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
}

export interface DocumentationSession {
  id: string;
  name: string;
  url: string;
  status: 'crawling' | 'ready' | 'error';
  progress?: number;
  error?: string;
  createdAt: Date;
} 

--- documentation-buddy/src/lib/utils.ts ---
import { type ClassValue, clsx } from "clsx";
import { twMerge } from "tailwind-merge";

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs));
}

export function extractDomainName(url: string): string {
  try {
    const domain = new URL(url).hostname;
    return domain.replace('www.', '');
  } catch {
    return 'Documentation';
  }
}

export function chunkText(text: string, maxLength: number = 1000): string[] {
  const chunks: string[] = [];
  const paragraphs = text.split('\n\n');
  
  let currentChunk = '';
  
  for (const paragraph of paragraphs) {
    if (currentChunk.length + paragraph.length > maxLength && currentChunk.length > 0) {
      chunks.push(currentChunk.trim());
      currentChunk = paragraph;
    } else {
      currentChunk += (currentChunk ? '\n\n' : '') + paragraph;
    }
  }
  
  if (currentChunk.trim()) {
    chunks.push(currentChunk.trim());
  }
  
  return chunks;
}

export function formatRelativeTime(date: Date): string {
  const now = new Date();
  const diffInSeconds = Math.floor((now.getTime() - date.getTime()) / 1000);
  
  if (diffInSeconds < 60) return 'Just now';
  if (diffInSeconds < 3600) return `${Math.floor(diffInSeconds / 60)}m ago`;
  if (diffInSeconds < 86400) return `${Math.floor(diffInSeconds / 3600)}h ago`;
  return `${Math.floor(diffInSeconds / 86400)}d ago`;
} 

--- flow-mapper/README.md ---
# FlowMapper - Interactive User Flow Generator

**Transform any website into interactive flow diagrams and automated tests using the Hyperbrowser SDK.**

FlowMapper is a powerful web application that crawls websites and generates:
- ðŸ“Š Interactive flow diagrams (Mermaid.js)
- ðŸŽ­ Playwright test automation code
- âš›ï¸ React XState components
- ðŸ“® Postman API collections
- ðŸ“¸ Visual screenshots of each step

## ðŸš€ Features

### Core Capabilities
- **Smart Website Crawling**: Uses Hyperbrowser SDK with stealth mode and proxy support
- **Visual Flow Generation**: Creates beautiful Mermaid diagrams
- **Multi-Format Export**: Playwright, React/XState, Postman collections
- **API Discovery**: Automatically detects and maps API endpoints
- **Screenshot Capture**: Visual documentation of each user flow step
- **Enterprise Features**: Proxy rotation, CAPTCHA solving, anti-detection

### Hyperbrowser Integration
- **Stealth Mode**: Bypass bot detection systems
- **Proxy Support**: Residential proxy rotation
- **Enterprise Security**: Advanced anti-detection capabilities
- **AI-Powered Extraction**: Intelligent content and link extraction
- **Session Management**: Persistent browser sessions
- **Network Traffic Recording**: Complete API endpoint discovery

## ðŸ› ï¸ Tech Stack

- **Frontend**: Next.js 14, React, TypeScript, Tailwind CSS
- **Visualization**: Mermaid.js, ReactFlow
- **Web Automation**: Hyperbrowser SDK v0.51.0
- **Code Generation**: EJS templating
- **File Processing**: JSZip for downloadable packages

## ðŸ“‹ Prerequisites

1. **Hyperbrowser API Key**: Get your API key from [Hyperbrowser](https://hyperbrowser.ai)
2. **Node.js**: Version 18 or higher
3. **npm**: Latest version

## ðŸš€ Quick Start

### 1. Clone and Install
```bash
git clone <repository-url>
cd flow-mapper
npm install
```

### 2. Start Development Server
```bash
npm run dev
```

### 3. Open Application
Navigate to `http://localhost:3000`

### 4. Configure Hyperbrowser
1. Enter your Hyperbrowser API key
2. Paste the target website URL
3. Set crawl depth (1-5 recommended)
4. Click "Start Crawling"

## ðŸ”§ Configuration

### Environment Variables
Create a `.env.local` file:
```env
HYPERBROWSER_API_KEY=your_api_key_here
NEXT_PUBLIC_SITE_DOMAIN=http://localhost:3000
```

### Hyperbrowser Features
The application uses these Hyperbrowser enterprise features:
- **Stealth Mode**: `useStealth: true`
- **Proxy Rotation**: `useProxy: true`
- **Ad Blocking**: `adblock: true`
- **Tracker Blocking**: `trackers: true`
- **Cookie Management**: `acceptCookies: true`
- **Screenshot Capture**: `screenshotOptions.fullPage: true`

## ðŸ“Š Generated Outputs

### 1. Interactive Flow Diagram
- Mermaid.js syntax for visual flows
- Clickable nodes with screenshots
- Relationship mapping between pages

### 2. Playwright Test Code
```javascript
// Generated Playwright test
import { test, expect } from '@playwright/test';

test('User Flow Test', async ({ page }) => {
  await page.goto('https://example.com');
  // ... automated test steps
});
```

### 3. React XState Component
```jsx
// Generated React component with state machine
import { useMachine } from '@xstate/react';

export const UserFlowComponent = () => {
  const [state, send] = useMachine(userFlowMachine);
  // ... component logic
};
```

### 4. Postman Collection
- Complete API endpoint documentation
- Request/response examples
- Environment variables
- Test scripts

## ðŸŽ¯ Use Cases

### For Developers
- **Test Automation**: Generate Playwright tests from user flows
- **API Documentation**: Auto-discover and document APIs
- **Flow Analysis**: Understand complex user journeys

### For QA Teams
- **Test Case Generation**: Automated test scenario creation
- **Regression Testing**: Comprehensive flow validation
- **Visual Testing**: Screenshot-based verification

### For Product Teams
- **User Journey Mapping**: Visual flow documentation
- **Conversion Analysis**: Identify drop-off points
- **Feature Planning**: Understand current user paths

## ðŸ”’ Security & Privacy

- **Hyperbrowser Security**: Enterprise-grade bot detection bypass
- **Data Privacy**: No data stored permanently
- **Secure Processing**: All crawling happens server-side
- **API Key Protection**: Keys are not logged or stored

## ðŸš€ Deployment

### Vercel Deployment
```bash
npm run build
vercel --prod
```

### Docker Deployment
```bash
docker build -t flow-mapper .
docker run -p 3000:3000 flow-mapper
```

### Environment Setup
Ensure these environment variables are set in production:
- `HYPERBROWSER_API_KEY`
- `NEXT_PUBLIC_SITE_DOMAIN`

## ðŸ› ï¸ Development

### Project Structure
```
flow-mapper/
â”œâ”€â”€ app/                 # Next.js app directory
â”œâ”€â”€ components/          # React components
â”œâ”€â”€ lib/                 # Core functionality
â”‚   â”œâ”€â”€ crawl.ts        # Hyperbrowser integration
â”‚   â”œâ”€â”€ graph.ts        # Flow graph generation
â”‚   â”œâ”€â”€ codegen.ts      # Code generation
â”‚   â””â”€â”€ zip.ts          # File packaging
â””â”€â”€ public/             # Static assets
```

### Key Components
- **Crawler**: Hyperbrowser SDK integration
- **GraphBuilder**: Flow diagram generation
- **CodeGenerator**: Multi-format code export
- **ZipBuilder**: Downloadable package creation

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with Hyperbrowser API
5. Submit a pull request

## ðŸ“„ License

MIT License - see LICENSE file for details

## ðŸ†˜ Support

- **Documentation**: [Hyperbrowser Docs](https://docs.hyperbrowser.ai)
- **Issues**: GitHub Issues
- **Contact**: [Hyperbrowser Support](https://hyperbrowser.ai/)

---

*FlowMapper demonstrates the power of Hyperbrowser SDK for enterprise web automation and testing. Built with modern web technologies and designed for scalability.*


## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)
- [Hyperbrowser Docs](https://docs.hyperbrowser.ai)
- [Hyperbrowser Support](https://hyperbrowser.ai/)

--- flow-mapper/next.config.js ---
/** @type {import('next').NextConfig} */
const nextConfig = {
  env: {
    NEXT_PUBLIC_SITE_DOMAIN: process.env.NEXT_PUBLIC_SITE_DOMAIN || 'http://localhost:3000',
  },
  experimental: {
    serverComponentsExternalPackages: ['archiver'],
  },
}

module.exports = nextConfig 

--- flow-mapper/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;


--- flow-mapper/postcss.config.js ---
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}


--- flow-mapper/tailwind.config.js ---
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    "./pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        'neon-yellow': '#FFFD39',
      },
      fontFamily: {
        mono: ['ui-monospace', 'SFMono-Regular', 'Monaco', 'Consolas', 'Liberation Mono', 'Courier New', 'monospace'],
      },
      backdropBlur: {
        'xs': '2px',
      },
    },
  },
  plugins: [],
} 

--- flow-mapper/lib/codegen.ts ---
import { FlowGraph, FlowNode } from './graph';
import { ApiEndpoint } from './crawl';
import * as ejs from 'ejs';

export interface CodegenResult {
  playwrightCode: string;
  reactCode: string;
  postmanCollection: any;
}

const PLAYWRIGHT_TEMPLATE = `import { test, expect, Page } from '@playwright/test';

/**
 * Generated Playwright test from FlowMapper
 * Website: <%= baseUrl %>
 * Generated: <%= timestamp %>
 */

test.describe('User Flow Test', () => {
  let page: Page;

  test.beforeEach(async ({ browser }) => {
    page = await browser.newPage();
  });

  test.afterEach(async () => {
    await page.close();
  });

<% nodes.forEach((node, index) => { %>
  <% if (node.type === 'page') { %>
  test('Navigate to <%= node.label %>', async () => {
    // Navigate to <%= node.url || 'page' %>
    await page.goto('<%= node.url || baseUrl %>');
    
    // Wait for page to load
    await page.waitForLoadState('networkidle');
    
    // Take screenshot for verification
    await page.screenshot({ 
      path: 'screenshots/<%= node.id %>-<%= index %>.png',
      fullPage: true 
    });
    
    // Verify page loaded correctly
    await expect(page).toHaveTitle(/<%= node.label.replace(/[^a-zA-Z0-9\s]/g, '') %>/i);
  });
  <% } %>

  <% if (node.type === 'form') { %>
  test('Fill and submit <%= node.label %>', async () => {
    <% if (node.payload && Array.isArray(node.payload)) { %>
    <% node.payload.forEach(field => { %>
    // Fill <%= field %> field
    await page.fill('[name="<%= field %>"]', 'test-value');
    <% }); %>
    <% } %>
    
    // Submit form
    await page.click('button[type="submit"], input[type="submit"]');
    
    // Wait for response
    await page.waitForLoadState('networkidle');
    
    // Take screenshot after form submission
    await page.screenshot({ 
      path: 'screenshots/<%= node.id %>-submitted.png',
      fullPage: true 
    });
  });
  <% } %>

  <% if (node.type === 'api') { %>
  test('Verify API call: <%= node.method %> <%= node.url %>', async () => {
    // Set up network interception
    const apiResponse = page.waitForResponse(response => 
      response.url().includes('<%= node.url %>') && 
      response.request().method() === '<%= node.method %>'
    );
    
    <% if (node.method === 'POST' || node.method === 'PUT') { %>
    // Trigger the API call (usually through form submission or button click)
    await page.click('button[data-testid="submit"], .submit-btn');
    <% } else { %>
    // Trigger the API call (usually through navigation or page load)
    await page.reload();
    <% } %>
    
    // Wait for and verify API response
    const response = await apiResponse;
    expect(response.status()).toBe(200);
    
    <% if (node.response) { %>
    // Verify response content
    const responseBody = await response.json();
    expect(responseBody).toBeDefined();
    <% } %>
  });
  <% } %>
<% }); %>

  test('Complete user flow', async () => {
    <% nodes.filter(n => n.type === 'page').forEach((node, index) => { %>
    // Step <%= index + 1 %>: <%= node.label %>
    await page.goto('<%= node.url || baseUrl %>');
    await page.waitForLoadState('networkidle');
    
    <% if (index < nodes.filter(n => n.type === 'page').length - 1) { %>
    // Continue to next step
    await page.waitForTimeout(1000);
    <% } %>
    <% }); %>
    
    // Final verification
    await page.screenshot({ 
      path: 'screenshots/complete-flow.png',
      fullPage: true 
    });
  });
});`;

const REACT_XSTATE_TEMPLATE = `import React from 'react';
import { createMachine, interpret } from 'xstate';
import { useMachine } from '@xstate/react';

/**
 * Generated XState React component from FlowMapper
 * Generated: <%= timestamp %>
 */

// XState Machine Definition
const userFlowMachine = createMachine({
  id: 'userFlow',
  initial: '<%= initialState %>',
  context: {
    userData: {},
    apiResponses: {},
    currentUrl: '',
  },
  states: {
    <% Object.entries(states).forEach(([stateName, state]) => { %>
    <%= stateName %>: {
      meta: {
        label: '<%= state.label %>',
        <% if (state.url) { %>url: '<%= state.url %>',<% } %>
        <% if (state.screenshot) { %>screenshot: '<%= state.screenshot %>',<% } %>
      },
      entry: ['log<%= stateName.charAt(0).toUpperCase() + stateName.slice(1) %>Entry'],
      on: {
        <% Object.entries(state.transitions || {}).forEach(([event, target]) => { %>
        <%= event %>: {
          target: '<%= typeof target === 'string' ? target : target.target %>',
          <% if (typeof target === 'object' && target.actions) { %>
          actions: ['<%= target.actions.join("', '") %>'],
          <% } %>
        },
        <% }); %>
      },
    },
    <% }); %>
  },
}, {
  actions: {
    <% Object.entries(states).forEach(([stateName, state]) => { %>
    log<%= stateName.charAt(0).toUpperCase() + stateName.slice(1) %>Entry: (context, event) => {
      console.log('Entered <%= stateName %>', { context, event });
    },
    <% }); %>
  },
});

// React Component
export const UserFlowComponent: React.FC = () => {
  const [state, send] = useMachine(userFlowMachine);

  const currentStateConfig = state.meta?.['userFlow.<%= initialState %>'] || {};

  return (
    <div className="user-flow-container">
      <div className="state-indicator">
        <h2>Current State: {state.value}</h2>
        <p>Label: {currentStateConfig.label}</p>
        {currentStateConfig.url && (
          <p>URL: <a href={currentStateConfig.url} target="_blank" rel="noopener noreferrer">
            {currentStateConfig.url}
          </a></p>
        )}
      </div>

      <div className="controls">
        <h3>Available Actions:</h3>
        {Object.keys(state.nextEvents || {}).map(event => (
          <button
            key={event}
            onClick={() => send(event)}
            className="action-button"
          >
            {event.replace(/_/g, ' ').toLowerCase()}
          </button>
        ))}
      </div>

      {currentStateConfig.screenshot && (
        <div className="screenshot-preview">
          <h4>Page Screenshot:</h4>
          <img 
            src={currentStateConfig.screenshot} 
            alt="Page screenshot"
            style={{ maxWidth: '100%', border: '1px solid #ccc' }}
          />
        </div>
      )}

      <div className="state-data">
        <h4>State Context:</h4>
        <pre>{JSON.stringify(state.context, null, 2)}</pre>
      </div>

      <style jsx>{\`
        .user-flow-container {
          padding: 20px;
          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
        }
        
        .state-indicator {
          background: #f5f5f5;
          padding: 15px;
          border-radius: 8px;
          margin-bottom: 20px;
        }
        
        .controls {
          margin-bottom: 20px;
        }
        
        .action-button {
          background: #007bff;
          color: white;
          border: none;
          padding: 8px 16px;
          margin: 4px;
          border-radius: 4px;
          cursor: pointer;
        }
        
        .action-button:hover {
          background: #0056b3;
        }
        
        .screenshot-preview {
          margin-bottom: 20px;
        }
        
        .state-data {
          background: #f8f9fa;
          padding: 15px;
          border-radius: 8px;
          border: 1px solid #dee2e6;
        }
        
        pre {
          background: #2d3748;
          color: #e2e8f0;
          padding: 12px;
          border-radius: 4px;
          overflow-x: auto;
          font-size: 12px;
        }
      \`}</style>
    </div>
  );
};

export default UserFlowComponent;`;

export class CodeGenerator {
  generateCode(graph: FlowGraph, endpoints: ApiEndpoint[], baseUrl: string): CodegenResult {
    const playwrightCode = this.generatePlaywrightCode(graph, baseUrl);
    const reactCode = this.generateReactCode(graph);
    const postmanCollection = this.generatePostmanCollection(endpoints, baseUrl);

    return {
      playwrightCode,
      reactCode,
      postmanCollection,
    };
  }

  private generatePlaywrightCode(graph: FlowGraph, baseUrl: string): string {
    const templateData = {
      nodes: graph.nodes,
      edges: graph.edges,
      baseUrl,
      timestamp: new Date().toISOString(),
    };

    return ejs.render(PLAYWRIGHT_TEMPLATE, templateData);
  }

  private generateReactCode(graph: FlowGraph): string {
    const states: Record<string, any> = {};
    const pageNodes = graph.nodes.filter(n => n.type === 'page');
    
    // Convert graph nodes to XState states
    graph.nodes.forEach(node => {
      const stateName = node.id;
      const outgoingEdges = graph.edges.filter(e => e.source === node.id);
      const transitions: Record<string, any> = {};

      outgoingEdges.forEach(edge => {
        const eventName = this.sanitizeEventName(edge.label || 'NEXT');
        transitions[eventName] = edge.target;
      });

      states[stateName] = {
        label: node.label,
        url: node.url,
        screenshot: node.screenshot,
        transitions,
      };
    });

    // Add a default state if no nodes exist
    if (Object.keys(states).length === 0) {
      states['initial'] = {
        label: 'No pages found',
        url: '',
        screenshot: '',
        transitions: {},
      };
    }

    const templateData = {
      states,
      initialState: pageNodes[0]?.id || 'initial',
      timestamp: new Date().toISOString(),
    };

    return ejs.render(REACT_XSTATE_TEMPLATE, templateData);
  }

  private generatePostmanCollection(endpoints: ApiEndpoint[], baseUrl: string): any {
    const collection = {
      info: {
        name: 'FlowMapper Generated API Collection',
        description: 'Generated from FlowMapper crawl',
        schema: 'https://schema.getpostman.com/json/collection/v2.1.0/collection.json',
      },
      item: endpoints.map((endpoint, index) => ({
        name: `${endpoint.method} ${this.extractPathFromUrl(endpoint.url)}`,
        event: [
          {
            listen: 'test',
            script: {
              exec: [
                'pm.test("Status code is success", function () {',
                '    pm.response.to.have.status(200) || pm.response.to.have.status(201) || pm.response.to.have.status(204);',
                '});',
                '',
                'pm.test("Response time is less than 2000ms", function () {',
                '    pm.expect(pm.response.responseTime).to.be.below(2000);',
                '});',
              ],
            },
          },
        ],
        request: {
          method: endpoint.method,
          header: [
            { key: 'Content-Type', value: 'application/json' },
            { key: 'Accept', value: 'application/json' }
          ],
          body: {
            mode: 'raw',
            raw: endpoint.payload ? JSON.stringify(endpoint.payload, null, 2) : ''
          },
          url: {
            raw: endpoint.url,
            protocol: this.extractProtocol(endpoint.url),
            host: this.extractHost(endpoint.url).split('.'),
            path: this.extractPath(endpoint.url).split('/').filter(Boolean)
          }
        },
        response: endpoint.response ? [{
          name: 'Default',
          originalRequest: {
            method: endpoint.method,
            url: endpoint.url
          },
          status: 'OK',
          code: 200,
          _postman_previewlanguage: 'json',
          header: [
            { key: 'Content-Type', value: 'application/json' }
          ],
          body: JSON.stringify(endpoint.response, null, 2)
        }] : []
      })),
      variable: [
        {
          key: 'baseUrl',
          value: baseUrl,
          type: 'string',
        },
      ],
    };

    return collection;
  }

  private extractPathFromUrl(url: string): string {
    try {
      const urlObj = new URL(url);
      return urlObj.pathname + urlObj.search;
    } catch {
      return url;
    }
  }

  private sanitizeEventName(label: string): string {
    return label
      .toUpperCase()
      .replace(/[^A-Z0-9]/g, '_')
      .replace(/_+/g, '_')
      .replace(/^_|_$/g, '');
  }

  private extractProtocol(url: string): string {
    const urlObj = new URL(url);
    return urlObj.protocol.replace(':', '');
  }

  private extractHost(url: string): string {
    const urlObj = new URL(url);
    return urlObj.hostname;
  }

  private extractPath(url: string): string {
    const urlObj = new URL(url);
    return urlObj.pathname + urlObj.search;
  }
} 

--- flow-mapper/lib/crawl.ts ---
import { Hyperbrowser } from '@hyperbrowser/sdk';
import { FlowGraph, FlowNode } from './graph';

export interface CrawlResult {
  dom: string[];
  endpoints: ApiEndpoint[];
  screenshots: string[];
  crawlId: string;
}

export interface ApiEndpoint {
  url: string;
  method: string;
  payload?: any;
  response?: any;
}

interface CrawlerOptions {
  hbKey: string;
  maxDepth?: number;
  includeApis?: boolean;
}

export class Crawler {
  private client: Hyperbrowser;
  private maxDepth: number;
  private includeApis: boolean;
  private visitedUrls: Set<string>;
  private graph: FlowGraph;
  private endpoints: ApiEndpoint[];

  constructor(options: CrawlerOptions) {
    this.maxDepth = options.maxDepth || 3;
    this.includeApis = options.includeApis || true;
    this.visitedUrls = new Set();
    this.graph = new FlowGraph();
    this.endpoints = [];

    // Initialize Hyperbrowser with API key and debug logging
    console.log(`Initializing Hyperbrowser with API key: ${options.hbKey.substring(0, 8)}...`);
    
    this.client = new Hyperbrowser({
      apiKey: options.hbKey,
      baseUrl: 'https://api.hyperbrowser.ai',
      timeout: 60000, // 60 seconds timeout
    });
  }

  async crawl(startUrl: string): Promise<{ graph: FlowGraph; endpoints: ApiEndpoint[] }> {
    await this.crawlPage(startUrl, 0);
    return { graph: this.graph, endpoints: this.endpoints };
  }

  private async crawlPage(url: string, depth: number): Promise<void> {
    if (depth >= this.maxDepth || this.visitedUrls.has(url)) {
      return;
    }

    this.visitedUrls.add(url);
    console.log(`Crawling ${url} at depth ${depth}`);

    try {
      // Start and wait for scrape job to complete with enterprise features
      console.log(`Making Hyperbrowser API call for ${url}`);
      const result = await this.client.scrape.startAndWait({
        url,
        sessionOptions: {
          useStealth: true,
          useProxy: true,
          enableWebRecording: true,
          saveDownloads: true,
          adblock: true,
          trackers: true,
          annoyances: true,
          acceptCookies: true,
        },
        scrapeOptions: {
          formats: ['html', 'markdown'],
          screenshotOptions: {
            fullPage: true,
          },
          waitUntil: 'networkidle',
        },
      });

      console.log(`Hyperbrowser API response status: ${result.status}`);

      if (result.status === 'completed' && result.data) {
        // Add the page to the graph
        const pageNode: FlowNode = {
          id: Buffer.from(url).toString('base64'),
          type: 'page',
          url,
          label: Array.isArray(result.data.metadata?.title) 
            ? result.data.metadata.title[0] || url
            : result.data.metadata?.title || url,
          screenshot: result.data.screenshot,
        };
        this.graph.addNode(pageNode);

        if (this.includeApis) {
          // Note: Network request extraction is not available in the current scrape API
          // This would require using a different Hyperbrowser service like sessions
          console.log('API extraction not available with scrape service');
        }

        // Extract all links from the page content
        const links = result.data.links || [];
        console.log(`Found ${links.length} links on ${url}`);
        console.log(`Sample links:`, links.slice(0, 5));

        // If no links found from API, try to extract from HTML
        let allLinks = links;
        if (links.length === 0 && result.data.html) {
          console.log('No links from API, extracting from HTML...');
          allLinks = this.extractLinksFromHTML(result.data.html, url);
          console.log(`Extracted ${allLinks.length} links from HTML`);
        }

        // Filter and process links for same domain
        const validLinks = allLinks.filter(link => {
          try {
            const absoluteUrl = new URL(link, url).toString();
            const isSameDomain = absoluteUrl.startsWith(new URL(url).origin);
            const isNotFragment = !link.startsWith('#');
            const isNotMailto = !link.startsWith('mailto:');
            const isNotTel = !link.startsWith('tel:');
            return isSameDomain && isNotFragment && isNotMailto && isNotTel;
          } catch {
            return false;
          }
        });

        console.log(`Found ${validLinks.length} valid same-domain links to crawl`);

        // Recursively crawl linked pages
        for (const link of validLinks.slice(0, 10)) { // Limit to first 10 links per page
          try {
            const absoluteUrl = new URL(link, url).toString();
            console.log(`Attempting to crawl: ${absoluteUrl} at depth ${depth + 1}`);
            
            await this.crawlPage(absoluteUrl, depth + 1);
            
            // Add edge from current page to the linked page
            const targetNodeId = Buffer.from(absoluteUrl).toString('base64');
            this.graph.addEdge(pageNode.id, targetNodeId);
            console.log(`Added edge from ${pageNode.id} to ${targetNodeId}`);
          } catch (error) {
            console.error(`Error processing link ${link}:`, error);
          }
        }
      } else {
        console.error(`Hyperbrowser API failed with status: ${result.status}`);
        if (result.error) {
          console.error(`Hyperbrowser API error: ${result.error}`);
        }
      }

    } catch (error) {
      console.error(`Hyperbrowser API call failed for ${url}:`, error);
      
      // Log detailed error information for debugging
      if (error instanceof Error) {
        console.error(`Error name: ${error.name}`);
        console.error(`Error message: ${error.message}`);
        console.error(`Error stack: ${error.stack}`);
      }
      
      // Check if it's an authentication error
      if (error && typeof error === 'object' && 'message' in error) {
        const errorMessage = (error as any).message;
        if (errorMessage.includes('NOT AUTHENTICATED') || errorMessage.includes('401') || errorMessage.includes('Unauthorized')) {
          console.error('Authentication failed. Please check your Hyperbrowser API key.');
          console.error('Make sure you are using the correct API key format and it has sufficient credits.');
        }
      }
      
      // Re-throw the error so the API route can handle it properly
      throw error;
    }
  }

  private extractLinksFromHTML(html: string, baseUrl: string): string[] {
    const links: string[] = [];
    
    // Simple regex to extract href attributes from anchor tags
    const hrefRegex = /<a[^>]+href\s*=\s*["']([^"']+)["'][^>]*>/gi;
    let match;
    
    while ((match = hrefRegex.exec(html)) !== null) {
      const href = match[1];
      if (href && !href.startsWith('javascript:') && !href.startsWith('#')) {
        try {
          // Convert relative URLs to absolute
          const absoluteUrl = new URL(href, baseUrl).toString();
          links.push(absoluteUrl);
        } catch {
          // Skip invalid URLs
        }
      }
    }
    
    // Remove duplicates
    return Array.from(new Set(links));
  }
} 

## Links discovered
- ["'](https://github.com/hyperbrowserai/hyperbrowser-app-examples/blob/main/flow-mapper/lib/[^"']+.md)

--- flow-mapper/lib/graph.ts ---
import { CrawlResult, ApiEndpoint } from './crawl';

export interface FlowNode {
  id: string;
  type: 'page' | 'form' | 'api' | 'decision';
  label: string;
  url?: string;
  method?: string;
  payload?: any;
  response?: any;
  screenshot?: string;
}

export interface FlowEdge {
  id: string;
  source: string;
  target: string;
  label?: string;
  condition?: string;
}

export class FlowGraph {
  nodes: FlowNode[] = [];
  edges: FlowEdge[] = [];
  mermaidSyntax: string = '';
  xstateConfig: any = {};

  addNode(node: FlowNode) {
    this.nodes.push(node);
  }

  addEdge(source: string, target: string, label?: string) {
    this.edges.push({
      id: `${source}-${target}`,
      source,
      target,
      label
    });
  }

  generateMermaidSyntax() {
    let mermaid = 'graph TD\n';
    
    // Add nodes
    this.nodes.forEach(node => {
      const shape = this.getMermaidShape(node.type);
      mermaid += `    ${node.id}${shape.start}"${node.label}"${shape.end}\n`;
    });

    mermaid += '\n';

    // Add edges
    this.edges.forEach(edge => {
      const arrow = edge.label ? `-- "${edge.label}" -->` : '-->';
      mermaid += `    ${edge.source} ${arrow} ${edge.target}\n`;
    });

    // Add styling
    mermaid += '\n';
    mermaid += '    classDef pageNode fill:#1a1a1a,stroke:#FFFD39,stroke-width:2px,color:#fff\n';
    mermaid += '    classDef formNode fill:#0f1419,stroke:#FFFD39,stroke-width:2px,color:#fff\n';
    mermaid += '    classDef apiNode fill:#0a0f16,stroke:#FFFD39,stroke-width:2px,color:#fff\n';
    mermaid += '    classDef decisionNode fill:#16161a,stroke:#FFFD39,stroke-width:2px,color:#fff\n';

    this.mermaidSyntax = mermaid;
    return mermaid;
  }

  private getMermaidShape(nodeType: string): { start: string; end: string } {
    switch (nodeType) {
      case 'page':
        return { start: '((', end: '))' };
      case 'form':
        return { start: '[', end: ']' };
      case 'api':
        return { start: '{', end: '}' };
      case 'decision':
        return { start: '{', end: '}' };
      default:
        return { start: '[', end: ']' };
    }
  }
}

export class GraphBuilder {
  private nodeCounter = 0;

  buildGraph(crawlResult: any): FlowGraph {
    const graph = new FlowGraph();
    
    // Process pages
    if (crawlResult.dom) {
      crawlResult.dom.forEach((domContent: string, index: number) => {
        const pageNode: FlowNode = {
          id: this.generateNodeId('page'),
          type: 'page',
          label: `Page ${index + 1}`,
          screenshot: crawlResult.screenshots?.[index]
        };
        graph.addNode(pageNode);
      });
    }
    
    // Process API endpoints
    if (crawlResult.endpoints) {
      crawlResult.endpoints.forEach((endpoint: any) => {
        const apiNode: FlowNode = {
          id: this.generateNodeId('api'),
          type: 'api',
          label: `${endpoint.method} ${this.extractPathFromUrl(endpoint.url)}`,
          url: endpoint.url,
          method: endpoint.method,
          payload: endpoint.payload,
          response: endpoint.response
        };
        graph.addNode(apiNode);
      });
    }

    // Generate graph syntax
    graph.generateMermaidSyntax();
    
    return graph;
  }

  private generateNodeId(type: string): string {
    return `${type}_${++this.nodeCounter}`;
  }

  private extractPathFromUrl(url: string): string {
    try {
      const urlObj = new URL(url);
      return urlObj.pathname;
    } catch {
      return url;
    }
  }
} 

--- flow-mapper/lib/zip.ts ---
import archiver from 'archiver';
import { Readable } from 'stream';

export interface ZipFile {
  name: string;
  content: string;
  path?: string;
}

export interface ZipResult {
  buffer: Buffer;
  filename: string;
}

export class ZipBuilder {
  async createZip(files: ZipFile[], zipName: string): Promise<ZipResult> {
    return new Promise((resolve, reject) => {
      const archive = archiver('zip', {
        zlib: { level: 9 } // Maximum compression
      });

      const chunks: Buffer[] = [];
      
      archive.on('data', (chunk) => {
        chunks.push(chunk);
      });

      archive.on('end', () => {
        const buffer = Buffer.concat(chunks);
        resolve({
          buffer,
          filename: `${zipName}.zip`
        });
      });

      archive.on('error', (err) => {
        reject(err);
      });

      // Add files to archive
      files.forEach(file => {
        const filePath = file.path || file.name;
        archive.append(file.content, { name: filePath });
      });

      archive.finalize();
    });
  }

  async createPlaywrightZip(playwrightCode: string, baseUrl: string): Promise<ZipResult> {
    const packageJson = {
      name: 'flowmapper-playwright-tests',
      version: '1.0.0',
      description: 'Generated Playwright tests from FlowMapper',
      scripts: {
        test: 'playwright test',
        'test:headed': 'playwright test --headed',
        'test:ui': 'playwright test --ui',
        'test:debug': 'playwright test --debug'
      },
      devDependencies: {
        '@playwright/test': '^1.40.0'
      }
    };

    const playwrightConfig = `import { defineConfig, devices } from '@playwright/test';

export default defineConfig({
  testDir: './tests',
  fullyParallel: true,
  forbidOnly: !!process.env.CI,
  retries: process.env.CI ? 2 : 0,
  workers: process.env.CI ? 1 : undefined,
  reporter: 'html',
  use: {
    baseURL: '${baseUrl}',
    trace: 'on-first-retry',
    screenshot: 'only-on-failure',
  },
  projects: [
    {
      name: 'chromium',
      use: { ...devices['Desktop Chrome'] },
    },
    {
      name: 'firefox',
      use: { ...devices['Desktop Firefox'] },
    },
    {
      name: 'webkit',
      use: { ...devices['Desktop Safari'] },
    },
  ],
});`;

    const readme = `# FlowMapper Generated Playwright Tests

This test suite was automatically generated by FlowMapper based on your website crawl.

## Setup

1. Install dependencies:
   \`\`\`bash
   npm install
   \`\`\`

2. Install Playwright browsers:
   \`\`\`bash
   npx playwright install
   \`\`\`

## Running Tests

- Run all tests: \`npm test\`
- Run tests in headed mode: \`npm run test:headed\`
- Run tests with UI: \`npm run test:ui\`
- Debug tests: \`npm run test:debug\`

## Generated Files

- \`tests/user-flow.spec.ts\` - Main test file with your user flow
- \`playwright.config.ts\` - Playwright configuration
- \`package.json\` - Project dependencies

## Customization

You can modify the tests in \`tests/user-flow.spec.ts\` to:
- Add assertions for specific elements
- Modify test data and inputs
- Add more detailed verification steps
- Extend the test coverage

Generated by FlowMapper - ${new Date().toISOString()}
`;

    const files: ZipFile[] = [
      {
        name: 'package.json',
        content: JSON.stringify(packageJson, null, 2)
      },
      {
        name: 'playwright.config.ts',
        content: playwrightConfig
      },
      {
        name: 'user-flow.spec.ts',
        content: playwrightCode,
        path: 'tests/user-flow.spec.ts'
      },
      {
        name: 'README.md',
        content: readme
      }
    ];

    return this.createZip(files, 'playwright-tests');
  }

  async createReactZip(reactCode: string): Promise<ZipResult> {
    const packageJson = {
      name: 'flowmapper-react-xstate',
      version: '1.0.0',
      description: 'Generated React XState component from FlowMapper',
      private: true,
      scripts: {
        dev: 'next dev',
        build: 'next build',
        start: 'next start',
        lint: 'next lint'
      },
      dependencies: {
        react: '^18.2.0',
        'react-dom': '^18.2.0',
        next: '^14.0.0',
        xstate: '^4.38.0',
        '@xstate/react': '^3.2.0'
      },
      devDependencies: {
        '@types/node': '^20.0.0',
        '@types/react': '^18.2.0',
        '@types/react-dom': '^18.2.0',
        typescript: '^5.0.0'
      }
    };

    const nextConfig = `/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    appDir: true,
  },
}

module.exports = nextConfig`;

    const tsConfig = {
      compilerOptions: {
        target: 'es5',
        lib: ['dom', 'dom.iterable', 'es6'],
        allowJs: true,
        skipLibCheck: true,
        strict: true,
        forceConsistentCasingInFileNames: true,
        noEmit: true,
        esModuleInterop: true,
        module: 'esnext',
        moduleResolution: 'bundler',
        resolveJsonModule: true,
        isolatedModules: true,
        jsx: 'preserve',
        incremental: true,
        plugins: [
          {
            name: 'next'
          }
        ],
        paths: {
          '@/*': ['./*']
        }
      },
      include: ['next-env.d.ts', '**/*.ts', '**/*.tsx', '.next/types/**/*.ts'],
      exclude: ['node_modules']
    };

    const appLayout = `export default function RootLayout({
  children,
}: {
  children: React.ReactNode
}) {
  return (
    <html lang="en">
      <body>{children}</body>
    </html>
  )
}`;

    const appPage = `import UserFlowComponent from '../components/UserFlow'

export default function Home() {
  return (
    <main className="container mx-auto p-8">
      <h1 className="text-3xl font-bold mb-8">FlowMapper Generated User Flow</h1>
      <UserFlowComponent />
    </main>
  )
}`;

    const readme = `# FlowMapper Generated React XState Component

This React component was automatically generated by FlowMapper and uses XState for state management.

## Setup

1. Install dependencies:
   \`\`\`bash
   npm install
   \`\`\`

2. Run the development server:
   \`\`\`bash
   npm run dev
   \`\`\`

3. Open [http://localhost:3000](http://localhost:3000) in your browser

## Generated Files

- \`components/UserFlow.tsx\` - Main XState component with your user flow
- \`app/page.tsx\` - Next.js page component
- \`app/layout.tsx\` - Root layout
- \`package.json\` - Project dependencies

## Usage

The generated component provides:
- Visual state representation
- Interactive controls to trigger state transitions
- Context data display
- Screenshot previews (if available)

## Customization

You can extend the XState machine in \`components/UserFlow.tsx\` to:
- Add more states and transitions
- Include guards and conditions
- Add side effects and services
- Integrate with APIs and external systems

Generated by FlowMapper - ${new Date().toISOString()}
`;

    const files: ZipFile[] = [
      {
        name: 'package.json',
        content: JSON.stringify(packageJson, null, 2)
      },
      {
        name: 'next.config.js',
        content: nextConfig
      },
      {
        name: 'tsconfig.json',
        content: JSON.stringify(tsConfig, null, 2)
      },
      {
        name: 'layout.tsx',
        content: appLayout,
        path: 'app/layout.tsx'
      },
      {
        name: 'page.tsx',
        content: appPage,
        path: 'app/page.tsx'
      },
      {
        name: 'UserFlow.tsx',
        content: reactCode,
        path: 'components/UserFlow.tsx'
      },
      {
        name: 'README.md',
        content: readme
      }
    ];

    return this.createZip(files, 'react-xstate-component');
  }
} 

## Links discovered
- [http://localhost:3000](http://localhost:3000)

--- hb-job-matcher/README.md ---
# HB Job Matcher

An AI-powered job matching application that uses **Hyperbrowser** to extract information from portfolio websites and resumes, then finds matching job opportunities.

![HB Job Matcher](https://img.shields.io/badge/Powered%20by-Hyperbrowser-yellow)
![Next.js](https://img.shields.io/badge/Next.js-15.3.3-blue)
![TypeScript](https://img.shields.io/badge/TypeScript-5-blue)

## ðŸš€ Features

- **Smart Profile Extraction**: Uses Hyperbrowser AI to extract skills, experience, and other relevant information from portfolio websites, Google Drive resumes, and Google Docs
- **Google Drive Integration**: Seamlessly works with Google Drive sharing links and Google Docs
- **Intelligent Job Matching**: Finds relevant job opportunities based on extracted profile data
- **Real-time Processing**: Live updates during profile extraction and job matching
- **Modern UI**: Clean, responsive interface with dark theme
- **API Key Management**: Secure local storage of Hyperbrowser API credentials
- **Match Scoring**: Intelligent scoring system for job relevance

## ðŸ› ï¸ Tech Stack

- **Frontend**: Next.js 15.3.3, React 19, TypeScript, Tailwind CSS
- **Backend**: Next.js API Routes
- **Automation**: Hyperbrowser SDK, Puppeteer Core
- **Styling**: Tailwind CSS with custom dark theme

## ðŸ“‹ Prerequisites

Before running this application, you need:

1. **Node.js** (version 18 or higher)
2. **Hyperbrowser API Key** - Get one at [hyperbrowser.ai](https://www.hyperbrowser.ai/)

## ðŸƒâ€â™‚ï¸ Getting Started

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd hb-job-matcher
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Install Hyperbrowser SDK** (when available)
   ```bash
   npm install @hyperbrowser/sdk puppeteer-core
   ```

4. **Start the development server**
```bash
npm run dev
   ```

5. **Open your browser**
   Navigate to [http://localhost:3000](http://localhost:3000)

## ðŸ”§ Configuration

### API Key Setup

1. Click the "Get Hyperbrowser API Key" button in the header
2. This opens a sidebar where you can:
   - Enter your Hyperbrowser API key
   - Get information about Hyperbrowser
   - Access the official Hyperbrowser website

Your API key is stored locally in your browser and is used to authenticate requests to the Hyperbrowser service.

## ðŸŽ¯ How to Use

1. **Configure API Key**: Click "Get Hyperbrowser API Key" and enter your API key
2. **Enter URL**: Paste the URL of a portfolio website, Google Drive resume, or Google Docs resume
3. **Extract Profile**: Click "Analyze & Find Jobs" to start the extraction process
4. **View Results**: See the extracted profile information and matching job opportunities
5. **Explore Matches**: Review job matches with relevance scores and apply directly

## ðŸ”„ How It Works

### Profile Extraction Process

1. **URL Validation**: The application validates the provided URL
2. **Hyperbrowser Session**: Creates a new browser session using your API key
3. **Smart Extraction**: Uses AI-powered selectors to extract:
   - Personal information (name, title, location)
   - Skills and technologies
   - Work experience
   - Education background
   - Professional summary

### Supported URL Formats

The application supports various URL formats:
- **Portfolio websites**: `https://yourname.dev`, `https://github.com/username`
- **Google Drive files**: `https://drive.google.com/file/d/FILE_ID/view`
- **Google Docs**: `https://docs.google.com/document/d/DOCUMENT_ID`
- **Direct PDF links**: Any direct PDF URL

### Job Matching Algorithm

1. **Skill Analysis**: Analyzes extracted skills against job requirements
2. **Experience Matching**: Matches experience level with job seniority
3. **Location Preferences**: Considers location compatibility
4. **Scoring System**: Calculates match percentage based on:
   - Skill overlap (70% weight)
   - Title similarity (20% weight)
   - Location match (10% weight)

## ðŸ—‚ï¸ Project Structure

```
hb-job-matcher/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ extract-profile/
â”‚   â”‚   â”‚   â””â”€â”€ route.ts          # Profile extraction API
â”‚   â”‚   â””â”€â”€ find-jobs/
â”‚   â”‚       â””â”€â”€ route.ts          # Job matching API
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ JobMatcher.tsx        # Main application component
â”‚   â”‚   â””â”€â”€ Sidebar.tsx           # API key management sidebar
â”‚   â”œâ”€â”€ globals.css               # Global styles
â”‚   â”œâ”€â”€ layout.tsx                # Root layout
â”‚   â””â”€â”€ page.tsx                  # Home page
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## ðŸ”Œ API Endpoints

### POST `/api/extract-profile`
Extracts profile information from a given URL.

**Request Body:**
```json
{
  "url": "https://example.com/portfolio",
  "apiKey": "your-hyperbrowser-api-key"
}
```

**Response:**
```json
{
  "profileData": {
    "name": "John Doe",
    "title": "Software Engineer",
    "skills": ["JavaScript", "React", "Node.js"],
    "experience": "5 years of experience...",
    "education": "BS Computer Science",
    "location": "San Francisco, CA",
    "summary": "Experienced developer..."
  }
}
```

### POST `/api/find-jobs`
Finds matching jobs based on extracted profile data.

**Request Body:**
```json
{
  "profile": { /* ExtractedProfile object */ },
  "apiKey": "your-hyperbrowser-api-key"
}
```

**Response:**
```json
{
  "jobMatches": [
    {
      "id": "job-1",
      "title": "Senior Software Engineer",
      "company": "TechCorp",
      "location": "Remote",
      "matchScore": 95,
      "description": "Job description...",
      "requirements": ["JavaScript", "React"],
      "url": "https://example.com/job"
    }
  ]
}
```

## ðŸ”® Future Enhancements

When the Hyperbrowser SDK is fully integrated, this application will support:

- **Real-time Job Scraping**: Live scraping from Indeed, LinkedIn, Stack Overflow Jobs
- **Advanced AI Extraction**: More sophisticated profile data extraction
- **Multiple Format Support**: PDF resume parsing, LinkedIn profile import
- **Job Application Automation**: Automated job application submission
- **Email Notifications**: Alerts for new matching opportunities
- **Analytics Dashboard**: Track application success rates

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Commit your changes: `git commit -m 'Add feature'`
4. Push to the branch: `git push origin feature-name`
5. Submit a pull request

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- **Hyperbrowser Team** for providing the powerful browser automation platform
- **Next.js Team** for the excellent React framework
- **Tailwind CSS** for the utility-first styling approach

## ðŸ“ž Support

- **Hyperbrowser Documentation**: [docs.hyperbrowser.ai](https://docs.hyperbrowser.ai)
- **Hyperbrowser Website**: [hyperbrowser.ai](https://www.hyperbrowser.ai/)
- **Issues**: Please report bugs and feature requests via GitHub issues

---

**Powered by [Hyperbrowser](https://www.hyperbrowser.ai/)**


## Links discovered
- [HB Job Matcher](https://img.shields.io/badge/Powered%20by-Hyperbrowser-yellow)
- [Next.js](https://img.shields.io/badge/Next.js-15.3.3-blue)
- [TypeScript](https://img.shields.io/badge/TypeScript-5-blue)
- [hyperbrowser.ai](https://www.hyperbrowser.ai/)
- [http://localhost:3000](http://localhost:3000)
- [LICENSE](https://github.com/hyperbrowserai/hyperbrowser-app-examples/blob/main/hb-job-matcher/LICENSE.md)
- [docs.hyperbrowser.ai](https://docs.hyperbrowser.ai)
- [Hyperbrowser](https://www.hyperbrowser.ai/)

--- hb-job-matcher/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;


--- hb-pitchdeck/README.md ---
# Pitch Deck Generator

**Built with [Hyperbrowser](https://hyperbrowser.ai)**

A Next.js application that turns any company website into a professional startup pitch deck PDF and JSON.

## Features

- Scrapes website content using Hyperbrowser's powerful SDK
- Generates structured pitch deck content with OpenAI
- Creates professionally formatted PDF pitch decks
- Supports multiple themes: modern, dark, and neon
- Downloads both PDF and JSON formats
- Sends generated pitch decks to Slack (optional)

## Quick Start

### Get an API key

Sign up for an API key at [https://hyperbrowser.ai](https://hyperbrowser.ai)

### Environment Setup

Create a `.env.local` file with the following variables:

```
HYPERBROWSER_API_KEY=your_hyperbrowser_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
SLACK_WEBHOOK_URL=your_slack_webhook_url_here  # Optional
```

### Installation

```bash
npm install
npm run dev
```

### Usage

1. Enter a company website URL
2. Select a theme (modern, dark, or neon)
3. Click "Generate Pitch Deck"
4. View the generated pitch deck content
5. Download as PDF or JSON

## API Reference

### `POST /api/pitchdeck`

Generates a pitch deck from a URL.

**Request Body:**

```json
{
  "url": "https://example.com",
  "theme": "modern" // Optional: "modern", "dark", or "neon"
}
```

**Response:**

```json
{
  "pitch": {
    "company": "Company Name",
    "one_liner": "One sentence description",
    "problem": ["Problem point 1", "Problem point 2", ...],
    "solution": ["Solution point 1", "Solution point 2", ...],
    // Other pitch deck sections...
  },
  "pdfBase64": "base64-encoded-pdf-data",
  "filename": "company-name-pitch-deck.pdf"
}
```

## Growth Use Case

This tool helps startups quickly generate professional pitch decks for investor meetings, saving hours of design work and content creation. It's perfect for founders who need to quickly communicate their value proposition and business model.

---

Follow [@hyperbrowser](https://twitter.com/hyperbrowser) for updates.

## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)
- [https://hyperbrowser.ai](https://hyperbrowser.ai)
- [@hyperbrowser](https://twitter.com/hyperbrowser)

--- hb-pitchdeck/next.config.js ---
/** @type {import('next').NextConfig} */
const nextConfig = {
  reactStrictMode: true,
  images: {
    domains: ['hyperbrowser.ai'],
  },
};

module.exports = nextConfig;

--- hb-pitchdeck/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;


--- hb-pitchdeck/postcss.config.js ---
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}

--- hb-pitchdeck/tailwind.config.js ---
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './pages/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    './app/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {
      colors: {
        'hb-yellow': '#F0FF26',
        'hb-green': '#00FF88',
        'hb-black': '#000000',
        'hb-dark': '#111111',
        'hb-gray': '#333333',
      },
    },
  },
  plugins: [],
}

--- hb-ui-bot-app/README.md ---
# UI Bot

**Built with [Hyperbrowser](https://hyperbrowser.ai)**

A minimal Next.js app that screenshots websites and analyzes UI/UX issues that might cause users to leave. Uses Hyperbrowser for automated browser analysis and OpenAI for detailed UX insights.

## Features

- ðŸ¤– Real browser testing using Hyperbrowser's AI agents
- ðŸ” AI-powered UI/UX analysis identifying why users leave websites
- âš¡ Automated URL formatting (just type "example.com")
- ðŸ“Š Categorized insights: Critical Issues, UX Problems, Performance Concerns, Conversion Barriers
- ðŸ’¡ Actionable recommendations for improving user experience
- ðŸŽ¯ Professional YC startup-style interface with minimal design

## Getting Started

### 1. Get API Keys

- **Hyperbrowser**: Get an API key from https://hyperbrowser.ai
- **OpenAI**: Get an API key from https://platform.openai.com/api-keys

### 2. Environment Setup

Copy `.env.example` to `.env.local` and add your API keys:

```bash
cp .env.example .env.local
```

Edit `.env.local`:
```
HYPERBROWSER_API_KEY=your_hyperbrowser_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
```

### 3. Install Dependencies

```bash
npm install
```

### 4. Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to use the app.

## How It Works

1. **Input URL**: Enter any website URL you want to analyze
2. **Screenshot & Analysis**: Hyperbrowser takes a full-page screenshot and performs initial UI analysis
3. **AI Processing**: OpenAI analyzes the results to identify specific issues and provide actionable recommendations
4. **Results**: Get categorized insights on critical issues, UX problems, performance concerns, and conversion barriers

## Example Use Cases

- **Landing Page Optimization**: Identify why visitors aren't converting
- **E-commerce Analysis**: Find checkout flow issues and product page problems  
- **SaaS Dashboard Review**: Improve user onboarding and feature discovery
- **Mobile Experience Audit**: Ensure responsive design works properly

## Tech Stack

- **Next.js 15** - React framework
- **Hyperbrowser SDK** - Browser automation and screenshot capture
- **OpenAI GPT-4** - AI-powered analysis and recommendations  
- **TypeScript** - Type safety
- **Tailwind CSS** - Minimal, utility-first styling

## API Reference

The app uses Hyperbrowser's Browser Use agent with vision capabilities enabled:

- `useVision: true` - Enables screenshot analysis
- `useVisionForPlanner: true` - Provides visual context for planning
- Session configured at 1920x1080 for full desktop analysis

Follow [@hyperbrowser_ai](https://x.com/hyperbrowser) for updates.


## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)
- [http://localhost:3000](http://localhost:3000)
- [@hyperbrowser_ai](https://x.com/hyperbrowser)

--- hb-ui-bot-app/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;


--- hyper-research/README.md ---
# AI Research Tool

Powered by **Hyperbrowser** + **Claude Opus 4.5**

A powerful multi-URL research tool that uses AI to analyze and synthesize information across multiple web sources.

## Features

- ðŸ” **Multi-URL Analysis**: Scrape and analyze multiple websites simultaneously
- ðŸ¤– **Claude Opus 4.5**: Uses Anthropic's latest and most capable AI model
- ðŸ“Š **Visual Comparisons**: Auto-generated comparison tables and charts
- ðŸ’¡ **AI Synthesis**: Get comprehensive insights across all sources
- ðŸŽ¯ **Research Questions**: Ask specific questions to guide the analysis

## Getting Started

### Prerequisites

- Node.js 18+ 
- [Hyperbrowser API key](https://hyperbrowser.ai) (required)
- [Anthropic API key](https://console.anthropic.com/settings/keys) (recommended for Claude Opus 4.5)

### Installation

```bash
npm install
```

### Setup API Keys

1. Copy `.env.example` to `.env.local`
2. Add your API keys:
   - **Hyperbrowser API Key**: Get from [hyperbrowser.ai](https://hyperbrowser.ai)
   - **Anthropic API Key**: Get from [console.anthropic.com](https://console.anthropic.com/settings/keys)

Or add them directly in the app via Settings (âš™ï¸ icon)

### Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

## How to Use

1. **Add URLs**: Enter 2-10 URLs you want to analyze
2. **Research Question** (optional): Add a specific question like "Compare pricing and features"
3. **Click Research**: The tool will:
   - Scrape all URLs using Hyperbrowser
   - Analyze content with Claude Opus 4.5
   - Generate AI synthesis and visual comparisons
4. **View Results**: See insights, comparison tables, and source links

## Example Use Cases

- **Competitor Analysis**: Compare features, pricing, and positioning
- **Market Research**: Analyze trends across industry websites
- **Product Comparison**: Evaluate multiple products side-by-side
- **Content Research**: Gather insights from multiple articles/blogs
- **Due Diligence**: Research multiple aspects of a company

## Tech Stack

- **Next.js 16** - React framework
- **Hyperbrowser SDK** - Web scraping and browser automation
- **Anthropic Claude Opus 4.5** - AI analysis and synthesis
- **Recharts** - Data visualization
- **Tailwind CSS** - Styling

## API Endpoints

- `POST /api/research` - Multi-URL research and analysis

## License

MIT


## Links discovered
- [Hyperbrowser API key](https://hyperbrowser.ai)
- [Anthropic API key](https://console.anthropic.com/settings/keys)
- [hyperbrowser.ai](https://hyperbrowser.ai)
- [console.anthropic.com](https://console.anthropic.com/settings/keys)
- [http://localhost:3000](http://localhost:3000)

--- hyper-research/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;


--- hyper-research/app/types.ts ---
// Research types
export interface ResearchResult {
  url: string;
  title?: string;
  content: string;
  metadata?: {
    description?: string;
  };
  error?: boolean;
}

export interface ComparisonData {
  category: string;
  items: {
    name: string;
    value: string | number;
  }[];
}

export interface SourceScore {
  name: string;
  scores: {
    pricing: number;
    features: number;
    ease_of_use: number;
    performance: number;
    support: number;
  };
  overall: number;
}

export interface ResearchResponse {
  synthesis: string;
  comparisons: ComparisonData[];
  results: ResearchResult[];
  scores?: SourceScore[];
}


--- hyperbuild/README.md ---
**Built with [Hyperbrowser](https://hyperbrowser.ai)**

# HyperBuild - Visual AI Agent Builder

- Pick a template or describe your idea
- See a ReactFlow graph instantly
- Run live with real Hyperbrowser
- Export results to `/public/runs/<uuid>/`

## Get an API key
- Hyperbrowser: https://hyperbrowser.ai
- OpenAI: https://platform.openai.com/

## Quick start

```bash
cp .env.example .env.local # or set env vars manually
npm i
npm run dev
```

Required env vars:

```bash
HYPERBROWSER_API_KEY=sk_live_...
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-5-nano
```

## Growth use-case
Demonstrates visual + natural-language agent creation for real-time web data workflows.

Follow @hyperbrowser for updates.



## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)

--- hyperbuild/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;


--- hyperbuild/lib/hyperbrowser.ts ---
import { Hyperbrowser } from "@hyperbrowser/sdk";
import type { WebAutomationProvider } from "./providers";

export type ScrapeParams = {
  url: string;
  viewport?: { width: number; height: number };
  screenshot?: boolean;
};

export type ExtractParams = {
  url: string;
  schema: unknown;
};

const hb = new Hyperbrowser({
  apiKey: process.env.HYPERBROWSER_API_KEY,
});

export async function scrapeMarkdown(params: ScrapeParams): Promise<string> {
  const resp = await hb.scrape.startAndWait(params);
  return resp?.data?.markdown ?? "";
}

export async function extractStructured<T = unknown>(params: ExtractParams): Promise<T | null> {
  const resp = await hb.extract.startAndWait({ urls: [params.url], schema: params.schema as object });
  return (resp?.data as T) ?? null;
}

export { hb };

export async function crawlMarkdown(params: { seedUrls: string[]; maxPages?: number }): Promise<string> {
  const resp = await hb.crawl.startAndWait({ url: params.seedUrls[0], maxPages: params.maxPages });
  let markdown = "";
  if (Array.isArray(resp?.data)) {
    for (const page of resp.data) {
      if (page?.markdown) markdown += `\n-----\nUrl: ${page.url}\n${page.markdown}`;
    }
  }
  return markdown.trim();
}

export const HyperbrowserProvider: WebAutomationProvider = {
  name: "hyperbrowser",
  scrapeMarkdown: (i) => scrapeMarkdown(i),
  extractStructured: (i) => extractStructured(i),
  crawlMarkdown: (i) => crawlMarkdown(i),
};




--- hyperbuild/lib/llm.ts ---
import OpenAI from "openai";

export type GenerateAgentInput = {
  idea: string;
};

export type WorkflowGraph = {
  nodes: Array<{ id: string; type: string; data: Record<string, unknown> }>;
  edges: Array<{ id: string; source: string; target: string }>;
};

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export async function generateAgentFromIdea(idea: string): Promise<WorkflowGraph> {
  
  const system = `You are a workflow generator. Output ONLY strict JSON with {nodes, edges}.

EXAMPLE OUTPUT FORMAT:
{
  "nodes": [
    {"id": "s1", "type": "Start", "data": {"input": ""}},
    {"id": "n1", "type": "Scrape", "data": {"url": "https://example.com"}},
    {"id": "n2", "type": "LLM", "data": {"instruction": "Analyze the scraped content and extract pricing information. Identify plans, costs, features, and billing cycles. Format as structured JSON with clear categories."}},
    {"id": "o1", "type": "Output", "data": {"filename": "output.json"}}
  ],
  "edges": [
    {"id": "e1", "source": "s1", "target": "n1"},
    {"id": "e2", "source": "n1", "target": "n2"},
    {"id": "e3", "source": "n2", "target": "o1"}
  ]
}

RULES:
- Always include exactly one Start node and one Output node (NOT End node)
- Always include AT LEAST one Hyperbrowser node (Scrape, Extract, or Crawl)
- Use <= 5 nodes total
- CRITICAL: Every LLM node MUST have an "instruction" field in its data object
- LLM instructions must be detailed and specific to the user's request
- Scrape nodes must have "url" field in data
- Connect nodes linearly: Start -> Scrape/Extract/Crawl -> LLM -> Output

For the user's idea "${idea}", create a workflow that includes a detailed LLM instruction tailored to their specific request.`;
  const user = `Create a workflow for: ${idea}

Remember: The LLM node MUST include a specific "instruction" field that tells the AI exactly what to do with the scraped content.`;
  const resp = await openai.chat.completions.create({
    model: process.env.OPENAI_MODEL || "gpt-5-nano",
    temperature: 0.1,
    messages: [
      { role: "system", content: system },
      { role: "user", content: user },
    ],
    response_format: { type: "json_object" },
  });
  const content = resp.choices?.[0]?.message?.content || "{}";
  console.log("Generated workflow:", content); // Debug log
  const parsed = JSON.parse(content) as WorkflowGraph;
  
  // Ensure LLM nodes have instructions
  if (parsed.nodes) {
    parsed.nodes.forEach(node => {
      if (node.type === "LLM" && (!node.data || !node.data.instruction)) {
        console.warn("LLM node missing instruction, adding default");
        node.data = { 
          ...node.data, 
          instruction: `Analyze the input content related to "${idea}". Extract key information and format as structured JSON with clear categories and actionable insights.`
        };
      }
    });
  }
  
  return parsed;
}

export async function runDeterministicLLM(input: string, instruction?: string): Promise<string> {
  const sys = instruction || "Summarize the input precisely in 5 bullet points. Keep neutral tone.";
  const resp = await openai.chat.completions.create({
    model: process.env.OPENAI_MODEL || "gpt-5-nano",
    temperature: 0.1,
    messages: [
      { role: "system", content: sys },
      { role: "user", content: input.slice(0, 6000) },
    ],
  });
  return resp.choices?.[0]?.message?.content ?? "";
}




--- hyperbuild/lib/providers.ts ---
export type ScrapeInput = { url: string };
export type ExtractInput = { url: string; schema: unknown };
export type CrawlInput = { seedUrls: string[]; maxPages?: number };

export interface WebAutomationProvider {
  name: string;
  scrapeMarkdown(input: ScrapeInput): Promise<string>;
  extractStructured<T = unknown>(input: ExtractInput): Promise<T | null>;
  crawlMarkdown(input: CrawlInput): Promise<string>;
}

type ProviderKey = "hyperbrowser";

const providers: Partial<Record<ProviderKey, WebAutomationProvider>> = {};

export function registerProvider(key: ProviderKey, impl: WebAutomationProvider) {
  providers[key] = impl;
}

export function getProvider(): WebAutomationProvider {
  const key = (process.env.WEB_PROVIDER || "hyperbrowser") as ProviderKey;
  const impl = providers[key];
  if (!impl) throw new Error(`No provider registered for ${key}`);
  return impl;
}




--- hyperbuild/lib/qna.ts ---
import { runDeterministicLLM } from "./llm";

export type QAPair = { q: string; a: string };
export type QNAResult = { qa_pairs: QAPair[] };

export async function generateQnaFromText(text: string): Promise<QNAResult> {
  // Use AI to generate meaningful Q&A pairs from the content
  const instruction = `Analyze the following content and generate 5-8 high-quality question-answer pairs that would be useful for someone learning about this topic. 

Focus on:
- Key facts and important information
- Definitions and explanations
- How-to questions if applicable
- Common concerns or frequently asked questions

Format your response as valid JSON with this structure:
{
  "qa_pairs": [
    {"q": "What is...", "a": "The answer explaining..."},
    {"q": "How does...", "a": "The process involves..."}
  ]
}

Content to analyze:
${text.slice(0, 4000)}`;

  try {
    const response = await runDeterministicLLM(text, instruction);
    
    // Try to parse the JSON response
    const parsed = JSON.parse(response);
    if (parsed.qa_pairs && Array.isArray(parsed.qa_pairs)) {
      return { qa_pairs: parsed.qa_pairs };
    }
    
    // Fallback if parsing fails
    return { qa_pairs: [{ q: "What is this content about?", a: text.slice(0, 200) + "..." }] };
  } catch (error) {
    console.error("QnA generation failed:", error);
    // Fallback Q&A
    return { 
      qa_pairs: [
        { q: "What is this content about?", a: text.slice(0, 200) + "..." },
        { q: "What are the key points?", a: "Please refer to the original content for detailed information." }
      ] 
    };
  }
}




--- hyperbuild/lib/utils.ts ---
import { randomUUID } from "crypto";
import { writeFileSync, mkdirSync } from "fs";
import { join } from "path";

export function generateRunPath(id?: string) {
  const runId = id ?? randomUUID();
  const dir = join(process.cwd(), "public", "runs", runId);
  mkdirSync(dir, { recursive: true });
  return { runId, dir };
}

export function writeRunFile(dir: string, name: string, data: unknown) {
  const file = join(dir, name);
  writeFileSync(file, typeof data === "string" ? data : JSON.stringify(data, null, 2), "utf8");
  return file;
}




--- hyperdatalab/README.md ---
**Built with [Hyperbrowser](https://hyperbrowser.ai)**

# HyperDataLab

A web data extraction and QnA generation tool that turns any webpage into structured question-answer pairs.

## Quick Start

1. **Get an API key** from [hyperbrowser.ai](https://hyperbrowser.ai)

2. **Set up environment variables**:

```bash
# Create a .env.local file with:
HYPERBROWSER_API_KEY=your_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
```

3. **Install dependencies**:

```bash
npm install
```

4. **Run the development server**:

```bash
npm run dev
```

5. Open [http://localhost:3000](http://localhost:3000) and enter a URL to analyze.

## Use Cases

HyperDataLab helps growth teams extract valuable information from websites by automatically generating question-answer pairs from any web page, enabling quick data analysis and content repurposing.

Follow [@hyperbrowser](https://x.com/hyperbrowser) for updates.

## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)
- [hyperbrowser.ai](https://hyperbrowser.ai)
- [http://localhost:3000](http://localhost:3000)
- [@hyperbrowser](https://x.com/hyperbrowser)

--- hyperdatalab/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;


--- hyperdatalab/lib/config.ts ---
export type RequiredEnv = {
  HYPERBROWSER_API_KEY: string;
  HB_BASE_URL?: string;
  OPENAI_API_KEY?: string; 
};

export function getEnv(): RequiredEnv {
  const {
    HYPERBROWSER_API_KEY,
    HB_BASE_URL,
    OPENAI_API_KEY,
  } = process.env as NodeJS.ProcessEnv & RequiredEnv;

  if (!HYPERBROWSER_API_KEY) throw new Error('Missing HYPERBROWSER_API_KEY');

  return { HYPERBROWSER_API_KEY, HB_BASE_URL, OPENAI_API_KEY };
}

export const HYPERBROWSER_DEFAULT_BASE_URL = 'https://api.hyperbrowser.ai';




--- hyperdatalab/lib/fs.ts ---
import fs from 'fs';
import path from 'path';

export function canWritePublic(): boolean {
  // Vercel (and many serverless platforms) mount the app directory read-only.
  // In those environments, avoid writing under process.cwd()/public.
  // We still allow writes locally (dev) and in non-Vercel environments.
  return !process.env.VERCEL;
}

export function ensureDir(dirPath: string): void {
  if (!fs.existsSync(dirPath)) {
    fs.mkdirSync(dirPath, { recursive: true });
  }
}

export function writeFileSafe(filePath: string, data: Buffer | string): void {
  ensureDir(path.dirname(filePath));
  fs.writeFileSync(filePath, data);
}

export function resolveRunPath(runId: string, ...segments: string[]): string {
  return path.join(process.cwd(), 'public', 'runs', runId, ...segments);
}




--- hyperdatalab/lib/hashing.ts ---
import crypto from 'crypto';

export function stableHash(input: string | object): string {
  const data = typeof input === 'string' ? input : JSON.stringify(input);
  return crypto.createHash('sha256').update(data).digest('hex').slice(0, 16);
}




--- hyperdatalab/lib/hooks.ts ---
import { useState, useEffect } from 'react';

export function useLocalStorage<T>(key: string, initialValue: T): [T, (value: T) => void] {
  const [storedValue, setStoredValue] = useState<T>(initialValue);
  
  useEffect(() => {
    try {
      const item = window.localStorage.getItem(key);
      const value = item ? JSON.parse(item) : initialValue;
      setStoredValue(value);
    } catch (error) {
      console.log(error);
    }
  }, [key, initialValue]);

  // Return a wrapped version of useState's setter function that persists the new value to localStorage
  const setValue = (value: T) => {
    try {
      // Allow value to be a function so we have same API as useState
      const valueToStore = value instanceof Function ? value(storedValue) : value;
      // Save state
      setStoredValue(valueToStore);
      // Save to local storage
      window.localStorage.setItem(key, JSON.stringify(valueToStore));
    } catch (error) {
      console.log(error);
    }
  };

  return [storedValue, setValue];
}


--- hyperdatalab/lib/hyperbrowser.ts ---
import { Hyperbrowser } from '@hyperbrowser/sdk';
import { getEnv, HYPERBROWSER_DEFAULT_BASE_URL } from './config';

export type Evidence = {
  htmlPath: string;
  screenshotPath: string;
};

export type FetchResult = {
  html: string;
  text: string;
  screenshotFile: string; 
  textPath: string; 
  evidence: Evidence;
};

export async function createHBClient(overrideApiKey?: string): Promise<Hyperbrowser> {
  // If we have an override API key, use it directly without checking environment variables
  if (overrideApiKey) {
    console.log('[HB] Using provided API key');
    const client = new Hyperbrowser({
      apiKey: overrideApiKey,
      baseUrl: process.env.HB_BASE_URL || HYPERBROWSER_DEFAULT_BASE_URL,
      timeout: 60000,
    });
    return client;
  }
  
  // Otherwise, fall back to environment variables
  const { HYPERBROWSER_API_KEY, HB_BASE_URL } = getEnv();
  const client = new Hyperbrowser({
    apiKey: HYPERBROWSER_API_KEY,
    baseUrl: HB_BASE_URL || HYPERBROWSER_DEFAULT_BASE_URL,
    timeout: 60000,
  });
  return client;
}

export async function fetchWithEvidence(params: {
  url: string;
  runId: string;
  apiKey?: string;
}): Promise<FetchResult> {
  const client = await createHBClient(params.apiKey);

  try {
    console.log(`[HB] Starting scrape for ${params.url}`);
    const result = await client.scrape.startAndWait({
      url: params.url,
      scrapeOptions: {
        formats: ["html", "markdown"],
      },
    });
    console.log(`[HB] Raw result:`, JSON.stringify(result, null, 2).slice(0, 500));

    type ScrapeData = { html?: string; markdown?: string; screenshot?: string };
    const html: string = (result.data as ScrapeData)?.html || '';
    const text: string = (result.data as ScrapeData)?.markdown || '';
    const screenshotBase64: string | undefined = (result.data as ScrapeData)?.screenshot;

    const htmlPath = '';
    const textPath = '';
    const screenshotPath = '';
    console.log(`[HB] Skipping evidence writes; returning inline content for run=${params.runId}`);

    const out = {
      html,
      text,
      screenshotFile: screenshotPath,
      textPath,
      evidence: { htmlPath, screenshotPath },
    };
    console.log(`[HB] Saved evidence run=${params.runId} html=${out.evidence.htmlPath} txt=${textPath}`);
    return out;
  } finally {
  }
}




--- hypergraph/README.md ---
**Built with [Hyperbrowser](https://hyperbrowser.ai)**

# HyperGraph

Turn any technical topic into a traversable skill graph your agent can navigate. This is the difference between an agent that follows instructions and one that understands a domain.

HyperGraph scrapes real sources with Hyperbrowser, then uses Claude to generate a network of interconnected markdown nodes â€” each one a complete thought, linked with wikilinks the agent can follow.

```
Search â†’ Hyperbrowser scrape â†’ Claude graph generation â†’ Interactive force graph
```

## What it generates

- **MOC** â€” Map of Content; the entry point your agent reads first
- **Concepts** â€” foundational ideas, theories, and frameworks
- **Patterns** â€” reusable techniques and approaches
- **Gotchas** â€” failure modes and counterintuitive findings

Every node has a YAML `description` the agent can scan without reading the full file. Wikilinks are woven into prose so the agent knows *why* to follow them.

## Get an API key

Get your Hyperbrowser API key at [https://hyperbrowser.ai](https://hyperbrowser.ai)

## Quick start

```bash
git clone https://github.com/yourusername/hypergraph
cd hypergraph
npm install
cp .env.example .env.local   # fill in your keys
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) and enter any technical topic.

## Environment variables

```bash
HYPERBROWSER_API_KEY=          # from hyperbrowser.ai
SERPER_API_KEY=                # from serper.dev â€” used to find source URLs
OPENAI_API_KEY=                # used for graph generation with GPT-4o
HYPERBROWSER_MAX_CONCURRENCY=1 # free plan: keep at 1. paid plans can increase this.
```

## Free plan & concurrency

The free Hyperbrowser plan supports **1 concurrent browser session**. By default the app scrapes URLs **sequentially** (one at a time), so it works out of the box on any plan.

If you hit a concurrency limit the app will show an amber warning banner with an **Upgrade plan** link rather than crashing silently. To unlock parallel scraping, upgrade at [hyperbrowser.ai](https://hyperbrowser.ai) and set `HYPERBROWSER_MAX_CONCURRENCY` to match your plan's limit.

## Stack

- **Next.js 16** â€” App Router, API routes
- **Hyperbrowser** â€” scrapes source material with `onlyMainContent` for clean output
- **OpenAI GPT-4o** â€” generates the skill graph JSON from scraped docs
- **Serper** â€” Google search to find the best source URLs for any topic
- **react-force-graph-2d** â€” force-directed graph canvas rendering
- **react-markdown** â€” renders node content in the VS Code-style preview panel

## Download

Every generated graph can be downloaded as a `.zip` containing ready-to-use markdown files. Drop the folder into `.claude/skills/` or `.cursor/skills/` and point your agent at `moc.md` as the entry point.

---

Follow [@hyperbrowser](https://x.com/hyperbrowser) for updates.


## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)
- [https://hyperbrowser.ai](https://hyperbrowser.ai)
- [http://localhost:3000](http://localhost:3000)
- [hyperbrowser.ai](https://hyperbrowser.ai)
- [@hyperbrowser](https://x.com/hyperbrowser)

--- hypergraph/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  serverExternalPackages: ["@hyperbrowser/sdk"],
};

export default nextConfig;


--- hypergraph/lib/generator.ts ---
import OpenAI from "openai";
import type { GraphNode, SkillGraph, GeneratedFile } from "@/types/graph";

const openai = new OpenAI();

const SYSTEM_PROMPT = `You are a domain knowledge graph architect. Given a topic and source material, produce a deeply interconnected JSON skill graph that enables an agent to UNDERSTAND the domain â€” not merely summarize it. This is the difference between an agent that follows instructions and an agent that understands a domain.

Output format (JSON only):
{
  "topic": "the topic",
  "nodes": [
    {
      "id": "kebab-case-id",
      "label": "Human Readable Label",
      "type": "moc" | "concept" | "pattern" | "gotcha",
      "description": "One-sentence description the agent can scan to decide whether to read the full file",
      "content": "Full markdown content with [[wikilinks]] woven into prose",
      "links": ["other-node-id"]
    }
  ]
}

Node type definitions:
- "moc" â€” exactly 1 per graph; the Map of Content and traversal entry point
- "concept" â€” a foundational idea, theory, or framework in the domain
- "pattern" â€” a reusable approach, technique, or methodology
- "gotcha" â€” a counterintuitive finding, failure mode, or common mistake

Rules:
- Generate 12â€“18 nodes total
- Exactly 1 node must be type "moc"
- Every [[wikilink]] must appear INSIDE a prose sentence that explains WHY the agent should follow it. Never list wikilinks as bare bullets â€” they must carry meaning through the sentence they live in.
- The "links" array must list every node ID referenced via [[wikilinks]] in the content
- Every non-moc node must begin with YAML frontmatter:
  ---
  title: Human Readable Label
  type: concept | pattern | gotcha
  description: One-sentence scan description
  ---
- Node IDs must be kebab-case
- Content must be rich, substantive markdown â€” not summaries. Each node is one complete thought or claim about the domain.

MOC node requirements (type "moc"):
- Opens with a 2-3 sentence overview of the domain and why structured knowledge of it matters
- Contains a "## Domain Clusters" section where each cluster is described in 1-2 sentences with [[wikilinks]] to relevant concept nodes woven into the prose
- Contains an "## Explorations Needed" section with 2-3 open questions the graph does not yet answer â€” gaps in the current knowledge structure
- The MOC is a navigable entry point, not a table of contents. Each link must be justified in prose.

Depth requirements:
- Concept nodes must explain the underlying mechanism or theory, not just define terms
- Pattern nodes must include when to apply the pattern and what breaks it
- Gotcha nodes must explain why the mistake is made and what the correct mental model is
- This graph should give an agent enough structured knowledge to reason about novel situations in the domain`;


export async function generateGraph(
  topic: string,
  docs: { url: string; markdown: string }[]
): Promise<{ graph: SkillGraph; files: GeneratedFile[] }> {
  const truncatedDocs = docs
    .map((d) => `## Source: ${d.url}\n\n${d.markdown.slice(0, 4000)}`)
    .join("\n\n---\n\n");

  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    response_format: { type: "json_object" },
    messages: [
      { role: "system", content: SYSTEM_PROMPT },
      {
        role: "user",
        content: `Topic: ${topic}\n\nScraped documentation:\n\n${truncatedDocs}`,
      },
    ],
    temperature: 0.7,
    max_tokens: 8192,
  });

  const raw = response.choices[0].message.content;
  if (!raw) throw new Error("Empty response from OpenAI");

  const parsed = JSON.parse(raw) as SkillGraph;

  if (!parsed.nodes || parsed.nodes.length < 3) {
    throw new Error("Generated graph has too few nodes");
  }

  const files: GeneratedFile[] = parsed.nodes.map((node: GraphNode) => ({
    path: `${slugify(topic)}/${node.id}.md`,
    content: node.content,
  }));

  return { graph: parsed, files };
}

function slugify(text: string): string {
  return text
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, "-")
    .replace(/(^-|-$)/g, "");
}


--- hypergraph/types/graph.ts ---
export type NodeType = "moc" | "concept" | "pattern" | "gotcha";

export interface GraphNode {
  id: string;
  label: string;
  type: NodeType;
  description: string;
  content: string;
  links: string[];
}

export interface SkillGraph {
  topic: string;
  nodes: GraphNode[];
}

export interface GeneratedFile {
  path: string;
  content: string;
}

export interface GenerateResponse {
  graph: SkillGraph;
  files: GeneratedFile[];
}

export interface ForceGraphNode {
  id: string;
  label: string;
  type: NodeType;
  val: number;
}

export interface ForceGraphLink {
  source: string;
  target: string;
}

export interface ForceGraphData {
  nodes: ForceGraphNode[];
  links: ForceGraphLink[];
}

export const NODE_COLORS: Record<NodeType, string> = {
  moc: "#000000",
  concept: "#404040",
  pattern: "#737373",
  gotcha: "#a3a3a3",
};

export const NODE_SIZES: Record<NodeType, number> = {
  moc: 3,
  concept: 2,
  pattern: 1.5,
  gotcha: 1,
};


--- hypergraph/lib/hyperbrowser.ts ---
import Hyperbrowser from "@hyperbrowser/sdk";

let client: Hyperbrowser | null = null;

function getClient(): Hyperbrowser {
  if (!client) {
    const apiKey = process.env.HYPERBROWSER_API_KEY;
    if (!apiKey) throw new Error("HYPERBROWSER_API_KEY is not set");
    client = new Hyperbrowser({ apiKey });
  }
  return client;
}

const MAX_CONCURRENCY = Math.max(
  1,
  parseInt(process.env.HYPERBROWSER_MAX_CONCURRENCY ?? "1", 10)
);

/** Identify errors caused by exceeding the plan's concurrency limit. */
function isConcurrencyError(err: unknown): boolean {
  const msg =
    err instanceof Error
      ? err.message.toLowerCase()
      : String(err).toLowerCase();
  return (
    msg.includes("concurrent") ||
    msg.includes("concurrency") ||
    msg.includes("session limit") ||
    msg.includes("too many") ||
    msg.includes("rate limit") ||
    msg.includes("upgrade") ||
    msg.includes("plan")
  );
}

export class ConcurrencyPlanError extends Error {
  constructor() {
    super(
      "Your Hyperbrowser plan only supports 1 concurrent browser. " +
        "The app is running in sequential mode, but multiple scrapes still " +
        "exceeded the limit. Upgrade at https://hyperbrowser.ai to unlock " +
        "parallel execution."
    );
    this.name = "ConcurrencyPlanError";
  }
}

interface ScrapeResult {
  url: string;
  markdown: string;
}

/** Scrape a single URL, re-throwing concurrency errors as ConcurrencyPlanError. */
async function scrapeOne(
  hb: Hyperbrowser,
  url: string
): Promise<ScrapeResult> {
  try {
    const result = await hb.scrape.startAndWait({
      url,
      scrapeOptions: { formats: ["markdown"], onlyMainContent: true },
    });
    return { url, markdown: result.data?.markdown ?? "" };
  } catch (err) {
    if (isConcurrencyError(err)) throw new ConcurrencyPlanError();
    throw err;
  }
}

/**
 * Scrape an array of URLs with bounded concurrency.
 * Defaults to MAX_CONCURRENCY=1 (safe for free-plan users).
 * Set HYPERBROWSER_MAX_CONCURRENCY env var to increase for paid plans.
 */
export async function scrapeUrls(urls: string[]): Promise<ScrapeResult[]> {
  const hb = getClient();

  if (MAX_CONCURRENCY === 1) {
    // Sequential â€” guaranteed safe on the free plan.
    const results: ScrapeResult[] = [];
    for (const url of urls) {
      try {
        const r = await scrapeOne(hb, url);
        if (r.markdown.length >= 100) results.push(r);
      } catch (err) {
        if (err instanceof ConcurrencyPlanError) throw err;
        console.warn(`[hyperbrowser] Failed to scrape ${url}:`, err);
      }
    }
    return results;
  }

  // Parallel with a concurrency cap (paid plans).
  const queue = [...urls];
  const results: ScrapeResult[] = [];
  const errors: unknown[] = [];

  async function worker() {
    while (queue.length > 0) {
      const url = queue.shift();
      if (!url) break;
      try {
        const r = await scrapeOne(hb, url);
        if (r.markdown.length >= 100) results.push(r);
      } catch (err) {
        if (err instanceof ConcurrencyPlanError) throw err;
        errors.push(err);
        console.warn(`[hyperbrowser] Failed to scrape ${url}:`, err);
      }
    }
  }

  await Promise.all(
    Array.from({ length: MAX_CONCURRENCY }, () => worker())
  );

  if (errors.length > 0 && results.length === 0) {
    console.error("[hyperbrowser] All scrapes failed:", errors);
  }

  return results;
}


--- hypergraph/lib/serper.ts ---
export async function searchDocs(topic: string): Promise<string[]> {
  const apiKey = process.env.SERPER_API_KEY;
  if (!apiKey) throw new Error("SERPER_API_KEY is not set");

  const res = await fetch("https://google.serper.dev/search", {
    method: "POST",
    headers: {
      "X-API-KEY": apiKey,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      q: `${topic} knowledge framework principles theory guide`,
      num: 20,
    }),
  });

  if (!res.ok) {
    throw new Error(`Serper search failed: ${res.status}`);
  }

  const data = await res.json();
  const blocked = [
    "youtube.com",
    "twitter.com",
    "x.com",
    "reddit.com",
    ".pdf",
  ];

  const urls: string[] = (data.organic ?? [])
    .map((r: { link: string }) => r.link)
    .filter(
      (url: string) => !blocked.some((b) => url.toLowerCase().includes(b))
    )
    .slice(0, 8);

  return urls;
}


--- hyperpages/README.md ---
**Built with [Hyperbrowser](https://hyperbrowser.ai)**

# HyperPages

A production-ready AI-powered research page builder. Create beautiful, long-form content pages with automatic research (Hyperbrowser), AI content generation (GPT-5-nano), and image integration (Unsplash + Google Gemini).

## Features

- **AI-Powered Content Generation**: GPT-5-nano generates comprehensive, well-structured content
- **Smart Web Research**: Hyperbrowser scrapes and analyzes topics for accurate content
- **Unsplash Integration**: Beautiful stock photos from Unsplash
- **Real-Time Streaming**: Watch content generate word-by-word with smooth animations
- **Local Storage**: All pages saved to browser localStorage (perfect for open-source)
- **Beautiful Typography**: Premium Manrope + Playfair Display fonts with -2% letter spacing
- **Smart TOC**: Auto-generated table of contents with smooth scrolling
- **Responsive Design**: Looks great on all devices

## Get Started

### Prerequisites

- Node.js 18+ installed
- Get an API key from [Hyperbrowser](https://hyperbrowser.ai)
- OpenAI API key for GPT-5-nano
- Unsplash API key for stock photos

### Installation

```bash
npm install
```

### Environment Setup

1. Copy `.env.local.example` to `.env.local`
2. Fill in your API keys:

```env
HYPERBROWSER_API_KEY=your_key
OPENAI_API_KEY=your_key
UNSPLASH_ACCESS_KEY=your_unsplash_key
```

### Storage

Pages are automatically saved to browser localStorage. No database setup required! Perfect for open-source projects.

### Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to start creating pages.

## Tech Stack

- **Next.js 16** - React framework with App Router
- **TypeScript** - Type safety
- **Tailwind CSS** - Styling
- **Framer Motion** - Smooth animations
- **Lucide React** - Consistent icons
- **localStorage** - Browser storage (no database needed)
- **OpenAI GPT-5-nano** - Fast, cost-effective content generation
- **Hyperbrowser SDK** - Web research and scraping
- **Unsplash** - Stock photo library

## Project Structure

```
/app
  /page.tsx              # Homepage with topic input
  /editor/page.tsx       # Main editor interface
  /p/[slug]/page.tsx     # Published page view
  /api
    /generate/route.ts   # Content generation endpoint
    /pages/route.ts      # CRUD for pages
    /pages/[slug]/route.ts
    /images
      /unsplash/route.ts # Unsplash search
/components
  /sidebar.tsx           # Navigation with saved pages
  /navbar.tsx            # Top nav with Launch Hyperbrowser button
  /logo.tsx              # HyperPages logo
  /button.tsx            # Reusable button component
  /share-modal.tsx       # Share & export modal
  /streaming-text.tsx    # Text streaming animation
/lib
  /storage.ts            # localStorage utilities
  /openai.ts             # OpenAI GPT-5-nano client
  /hyperbrowser.ts       # Hyperbrowser SDK
  /unsplash.ts           # Unsplash API helper
```

## Usage

### Creating a Page

1. Enter a topic on the homepage
2. Select your target audience
3. Watch as AI researches and generates content
4. Add images to sections (Unsplash or AI-generated)
5. Edit and refine the content
6. Page auto-saves to localStorage

### Accessing Pages

- View all your pages in the sidebar under "Your Pages"
- Click any page to view the published version
- Pages are stored in your browser's localStorage
- Share pages with the unique URL: `pages.hyperbrowser.ai/p/[slug]`

### Image Integration

Add beautiful stock photos from Unsplash to your sections with smart search.

## Growth Use Case

HyperPages demonstrates the power of [Hyperbrowser](https://hyperbrowser.ai) for:

- Automated content research and curation
- Fast, AI-powered page generation
- Beautiful, shareable content pages
- Perfect for content marketers, researchers, educators, and creators

## API Architecture

```
User Input â†’ /api/generate
  â†“
Hyperbrowser (research) + GPT-5-nano (content)
  â†“
Stream to Editor â†’ Save to localStorage
  â†“
Published at /p/[slug]
```

## Follow for Updates

Follow [@hyperbrowser](https://x.com/hyperbrowser) for updates.

---

Built with â¤ï¸ using [Hyperbrowser](https://hyperbrowser.ai)


## Links discovered
- [Hyperbrowser](https://hyperbrowser.ai)
- [http://localhost:3000](http://localhost:3000)
- [@hyperbrowser](https://x.com/hyperbrowser)

--- hyperpages/next.config.ts ---
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;


--- hyperpages/SETUP.md ---
# HyperPages Setup Guide

Follow these steps to get HyperPages running in production.

## 1. Install Dependencies

```bash
npm install
```

## 2. Get API Keys

### Hyperbrowser
1. Visit [https://hyperbrowser.ai](https://hyperbrowser.ai)
2. Sign up for an account
3. Get your API key from the dashboard

### OpenAI (GPT-5-nano)
1. Visit [https://platform.openai.com](https://platform.openai.com)
2. Create an account
3. Generate an API key
4. Make sure you have access to GPT-5-nano model

### Unsplash
1. Visit [https://unsplash.com/developers](https://unsplash.com/developers)
2. Create a new application
3. Get your Access Key

## 3. Configure Environment

Create `.env.local` file:

```env
HYPERBROWSER_API_KEY=your_hyperbrowser_key
OPENAI_API_KEY=your_openai_key
UNSPLASH_ACCESS_KEY=your_unsplash_key
```

**Note**: Pages are stored in browser localStorage. No database setup required!

## 4. Run Development Server

```bash
npm run dev
```

Visit `http://localhost:3000` to test the application.

## 5. Test the Features

### Create a Page
1. Enter a topic (e.g., "Machine Learning Basics")
2. Select audience
3. Watch content generate in real-time
4. Add images to sections

### View Pages
1. Check sidebar for saved pages
2. Click to view published version
3. Test share functionality

## 6. Deploy to Production

### Vercel (Recommended)

1. Push your code to GitHub
2. Import project in Vercel
3. Add environment variables in Vercel dashboard
4. Deploy!

### Environment Variables in Vercel
Go to Project Settings â†’ Environment Variables and add all keys from your `.env.local`.

## 7. Troubleshooting

### Content not generating
- Check OpenAI API key is valid
- Verify Hyperbrowser key has credits
- Check browser console for errors

### Images not loading
- Verify Unsplash API key is active
- Check for API rate limits

### Pages not saving
- Check browser console for localStorage errors
- Verify localStorage is not full (quota exceeded)
- Make sure browser allows localStorage

### Build errors
- Run `npm install` again
- Check Node.js version (18+)
- Clear `.next` folder and rebuild

## Production Checklist

- [ ] All API keys configured
- [ ] Environment variables set in production
- [ ] Test page creation end-to-end
- [ ] Test image upload/generation
- [ ] Test localStorage saving/loading
- [ ] Check mobile responsiveness
- [ ] Monitor API usage and costs

## Need Help?

- Check [Hyperbrowser Docs](https://docs.hyperbrowser.ai)
- Review [Next.js Documentation](https://nextjs.org/docs)

Built with [Hyperbrowser](https://hyperbrowser.ai)



## Links discovered
- [https://hyperbrowser.ai](https://hyperbrowser.ai)
- [https://platform.openai.com](https://platform.openai.com)
- [https://unsplash.com/developers](https://unsplash.com/developers)
- [Hyperbrowser Docs](https://docs.hyperbrowser.ai)
- [Next.js Documentation](https://nextjs.org/docs)
- [Hyperbrowser](https://hyperbrowser.ai)

--- hyperpages/lib/hyperbrowser.ts ---
import { Hyperbrowser } from '@hyperbrowser/sdk';

export const hyperbrowser = new Hyperbrowser({
  apiKey: process.env.HYPERBROWSER_API_KEY!,
});

export type Source = {
  title: string;
  url: string;
  snippet: string;
};

export const fetchSources = async (topic: string): Promise<Source[]> => {
  try {
    console.log(`Scraping real sources for: ${topic}`);
    
    const sources: Source[] = [];
    
    // Prepare URLs to scrape
    const topicForWiki = topic.replace(/\s+/g, '_');
    const urlsToScrape = [
      {
        url: `https://en.wikipedia.org/wiki/${topicForWiki}`,
        name: 'Wikipedia',
      },
      {
        url: `https://www.britannica.com/search?query=${encodeURIComponent(topic)}`,
        name: 'Britannica',
      },
      {
        url: `https://simple.wikipedia.org/wiki/${topicForWiki}`,
        name: 'Simple Wikipedia',
      },
    ];
    
    // Scrape each source with Hyperbrowser
    const scrapePromises = urlsToScrape.map(async ({ url, name }) => {
      try {
        console.log(`Scraping ${name}...`);
        const result = await hyperbrowser.scrape.startAndWait({
          url,
        });
        
        const markdown = result.data?.markdown || '';
        
        // Extract a meaningful snippet from the content
        let snippet = '';
        if (markdown) {
          // Get first paragraph that's not too short
          const paragraphs = markdown.split('\n\n').filter(p => p.length > 50);
          snippet = paragraphs[0]?.slice(0, 200) || '';
        }
        
        // Clean up the snippet
        snippet = snippet
          .replace(/\[edit\]/g, '')
          .replace(/\[\d+\]/g, '')
          .trim();
        
        console.log(`âœ“ Scraped ${name}: ${snippet.length} chars`);
        
        if (snippet.length > 30) {
          return {
            title: name,
            url,
            snippet: snippet + '...',
          };
        }
        
        return null;
      } catch (error) {
        console.log(`âœ— Failed to scrape ${name}:`, error);
        return null;
      }
    });
    
    // Wait for all scrapes to complete
    const results = await Promise.all(scrapePromises);
    
    // Filter out failed scrapes and add to sources
    for (const result of results) {
      if (result && sources.length < 3) {
        sources.push(result);
      }
    }
    
    console.log(`Successfully scraped ${sources.length} sources`);
    
    // If we got at least one source, return them
    if (sources.length > 0) {
      return sources;
    }
    
    // Fallback if all scrapes failed
    console.log('All scrapes failed - returning fallback sources');
    return generateFallbackSources(topic);
    
  } catch (error) {
    console.error('Hyperbrowser source fetching error:', error);
    return generateFallbackSources(topic);
  }
};

// Generate fallback sources when scraping fails
function generateFallbackSources(topic: string): Source[] {
  const topicSlug = topic.toLowerCase().replace(/\s+/g, '-');
  
  return [
    {
      title: 'Wikipedia',
      url: `https://en.wikipedia.org/wiki/${topicSlug}`,
      snippet: `Encyclopedia article about ${topic}`,
    },
    {
      title: 'Research Papers',
      url: `https://scholar.google.com/scholar?q=${encodeURIComponent(topic)}`,
      snippet: `Academic research on ${topic}`,
    },
    {
      title: 'Latest News',
      url: `https://news.google.com/search?q=${encodeURIComponent(topic)}`,
      snippet: `Recent news about ${topic}`,
    },
  ];
}



--- hyperpages/lib/image-query.ts ---
import { openai, MODEL } from './openai';

/**
 * Generate a meaningful Unsplash search query from a topic and section title
 * Removes dates, numbers, and converts concepts to visual search terms
 */
export const generateImageQuery = async (topic: string, sectionTitle: string): Promise<string> => {
  // Fast fallback: use simple text processing instead of AI
  // This is much faster and more reliable
  
  // Combine topic and section title, clean it up
  const combined = `${topic} ${sectionTitle}`;
  
  // Remove common words and clean up
  let query = combined
    .replace(/\d{4}/g, '') // Remove years
    .replace(/in\s+\d+/gi, '') // Remove "in 2026"
    .replace(/\b(the|a|an|is|are|was|were|be|been|being|have|has|had|do|does|did|will|would|should|could|may|might|can|must|hero|image)\b/gi, '') // Remove common words
    .replace(/\s+/g, ' ') // Multiple spaces to single
    .trim()
    .split(' ')
    .filter(word => word.length > 2) // Remove very short words
    .slice(0, 2) // Take first 2 meaningful words
    .join(' ');
  
  // If empty, use topic
  if (!query || query.length < 3) {
    query = topic.replace(/\d{4}/g, '').trim();
  }
  
  console.log(`ðŸŽ¨ Image query for "${sectionTitle}": "${query}"`);
  return query;
};



--- hyperpages/lib/openai.ts ---
import OpenAI from 'openai';

// Validate API key exists
if (!process.env.OPENAI_API_KEY) {
  console.error('âš ï¸  OPENAI_API_KEY is not set in environment variables');
}

export const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  timeout: 30000, // 30 second timeout
  maxRetries: 2,
});

// Using GPT-4o-mini - fast, cost-effective, and reliable
export const MODEL = 'gpt-4o-mini';

export const generateOutline = async (topic: string, audience: string, research: string) => {
  const prompt = `Create an outline for a comprehensive page about "${topic}" for ${audience} audience.

Generate exactly 6 section titles that would make sense for this topic. Return ONLY a JSON array of strings, no other text.

Example format: ["Introduction", "Key Concepts", "Applications", "Best Practices", "Common Challenges", "Conclusion"]`;

  try {
    const response = await openai.chat.completions.create({
      model: MODEL,
      messages: [{ role: 'user', content: prompt }],
    });

    const content = response.choices[0].message.content || '[]';
    return JSON.parse(content) as string[];
  } catch (error) {
    console.error('Error generating outline:', error);
    // Return fallback outline
    return [
      "Introduction",
      "Key Features",
      "How It Works",
      "Benefits",
      "Use Cases",
      "Conclusion"
    ];
  }
};

export const generateSectionContent = async (
  topic: string,
  sectionTitle: string,
  audience: string,
  research: string
) => {
  const prompt = `Write detailed, engaging content for the "${sectionTitle}" section of a page about "${topic}" for ${audience} audience.

Requirements:
- Write 3-4 substantial paragraphs
- Be informative and comprehensive
- Use clear, accessible language
- Include specific examples where relevant
- No markdown formatting, just plain text paragraphs

Write the content now:`;

  try {
    const stream = await Promise.race([
      openai.chat.completions.create({
        model: MODEL,
        messages: [{ role: 'user', content: prompt }],
        stream: true,
      }),
      new Promise<never>((_, reject) => 
        setTimeout(() => reject(new Error('Content generation timeout')), 60000) // 60 second timeout
      )
    ]);

    return stream;
  } catch (error) {
    console.error('Error generating section content:', error);
    throw error;
  }
};



--- hyperpages/lib/storage.ts ---
export type Page = {
  id: string;
  slug: string;
  title: string;
  topic?: string;
  audience: string;
  sections: Section[];
  hero_image?: string;
  created_at: string;
  updated_at: string;
};

export type Section = {
  id: string;
  title: string;
  content: string;
  image?: string;
};

const STORAGE_KEY = 'hyperpages_pages';

export const getPages = (): Page[] => {
  if (typeof window === 'undefined') return [];
  
  try {
    const stored = localStorage.getItem(STORAGE_KEY);
    return stored ? JSON.parse(stored) : [];
  } catch (error) {
    console.error('Error reading from localStorage:', error);
    return [];
  }
};

export const savePage = (page: Page): void => {
  if (typeof window === 'undefined') return;
  
  try {
    const pages = getPages();
    // Check by id first, then slug
    const existingIndex = pages.findIndex(p => p.id === page.id || p.slug === page.slug);
    
    if (existingIndex >= 0) {
      // Update existing page
      pages[existingIndex] = {
        ...page,
        updated_at: new Date().toISOString(),
      };
    } else {
      pages.push(page);
    }
    
    localStorage.setItem(STORAGE_KEY, JSON.stringify(pages));
  } catch (error) {
    console.error('Error saving to localStorage:', error);
  }
};

export const getPageBySlug = (slug: string): Page | null => {
  const pages = getPages();
  return pages.find(p => p.slug === slug) || null;
};

export const deletePage = (slug: string): void => {
  if (typeof window === 'undefined') return;
  
  try {
    const pages = getPages();
    const filtered = pages.filter(p => p.slug !== slug);
    localStorage.setItem(STORAGE_KEY, JSON.stringify(filtered));
  } catch (error) {
    console.error('Error deleting from localStorage:', error);
  }
};

export const generateSlug = (title: string): string => {
  return title
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, '-')
    .replace(/(^-|-$)/g, '');
};
