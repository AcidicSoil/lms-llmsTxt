<!-- llmstxt:source=dspy -->
# LM Studio Documentation

## Project Purpose
The primary purpose of this documentation is to guide users through the setup, usage, and customization of LM Studio — a local LLM environment — for both casual users and developers. The documentation supports multiple programming languages (Python, TypeScript) and integrates with various APIs (OpenAI, Anthropic) while offering features like model management, prompt templating, speculative decoding, and multi-modal support. It aims to empower users to deploy, manage, and extend local LLMs effectively.

## Key Concepts
- **LM Studio**: The core local LLM runtime environment.
- **Local LLM Deployment**: Deploying models without cloud dependency.
- **Model Management**: Adding, removing, and configuring models.
- **API Compatibility (OpenAI/Anthropic)**: Seamless integration with industry-standard APIs.
- **Prompt Templates**: Predefined prompt structures for consistent outputs.
- **Speculative Decoding**: Optimized text generation performance.
- **Multi-Language Support**: Full support for Python, TypeScript, and more.
- **Headless Server Mode**: Run LM Studio without GUI for server environments.
- **Tokenization & Embeddings**: Underlying mechanics for model input/output handling.
- **Custom Plugins and Tools**: Extend functionality via developer plugins.

## Architecture Overview
The documentation is structured as a hierarchical, modular system organized into logical sections such as “User Interface,” “Developer API,” “Python SDK,” “TypeScript SDK,” and “Command-Line Interface.” Each section contains articles that detail specific functionalities. The architecture supports extensibility through plugins (e.g., tools providers, prompt preprocessors) and features like custom configurations and multi-language code examples. It also includes infrastructure documentation for serving models over networks and managing model lifecycles via CLI or API endpoints. The system uses markdown with custom frontmatter to control section ordering and article placement, making it dynamic and user-friendly.

## Important Directories
- `0_app/`: Core application logic and UI components.
- `1_developer/`: Developer-focused tools and APIs.
- `1_python/`: Python SDK documentation and modules.
- `2_typescript/`: TypeScript SDK documentation and modules.
- `3_cli/`: Command-line interface documentation and scripts.
- `4_integrations/`: Integration guides for OpenAI, Anthropic, etc.
- `assets/`: Static assets like icons, images, or templates.
- `_configuration/`: Configuration files and environment settings.
- `docs/`: Main documentation structure with markdown articles.

## Entry Points
- `3_cli/index.md`: CLI commands for managing models and servers.
- `1_python/index.md`: Python SDK usage guide and examples.
- `2_typescript/index.md`: TypeScript SDK usage guide and examples.
- `README.md`: Project overview and installation instructions.
- `CONTRIBUTING.md`: Guidelines for contributing to the project.

## Development Info
No explicit package.json, pyproject.toml, or requirements.txt files are visible in the provided file tree. However, the presence of TypeScript and Python documentation directories suggests this is a multi-language SDK with likely npm and pip-based dependencies. The structure implies:
- Development occurs via CLI (`3_cli/`) for local model management and server operations.
- Plugin development is supported via `2_typescript/3_plugins/` and `1_python/` directories.
- API reference documentation is extensive, suggesting a well-documented SDK.
- Integration with platforms like OpenAI and Anthropic indicates compatibility layers.

No explicit build scripts or dev dependencies are visible. The project likely uses standard Python/Node.js tooling (e.g., `npm`, `pip`, `pre-commit`, `lint-staged`) but these are not reflected in the provided tree.

## Usage Examples
```bash
# CLI: Start LM Studio in headless mode with OpenAI API compatibility
lm-studio --headless --api-key YOUR_API_KEY

# Python: Load a model and generate text using prompt templating
from lmstudio import LLMClient
client = LLMClient("path/to/model")
response = client.generate(prompt="Hello, how are you?", temperature=0.7)
print(response)

# TypeScript: Use LM Studio with Anthropic API integration
import { LMStudio } from "@lm-studio/client";
const studio = new LMStudio();
await studio.setModel("anthropic:claude-3");
const result = await studio.generate("Explain quantum computing in simple terms.");
console.log(result);
```
