# llms-full (private-aware)
> Built from GitHub files and website pages. Large files may be truncated.

--- tools/code_reference_indexer.py ---
#!/usr/bin/env python3
"""
Code Reference Indexer MCP Tool - Unified Version

Specialized MCP tool for searching relevant index content in indexes folder
and formatting it for LLM code implementation reference.

Core Features:
1. **UNIFIED TOOL**: Combined search_code_references that handles directory setup, loading, and searching in one call
2. Match relevant reference code based on target file path and functionality requirements
3. Format output of relevant code examples, functions and concepts
4. Provide structured reference information for LLM use

Key Improvement:
- Single tool call that handles all steps internally
- Agent only needs to provide indexes_path and target_file
- No dependency on calling order or global state management
"""

import json
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass
import logging

# Import MCP modules
from mcp.server.fastmcp import FastMCP

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create FastMCP server instance
mcp = FastMCP("code-reference-indexer")


@dataclass
class CodeReference:
    """Code reference information structure"""

    file_path: str
    file_type: str
    main_functions: List[str]
    key_concepts: List[str]
    dependencies: List[str]
    summary: str
    lines_of_code: int
    repo_name: str
    confidence_score: float = 0.0


@dataclass
class RelationshipInfo:
    """Relationship information structure"""

    repo_file_path: str
    target_file_path: str
    relationship_type: str
    confidence_score: float
    helpful_aspects: List[str]
    potential_contributions: List[str]
    usage_suggestions: str


def load_index_files_from_directory(indexes_directory: str) -> Dict[str, Dict]:
    """Load all index files from specified directory"""
    indexes_path = Path(indexes_directory).resolve()

    if not indexes_path.exists():
        logger.warning(f"Indexes directory does not exist: {indexes_path}")
        return {}

    index_cache = {}

    for index_file in indexes_path.glob("*.json"):
        try:
            with open(index_file, "r", encoding="utf-8") as f:
                index_data = json.load(f)
                index_cache[index_file.stem] = index_data
                logger.info(f"Loaded index file: {index_file.name}")
        except Exception as e:
            logger.error(f"Failed to load index file {index_file.name}: {e}")

    logger.info(f"Loaded {len(index_cache)} index files from {indexes_path}")
    return index_cache


def extract_code_references(index_data: Dict) -> List[CodeReference]:
    """Extract code reference information from index data"""
    references = []

    repo_name = index_data.get("repo_name", "Unknown")
    file_summaries = index_data.get("file_summaries", [])

    for file_summary in file_summaries:
        reference = CodeReference(
            file_path=file_summary.get("file_path", ""),
            file_type=file_summary.get("file_type", ""),
            main_functions=file_summary.get("main_functions", []),
            key_concepts=file_summary.get("key_concepts", []),
            dependencies=file_summary.get("dependencies", []),
            summary=file_summary.get("summary", ""),
            lines_of_code=file_summary.get("lines_of_code", 0),
            repo_name=repo_name,
        )
        references.append(reference)

    return references


def extract_relationships(index_data: Dict) -> List[RelationshipInfo]:
    """Extract relationship information from index data"""
    relationships = []

    relationship_list = index_data.get("relationships", [])

    for rel in relationship_list:
        relationship = RelationshipInfo(
            repo_file_path=rel.get("repo_file_path", ""),
            target_file_path=rel.get("target_file_path", ""),
            relationship_type=rel.get("relationship_type", ""),
            confidence_score=rel.get("confidence_score", 0.0),
            helpful_aspects=rel.get("helpful_aspects", []),
            potential_contributions=rel.get("potential_contributions", []),
            usage_suggestions=rel.get("usage_suggestions", ""),
        )
        relationships.append(relationship)

    return relationships


def calculate_relevance_score(
    target_file: str, reference: CodeReference, keywords: List[str] = None
) -> float:
    """Calculate relevance score between reference code and target file"""
    score = 0.0

    # File name similarity
    target_name = Path(target_file).stem.lower()
    ref_name = Path(reference.file_path).stem.lower()

    if target_name in ref_name or ref_name in target_name:
        score += 0.3

    # File type matching
    target_extension = Path(target_file).suffix
    ref_extension = Path(reference.file_path).suffix

    if target_extension == ref_extension:
        score += 0.2

    # Keyword matching
    if keywords:
        keyword_matches = 0
        total_searchable_text = (
            " ".join(reference.key_concepts)
            + " "
            + " ".join(reference.main_functions)
            + " "
            + reference.summary
            + " "
            + reference.file_type
        ).lower()

        for keyword in keywords:
            if keyword.lower() in total_searchable_text:
                keyword_matches += 1

        if keywords:
            score += (keyword_matches / len(keywords)) * 0.5

    return min(score, 1.0)


def find_relevant_references_in_cache(
    target_file: str,
    index_cache: Dict[str, Dict],
    keywords: List[str] = None,
    max_results: int = 10,
) -> List[Tuple[CodeReference, float]]:
    """Find reference code relevant to target file from provided cache"""
    all_references = []

    # Collect reference information from all index files
    for repo_name, index_data in index_cache.items():
        references = extract_code_references(index_data)
        for ref in references:
            relevance_score = calculate_relevance_score(target_file, ref, keywords)
            if relevance_score > 0.1:  # Only keep results with certain relevance
                all_references.append((ref, relevance_score))

    # Sort by relevance score
    all_references.sort(key=lambda x: x[1], reverse=True)

    return all_references[:max_results]


def find_direct_relationships_in_cache(
    target_file: str, index_cache: Dict[str, Dict]
) -> List[RelationshipInfo]:
    """Find direct relationships with target file from provided cache"""
    relationships = []

    # Normalize target file path (remove common prefixes if exists)
    common_prefixes = ["src/", "core/", "lib/", "main/", "./"]
    normalized_target = target_file.strip("/")
    for prefix in common_prefixes:
        if normalized_target.startswith(prefix):
            normalized_target = normalized_target[len(prefix) :]
            break

    # Collect relationship information from all index files
    for repo_name, index_data in index_cache.items():
        repo_relationships = extract_relationships(index_data)
        for rel in repo_relationships:
            # Normalize target file path in relationship
            normalized_rel_target = rel.target_file_path.strip("/")
            for prefix in common_prefixes:
                if normalized_rel_target.startswith(prefix):
                    normalized_rel_target = normalized_rel_target[len(prefix) :]
                    break

            # Check target file path matching (support multiple matching methods)
            if (
                normalized_target == normalized_rel_target
                or normalized_target in normalized_rel_target
                or normalized_rel_target in normalized_target
                or target_file in rel.target_file_path
                or rel.target_file_path in target_file
            ):
                relationships.append(rel)

    # Sort by confidence score
    relationships.sort(key=lambda x: x.confidence_score, reverse=True)

    return relationships


def format_reference_output(
    target_file: str,
    relevant_refs: List[Tuple[CodeReference, float]],
    relationships: List[RelationshipInfo],
) -> str:
    """Format reference information output"""
    output_lines = []

    output_lines.append(f"# Code Reference Information - {target_file}")
    output_lines.append("=" * 80)
    output_lines.append("")

    # Direct relationship information
    if relationships:
        output_lines.append("## üéØ Direct Relationships")
        output_lines.append("")

        for i, rel in enumerate(relationships[:5], 1):
            output_lines.append(f"### {i}. {rel.repo_file_path}")
            output_lines.append(f"**Relationship Type**: {rel.relationship_type}")
            output_lines.append(f"**Confidence Score**: {rel.confidence_score:.2f}")
            output_lines.append(
                f"**Helpful Aspects**: {', '.join(rel.helpful_aspects)}"
            )
            output_lines.append(
                f"**Potential Contributions**: {', '.join(rel.potential_contributions)}"
            )
            output_lines.append(f"**Usage Suggestions**: {rel.usage_suggestions}")
            output_lines.append("")

    # Relevant code references
    if relevant_refs:
        output_lines.append("## üìö Relevant Code References")
        output_lines.append("")

        for i, (ref, score) in enumerate(relevant_refs[:8], 1):
            output_lines.append(f"### {i}. {ref.file_path} (Relevance: {score:.2f})")
            output_lines.append(f"**Repository**: {ref.repo_name}")
            output_lines.append(f"**File Type**: {ref.file_type}")
            output_lines.append(
                f"**Main Functions**: {', '.join(ref.main_functions[:5])}"
            )
            output_lines.append(f"**Key Concepts**: {', '.join(ref.key_concepts[:8])}")
            output_lines.append(f"**Dependencies**: {', '.join(ref.dependencies[:6])}")
            output_lines.append(f"**Lines of Code**: {ref.lines_of_code}")
            output_lines.append(f"**Summary**: {ref.summary[:300]}...")
            output_lines.append("")

    # Implementation suggestions
    output_lines.append("## üí° Implementation Suggestions")
    output_lines.append("")

    if relevant_refs:
        # Collect all function names and concepts
        all_functions = set()
        all_concepts = set()
        all_dependencies = set()

        for ref, _ in relevant_refs[:5]:
            all_functions.update(ref.main_functions)
            all_concepts.update(ref.key_concepts)
            all_dependencies.update(ref.dependencies)

        output_lines.append("**Reference Function Name Patterns**:")
        for func in sorted(list(all_functions))[:10]:
            output_lines.append(f"- {func}")
        output_lines.append("")

        output_lines.append("**Important Concepts and Patterns**:")
        for concept in sorted(list(all_concepts))[:15]:
            output_lines.append(f"- {concept}")
        output_lines.append("")

        output_lines.append("**Potential Dependencies Needed**:")
        for dep in sorted(list(all_dependencies))[:10]:
            output_lines.append(f"- {dep}")
        output_lines.append("")

    output_lines.append("## üöÄ Next Actions")
    output_lines.append(
        "1. Analyze design patterns and architectural styles from the above reference code"
    )
    output_lines.append("2. Determine core functionalities and interfaces to implement")
    output_lines.append("3. Choose appropriate dependency libraries and tools")
    output_lines.append(
        "4. Design implementation solution consistent with existing code style"
    )
    output_lines.append("5. Start writing specific code implementation")

    return "\n".join(output_lines)


# ==================== MCP Tool Definitions ====================


@mcp.tool()
async def search_code_references(
    indexes_path: str, target_file: str, keywords: str = "", max_results: int = 10
) -> str:
    """
    **UNIFIED TOOL**: Search relevant reference code from index files for target file implementation.
    This tool combines directory setup, index loading, and searching in a single call.

    Args:
        indexes_path: Path to the indexes directory containing JSON index files
        target_file: Target file path (file to be implemented)
        keywords: Search keywords, comma-separated
        max_results: Maximum number of results to return

    Returns:
        Formatted reference code information JSON string
    """
    try:
        # Step 1: Load index files from specified directory
        logger.info(f"Loading index files from: {indexes_path}")
        index_cache = load_index_files_from_directory(indexes_path)

        if not index_cache:
            result = {
                "status": "error",
                "message": f"No index files found or failed to load from: {indexes_path}",
                "target_file": target_file,
                "indexes_path": indexes_path,
            }
            return json.dumps(result, ensure_ascii=False, indent=2)

        # Step 2: Parse keywords
        keyword_list = (
            [kw.strip() for kw in keywords.split(",") if kw.strip()] if keywords else []
        )

        # Step 3: Find relevant reference code
        relevant_refs = find_relevant_references_in_cache(
            target_file, index_cache, keyword_list, max_results
        )

        # Step 4: Find direct relationships
        relationships = find_direct_relationships_in_cache(target_file, index_cache)

        # Step 5: Format output
        formatted_output = format_reference_output(
            target_file, relevant_refs, relationships
        )

        result = {
            "status": "success",
            "target_file": target_file,
            "indexes_path": indexes_path,
            "keywords_used": keyword_list,
            "total_references_found": len(relevant_refs),
            "total_relationships_found": len(relationships),
            "formatted_content": formatted_output,
            "indexes_loaded": list(index_cache.keys()),
            "total_indexes_loaded": len(index_cache),
        }

        logger.info(
            f"Successfully found {len(relevant_refs)} references and {len(relationships)} relationships for {target_file}"
        )
        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error(f"Error in search_code_references: {str(e)}")
        result = {
            "status": "error",
            "message": f"Failed to search reference code: {str(e)}",
            "target_file": target_file,
            "indexes_path": indexes_path,
        }
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def get_indexes_overview(indexes_path: str) -> str:
    """
    Get overview of all available reference code index information from specified directory

    Args:
        indexes_path: Path to the indexes directory containing JSON index files

    Returns:
        Overview information of all available reference code JSON string
    """
    try:
        # Load index files from specified directory
        index_cache = load_index_files_from_directory(indexes_path)

        if not index_cache:
            result = {
                "status": "error",
                "message": f"No index files found in: {indexes_path}",
                "indexes_path": indexes_path,
            }
            return json.dumps(result, ensure_ascii=False, indent=2)

        overview = {"total_repos": len(index_cache), "repositories": {}}

        for repo_name, index_data in index_cache.items():
            repo_info = {
                "repo_name": index_data.get("repo_name", repo_name),
                "total_files": index_data.get("total_files", 0),
                "file_types": [],
                "main_concepts": [],
                "total_relationships": len(index_data.get("relationships", [])),
            }

            # Collect file types and concepts
            file_summaries = index_data.get("file_summaries", [])
            file_types = set()
            concepts = set()

            for file_summary in file_summaries:
                file_types.add(file_summary.get("file_type", "Unknown"))
                concepts.update(file_summary.get("key_concepts", []))

            repo_info["file_types"] = sorted(list(file_types))
            repo_info["main_concepts"] = sorted(list(concepts))[
                :20
            ]  # Limit concept count

            overview["repositories"][repo_name] = repo_info

        result = {
            "status": "success",
            "overview": overview,
            "indexes_directory": str(Path(indexes_path).resolve()),
            "total_indexes_loaded": len(index_cache),
        }

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to get indexes overview: {str(e)}",
            "indexes_path": indexes_path,
        }
        return json.dumps(result, ensure_ascii=False, indent=2)


def main():
    """Main function"""
    logger.info("Starting unified Code Reference Indexer MCP server")
    logger.info("Available tools:")
    logger.info(
        "1. search_code_references(indexes_path, target_file, keywords, max_results) - UNIFIED TOOL"
    )
    logger.info(
        "2. get_indexes_overview(indexes_path) - Get overview of available indexes"
    )

    # Run MCP server
    mcp.run()


if __name__ == "__main__":
    main()


--- new_ui/backend/api/__init__.py ---
"""API package"""


--- new_ui/frontend/src/services/api.ts ---
import axios from 'axios';
import type {
  TaskResponse,
  WorkflowStatusResponse,
  QuestionsResponse,
  RequirementsSummaryResponse,
  ConfigResponse,
  SettingsResponse,
  FileUploadResponse,
} from '../types/api';

const api = axios.create({
  baseURL: '/api/v1',
  timeout: 30000,
  headers: {
    'Content-Type': 'application/json',
  },
});

// Workflows API
export const workflowsApi = {
  startPaperToCode: async (
    inputSource: string,
    inputType: 'file' | 'url',
    enableIndexing: boolean = false
  ): Promise<TaskResponse> => {
    const response = await api.post<TaskResponse>('/workflows/paper-to-code', {
      input_source: inputSource,
      input_type: inputType,
      enable_indexing: enableIndexing,
    });
    return response.data;
  },

  startChatPlanning: async (
    requirements: string,
    enableIndexing: boolean = false
  ): Promise<TaskResponse> => {
    const response = await api.post<TaskResponse>('/workflows/chat-planning', {
      requirements,
      enable_indexing: enableIndexing,
    });
    return response.data;
  },

  getStatus: async (taskId: string): Promise<WorkflowStatusResponse> => {
    const response = await api.get<WorkflowStatusResponse>(
      `/workflows/status/${taskId}`
    );
    return response.data;
  },

  cancel: async (taskId: string): Promise<void> => {
    await api.post(`/workflows/cancel/${taskId}`);
  },

  getActiveTasks: async (): Promise<{ tasks: Array<{
    task_id: string;
    status: string;
    progress: number;
    message: string;
    started_at: string | null;
  }> }> => {
    const response = await api.get('/workflows/active');
    return response.data;
  },

  getRecentTasks: async (limit: number = 10): Promise<{ tasks: Array<{
    task_id: string;
    status: string;
    progress: number;
    message: string;
    result: Record<string, unknown> | null;
    error: string | null;
    started_at: string | null;
    completed_at: string | null;
  }> }> => {
    const response = await api.get(`/workflows/recent?limit=${limit}`);
    return response.data;
  },

  // User-in-Loop interaction APIs
  respondToInteraction: async (
    taskId: string,
    action: string,
    data: Record<string, unknown> = {},
    skipped: boolean = false
  ): Promise<{ status: string; task_id: string; action: string }> => {
    const response = await api.post(`/workflows/respond/${taskId}`, {
      action,
      data,
      skipped,
    });
    return response.data;
  },

  getInteraction: async (taskId: string): Promise<{
    has_interaction: boolean;
    task_id: string;
    status: string;
    interaction?: {
      type: string;
      title: string;
      description: string;
      data: Record<string, unknown>;
      options: Record<string, string>;
      required: boolean;
    };
  }> => {
    const response = await api.get(`/workflows/interaction/${taskId}`);
    return response.data;
  },
};

// Requirements API
export const requirementsApi = {
  generateQuestions: async (
    initialRequirement: string
  ): Promise<QuestionsResponse> => {
    const response = await api.post<QuestionsResponse>('/requirements/questions', {
      initial_requirement: initialRequirement,
    });
    return response.data;
  },

  summarize: async (
    initialRequirement: string,
    userAnswers: Record<string, string>
  ): Promise<RequirementsSummaryResponse> => {
    const response = await api.post<RequirementsSummaryResponse>(
      '/requirements/summarize',
      {
        initial_requirement: initialRequirement,
        user_answers: userAnswers,
      }
    );
    return response.data;
  },

  modify: async (
    currentRequirements: string,
    modificationFeedback: string
  ): Promise<RequirementsSummaryResponse> => {
    const response = await api.put<RequirementsSummaryResponse>(
      '/requirements/modify',
      {
        current_requirements: currentRequirements,
        modification_feedback: modificationFeedback,
      }
    );
    return response.data;
  },
};

// Config API
export const configApi = {
  getSettings: async (): Promise<SettingsResponse> => {
    const response = await api.get<SettingsResponse>('/config/settings');
    return response.data;
  },

  getLLMProviders: async (): Promise<ConfigResponse> => {
    const response = await api.get<ConfigResponse>('/config/llm-providers');
    return response.data;
  },

  setLLMProvider: async (provider: string): Promise<void> => {
    await api.put('/config/llm-provider', { provider });
  },
};

// Files API
export const filesApi = {
  upload: async (file: File): Promise<FileUploadResponse> => {
    const formData = new FormData();
    formData.append('file', file);

    const response = await api.post<FileUploadResponse>('/files/upload', formData, {
      headers: {
        'Content-Type': 'multipart/form-data',
      },
    });
    return response.data;
  },

  delete: async (fileId: string): Promise<void> => {
    await api.delete(`/files/delete/${fileId}`);
  },

  getInfo: async (fileId: string): Promise<FileUploadResponse> => {
    const response = await api.get<FileUploadResponse>(`/files/info/${fileId}`);
    return response.data;
  },
};

export default api;


--- new_ui/frontend/src/types/api.ts ---
// API types

export interface TaskResponse {
  task_id: string;
  status: string;
  message: string;
  created_at?: string;
}

export interface WorkflowStatusResponse {
  task_id: string;
  status: string;
  progress: number;
  message: string;
  result?: Record<string, unknown>;
  error?: string;
  started_at?: string;
  completed_at?: string;
}

export interface QuestionsResponse {
  questions: Question[];
  status: string;
}

export interface Question {
  id: string;
  question: string;
  category?: string;
  importance?: string;
  hint?: string;
}

export interface RequirementsSummaryResponse {
  summary: string;
  status: string;
}

export interface ConfigResponse {
  llm_provider: string;
  available_providers: string[];
  models: Record<string, string>;
  indexing_enabled: boolean;
}

export interface SettingsResponse {
  llm_provider: string;
  models: Record<string, string>;
  indexing_enabled: boolean;
  document_segmentation: Record<string, unknown>;
}

export interface FileUploadResponse {
  file_id: string;
  filename: string;
  path: string;
  size: number;
}

export interface ErrorResponse {
  error: string;
  detail?: string;
  code?: string;
}

// WebSocket message types
export interface WSProgressMessage {
  type: 'progress' | 'status' | 'heartbeat';
  task_id: string;
  progress?: number;
  message?: string;
  status?: string;
  timestamp: string;
}

export interface WSCompleteMessage {
  type: 'complete';
  task_id: string;
  status: string;
  result: Record<string, unknown>;
  timestamp: string;
}

export interface WSErrorMessage {
  type: 'error';
  task_id: string;
  error: string;
  timestamp: string;
}

export interface WSCodeChunkMessage {
  type: 'code_chunk' | 'file_start' | 'file_end';
  task_id: string;
  content?: string;
  filename?: string;
  timestamp: string;
}

export interface WSLogMessage {
  type: 'log';
  level: 'INFO' | 'WARNING' | 'ERROR' | 'DEBUG';
  message: string;
  namespace: string;
  timestamp: string;
}

// User-in-Loop interaction message
export interface WSInteractionMessage {
  type: 'interaction_required';
  task_id: string;
  interaction_type: 'requirement_questions' | 'plan_review' | 'code_review' | string;
  title: string;
  description: string;
  data: {
    questions?: Question[];
    plan?: string;
    plan_preview?: string;
    original_input?: string;
    [key: string]: unknown;
  };
  options: Record<string, string>;
  required: boolean;
  timestamp: string;
}

export type WSMessage =
  | WSProgressMessage
  | WSCompleteMessage
  | WSErrorMessage
  | WSCodeChunkMessage
  | WSLogMessage
  | WSInteractionMessage;


--- new_ui/backend/api/websockets/code_stream_ws.py ---
"""
Code Stream WebSocket Handler
Provides real-time streaming of generated code
"""

import asyncio
from datetime import datetime
from fastapi import APIRouter, WebSocket, WebSocketDisconnect

from services.workflow_service import workflow_service


router = APIRouter()


@router.websocket("/code-stream/{task_id}")
async def code_stream_websocket(websocket: WebSocket, task_id: str):
    """
    WebSocket endpoint for real-time code streaming.

    Streams generated code as it's being written, similar to ChatGPT.

    Message format:
    {
        "type": "code_chunk" | "file_start" | "file_end" | "complete",
        "task_id": str,
        "content": str,  # Code content for code_chunk
        "filename": str | null,  # For file_start/file_end
        "timestamp": str
    }
    """
    await websocket.accept()

    task = workflow_service.get_task(task_id)
    # Subscribe to get our own queue for this task
    queue = workflow_service.subscribe(task_id)

    if not task:
        await websocket.send_json(
            {
                "type": "error",
                "task_id": task_id,
                "error": "Task not found",
                "timestamp": datetime.utcnow().isoformat(),
            }
        )
        await websocket.close()
        return

    try:
        # Track current file being streamed
        current_file = None

        if queue:
            while True:
                try:
                    message = await asyncio.wait_for(queue.get(), timeout=60.0)

                    # Transform progress messages into code stream format
                    if message.get("type") == "progress":
                        msg_text = message.get("message", "")

                        # Detect file creation events
                        if "Creating file:" in msg_text or "Writing:" in msg_text:
                            filename = msg_text.split(":")[-1].strip()
                            if current_file:
                                await websocket.send_json(
                                    {
                                        "type": "file_end",
                                        "task_id": task_id,
                                        "filename": current_file,
                                        "timestamp": datetime.utcnow().isoformat(),
                                    }
                                )
                            current_file = filename
                            await websocket.send_json(
                                {
                                    "type": "file_start",
                                    "task_id": task_id,
                                    "filename": filename,
                                    "timestamp": datetime.utcnow().isoformat(),
                                }
                            )

                        # Forward progress message
                        await websocket.send_json(
                            {
                                "type": "progress",
                                "task_id": task_id,
                                "progress": message.get("progress", 0),
                                "message": msg_text,
                                "timestamp": datetime.utcnow().isoformat(),
                            }
                        )

                    elif message.get("type") == "code_chunk":
                        # Direct code chunk forwarding
                        await websocket.send_json(
                            {
                                "type": "code_chunk",
                                "task_id": task_id,
                                "content": message.get("content", ""),
                                "filename": message.get("filename"),
                                "timestamp": datetime.utcnow().isoformat(),
                            }
                        )

                    elif message.get("type") in ("complete", "error"):
                        msg_type = message.get("type")
                        print(
                            f"[CodeStreamWS] Workflow finished: task={task_id[:8]}... type={msg_type}"
                        )
                        if current_file:
                            await websocket.send_json(
                                {
                                    "type": "file_end",
                                    "task_id": task_id,
                                    "filename": current_file,
                                    "timestamp": datetime.utcnow().isoformat(),
                                }
                            )
                        await websocket.send_json(message)
                        # Wait a bit before closing to ensure frontend processes the message
                        await asyncio.sleep(0.5)
                        await websocket.close()
                        break

                except asyncio.TimeoutError:
                    await websocket.send_json(
                        {
                            "type": "heartbeat",
                            "task_id": task_id,
                            "timestamp": datetime.utcnow().isoformat(),
                        }
                    )

    except WebSocketDisconnect:
        pass
    finally:
        # Unsubscribe from task updates
        if queue:
            workflow_service.unsubscribe(task_id, queue)


--- new_ui/backend/api/routes/config.py ---
"""
Configuration API Routes
Handles LLM provider and settings management
"""

from fastapi import APIRouter, HTTPException
import yaml

from settings import (
    load_mcp_config,
    load_secrets,
    get_llm_provider,
    get_llm_models,
    is_indexing_enabled,
    CONFIG_PATH,
)
from models.requests import LLMProviderUpdateRequest
from models.responses import ConfigResponse, SettingsResponse


router = APIRouter()


@router.get("/settings", response_model=SettingsResponse)
async def get_settings():
    """Get current application settings"""
    config = load_mcp_config()
    provider = get_llm_provider()
    models = get_llm_models(provider)

    return SettingsResponse(
        llm_provider=provider,
        models=models,
        indexing_enabled=is_indexing_enabled(),
        document_segmentation=config.get("document_segmentation", {}),
    )


@router.get("/llm-providers", response_model=ConfigResponse)
async def get_llm_providers():
    """Get available LLM providers and their configurations"""
    secrets = load_secrets()

    # Get available providers (those with API keys configured)
    available_providers = []
    for provider in ["google", "anthropic", "openai"]:
        if secrets.get(provider, {}).get("api_key"):
            available_providers.append(provider)

    current_provider = get_llm_provider()
    models = get_llm_models(current_provider)

    return ConfigResponse(
        llm_provider=current_provider,
        available_providers=available_providers,
        models=models,
        indexing_enabled=is_indexing_enabled(),
    )


@router.put("/llm-provider")
async def set_llm_provider(request: LLMProviderUpdateRequest):
    """Update the preferred LLM provider"""
    secrets = load_secrets()

    # Verify provider has an API key
    if not secrets.get(request.provider, {}).get("api_key"):
        raise HTTPException(
            status_code=400,
            detail=f"Provider '{request.provider}' does not have an API key configured",
        )

    # Update config file
    try:
        config = load_mcp_config()
        config["llm_provider"] = request.provider

        with open(CONFIG_PATH, "w", encoding="utf-8") as f:
            yaml.dump(config, f, default_flow_style=False)

        return {
            "status": "success",
            "message": f"LLM provider updated to '{request.provider}'",
            "provider": request.provider,
        }

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to update configuration: {str(e)}",
        )


--- new_ui/backend/api/routes/files.py ---
"""
Files API Routes
Handles file upload and download operations
"""

import uuid
import shutil
from pathlib import Path

from fastapi import APIRouter, File, UploadFile, HTTPException
from fastapi.responses import FileResponse

from settings import settings


router = APIRouter()

# In-memory file registry (in production, use a database)
_file_registry: dict = {}


@router.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """Upload a file (PDF, markdown, etc.)"""
    # Validate file type
    allowed_types = {".pdf", ".md", ".txt", ".markdown"}
    file_ext = Path(file.filename).suffix.lower()

    if file_ext not in allowed_types:
        raise HTTPException(
            status_code=400,
            detail=f"File type '{file_ext}' not allowed. Allowed: {', '.join(allowed_types)}",
        )

    # Generate unique file ID
    file_id = str(uuid.uuid4())
    safe_filename = f"{file_id}{file_ext}"
    file_path = Path(settings.upload_dir) / safe_filename

    try:
        # Ensure upload directory exists
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # Save file
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        # Get file size
        file_size = file_path.stat().st_size

        # Check size limit
        if file_size > settings.max_upload_size:
            file_path.unlink()  # Delete oversized file
            raise HTTPException(
                status_code=400,
                detail=f"File size exceeds limit of {settings.max_upload_size // (1024*1024)}MB",
            )

        # Register file
        _file_registry[file_id] = {
            "id": file_id,
            "original_name": file.filename,
            "path": str(file_path),
            "size": file_size,
            "type": file_ext,
        }

        return {
            "file_id": file_id,
            "filename": file.filename,
            "path": str(file_path),
            "size": file_size,
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to upload file: {str(e)}",
        )


@router.get("/download/{file_id}")
async def download_file(file_id: str):
    """Download a file by ID"""
    file_info = _file_registry.get(file_id)

    if not file_info:
        raise HTTPException(status_code=404, detail="File not found")

    file_path = Path(file_info["path"])

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="File no longer exists")

    return FileResponse(
        path=str(file_path),
        filename=file_info["original_name"],
        media_type="application/octet-stream",
    )


@router.delete("/delete/{file_id}")
async def delete_file(file_id: str):
    """Delete an uploaded file"""
    file_info = _file_registry.get(file_id)

    if not file_info:
        raise HTTPException(status_code=404, detail="File not found")

    file_path = Path(file_info["path"])

    try:
        if file_path.exists():
            file_path.unlink()

        del _file_registry[file_id]

        return {"status": "deleted", "file_id": file_id}

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete file: {str(e)}",
        )


@router.get("/info/{file_id}")
async def get_file_info(file_id: str):
    """Get information about an uploaded file"""
    file_info = _file_registry.get(file_id)

    if not file_info:
        raise HTTPException(status_code=404, detail="File not found")

    return file_info


--- new_ui/backend/api/routes/__init__.py ---
"""API Routes"""


--- new_ui/backend/api/websockets/__init__.py ---
"""WebSocket handlers"""


--- new_ui/backend/api/websockets/logs_ws.py ---
"""
Logs WebSocket Handler
Provides real-time log streaming
"""

import asyncio
import json
from datetime import datetime
from fastapi import APIRouter, WebSocket, WebSocketDisconnect

from settings import PROJECT_ROOT


router = APIRouter()


@router.websocket("/logs/{session_id}")
async def logs_websocket(websocket: WebSocket, session_id: str):
    """
    WebSocket endpoint for real-time log streaming.

    Streams log entries from the logs directory.

    Message format:
    {
        "type": "log",
        "level": "INFO" | "WARNING" | "ERROR" | "DEBUG",
        "message": str,
        "namespace": str,
        "timestamp": str
    }
    """
    await websocket.accept()

    logs_dir = PROJECT_ROOT / "logs"
    last_position = 0
    current_log_file = None

    try:
        while True:
            try:
                # Find the most recent log file
                if logs_dir.exists():
                    log_files = sorted(
                        logs_dir.glob("*.jsonl"),
                        key=lambda p: p.stat().st_mtime,
                        reverse=True,
                    )

                    if log_files:
                        newest_log = log_files[0]

                        # Check if we switched to a new log file
                        if current_log_file != newest_log:
                            current_log_file = newest_log
                            last_position = 0

                        # Read new entries
                        try:
                            with open(current_log_file, "r", encoding="utf-8") as f:
                                f.seek(last_position)
                                new_lines = f.readlines()
                                last_position = f.tell()

                            for line in new_lines:
                                line = line.strip()
                                if not line:
                                    continue

                                try:
                                    log_entry = json.loads(line)
                                    await websocket.send_json(
                                        {
                                            "type": "log",
                                            "level": log_entry.get("level", "INFO"),
                                            "message": log_entry.get("message", ""),
                                            "namespace": log_entry.get("namespace", ""),
                                            "timestamp": log_entry.get(
                                                "timestamp",
                                                datetime.utcnow().isoformat(),
                                            ),
                                        }
                                    )
                                except json.JSONDecodeError:
                                    # Raw text log
                                    await websocket.send_json(
                                        {
                                            "type": "log",
                                            "level": "INFO",
                                            "message": line,
                                            "namespace": "",
                                            "timestamp": datetime.utcnow().isoformat(),
                                        }
                                    )

                        except Exception as e:
                            await websocket.send_json(
                                {
                                    "type": "error",
                                    "message": f"Error reading log file: {str(e)}",
                                    "timestamp": datetime.utcnow().isoformat(),
                                }
                            )

                # Wait before checking for more logs
                await asyncio.sleep(0.5)

            except asyncio.CancelledError:
                break

    except WebSocketDisconnect:
        pass


--- nanobot/SECURITY.md ---
# Security Policy

## Reporting a Vulnerability

If you discover a security vulnerability in nanobot, please report it by:

1. **DO NOT** open a public GitHub issue
2. Create a private security advisory on GitHub or contact the repository maintainers
3. Include:
   - Description of the vulnerability
   - Steps to reproduce
   - Potential impact
   - Suggested fix (if any)

We aim to respond to security reports within 48 hours.

## Security Best Practices

### 1. API Key Management

**CRITICAL**: Never commit API keys to version control.

```bash
# ‚úÖ Good: Store in config file with restricted permissions
chmod 600 ~/.nanobot/config.json

# ‚ùå Bad: Hardcoding keys in code or committing them
```

**Recommendations:**
- Store API keys in `~/.nanobot/config.json` with file permissions set to `0600`
- Consider using environment variables for sensitive keys
- Use OS keyring/credential manager for production deployments
- Rotate API keys regularly
- Use separate API keys for development and production

### 2. Channel Access Control

**IMPORTANT**: Always configure `allowFrom` lists for production use.

```json
{
  "channels": {
    "telegram": {
      "enabled": true,
      "token": "YOUR_BOT_TOKEN",
      "allowFrom": ["123456789", "987654321"]
    },
    "whatsapp": {
      "enabled": true,
      "allowFrom": ["+1234567890"]
    }
  }
}
```

**Security Notes:**
- Empty `allowFrom` list will **ALLOW ALL** users (open by default for personal use)
- Get your Telegram user ID from `@userinfobot`
- Use full phone numbers with country code for WhatsApp
- Review access logs regularly for unauthorized access attempts

### 3. Shell Command Execution

The `exec` tool can execute shell commands. While dangerous command patterns are blocked, you should:

- ‚úÖ Review all tool usage in agent logs
- ‚úÖ Understand what commands the agent is running
- ‚úÖ Use a dedicated user account with limited privileges
- ‚úÖ Never run nanobot as root
- ‚ùå Don't disable security checks
- ‚ùå Don't run on systems with sensitive data without careful review

**Blocked patterns:**
- `rm -rf /` - Root filesystem deletion
- Fork bombs
- Filesystem formatting (`mkfs.*`)
- Raw disk writes
- Other destructive operations

### 4. File System Access

File operations have path traversal protection, but:

- ‚úÖ Run nanobot with a dedicated user account
- ‚úÖ Use filesystem permissions to protect sensitive directories
- ‚úÖ Regularly audit file operations in logs
- ‚ùå Don't give unrestricted access to sensitive files

### 5. Network Security

**API Calls:**
- All external API calls use HTTPS by default
- Timeouts are configured to prevent hanging requests
- Consider using a firewall to restrict outbound connections if needed

**WhatsApp Bridge:**
- The bridge runs on `localhost:3001` by default
- If exposing to network, use proper authentication and TLS
- Keep authentication data in `~/.nanobot/whatsapp-auth` secure (mode 0700)

### 6. Dependency Security

**Critical**: Keep dependencies updated!

```bash
# Check for vulnerable dependencies
pip install pip-audit
pip-audit

# Update to latest secure versions
pip install --upgrade nanobot-ai
```

For Node.js dependencies (WhatsApp bridge):
```bash
cd bridge
npm audit
npm audit fix
```

**Important Notes:**
- Keep `litellm` updated to the latest version for security fixes
- We've updated `ws` to `>=8.17.1` to fix DoS vulnerability
- Run `pip-audit` or `npm audit` regularly
- Subscribe to security advisories for nanobot and its dependencies

### 7. Production Deployment

For production use:

1. **Isolate the Environment**
   ```bash
   # Run in a container or VM
   docker run --rm -it python:3.11
   pip install nanobot-ai
   ```

2. **Use a Dedicated User**
   ```bash
   sudo useradd -m -s /bin/bash nanobot
   sudo -u nanobot nanobot gateway
   ```

3. **Set Proper Permissions**
   ```bash
   chmod 700 ~/.nanobot
   chmod 600 ~/.nanobot/config.json
   chmod 700 ~/.nanobot/whatsapp-auth
   ```

4. **Enable Logging**
   ```bash
   # Configure log monitoring
   tail -f ~/.nanobot/logs/nanobot.log
   ```

5. **Use Rate Limiting**
   - Configure rate limits on your API providers
   - Monitor usage for anomalies
   - Set spending limits on LLM APIs

6. **Regular Updates**
   ```bash
   # Check for updates weekly
   pip install --upgrade nanobot-ai
   ```

### 8. Development vs Production

**Development:**
- Use separate API keys
- Test with non-sensitive data
- Enable verbose logging
- Use a test Telegram bot

**Production:**
- Use dedicated API keys with spending limits
- Restrict file system access
- Enable audit logging
- Regular security reviews
- Monitor for unusual activity

### 9. Data Privacy

- **Logs may contain sensitive information** - secure log files appropriately
- **LLM providers see your prompts** - review their privacy policies
- **Chat history is stored locally** - protect the `~/.nanobot` directory
- **API keys are in plain text** - use OS keyring for production

### 10. Incident Response

If you suspect a security breach:

1. **Immediately revoke compromised API keys**
2. **Review logs for unauthorized access**
   ```bash
   grep "Access denied" ~/.nanobot/logs/nanobot.log
   ```
3. **Check for unexpected file modifications**
4. **Rotate all credentials**
5. **Update to latest version**
6. **Report the incident** to maintainers

## Security Features

### Built-in Security Controls

‚úÖ **Input Validation**
- Path traversal protection on file operations
- Dangerous command pattern detection
- Input length limits on HTTP requests

‚úÖ **Authentication**
- Allow-list based access control
- Failed authentication attempt logging
- Open by default (configure allowFrom for production use)

‚úÖ **Resource Protection**
- Command execution timeouts (60s default)
- Output truncation (10KB limit)
- HTTP request timeouts (10-30s)

‚úÖ **Secure Communication**
- HTTPS for all external API calls
- TLS for Telegram API
- WebSocket security for WhatsApp bridge

## Known Limitations

‚ö†Ô∏è **Current Security Limitations:**

1. **No Rate Limiting** - Users can send unlimited messages (add your own if needed)
2. **Plain Text Config** - API keys stored in plain text (use keyring for production)
3. **No Session Management** - No automatic session expiry
4. **Limited Command Filtering** - Only blocks obvious dangerous patterns
5. **No Audit Trail** - Limited security event logging (enhance as needed)

## Security Checklist

Before deploying nanobot:

- [ ] API keys stored securely (not in code)
- [ ] Config file permissions set to 0600
- [ ] `allowFrom` lists configured for all channels
- [ ] Running as non-root user
- [ ] File system permissions properly restricted
- [ ] Dependencies updated to latest secure versions
- [ ] Logs monitored for security events
- [ ] Rate limits configured on API providers
- [ ] Backup and disaster recovery plan in place
- [ ] Security review of custom skills/tools

## Updates

**Last Updated**: 2026-02-03

For the latest security updates and announcements, check:
- GitHub Security Advisories: https://github.com/HKUDS/nanobot/security/advisories
- Release Notes: https://github.com/HKUDS/nanobot/releases

## License

See LICENSE file for details.


--- .github/pull_request_template.md ---
<!--
Thanks for contributing to DeepCode!

Please ensure your pull request is ready for review before submitting.

About this template

This template helps contributors provide a clear and concise description of their changes. Feel free to adjust it as needed.
-->

## Description

[Briefly describe the changes made in this pull request.]

## Related Issues

[Reference any related issues or tasks addressed by this pull request.]

## Changes Made

[List the specific changes made in this pull request.]

## Checklist

- [ ] Changes tested locally
- [ ] Code reviewed
- [ ] Documentation updated (if necessary)
- [ ] Unit tests added (if applicable)

## Additional Notes

[Add any additional notes or context for the reviewer(s).]


--- README.md ---
<div align="center">

<table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;">
<tr>
<td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;">
  <img src="assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;"/>
</td>
<td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;">
  <pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;">    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù</pre>
</td>
</tr>
</table>

<div align="center">
<a href="https://trendshift.io/repositories/14665" target="_blank"><img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>

<!-- <img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&size=28&duration=2000&pause=800&color=06B6D4&background=00000000&center=true&vCenter=true&width=800&height=50&lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/> -->

# <img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;"/> DeepCode: Open Agentic Coding

### *Advancing Code Generation with Multi-Agent Systems*

<!-- <p align="center">
  <img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&logo=rocket&logoColor=white" alt="Version">

  <img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&logo=opensourceinitiative&logoColor=white" alt="License">
  <img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&logo=brain&logoColor=white" alt="AI">
  <img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&logo=university&logoColor=white" alt="HKU">
</p> -->
<p>
  <a href="https://github.com/HKUDS/DeepCode/stargazers"><img src='https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&style=for-the-badge&logo=star&logoColor=white&labelColor=1a1a2e' /></a>
  <a href='https://arxiv.org/abs/2512.07921'><img src="https://img.shields.io/badge/Paper-arXiv-orange?style=for-the-badge&logo=arxiv&logoColor=white&labelColor=1a1a2e"></a>
  <img src="https://img.shields.io/badge/üêçPython-3.13-4ecdc4?style=for-the-badge&logo=python&logoColor=white&labelColor=1a1a2e">
  <!-- <a href="https://pypi.org/project/deepcode-hku/"><img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&logo=pypi&logoColor=white&labelColor=1a1a2e&color=ff6b6b"></a> -->
</p>
<p>
  <a href="https://discord.gg/yF2MmDJyGJ"><img src="https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&logo=discord&logoColor=white&labelColor=1a1a2e"></a>
  <a href="https://github.com/HKUDS/DeepCode/issues/11"><img src="https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&logo=wechat&logoColor=white&labelColor=1a1a2e"></a>
</p>
<div align="center">
  <div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"></div>
</div>

<div align="center">
  <a href="#-quick-start" style="text-decoration: none;">
    <img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&logo=rocket&logoColor=white&labelColor=1a1a2e">
  </a>
</div>

<div align="center" style="margin-top: 10px;">
  <a href="README.md">
    <img src="https://img.shields.io/badge/English-00d4ff?style=for-the-badge&logo=readme&logoColor=white&labelColor=1a1a2e" alt="English">
  </a>
  <a href="README_ZH.md">
    <img src="https://img.shields.io/badge/‰∏≠Êñá-00d4ff?style=for-the-badge&logo=readme&logoColor=white&labelColor=1a1a2e" alt="‰∏≠Êñá">
  </a>
</div>

### üñ•Ô∏è **Interface Showcase**

<table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;">
<tr>
<td width="50%" align="center" style="vertical-align: top; padding: 20px;">

#### üñ•Ô∏è **CLI Interface**
**Terminal-Based Development**

<div align="center">

  <img src="https://github.com/Zongwei9888/Experiment_Images/blob/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;"/>

  <div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;">
    <strong>üöÄ Advanced Terminal Experience</strong><br/>
    <small>‚ö° Fast command-line workflow<br/>üîß Developer-friendly interface<br/>üìä Real-time progress tracking</small>
  </div>

  *Professional terminal interface for advanced users and CI/CD integration*
</div>

</td>
<td width="50%" align="center" style="vertical-align: top; padding: 20px;">

#### üåê **Web Interface**
**Visual Interactive Experience**

<div align="center">

  <img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;"/>

  <div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;">
    <strong>üé® Modern Web Dashboard</strong><br/>
    <small>üñ±Ô∏è Intuitive drag-and-drop<br/>üì± Responsive design<br/>üéØ Visual progress tracking</small>
  </div>

  *Beautiful web interface with streamlined workflow for all skill levels*
</div>

</td>
</tr>
</table>

---

<div align="center">

### üé¨ **Introduction Video**

<div style="margin: 20px 0;">
  <a href="https://youtu.be/PRgmP8pOI08" target="_blank">
    <img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg"
         alt="DeepCode Introduction Video"
         width="75%"
         style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;"/>
  </a>
</div>

*üéØ **Watch our complete introduction** - See how DeepCode transforms research papers and natural language into production-ready code*

<p>
  <a href="https://youtu.be/PRgmP8pOI08" target="_blank">
    <img src="https://img.shields.io/badge/‚ñ∂Ô∏è_Watch_Video-FF0000?style=for-the-badge&logo=youtube&logoColor=white" alt="Watch Video"/>
  </a>
</p>

</div>

---




> *"Where AI Agents Transform Ideas into Production-Ready Code"*

</div>

---

## üìë Table of Contents

- [üì∞ News](#-news)
- [üöÄ Key Features](#-key-features)
- [üèóÔ∏è Architecture](#Ô∏è-architecture)
- [üìä Experimental Results](#-experimental-results)
- [üöÄ Quick Start](#-quick-start)
- [ü§ñ nanobot Integration (Feishu Chatbot)](#-nanobot-integration-feishu-chatbot)
- [üí° Examples](#-examples)
  - [üé¨ Live Demonstrations](#-live-demonstrations)
- [‚≠ê Star History](#-star-history)
- [üìÑ License](#-license)


---

## üì∞ News

üéâ **[2025-02] nanobot ‚úñÔ∏è DeepCode. Just chat naturally with openclaw/nanobot to handle your coding tasks:**

<div align="center">
<table><tr>
<td align="center"><a href="https://github.com/HKUDS/DeepCode"><img src="./assets/logo.png" alt="DeepCode" height="60"/></a></td>
<td align="center"><h2>‚ú¶</h2></td>
<td align="center"><a href="https://github.com/HKUDS/nanobot"><img src="./assets/nanobot.png" alt="nanobot" height="60"/></a></td>
</tr></table>
</div>

- [nanobot](https://github.com/HKUDS/nanobot) nanobot now powers your agentic coding & engineering! ü§ñüíª
- Step away from your laptop ‚Äî make vibe coding even more vibe! Code directly from your phone! üì±‚ú®
- One-command deploy: `./nanobot/run_nanobot.sh` ‚Üí **[Setup Guide ‚Üí](#-nanobot-integration-feishu-chatbot)**

<div align="center">
<table width="100%"><tr>
<td width="50%" align="center">
  <img src="./assets/IMG_8098.jpeg" alt="Feishu Chat Example 1" width="95%" style="border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.2);"/>
</td>
<td width="50%" align="center">
  <img src="./assets/IMG_8099.jpeg" alt="Feishu Chat Example 2" width="95%" style="border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.2);"/>
</td>
</tr></table>
<sub><em>Feishu Bot in Action ‚Äî Natural language ‚Üí Full code generation with setup instructions</em></sub>
</div>

---

üéâ **[2025-02] New Web UI Experience Upgrade!**

- üîÑ **User-in-Loop Interaction**: Support real-time user interaction during workflows - AI asks clarifying questions directly in the chat
- üí¨ **Inline Interaction Design**: Interaction prompts appear naturally within the chat flow for a seamless experience
- üöÄ **One-Click Launch**: Simply run `deepcode` to start the new UI (cross-platform: Windows/macOS/Linux)
- üîß **Improved Process Management**: Enhanced service start/stop mechanism with automatic port cleanup
- üì° **WebSocket Real-time Communication**: Fixed message loss issues, ensuring proper interaction state synchronization

<div align="center">
  <img src="./assets/NewUI.png" alt="DeepCode New UI" width="85%" style="border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.15);" />
  <br/>
  <sub><em>DeepCode New Web UI - Modern React-based Interface</em></sub>
</div>

---

üéâ **[2025-10-28] DeepCode Achieves SOTA on PaperBench!**

DeepCode sets new benchmarks on OpenAI's PaperBench Code-Dev across all categories:

- üèÜ **Surpasses Human Experts**: **75.9%** (DeepCode) vs Top Machine Learning PhDs 72.4% (+3.5%).
- ü•á **Outperforms SOTA Commercial Code Agents**: **84.8%** (DeepCode) vs Leading Commercial Code Agents (+26.1%) (Cursor, Claude Code, and Codex).
- üî¨ **Advances Scientific Coding**: **73.5%** (DeepCode) vs PaperCoder 51.1% (+22.4%).
- üöÄ **Beats LLM Agents**: **73.5%** (DeepCode) vs best LLM frameworks 43.3% (+30.2%).

---

## üöÄ Key Features

<br/>

<table align="center" width="100%" style="border: none; table-layout: fixed;">
<tr>
<td width="30%" align="center" style="vertical-align: top; padding: 20px;">

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<h3 style="margin: 0; padding: 0;">üöÄ <strong>Paper2Code</strong></h3>
</div>

<div align="center" style="margin: 15px 0;">
  <img src="https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&logo=algorithm&logoColor=white" alt="Algorithm Badge" />
</div>

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<p align="center"><strong>Automated Implementation of Complex Algorithms</strong></p>
</div>

<div style="height: 60px; display: flex; align-items: center; justify-content: center;">
<p align="center">Effortlessly converts complex algorithms from research papers into <strong>high-quality</strong>, <strong>production-ready</strong> code, accelerating algorithm reproduction.</p>
</div>



</td>
<td width="30%" align="center" style="vertical-align: top; padding: 20px;">

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<h3 style="margin: 0; padding: 0;">üé® <strong>Text2Web</strong></h3>
</div>

<div align="center" style="margin: 15px 0;">
  <img src="https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&logo=react&logoColor=white" alt="Frontend Badge" />
</div>

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<p align="center"><strong>Automated Front-End Web Development</strong></p>
</div>

<div style="height: 60px; display: flex; align-items: center; justify-content: center;">
<p align="center">Translates plain textual descriptions into <strong>fully functional</strong>, <strong>visually appealing</strong> front-end web code for rapid interface creation.</p>
</div>



</td>
<td width="30%" align="center" style="vertical-align: top; padding: 20px;">

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<h3 style="margin: 0; padding: 0;">‚öôÔ∏è <strong>Text2Backend</strong></h3>
</div>

<div align="center" style="margin: 15px 0;">
  <img src="https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&logo=server&logoColor=white" alt="Backend Badge" />
</div>

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<p align="center"><strong>Automated Back-End Development</strong></p>
</div>

<div style="height: 60px; display: flex; align-items: center; justify-content: center;">
<p align="center">Generates <strong>efficient</strong>, <strong>scalable</strong>, and <strong>feature-rich</strong> back-end code from simple text inputs, streamlining server-side development.</p>
</div>



</td>
</tr>
</table>

<br/>

---

## üìä Experimental Results

<div align="center">
    <img src='./assets/result_main02.jpg' /><br>
</div>
<br/>

We evaluate **DeepCode** on the [*PaperBench*](https://openai.com/index/paperbench/) benchmark (released by OpenAI), a rigorous testbed requiring AI agents to independently reproduce 20 ICML 2024 papers from scratch. The benchmark comprises 8,316 gradable components assessed using SimpleJudge with hierarchical weighting.

Our experiments compare DeepCode against four baseline categories: **(1) Human Experts**, **(2) State-of-the-Art Commercial Code Agents**, **(3) Scientific Code Agents**, and **(4) LLM-Based Agents**.

### ‚ë† üß† Human Expert Performance (Top Machine Learning PhD)

**DeepCode: 75.9% vs. Top Machine Learning PhD: 72.4% (+3.5%)**

DeepCode achieves **75.9%** on the 3-paper human evaluation subset, **surpassing the best-of-3 human expert baseline (72.4%) by +3.5 percentage points**. This demonstrates that our framework not only matches but exceeds expert-level code reproduction capabilities, representing a significant milestone in autonomous scientific software engineering.

### ‚ë° üíº State-of-the-Art Commercial Code Agents

**DeepCode: 84.8% vs. Best Commercial Agent: 58.7% (+26.1%)**

On the 5-paper subset, DeepCode substantially outperforms leading commercial coding tools:
- Cursor: 58.4%
- Claude Code: 58.7%
- Codex: 40.0%
- **DeepCode: 84.8%**

This represents a **+26.1% improvement** over the leading commercial code agent. All commercial agents utilize Claude Sonnet 4.5 or GPT-5 Codex-high, highlighting that **DeepCode's superior architecture**‚Äîrather than base model capability‚Äîdrives this performance gap.

### ‚ë¢ üî¨ Scientific Code Agents

**DeepCode: 73.5% vs. PaperCoder: 51.1% (+22.4%)**

Compared to PaperCoder (**51.1%**), the state-of-the-art scientific code reproduction framework, DeepCode achieves **73.5%**, demonstrating a **+22.4% relative improvement**. This substantial margin validates our multi-module architecture combining planning, hierarchical task decomposition, code generation, and iterative debugging over simpler pipeline-based approaches.

### ‚ë£ ü§ñ LLM-Based Agents

**DeepCode: 73.5% vs. Best LLM Agent: 43.3% (+30.2%)**

DeepCode significantly outperforms all tested LLM agents:
- Claude 3.5 Sonnet + IterativeAgent: 27.5%
- o1 + IterativeAgent (36 hours): 42.4%
- o1 BasicAgent: 43.3%
- **DeepCode: 73.5%**

The **+30.2% improvement** over the best-performing LLM agent demonstrates that sophisticated agent scaffolding, rather than extended inference time or larger models, is critical for complex code reproduction tasks.

---

### üéØ **Autonomous Self-Orchestrating Multi-Agent Architecture**

**The Challenges**:

- üìÑ **Implementation Complexity**: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise

- üî¨ **Research Bottleneck**: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work

- ‚è±Ô∏è **Development Delays**: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles

- üîÑ **Repetitive Coding**: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions

**DeepCode** addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.

<div align="center">

```mermaid
flowchart LR
    A["üìÑ Research Papers<br/>üí¨ Text Prompts<br/>üåê URLs & Document<br/>üìé Files: PDF, DOC, PPTX, TXT, HTML"] --> B["üß† DeepCode<br/>Multi-Agent Engine"]
    B --> C["üöÄ Algorithm Implementation <br/>üé® Frontend Development <br/>‚öôÔ∏è Backend Development"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
```

</div>

---

## üèóÔ∏è Architecture

### üìä **System Overview**

**DeepCode** is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.

üéØ **Technical Capabilities**:

üß¨ **Research-to-Production Pipeline**<br>
Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.

ü™Ñ **Natural Language Code Synthesis**<br>
Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.

‚ö° **Automated Prototyping Engine**<br>
Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.

üíé **Quality Assurance Automation**<br>
Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.

üîÆ **CodeRAG Integration System**<br>
Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.

---

### üîß **Core Techniques**

- üß† **Intelligent Orchestration Agent**: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. <br>

- üíæ **Efficient Memory Mechanism**: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. <br>

- üîç **Advanced CodeRAG System**: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.

---

### ü§ñ **Multi-Agent Architecture of DeepCode**:

- **üéØ Central Orchestrating Agent**: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. <br>

- **üìù Intent Understanding Agent**: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. <br>

- **üìÑ Document Parsing Agent**: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. <br>

- **üèóÔ∏è Code Planning Agent**: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.<br>

- **üîç Code Reference Mining Agent**: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. <br>

- **üìö Code Indexing Agent**: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. <br>

- **üß¨ Code Generation Agent**: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.

---

#### üõ†Ô∏è **Implementation Tools Matrix**

**üîß Powered by MCP (Model Context Protocol)**

DeepCode leverages the **Model Context Protocol (MCP)** standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.

##### üì° **MCP Servers & Tools**

| üõ†Ô∏è **MCP Server** | üîß **Primary Function** | üí° **Purpose & Capabilities** |
|-------------------|-------------------------|-------------------------------|
| **üîç brave** | Web Search Engine | Real-time information retrieval via Brave Search API |
| **üåê bocha-mcp** | Alternative Search | Secondary search option with independent API access |
| **üìÇ filesystem** | File System Operations | Local file and directory management, read/write operations |
| **üåê fetch** | Web Content Retrieval | Fetch and extract content from URLs and web resources |
| **üì• github-downloader** | Repository Management | Clone and download GitHub repositories for analysis |
| **üìã file-downloader** | Document Processing | Download and convert files (PDF, DOCX, etc.) to Markdown |
| **‚ö° command-executor** | System Commands | Execute bash/shell commands for environment management |
| **üß¨ code-implementation** | Code Generation Hub | Comprehensive code reproduction with execution and testing |
| **üìö code-reference-indexer** | Smart Code Search | Intelligent indexing and search of code repositories |
| **üìÑ document-segmentation** | Smart Document Analysis | Intelligent document segmentation for large papers and technical documents |

##### üîß **Legacy Tool Functions** *(for reference)*

| üõ†Ô∏è **Function** | üéØ **Usage Context** |
|-----------------|---------------------|
| **üìÑ read_code_mem** | Efficient code context retrieval from memory |
| **‚úçÔ∏è write_file** | Direct file content generation and modification |
| **üêç execute_python** | Python code testing and validation |
| **üìÅ get_file_structure** | Project structure analysis and organization |
| **‚öôÔ∏è set_workspace** | Dynamic workspace and environment configuration |
| **üìä get_operation_history** | Process monitoring and operation tracking |


---

üéõÔ∏è **Multi-Interface Framework**<br>
RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.

**üöÄ Multi-Agent Intelligent Pipeline:**

<div align="center">

### üåü **Intelligence Processing Flow**

<table align="center" width="100%" style="border: none; border-collapse: collapse;">
<tr>
<td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;">
üí° <strong>INPUT LAYER</strong><br/>
üìÑ Research Papers ‚Ä¢ üí¨ Natural Language ‚Ä¢ üåê URLs ‚Ä¢ üìã Requirements
</td>
</tr>
<tr><td colspan="3" height="20"></td></tr>
<tr>
<td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;">
üéØ <strong>CENTRAL ORCHESTRATION</strong><br/>
Strategic Decision Making ‚Ä¢ Workflow Coordination ‚Ä¢ Agent Management
</td>
</tr>
<tr><td colspan="3" height="15"></td></tr>
<tr>
<td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;">
üìù <strong>TEXT ANALYSIS</strong><br/>
<small>Requirement Processing</small>
</td>
<td width="10"></td>
<td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;">
üìÑ <strong>DOCUMENT ANALYSIS</strong><br/>
<small>Paper & Spec Processing</small>
</td>
</tr>
<tr><td colspan="3" height="15"></td></tr>
<tr>
<td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;">
üìã <strong>REPRODUCTION PLANNING</strong><br/>
Deep Paper Analysis ‚Ä¢ Code Requirements Parsing ‚Ä¢ Reproduction Strategy Development
</td>
</tr>
<tr><td colspan="3" height="15"></td></tr>
<tr>
<td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;">
üîç <strong>REFERENCE ANALYSIS</strong><br/>
<small>Repository Discovery</small>
</td>
<td width="10"></td>
<td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;">
üìö <strong>CODE INDEXING</strong><br/>
<small>Knowledge Graph Building</small>
</td>
</tr>
<tr><td colspan="3" height="15"></td></tr>
<tr>
<td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;">
üß¨ <strong>CODE IMPLEMENTATION</strong><br/>
Implementation Generation ‚Ä¢ Testing ‚Ä¢ Documentation
</td>
</tr>
<tr><td colspan="3" height="15"></td></tr>
<tr>
<td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;">
‚ö° <strong>OUTPUT DELIVERY</strong><br/>
üì¶ Complete Codebase ‚Ä¢ üß™ Test Suite ‚Ä¢ üìö Documentation ‚Ä¢ üöÄ Deployment Ready
</td>
</tr>
</table>

</div>

<div align="center">
<br/>

### üîÑ **Process Intelligence Features**

<table align="center" style="border: none;">
<tr>
<td align="center" width="25%" style="padding: 15px;">
<div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;">
<h4>üéØ Adaptive Flow</h4>
<p><small>Dynamic agent selection based on input complexity</small></p>
</div>
</td>
<td align="center" width="25%" style="padding: 15px;">
<div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;">
<h4>üß† Smart Coordination</h4>
<p><small>Intelligent task distribution and parallel processing</small></p>
</div>
</td>
<td align="center" width="25%" style="padding: 15px;">
<div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;">
<h4>üîç Context Awareness</h4>
<p><small>Deep understanding through CodeRAG integration</small></p>
</div>
</td>
<td align="center" width="25%" style="padding: 15px;">
<div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;">
<h4>‚ö° Quality Assurance</h4>
<p><small>Automated testing and validation throughout</small></p>
</div>
</td>
</tr>
</table>

</div>

---


## üöÄ Quick Start

### üìã **Prerequisites**

Before installing DeepCode, ensure you have the following:

| Requirement | Version | Purpose |
|-------------|---------|---------|
| **Python** | 3.9+ | Core runtime |
| **Node.js** | 18+ | New UI frontend |
| **npm** | 8+ | Package management |

```bash
# Check your versions
python --version   # Should be 3.9+
node --version     # Should be 18+
npm --version      # Should be 8+
```

<details>
<summary><strong>üì• Install Node.js (if not installed)</strong></summary>

```bash
# macOS (using Homebrew)
brew install node

# Ubuntu/Debian
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs

# Windows
# Download from https://nodejs.org/
```

</details>

### üì¶ **Step 1: Installation**

Choose one of the following installation methods:

#### ‚ö° **Direct Installation (Recommended)**

```bash
# üöÄ Install DeepCode package directly
pip install deepcode-hku

# üîë Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml
```

#### üîß **Development Installation (From Source)**

<details>
<summary><strong>üìÇ Click to expand development installation options</strong></summary>

##### üî• **Using UV (Recommended for Development)**

```bash
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

curl -LsSf https://astral.sh/uv/install.sh | sh
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# Install frontend dependencies
npm install --prefix new_ui/frontend
```

##### üêç **Using Traditional pip**

```bash
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

pip install -r requirements.txt

# Install frontend dependencies
npm install --prefix new_ui/frontend
```

</details>

### üîß **Step 2: Configuration**

> The following configuration applies to **all installation methods** (pip, UV, source, and Docker).

#### üîë API Keys *(required)*

Edit `mcp_agent.secrets.yaml` with your API keys:

```yaml
# At least ONE provider API key is required
openai:
  api_key: "your_openai_api_key"
  base_url: "https://openrouter.ai/api/v1"  # Optional: for OpenRouter or custom endpoints

anthropic:
  api_key: "your_anthropic_api_key"  # For Claude models

google:
  api_key: "your_google_api_key"     # For Gemini models
```

#### ü§ñ LLM Provider *(optional)*

Edit `mcp_agent.config.yaml` to choose your preferred LLM provider (line ~106):

```yaml
# Options: "google", "anthropic", "openai"
# If not set or unavailable, will automatically fallback to first available provider
llm_provider: "google"
```

#### üîç Search API Keys *(optional)*

Configure web search in `mcp_agent.config.yaml`:

```yaml
# For Brave Search (default) ‚Äî set in brave.env section (line ~28)
brave:
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) ‚Äî set in bocha-mcp.env section (line ~74)
bocha-mcp:
  env:
    BOCHA_API_KEY: "your_bocha_api_key_here"
```

#### üìÑ Document Segmentation *(optional)*

Control document processing in `mcp_agent.config.yaml`:

```yaml
document_segmentation:
  enabled: true          # true/false ‚Äî whether to use intelligent document segmentation
  size_threshold_chars: 50000  # Document size threshold to trigger segmentation
```

<details>
<summary><strong>ü™ü Windows Users: Additional MCP Server Configuration</strong></summary>

If you're using Windows, you may need to configure MCP servers manually in `mcp_agent.config.yaml`:

```bash
# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
```

Then update your `mcp_agent.config.yaml` to use absolute paths:

```yaml
mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
```

> **Note**: Replace the path with your actual global node_modules path from step 2.

</details>

<details>
<summary><strong>üîç Search Server Configuration (Optional)</strong></summary>

DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in `mcp_agent.config.yaml`:

```yaml
# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"
```

**Available Options:**
- **üîç Brave Search** (`"brave"`): Default option with high-quality search results. Requires `BRAVE_API_KEY`. Recommended for most users.
- **üåê Bocha-MCP** (`"bocha-mcp"`): Alternative search server. Requires `BOCHA_API_KEY`. Uses local Python server implementation.

**Full MCP server configuration in mcp_agent.config.yaml:**
```yaml
# For Brave Search (default) - around line 28
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
```

> **üí° Tip**: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.

</details>

### ‚ö° **Step 3: Launch Application**

Choose your preferred launch method:

<table width="100%">
<tr>
<th width="33%">üê≥ Docker (Recommended)</th>
<th width="33%">üöÄ Local (<code>deepcode</code> command)</th>
<th width="33%">üõ†Ô∏è Other Methods</th>
</tr>
<tr><td>

No Python/Node needed ‚Äî everything in container.

```bash
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/
cp mcp_agent.secrets.yaml.example \
   mcp_agent.secrets.yaml
# Edit secrets with your API keys

./deepcode_docker/run_docker.sh
# Access ‚Üí http://localhost:8000
```

</td><td>

Auto-installs deps on first run.

```bash
deepcode
# Frontend ‚Üí http://localhost:5173
# Backend  ‚Üí http://localhost:8000
# Ctrl+C to stop
```

Features: User-in-Loop, real-time progress, inline chat.

</td><td>

```bash
# macOS / Linux
./run.sh
# or: python deepcode.py

# Windows
run.bat
# or: python deepcode.py

# Classic Streamlit UI
deepcode --classic

# CLI mode
deepcode --cli
# or: python cli/main_cli.py
```

</td></tr>
</table>

<details>
<summary><strong>üê≥ Docker Management Commands</strong></summary>

```bash
./deepcode_docker/run_docker.sh stop      # Stop
./deepcode_docker/run_docker.sh restart   # Restart (no rebuild needed for config changes)
./deepcode_docker/run_docker.sh --build   # Force rebuild
./deepcode_docker/run_docker.sh logs      # Real-time logs
./deepcode_docker/run_docker.sh status    # Health check
./deepcode_docker/run_docker.sh clean     # Remove containers & images
```

Or with Docker Compose directly:
```bash
docker compose -f deepcode_docker/docker-compose.yml up --build   # Build & start
docker compose -f deepcode_docker/docker-compose.yml down         # Stop
docker compose -f deepcode_docker/docker-compose.yml logs -f      # Logs
```

> **üí°** Config files are mounted as volumes ‚Äî edit and restart, no rebuild needed.
> **üí°** Windows users: run `docker compose` commands directly if shell scripts aren't available.

</details>

### üéØ **Step 4: Generate Code**

1. **üìÑ Input** ‚Äî Upload a research paper, type requirements, or paste a URL
2. **ü§ñ Processing** ‚Äî The multi-agent system analyzes, plans, and generates
3. **‚ö° Output** ‚Äî Receive production-ready code with tests and documentation

---

### üîß **Troubleshooting**

<details>
<summary><strong>‚ùì Common Issues & Solutions</strong></summary>

| Problem | Cause | Fix |
|---|---|---|
| Docker build fails with `tsc: not found` | Corrupted build cache | `docker builder prune -f` then rebuild with `--no-cache` |
| `error during connect` / `cannot find the file` | Docker Desktop not running | Start Docker Desktop, wait until ready, retry |
| Frontend blank page | Corrupted `node_modules` | `cd new_ui/frontend && rm -rf node_modules && npm install` |
| `ERR_CONNECTION_REFUSED` | Wrong port / backend not running | Docker: `http://localhost:8000`. Local: `http://localhost:5173` |
| `npm install` ‚Üí `Could not read package.json` | Wrong directory | Use `npm install --prefix new_ui/frontend` |
| Windows: MCP servers not working | Need absolute paths | See [Windows MCP Configuration](#-step-2-configuration) above |

</details>

  ---

## ü§ñ nanobot Integration (Feishu Chatbot)

> Chat with DeepCode from **Feishu** ‚Äî powered by [nanobot](https://github.com/HKUDS/nanobot).

<div align="center">

```mermaid
flowchart LR
    subgraph Clients["üí¨ Chat Platforms"]
        direction TB
        F["<b>Feishu</b><br/>WebSocket"]
        T["<b>Telegram</b><br/>Polling"]
        D["<b>Discord</b><br/>Gateway"]
    end

    subgraph Gateway["üêà nanobot Gateway"]
        direction TB
        A["Agent Loop<br/><i>LLM + Tool Calls</i>"]
    end

    subgraph Engine["üß† DeepCode Engine"]
        direction TB
        P2C["Paper ‚Üí Code"]
        C2C["Chat ‚Üí Code"]
        TRK["Task Tracking"]
    end

    F & T & D <-->|"messages"| A
    A -->|"HTTP API"| P2C & C2C & TRK
    A -.->|"LLM API"| LLM["‚òÅÔ∏è OpenRouter"]

    style Clients fill:#1a1a2e,stroke:#00d9ff,color:#fff
    style Gateway fill:#1a1a2e,stroke:#4ecdc4,color:#fff
    style Engine fill:#1a1a2e,stroke:#ff6b6b,color:#fff
    style LLM fill:#1a1a2e,stroke:#9b59b6,color:#fff
```

</div>

<div align="center">
<table><tr>
<td align="center"><a href="https://github.com/HKUDS/DeepCode"><img src="./assets/logo.png" alt="DeepCode" height="55"/></a></td>
<td align="center"><h2>‚ú¶</h2></td>
<td align="center"><a href="https://github.com/HKUDS/nanobot"><img src="./assets/nanobot.png" alt="nanobot" height="55"/></a></td>
</tr></table>
</div>

Both services run inside the same **Docker Compose** network. Prerequisites: **Docker Desktop** + **OpenRouter API Key** ([get one](https://openrouter.ai/keys)) + **Feishu App**.

---

### Step 1 ¬∑ Create a Feishu Bot

<details open>
<summary><b>Feishu / Lark</b> (Recommended ‚Äî WebSocket, no public IP needed)</summary>

1. Go to [Feishu Open Platform](https://open.feishu.cn/app) ‚Üí **Create Custom App**
2. Enable **Bot** capability in App Features
3. Add permissions: `im:message` ¬∑ `im:message:send_as_bot`
4. Event Subscription ‚Üí select **Long Connection** ‚Üí add `im.message.receive_v1`
5. Note your **App ID** (`cli_xxx`) and **App Secret** ‚Üí Publish the app

> **Note**: Feishu requires an active WebSocket connection before you can save "Long Connection" mode. Start nanobot first (Step 3), then come back to configure Event Subscription.

</details>

### Step 2 ¬∑ Configure

```bash
cp nanobot_config.json.example nanobot_config.json
```

Edit `nanobot_config.json` ‚Äî fill in the 3 required fields:

```jsonc
{
  "channels": {
    "feishu": {
      "enabled": true,
      "appId": "cli_xxx",              // ‚Üê Feishu App ID
      "appSecret": "xxx",              // ‚Üê Feishu App Secret
      "allowFrom": []                  // [] = allow all users
    }
  },
  "providers": {
    "openrouter": {
      "apiKey": "sk-or-v1-xxx"         // ‚Üê OpenRouter API Key
    }
  },
  "agents": {
    "defaults": {
      "model": "anthropic/claude-sonnet-4-20250514"
    }
  }
}
```

> **Model choice**: Any model on [openrouter.ai/models](https://openrouter.ai/models). Use `anthropic/claude-sonnet-4-20250514` for English, `minimax/minimax-m2.1` for Chinese.

---

### Step 3 ¬∑ Launch

Make sure `mcp_agent.secrets.yaml` has your DeepCode API keys (see [Configuration](#-step-2-configuration)), then:

```bash
./nanobot/run_nanobot.sh -d          # Start both DeepCode + nanobot in background
```

The script checks Docker, validates configs, builds images (first run only), and starts both containers.

```
‚úì DeepCode API:  http://localhost:8000
‚úì Nanobot:       http://localhost:18790
```

Now open Feishu ‚Üí find your bot ‚Üí send a message!

<details>
<summary><b>Management Commands</b></summary>

```bash
./nanobot/run_nanobot.sh              # Start (foreground)
./nanobot/run_nanobot.sh -d           # Start (background)
./nanobot/run_nanobot.sh stop         # Stop all services
./nanobot/run_nanobot.sh restart      # Restart (config changes take effect immediately)
./nanobot/run_nanobot.sh --build      # Force rebuild Docker images
./nanobot/run_nanobot.sh logs         # View real-time logs
./nanobot/run_nanobot.sh status       # Health check
./nanobot/run_nanobot.sh clean        # Remove containers & images
```

</details>

<details>
<summary><b>Troubleshooting</b></summary>

| Problem | Fix |
|---|---|
| Feishu bot doesn't respond | Check logs (`./nanobot/run_nanobot.sh logs`), verify `appId`/`appSecret`, ensure app is published with Long Connection mode |
| Can't connect to DeepCode | Verify `deepcode` container is healthy: `curl http://localhost:8000/health` |
| Wrong language output | Switch model ‚Äî `minimax-m2.1` defaults to Chinese, use Claude/GPT for English |
| Config not taking effect | Just restart: `./nanobot/run_nanobot.sh restart` (no rebuild needed) |
| Clear chat history | Send `/clear` in chat, or: `docker exec nanobot sh -c 'rm -rf /root/.nanobot/sessions/*.jsonl'` |

</details>

---

## üí° Examples



### üé¨ **Live Demonstrations**



<table align="center">
<tr>
<td width="33%" align="center">

#### üìÑ **Paper2Code Demo**
**Research to Implementation**

<div align="center">
  <a href="https://www.youtube.com/watch?v=MQZYpLkzsbw">
    <img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
  </a>

  **[‚ñ∂Ô∏è Watch Demo](https://www.youtube.com/watch?v=MQZYpLkzsbw)**

  *Transform academic papers into production-ready code automatically*
</div>

</td>
<td width="33%" align="center">

#### üñºÔ∏è **Image Processing Demo**
**AI-Powered Image Tools**

<div align="center">
  <a href="https://www.youtube.com/watch?v=nFt5mLaMEac">
    <img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
  </a>

  **[‚ñ∂Ô∏è Watch Demo](https://www.youtube.com/watch?v=nFt5mLaMEac)**

  *Intelligent image processing with background removal and enhancement*
</div>

</td>
<td width="33%" align="center">

#### üåê **Frontend Implementation**
**Complete Web Application**

<div align="center">
  <a href="https://www.youtube.com/watch?v=78wx3dkTaAU">
    <img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
  </a>

  **[‚ñ∂Ô∏è Watch Demo](https://www.youtube.com/watch?v=78wx3dkTaAU)**

  *Full-stack web development from concept to deployment*
</div>

</td>
</tr>
</table>



### üÜï **Recent Updates**

#### üìÑ **Smart Document Segmentation (v1.2.0)**
- **Intelligent Processing**: Automatically handles large research papers and technical documents that exceed LLM token limits
- **Configurable Control**: Toggle segmentation via configuration with size-based thresholds
- **Semantic Analysis**: Advanced content understanding with algorithm, concept, and formula preservation
- **Backward Compatibility**: Seamlessly falls back to traditional processing for smaller documents

### üöÄ **Coming Soon**

We're continuously enhancing DeepCode with exciting new features:

#### üîß **Enhanced Code Reliability & Validation**
- **Automated Testing**: Comprehensive functionality testing with execution verification and error detection.
- **Code Quality Assurance**: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.
- **Smart Debugging**: AI-powered error detection with automatic correction suggestions

#### üìä **PaperBench Performance Showcase**
- **Benchmark Dashboard**: Comprehensive performance metrics on the PaperBench evaluation suite.
- **Accuracy Metrics**: Detailed comparison with state-of-the-art paper reproduction systems.
- **Success Analytics**: Statistical analysis across paper categories and complexity levels.

#### ‚ö° **System-wide Optimizations**
- **Performance Boost**: Multi-threaded processing and optimized agent coordination for faster generation.
- **Enhanced Reasoning**: Advanced reasoning capabilities with improved context understanding.
- **Expanded Support**: Extended compatibility with additional programming languages and frameworks.

---

## ‚≠ê Star History

<div align="center">

*Community Growth Trajectory*

<a href="https://star-history.com/#HKUDS/DeepCode&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" />
  </picture>
</a>

</div>

---

### üöÄ **Ready to Transform Development?**

<div align="center">

<p>
  <a href="#-quick-start"><img src="https://img.shields.io/badge/üöÄ_Get_Started-00d4ff?style=for-the-badge&logo=rocket&logoColor=white" alt="Get Started"></a>
  <a href="https://github.com/HKUDS"><img src="https://img.shields.io/badge/üèõÔ∏è_View_on_GitHub-00d4ff?style=for-the-badge&logo=github&logoColor=white" alt="View on GitHub"></a>
  <a href="https://github.com/HKUDS/deepcode-agent"><img src="https://img.shields.io/badge/‚≠ê_Star_Project-00d4ff?style=for-the-badge&logo=star&logoColor=white" alt="Star Project"></a>
</p>

---

<div align="left">

### üìñ **Citation**


If you find DeepCode useful in your research or applications, please kindly cite:

```
@misc{li2025deepcodeopenagenticcoding,
      title={DeepCode: Open Agentic Coding},
      author={Zongwei Li and Zhonghang Li and Zirui Guo and Xubin Ren and Chao Huang},
      year={2025},
      eprint={2512.07921},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2512.07921},
}
```

---


### üìÑ **License**

<div align="center">

<img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&logo=opensourceinitiative&logoColor=white" alt="MIT License">

**MIT License** - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong

---


<img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&style=for-the-badge&color=00d4ff" alt="Visitors">

</div>


## Links discovered
- [nanobot](https://github.com/HKUDS/nanobot)
- [*PaperBench*](https://openai.com/index/paperbench/)
- [get one](https://openrouter.ai/keys)
- [Feishu Open Platform](https://open.feishu.cn/app)
- [openrouter.ai/models](https://openrouter.ai/models)
- [‚ñ∂Ô∏è Watch Demo](https://www.youtube.com/watch?v=MQZYpLkzsbw)
- [‚ñ∂Ô∏è Watch Demo](https://www.youtube.com/watch?v=nFt5mLaMEac)
- [‚ñ∂Ô∏è Watch Demo](https://www.youtube.com/watch?v=78wx3dkTaAU)
- [<img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/>](https://trendshift.io/repositories/14665)
- [<img src='https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&style=for-the-badge&logo=star&logoColor=white&labelColor=1a1a2e' />](https://github.com/HKUDS/DeepCode/stargazers)
- [<img src="https://img.shields.io/badge/Paper-arXiv-orange?style=for-the-badge&logo=arxiv&logoColor=white&labelColor=1a1a2e">](https://arxiv.org/abs/2512.07921)
- [<img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&logo=pypi&logoColor=white&labelColor=1a1a2e&color=ff6b6b">](https://pypi.org/project/deepcode-hku/)
- [<img src="https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&logo=discord&logoColor=white&labelColor=1a1a2e">](https://discord.gg/yF2MmDJyGJ)
- [<img src="https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&logo=wechat&logoColor=white&labelColor=1a1a2e">](https://github.com/HKUDS/DeepCode/issues/11)
- [<img src="https://img.shields.io/badge/English-00d4ff?style=for-the-badge&logo=readme&logoColor=white&labelColor=1a1a2e" alt="English">](https://github.com/HKUDS/DeepCode/blob/main/README.md)
- [<img src="https://img.shields.io/badge/‰∏≠Êñá-00d4ff?style=for-the-badge&logo=readme&logoColor=white&labelColor=1a1a2e" alt="‰∏≠Êñá">](https://github.com/HKUDS/DeepCode/blob/main/README_ZH.md)
- [<img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;"/>](https://youtu.be/PRgmP8pOI08)
- [<img src="https://img.shields.io/badge/‚ñ∂Ô∏è_Watch_Video-FF0000?style=for-the-badge&logo=youtube&logoColor=white" alt="Watch Video"/>](https://youtu.be/PRgmP8pOI08)
- [<img src="./assets/logo.png" alt="DeepCode" height="60"/>](https://github.com/HKUDS/DeepCode)
- [<img src="./assets/nanobot.png" alt="nanobot" height="60"/>](https://github.com/HKUDS/nanobot)
- [<img src="./assets/logo.png" alt="DeepCode" height="55"/>](https://github.com/HKUDS/DeepCode)
- [<img src="./assets/nanobot.png" alt="nanobot" height="55"/>](https://github.com/HKUDS/nanobot)
- [<img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>](https://www.youtube.com/watch?v=MQZYpLkzsbw)
- [<img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>](https://www.youtube.com/watch?v=nFt5mLaMEac)
- [<img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>](https://www.youtube.com/watch?v=78wx3dkTaAU)
- [<img src="https://img.shields.io/badge/üèõÔ∏è_View_on_GitHub-00d4ff?style=for-the-badge&logo=github&logoColor=white" alt="View on GitHub">](https://github.com/HKUDS)
- [<img src="https://img.shields.io/badge/‚≠ê_Star_Project-00d4ff?style=for-the-badge&logo=star&logoColor=white" alt="Star Project">](https://github.com/HKUDS/deepcode-agent)

--- README_ZH.md ---
<div align="center">

<table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;">
<tr>
<td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;">
  <img src="assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;"/>
</td>
<td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;">
  <pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;">    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù</pre>
</td>
</tr>
</table>

<div align="center">
<a href="https://trendshift.io/repositories/14665" target="_blank"><img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>

<!-- <img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&size=28&duration=2000&pause=800&color=06B6D4&background=00000000&center=true&vCenter=true&width=800&height=50&lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/> -->

# <img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;"/> DeepCode: ÂºÄÊ∫êÊô∫ËÉΩ‰ΩìÁºñÁ®ã

### *Âü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÊé®Ëøõ‰ª£Á†ÅÁîüÊàêÊäÄÊúØ*

<!-- <p align="center">
  <img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&logo=rocket&logoColor=white" alt="Version">

  <img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&logo=opensourceinitiative&logoColor=white" alt="License">
  <img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&logo=brain&logoColor=white" alt="AI">
  <img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&logo=university&logoColor=white" alt="HKU">
</p> -->
<p>
  <a href="https://github.com/HKUDS/DeepCode/stargazers"><img src='https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&style=for-the-badge&logo=star&logoColor=white&labelColor=1a1a2e' /></a>
  <img src="https://img.shields.io/badge/üêçPython-3.13-4ecdc4?style=for-the-badge&logo=python&logoColor=white&labelColor=1a1a2e">
  <a href="https://pypi.org/project/deepcode-hku/"><img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&logo=pypi&logoColor=white&labelColor=1a1a2e&color=ff6b6b"></a>
</p>
<p>
  <a href="https://discord.gg/yF2MmDJyGJ"><img src="https://img.shields.io/badge/üí¨Discord-Á§æÂå∫-7289da?style=for-the-badge&logo=discord&logoColor=white&labelColor=1a1a2e"></a>
  <a href="https://github.com/HKUDS/DeepCode/issues/11"><img src="https://img.shields.io/badge/üí¨ÂæÆ‰ø°-Áæ§ÁªÑ-07c160?style=for-the-badge&logo=wechat&logoColor=white&labelColor=1a1a2e"></a>
</p>
<div align="center">
  <div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"></div>
</div>

<div align="center">
  <a href="#-Âø´ÈÄüÂºÄÂßã" style="text-decoration: none;">
    <img src="https://img.shields.io/badge/Âø´ÈÄüÂºÄÂßã-Á´ãÂç≥ÂºÄÂßã-00d9ff?style=for-the-badge&logo=rocket&logoColor=white&labelColor=1a1a2e">
  </a>
</div>

<div align="center" style="margin-top: 10px;">
  <a href="README.md">
    <img src="https://img.shields.io/badge/English-00d4ff?style=for-the-badge&logo=readme&logoColor=white&labelColor=1a1a2e" alt="English">
  </a>
  <a href="README_ZH.md">
    <img src="https://img.shields.io/badge/‰∏≠Êñá-00d4ff?style=for-the-badge&logo=readme&logoColor=white&labelColor=1a1a2e" alt="‰∏≠Êñá">
  </a>
</div>

### üñ•Ô∏è **ÁïåÈù¢Â±ïÁ§∫**

<table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;">
<tr>
<td width="50%" align="center" style="vertical-align: top; padding: 20px;">

#### üñ•Ô∏è **ÂëΩ‰ª§Ë°åÁïåÈù¢**
**Âü∫‰∫éÁªàÁ´ØÁöÑÂºÄÂèëÁéØÂ¢É**

<div align="center">

  <img src="https://github.com/Zongwei9888/Experiment_Images/blob/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;"/>

  <div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;">
    <strong>üöÄ È´òÁ∫ßÁªàÁ´Ø‰ΩìÈ™å</strong><br/>
    <small>‚ö° Âø´ÈÄüÂëΩ‰ª§Ë°åÂ∑•‰ΩúÊµÅ<br/>üîß ÂºÄÂèëËÄÖÂèãÂ•ΩÁïåÈù¢<br/>üìä ÂÆûÊó∂ËøõÂ∫¶Ë∑üË∏™</small>
  </div>

  *‰∏ì‰∏öÁªàÁ´ØÁïåÈù¢ÔºåÈÄÇÂêàÈ´òÁ∫ßÁî®Êà∑ÂíåCI/CDÈõÜÊàê*
</div>

</td>
<td width="50%" align="center" style="vertical-align: top; padding: 20px;">

#### üåê **WebÁïåÈù¢**
**ÂèØËßÜÂåñ‰∫§‰∫í‰ΩìÈ™å**

<div align="center">

  <img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;"/>

  <div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;">
    <strong>üé® Áé∞‰ª£ÂåñWeb‰ª™Ë°®Êùø</strong><br/>
    <small>üñ±Ô∏è Áõ¥ËßÇÁöÑÊãñÊãΩÊìç‰Ωú<br/>üì± ÂìçÂ∫îÂºèËÆæËÆ°<br/>üéØ ÂèØËßÜÂåñËøõÂ∫¶Ë∑üË∏™</small>
  </div>

  *ÁæéËßÇÁöÑWebÁïåÈù¢Ôºå‰∏∫ÊâÄÊúâÊäÄËÉΩÊ∞¥Âπ≥Áî®Êà∑Êèê‰æõÊµÅÁïÖÁöÑÂ∑•‰ΩúÊµÅÁ®ã*
</div>

</td>
</tr>
</table>

---

<div align="center">

### üé¨ **‰ªãÁªçËßÜÈ¢ë**

<div style="margin: 20px 0;">
  <a href="https://youtu.be/PRgmP8pOI08" target="_blank">
    <img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg"
         alt="DeepCode Introduction Video"
         width="75%"
         style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;"/>
  </a>
</div>

*üéØ **ËßÇÁúãÊàë‰ª¨ÁöÑÂÆåÊï¥‰ªãÁªç** - ‰∫ÜËß£DeepCodeÂ¶Ç‰ΩïÂ∞ÜÁ†îÁ©∂ËÆ∫ÊñáÂíåËá™ÁÑ∂ËØ≠Ë®ÄËΩ¨Êç¢‰∏∫Áîü‰∫ßÂ∞±Áª™ÁöÑ‰ª£Á†Å*

<p>
  <a href="https://youtu.be/PRgmP8pOI08" target="_blank">
    <img src="https://img.shields.io/badge/‚ñ∂Ô∏è_ËßÇÁúãËßÜÈ¢ë-FF0000?style=for-the-badge&logo=youtube&logoColor=white" alt="Watch Video"/>
  </a>
</p>

</div>

---




> *"AIÊô∫ËÉΩ‰ΩìÂ∞ÜÂàõÊÑèËΩ¨Âåñ‰∏∫Áîü‰∫ßÂ∞±Áª™‰ª£Á†ÅÁöÑÂú∞Êñπ"*

</div>

---

## üìë ÁõÆÂΩï

- [üì∞ Êñ∞Èóª](#-Êñ∞Èóª)
- [üöÄ Ê†∏ÂøÉÁâπÊÄß](#-Ê†∏ÂøÉÁâπÊÄß)
- [üèóÔ∏è Êû∂ÊûÑ](#Ô∏è-Êû∂ÊûÑ)
- [üìä ÂÆûÈ™åÁªìÊûú](#-ÂÆûÈ™åÁªìÊûú)
- [üöÄ Âø´ÈÄüÂºÄÂßã](#-Âø´ÈÄüÂºÄÂßã)
- [ü§ñ nanobot ÈõÜÊàêÔºàÈ£û‰π¶ËÅäÂ§©Êú∫Âô®‰∫∫Ôºâ](#-nanobot-ÈõÜÊàêÈ£û‰π¶ËÅäÂ§©Êú∫Âô®‰∫∫)
- [üí° Á§∫‰æã](#-Á§∫‰æã)
  - [üé¨ ÂÆûÊó∂ÊºîÁ§∫](#-ÂÆûÊó∂ÊºîÁ§∫)
- [‚≠ê ÊòüÊ†áÂéÜÂè≤](#-ÊòüÊ†áÂéÜÂè≤)
- [üìÑ ËÆ∏ÂèØËØÅ](#-ËÆ∏ÂèØËØÅ)

---

## üì∞ Êñ∞Èóª

üéâ **[2025-02] DeepCode + nanobot ÈõÜÊàê ‚Äî ÈÄöËøáÈ£û‰π¶ËÅäÂ§©‰ΩøÁî® DeepCodeÔºÅ**

<div align="center">
<table><tr>
<td align="center"><a href="https://github.com/HKUDS/DeepCode"><img src="./assets/logo.png" alt="DeepCode" height="60"/></a></td>
<td align="center"><h2>‚ú¶</h2></td>
<td align="center"><a href="https://github.com/HKUDS/nanobot"><img src="./assets/nanobot.png" alt="nanobot" height="60"/></a></td>
</tr></table>
</div>

- [nanobot](https://github.com/HKUDS/nanobot) Áé∞Â∑≤ËøûÊé•Âà∞ DeepCode ‚Äî Âú®**È£û‰π¶**‰∏≠ÂèëÈÄÅÊ∂àÊÅØÂç≥ÂèØËá™Âä®ÁîüÊàê‰ª£Á†Å
- ÊîØÊåÅ**ËÆ∫ÊñáËΩ¨‰ª£Á†Å**Âíå**ÂØπËØùËΩ¨‰ª£Á†Å**Ôºå‰ª•ÂèäÂÆûÊó∂‰ªªÂä°Ë∑üË∏™ÔºåÂÖ®ÈÉ®Âú®ËÅäÂ§©Â∫îÁî®‰∏≠ÂÆåÊàê
- ‰∏ÄÈîÆÈÉ®ÁΩ≤Ôºö`./nanobot/run_nanobot.sh` ‚Üí **[ËÆæÁΩÆÊåáÂçó ‚Üí](#-nanobot-ÈõÜÊàêÈ£û‰π¶ËÅäÂ§©Êú∫Âô®‰∫∫)**

<div align="center">
<table width="100%"><tr>
<td width="50%" align="center">
  <img src="./assets/IMG_8098.jpeg" alt="È£û‰π¶ËÅäÂ§©Á§∫‰æã 1" width="95%" style="border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.2);"/>
</td>
<td width="50%" align="center">
  <img src="./assets/IMG_8099.jpeg" alt="È£û‰π¶ËÅäÂ§©Á§∫‰æã 2" width="95%" style="border-radius: 10px; box-shadow: 0 4px 15px rgba(0,0,0,0.2);"/>
</td>
</tr></table>
<sub><em>È£û‰π¶Êú∫Âô®‰∫∫ÂÆûÊàò ‚Äî Ëá™ÁÑ∂ËØ≠Ë®Ä ‚Üí ÂÆåÊï¥‰ª£Á†ÅÁîüÊàêÔºåÂ∏¶ËÆæÁΩÆËØ¥Êòé</em></sub>
</div>

---

üéâ **[2025-02] ÂÖ®Êñ∞ Web UI ‰ΩìÈ™åÂçáÁ∫ßÔºÅ**

- üîÑ **Áî®Êà∑‰∫§‰∫íÂæ™ÁéØ (User-in-Loop)**: ÊîØÊåÅÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÁöÑÂÆûÊó∂Áî®Êà∑‰∫§‰∫íÔºåAI ‰ºöÂú®ÂØπËØù‰∏≠ÂêëÊÇ®ÊèêÈóÆ‰ª•ÊæÑÊ∏ÖÈúÄÊ±Ç
- üí¨ **ÂÜÖËÅî‰∫§‰∫íËÆæËÆ°**: ‰∫§‰∫íÈóÆÈ¢òÁõ¥Êé•ÊòæÁ§∫Âú®ÂØπËØùÊ°Ü‰∏≠Ôºå‰ΩìÈ™åÊõ¥Ëá™ÁÑ∂ÊµÅÁïÖ
- üöÄ **‰∏ÄÈîÆÂêØÂä®**: ËøêË°å `deepcode` Âç≥ÂèØÂêØÂä®Êñ∞Áâà UIÔºàË∑®Âπ≥Âè∞ÊîØÊåÅÔºöWindows/macOS/LinuxÔºâ
- üîß **‰ºòÂåñÁöÑËøõÁ®ãÁÆ°ÁêÜ**: ÊîπËøõ‰∫ÜÊúçÂä°ÂêØÂÅúÊú∫Âà∂ÔºåËá™Âä®Ê∏ÖÁêÜÁ´ØÂè£Âç†Áî®
- üì° **WebSocket ÂÆûÊó∂ÈÄö‰ø°**: ‰øÆÂ§ç‰∫ÜÊ∂àÊÅØ‰∏¢Â§±ÈóÆÈ¢òÔºåÁ°Æ‰øù‰∫§‰∫íÁä∂ÊÄÅÊ≠£Á°ÆÂêåÊ≠•

<div align="center">
  <img src="./assets/NewUI.png" alt="DeepCode ÂÖ®Êñ∞ UI" width="85%" style="border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.15);" />
  <br/>
  <sub><em>DeepCode ÂÖ®Êñ∞ Web UI - Âü∫‰∫é React ÁöÑÁé∞‰ª£ÁïåÈù¢</em></sub>
</div>

---

üéâ **[2025-10-28] DeepCodeÂú®PaperBench‰∏äËææÂà∞ÊúÄÂÖàËøõÊ∞¥Âπ≥ÔºÅ**

DeepCodeÂú®OpenAIÁöÑPaperBench Code-DevÊâÄÊúâÁ±ªÂà´‰∏≠ÂàõÈÄ†Êñ∞Âü∫ÂáÜÔºö

- üèÜ **Ë∂ÖË∂ä‰∫∫Á±ª‰∏ìÂÆ∂**: **75.9%** (DeepCode) vs È°∂Á∫ßÊú∫Âô®Â≠¶‰π†ÂçöÂ£´ 72.4% (+3.5%)„ÄÇ
- ü•á **Ë∂ÖË∂äÊúÄÂÖàËøõÂïÜ‰∏ö‰ª£Á†ÅÊô∫ËÉΩ‰Ωì**: **84.8%** (DeepCode) vs È¢ÜÂÖàÂïÜ‰∏ö‰ª£Á†ÅÊô∫ËÉΩ‰Ωì (+26.1%) (Cursor, Claude Code, Âíå Codex)„ÄÇ
- üî¨ **Êé®ËøõÁßëÂ≠¶ÁºñÁ®ã**: **73.5%** (DeepCode) vs PaperCoder 51.1% (+22.4%)„ÄÇ
- üöÄ **ÂáªË¥•LLMÊô∫ËÉΩ‰Ωì**: **73.5%** (DeepCode) vs ÊúÄ‰Ω≥LLMÊ°ÜÊû∂ 43.3% (+30.2%)„ÄÇ

---

## üöÄ Ê†∏ÂøÉÁâπÊÄß

<br/>

<table align="center" width="100%" style="border: none; table-layout: fixed;">
<tr>
<td width="30%" align="center" style="vertical-align: top; padding: 20px;">

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<h3 style="margin: 0; padding: 0;">üöÄ <strong>ËÆ∫ÊñáËΩ¨‰ª£Á†Å</strong></h3>
</div>

<div align="center" style="margin: 15px 0;">
  <img src="https://img.shields.io/badge/ÁÆóÊ≥ï-ÂÆûÁé∞-ff6b6b?style=for-the-badge&logo=algorithm&logoColor=white" alt="Algorithm Badge" />
</div>

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<p align="center"><strong>Â§çÊùÇÁÆóÊ≥ïÁöÑËá™Âä®ÂåñÂÆûÁé∞</strong></p>
</div>

<div style="height: 60px; display: flex; align-items: center; justify-content: center;">
<p align="center">ËΩªÊùæÂ∞ÜÁ†îÁ©∂ËÆ∫Êñá‰∏≠ÁöÑÂ§çÊùÇÁÆóÊ≥ïËΩ¨Êç¢‰∏∫<strong>È´òË¥®Èáè</strong>„ÄÅ<strong>Áîü‰∫ßÂ∞±Áª™</strong>ÁöÑ‰ª£Á†ÅÔºåÂä†ÈÄüÁÆóÊ≥ïÂ§çÁé∞„ÄÇ</p>
</div>



</td>
<td width="30%" align="center" style="vertical-align: top; padding: 20px;">

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<h3 style="margin: 0; padding: 0;">üé® <strong>ÊñáÊú¨ËΩ¨Web</strong></h3>
</div>

<div align="center" style="margin: 15px 0;">
  <img src="https://img.shields.io/badge/ÂâçÁ´Ø-ÂºÄÂèë-4ecdc4?style=for-the-badge&logo=react&logoColor=white" alt="Frontend Badge" />
</div>

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<p align="center"><strong>Ëá™Âä®ÂåñÂâçÁ´ØWebÂºÄÂèë</strong></p>
</div>

<div style="height: 60px; display: flex; align-items: center; justify-content: center;">
<p align="center">Â∞ÜÁ∫ØÊñáÊú¨ÊèèËø∞ËΩ¨Êç¢‰∏∫<strong>ÂäüËÉΩÂÆåÊï¥</strong>„ÄÅ<strong>ËßÜËßâÁæéËßÇ</strong>ÁöÑÂâçÁ´ØWeb‰ª£Á†ÅÔºåÂø´ÈÄüÂàõÂª∫ÁïåÈù¢„ÄÇ</p>
</div>



</td>
<td width="30%" align="center" style="vertical-align: top; padding: 20px;">

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<h3 style="margin: 0; padding: 0;">‚öôÔ∏è <strong>ÊñáÊú¨ËΩ¨ÂêéÁ´Ø</strong></h3>
</div>

<div align="center" style="margin: 15px 0;">
  <img src="https://img.shields.io/badge/ÂêéÁ´Ø-ÂºÄÂèë-9b59b6?style=for-the-badge&logo=server&logoColor=white" alt="Backend Badge" />
</div>

<div style="height: 80px; display: flex; align-items: center; justify-content: center;">
<p align="center"><strong>Ëá™Âä®ÂåñÂêéÁ´ØÂºÄÂèë</strong></p>
</div>

<div style="height: 60px; display: flex; align-items: center; justify-content: center;">
<p align="center">‰ªéÁÆÄÂçïÁöÑÊñáÊú¨ËæìÂÖ•ÁîüÊàê<strong>È´òÊïà</strong>„ÄÅ<strong>ÂèØÊâ©Â±ï</strong>Âíå<strong>ÂäüËÉΩ‰∏∞ÂØå</strong>ÁöÑÂêéÁ´Ø‰ª£Á†ÅÔºåÁÆÄÂåñÊúçÂä°Âô®Á´ØÂºÄÂèë„ÄÇ</p>
</div>



</td>
</tr>
</table>

<br/>

---

## üìä ÂÆûÈ™åÁªìÊûú

<div align="center">
    <img src='./assets/result_main02.jpg' /><br>
</div>
<br/>

Êàë‰ª¨Âú®[*PaperBench*](https://openai.com/index/paperbench/)Âü∫ÂáÜÊµãËØïÔºàÁî±OpenAIÂèëÂ∏ÉÔºâ‰∏äËØÑ‰º∞**DeepCode**ÔºåËøôÊòØ‰∏Ä‰∏™‰∏•Ê†ºÁöÑÊµãËØïÂπ≥Âè∞ÔºåË¶ÅÊ±ÇAIÊô∫ËÉΩ‰Ωì‰ªéÂ§¥Áã¨Á´ãÂ§çÁé∞20ÁØáICML 2024ËÆ∫Êñá„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´8,316‰∏™ÂèØËØÑÂàÜÁªÑ‰ª∂Ôºå‰ΩøÁî®Â∏¶ÊúâÂàÜÂ±ÇÊùÉÈáçÁöÑSimpleJudgeËøõË°åËØÑ‰º∞„ÄÇ

Êàë‰ª¨ÁöÑÂÆûÈ™åÂ∞ÜDeepCode‰∏éÂõõ‰∏™Âü∫Á∫øÁ±ªÂà´ËøõË°åÊØîËæÉÔºö**(1) ‰∫∫Á±ª‰∏ìÂÆ∂**Ôºå**(2) ÊúÄÂÖàËøõÂïÜ‰∏ö‰ª£Á†ÅÊô∫ËÉΩ‰Ωì**Ôºå**(3) ÁßëÂ≠¶‰ª£Á†ÅÊô∫ËÉΩ‰Ωì**Ôºå‰ª•Âèä **(4) Âü∫‰∫éLLMÁöÑÊô∫ËÉΩ‰Ωì**„ÄÇ

### ‚ë† üß† ‰∫∫Á±ª‰∏ìÂÆ∂Ë°®Áé∞ÔºàÈ°∂Á∫ßÊú∫Âô®Â≠¶‰π†ÂçöÂ£´Ôºâ

**DeepCode: 75.9% vs. È°∂Á∫ßÊú∫Âô®Â≠¶‰π†ÂçöÂ£´: 72.4% (+3.5%)**

DeepCodeÂú®3ÁØáËÆ∫ÊñáÁöÑ‰∫∫Á±ªËØÑ‰º∞Â≠êÈõÜ‰∏äËææÂà∞**75.9%**Ôºå**Ë∂ÖË∂ä3Ê¨°‰∫∫Á±ª‰∏ìÂÆ∂Âü∫Á∫øÔºà72.4%Ôºâ+3.5‰∏™ÁôæÂàÜÁÇπ**„ÄÇËøôË°®ÊòéÊàë‰ª¨ÁöÑÊ°ÜÊû∂‰∏ç‰ªÖÂåπÈÖçËÄå‰∏îË∂ÖË∂ä‰∫Ü‰∏ìÂÆ∂Á∫ß‰ª£Á†ÅÂ§çÁé∞ËÉΩÂäõÔºå‰ª£Ë°®‰∫ÜËá™‰∏ªÁßëÂ≠¶ËΩØ‰ª∂Â∑•Á®ãÁöÑÈáçË¶ÅÈáåÁ®ãÁ¢ë„ÄÇ

### ‚ë° üíº ÊúÄÂÖàËøõÂïÜ‰∏ö‰ª£Á†ÅÊô∫ËÉΩ‰Ωì

**DeepCode: 84.8% vs. ÊúÄ‰Ω≥ÂïÜ‰∏öÊô∫ËÉΩ‰Ωì: 58.7% (+26.1%)**

Âú®5ÁØáËÆ∫ÊñáÁöÑÂ≠êÈõÜ‰∏äÔºåDeepCodeÂ§ßÂπÖË∂ÖË∂äÈ¢ÜÂÖàÁöÑÂïÜ‰∏öÁºñÁ†ÅÂ∑•ÂÖ∑Ôºö
- Cursor: 58.4%
- Claude Code: 58.7%
- Codex: 40.0%
- **DeepCode: 84.8%**

Ëøô‰ª£Ë°®‰∫ÜÁõ∏ÂØπ‰∫éÈ¢ÜÂÖàÂïÜ‰∏ö‰ª£Á†ÅÊô∫ËÉΩ‰ΩìÁöÑ**+26.1%ÊîπËøõ**„ÄÇÊâÄÊúâÂïÜ‰∏öÊô∫ËÉΩ‰ΩìÈÉΩ‰ΩøÁî®Claude Sonnet 4.5ÊàñGPT-5 Codex-highÔºåÁ™ÅÂá∫‰∫Ü**DeepCodeÁöÑÂçìË∂äÊû∂ÊûÑ**‚Äî‚ÄîËÄåÈùûÂü∫Á°ÄÊ®°ÂûãËÉΩÂäõ‚Äî‚ÄîÊé®Âä®‰∫ÜËøô‰∏ÄÊÄßËÉΩÂ∑ÆË∑ù„ÄÇ

### ‚ë¢ üî¨ ÁßëÂ≠¶‰ª£Á†ÅÊô∫ËÉΩ‰Ωì

**DeepCode: 73.5% vs. PaperCoder: 51.1% (+22.4%)**

‰∏éÊúÄÂÖàËøõÁöÑÁßëÂ≠¶‰ª£Á†ÅÂ§çÁé∞Ê°ÜÊû∂PaperCoderÔºà**51.1%**ÔºâÁõ∏ÊØîÔºåDeepCodeËææÂà∞**73.5%**ÔºåÂ±ïÁ§∫‰∫Ü**+22.4%ÁöÑÁõ∏ÂØπÊîπËøõ**„ÄÇËøô‰∏ÄÊòæËëóÂ∑ÆË∑ùÈ™åËØÅ‰∫ÜÊàë‰ª¨ÁªìÂêàËßÑÂàí„ÄÅÂàÜÂ±Ç‰ªªÂä°ÂàÜËß£„ÄÅ‰ª£Á†ÅÁîüÊàêÂíåËø≠‰ª£Ë∞ÉËØïÁöÑÂ§öÊ®°ÂùóÊû∂ÊûÑ‰ºò‰∫éÁÆÄÂçïÁöÑÁÆ°ÈÅìÂºèÊñπÊ≥ï„ÄÇ

### ‚ë£ ü§ñ Âü∫‰∫éLLMÁöÑÊô∫ËÉΩ‰Ωì

**DeepCode: 73.5% vs. ÊúÄ‰Ω≥LLMÊô∫ËÉΩ‰Ωì: 43.3% (+30.2%)**

DeepCodeÊòæËëóË∂ÖË∂äÊâÄÊúâÊµãËØïÁöÑLLMÊô∫ËÉΩ‰ΩìÔºö
- Claude 3.5 Sonnet + IterativeAgent: 27.5%
- o1 + IterativeAgent (36Â∞èÊó∂): 42.4%
- o1 BasicAgent: 43.3%
- **DeepCode: 73.5%**

Áõ∏ÂØπ‰∫éË°®Áé∞ÊúÄ‰Ω≥ÁöÑLLMÊô∫ËÉΩ‰ΩìÁöÑ**+30.2%ÊîπËøõ**Ë°®ÊòéÔºåÂ§çÊùÇÁöÑÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåËÄåÈùûÂª∂ÈïøÁöÑÊé®ÁêÜÊó∂Èó¥ÊàñÊõ¥Â§ßÁöÑÊ®°ÂûãÔºåÂØπ‰∫éÂ§çÊùÇÁöÑ‰ª£Á†ÅÂ§çÁé∞‰ªªÂä°Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ

---

### üéØ **Ëá™‰∏ªÂ§öÊô∫ËÉΩ‰ΩìÂ∑•‰ΩúÊµÅ**

**Èù¢‰∏¥ÁöÑÊåëÊàò**:

- üìÑ **ÂÆûÁé∞Â§çÊùÇÊÄß**: Â∞ÜÂ≠¶ÊúØËÆ∫ÊñáÂíåÂ§çÊùÇÁÆóÊ≥ïËΩ¨Êç¢‰∏∫ÂèØËøêË°å‰ª£Á†ÅÈúÄË¶ÅÂ§ßÈáèÊäÄÊúØÊäïÂÖ•ÂíåÈ¢ÜÂüü‰∏ì‰∏öÁü•ËØÜ

- üî¨ **Á†îÁ©∂Áì∂È¢à**: Á†îÁ©∂‰∫∫ÂëòÂ∞ÜÂÆùË¥µÊó∂Èó¥Ëä±Âú®ÁÆóÊ≥ïÂÆûÁé∞‰∏äÔºåËÄå‰∏çÊòØ‰∏ìÊ≥®‰∫éÊ†∏ÂøÉÁ†îÁ©∂ÂíåÂèëÁé∞Â∑•‰Ωú

- ‚è±Ô∏è **ÂºÄÂèëÂª∂Ëøü**: ‰∫ßÂìÅÂõ¢ÈòüÂú®Ê¶ÇÂøµÂíåÂèØÊµãËØïÂéüÂûã‰πãÈó¥ÁªèÂéÜÈïøÊó∂Èó¥Á≠âÂæÖÔºåÂáèÊÖ¢ÂàõÊñ∞Âë®Êúü

- üîÑ **ÈáçÂ§çÁºñÁ†Å**: ÂºÄÂèëËÄÖÈáçÂ§çÂÆûÁé∞Áõ∏‰ººÁöÑÊ®°ÂºèÂíåÂäüËÉΩÔºåËÄå‰∏çÊòØÂü∫‰∫éÁé∞ÊúâËß£ÂÜ≥ÊñπÊ°àÊûÑÂª∫

**DeepCode** ÈÄöËøá‰∏∫Â∏∏ËßÅÂºÄÂèë‰ªªÂä°Êèê‰æõÂèØÈù†ÁöÑËá™Âä®ÂåñÊù•Ëß£ÂÜ≥Ëøô‰∫õÂ∑•‰ΩúÊµÅÁ®ã‰ΩéÊïàÈóÆÈ¢òÔºåÁÆÄÂåñ‰ªéÊ¶ÇÂøµÂà∞‰ª£Á†ÅÁöÑÂºÄÂèëÂ∑•‰ΩúÊµÅÁ®ã„ÄÇ

<div align="center">

```mermaid
flowchart LR
    A["üìÑ Á†îÁ©∂ËÆ∫Êñá<br/>üí¨ ÊñáÊú¨ÊèêÁ§∫<br/>üåê URLÂíåÊñáÊ°£<br/>üìé Êñá‰ª∂: PDF, DOC, PPTX, TXT, HTML"] --> B["üß† DeepCode<br/>Â§öÊô∫ËÉΩ‰ΩìÂºïÊìé"]
    B --> C["üöÄ ÁÆóÊ≥ïÂÆûÁé∞ <br/>üé® ÂâçÁ´ØÂºÄÂèë <br/>‚öôÔ∏è ÂêéÁ´ØÂºÄÂèë"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
```

</div>

---

## üèóÔ∏è Êû∂ÊûÑ

### üìä **Á≥ªÁªüÊ¶ÇËø∞**

**DeepCode** ÊòØ‰∏Ä‰∏™AIÈ©±Âä®ÁöÑÂºÄÂèëÂπ≥Âè∞ÔºåËá™Âä®Âåñ‰ª£Á†ÅÁîüÊàêÂíåÂÆûÁé∞‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÂ§ÑÁêÜÂ∞ÜÈúÄÊ±ÇËΩ¨Êç¢‰∏∫ÂäüËÉΩÊÄß„ÄÅÁªìÊûÑËâØÂ•Ω‰ª£Á†ÅÁöÑÂ§çÊùÇÊÄßÔºåËÆ©ÊÇ®‰∏ìÊ≥®‰∫éÂàõÊñ∞ËÄåÈùûÂÆûÁé∞ÁªÜËäÇ„ÄÇ

üéØ **ÊäÄÊúØËÉΩÂäõ**:

üß¨ **Á†îÁ©∂Âà∞Áîü‰∫ßÊµÅÊ∞¥Á∫ø**<br>
Â§öÊ®°ÊÄÅÊñáÊ°£ÂàÜÊûêÂºïÊìéÔºå‰ªéÂ≠¶ÊúØËÆ∫Êñá‰∏≠ÊèêÂèñÁÆóÊ≥ïÈÄªËæëÂíåÊï∞Â≠¶Ê®°Âûã„ÄÇÁîüÊàê‰ºòÂåñÁöÑÂÆûÁé∞Ôºå‰ΩøÁî®ÈÄÇÂΩìÁöÑÊï∞ÊçÆÁªìÊûÑÔºåÂêåÊó∂‰øùÊåÅËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÁâπÂæÅ„ÄÇ

ü™Ñ **Ëá™ÁÑ∂ËØ≠Ë®Ä‰ª£Á†ÅÂêàÊàê**<br>
‰ΩøÁî®Âú®Á≤æÈÄâ‰ª£Á†ÅÂ∫ì‰∏äËÆ≠ÁªÉÁöÑÂæÆË∞ÉËØ≠Ë®ÄÊ®°ÂûãËøõË°å‰∏ä‰∏ãÊñáÊÑüÁü•‰ª£Á†ÅÁîüÊàê„ÄÇÂú®ÊîØÊåÅÂ§öÁßçÁºñÁ®ãËØ≠Ë®ÄÂíåÊ°ÜÊû∂ÁöÑÂêåÊó∂‰øùÊåÅÊ®°ÂùóÈó¥Êû∂ÊûÑ‰∏ÄËá¥ÊÄß„ÄÇ

‚ö° **Ëá™Âä®ÂåñÂéüÂûãÂºïÊìé**<br>
Êô∫ËÉΩËÑöÊâãÊû∂Á≥ªÁªüÔºåÁîüÊàêÂåÖÊã¨Êï∞ÊçÆÂ∫ìÊ®°Âºè„ÄÅAPIÁ´ØÁÇπÂíåÂâçÁ´ØÁªÑ‰ª∂ÁöÑÂÆåÊï¥Â∫îÁî®Á®ãÂ∫èÁªìÊûÑ„ÄÇ‰ΩøÁî®‰æùËµñÂàÜÊûêÁ°Æ‰øù‰ªéÂàùÂßãÁîüÊàêÂºÄÂßãÁöÑÂèØÊâ©Â±ïÊû∂ÊûÑ„ÄÇ

üíé **Ë¥®Èáè‰øùËØÅËá™Âä®Âåñ**<br>
ÈõÜÊàêÈùôÊÄÅÂàÜÊûê‰∏éËá™Âä®ÂåñÂçïÂÖÉÊµãËØïÁîüÊàêÂíåÊñáÊ°£ÂêàÊàê„ÄÇÈááÁî®ASTÂàÜÊûêËøõË°å‰ª£Á†ÅÊ≠£Á°ÆÊÄßÊ£ÄÊü•ÂíåÂü∫‰∫éÂ±ûÊÄßÁöÑÊµãËØïËøõË°åÂÖ®Èù¢Ë¶ÜÁõñ„ÄÇ

üîÆ **CodeRAGÈõÜÊàêÁ≥ªÁªü**<br>
È´òÁ∫ßÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºåÁªìÂêàËØ≠‰πâÂêëÈáèÂµåÂÖ•ÂíåÂü∫‰∫éÂõæÁöÑ‰æùËµñÂàÜÊûê„ÄÇ‰ªéÂ§ßËßÑÊ®°‰ª£Á†ÅËØ≠ÊñôÂ∫ì‰∏≠Ëá™Âä®ÂèëÁé∞ÊúÄ‰ºòÂ∫ìÂíåÂÆûÁé∞Ê®°Âºè„ÄÇ

---

### üîß **Ê†∏ÂøÉÊäÄÊúØ**

- üß† **Êô∫ËÉΩÁºñÊéíÊô∫ËÉΩ‰Ωì**: ÂçèË∞ÉÂ∑•‰ΩúÊµÅÈò∂ÊÆµÂíåÂàÜÊûêÈúÄÊ±ÇÁöÑ‰∏≠Â§ÆÂÜ≥Á≠ñÁ≥ªÁªü„ÄÇÈááÁî®Âä®ÊÄÅËßÑÂàíÁÆóÊ≥ïÔºåÊ†πÊçÆ‰∏çÊñ≠ÂèëÂ±ïÁöÑÈ°πÁõÆÂ§çÊùÇÊÄßÂÆûÊó∂Ë∞ÉÊï¥ÊâßË°åÁ≠ñÁï•„ÄÇ‰∏∫ÊØè‰∏™ÂÆûÁé∞Ê≠•È™§Âä®ÊÄÅÈÄâÊã©ÊúÄ‰ºòÂ§ÑÁêÜÁ≠ñÁï•„ÄÇ <br>

- üíæ **È´òÊïàÂÜÖÂ≠òÊú∫Âà∂**: È´òÊïàÁÆ°ÁêÜÂ§ßËßÑÊ®°‰ª£Á†Å‰∏ä‰∏ãÊñáÁöÑÈ´òÁ∫ß‰∏ä‰∏ãÊñáÂ∑•Á®ãÁ≥ªÁªü„ÄÇÂÆûÁé∞ÂàÜÂ±ÇÂÜÖÂ≠òÁªìÊûÑÔºåÂÖ∑ÊúâÊô∫ËÉΩÂéãÁº©ÂäüËÉΩÔºåÁî®‰∫éÂ§ÑÁêÜÂ§çÊùÇ‰ª£Á†ÅÂ∫ì„ÄÇËØ•ÁªÑ‰ª∂ÂÆûÁé∞ÂÆûÁé∞Ê®°ÂºèÁöÑÂç≥Êó∂Ê£ÄÁ¥¢ÔºåÂπ∂Âú®Êâ©Â±ïÂºÄÂèë‰ºöËØù‰∏≠‰øùÊåÅËØ≠‰πâ‰∏ÄËá¥ÊÄß„ÄÇ <br>

- üîç **È´òÁ∫ßCodeRAGÁ≥ªÁªü**: ÂàÜÊûêË∑®Â≠òÂÇ®Â∫ìÂ§çÊùÇÁõ∏‰∫í‰æùËµñÂÖ≥Á≥ªÁöÑÂÖ®Â±Ä‰ª£Á†ÅÁêÜËß£ÂºïÊìé„ÄÇÊâßË°åË∑®‰ª£Á†ÅÂ∫ìÂÖ≥Á≥ªÊò†Â∞ÑÔºå‰ªéÊï¥‰ΩìËßíÂ∫¶ÁêÜËß£Êû∂ÊûÑÊ®°Âºè„ÄÇËØ•Ê®°ÂùóÂà©Áî®‰æùËµñÂõæÂíåËØ≠‰πâÂàÜÊûêÂú®ÂÆûÁé∞ËøáÁ®ã‰∏≠Êèê‰æõÂÖ®Â±ÄÊÑüÁü•ÁöÑ‰ª£Á†ÅÂª∫ËÆÆ„ÄÇ

---

### ü§ñ **DeepCodeÁöÑÂ§öÊô∫ËÉΩ‰ΩìÊû∂ÊûÑ**:

- **üéØ ‰∏≠Â§ÆÁºñÊéíÊô∫ËÉΩ‰Ωì**: ÁºñÊéíÊï¥‰∏™Â∑•‰ΩúÊµÅÁ®ãÊâßË°åÂπ∂ÂÅöÂá∫ÊàòÁï•ÂÜ≥Á≠ñ„ÄÇÂü∫‰∫éËæìÂÖ•Â§çÊùÇÊÄßÂàÜÊûêÂçèË∞É‰∏ìÈó®Êô∫ËÉΩ‰Ωì„ÄÇÂÆûÁé∞Âä®ÊÄÅ‰ªªÂä°ËßÑÂàíÂíåËµÑÊ∫êÂàÜÈÖçÁÆóÊ≥ï„ÄÇ <br>

- **üìù ÊÑèÂõæÁêÜËß£Êô∫ËÉΩ‰Ωì**: ÂØπÁî®Êà∑ÈúÄÊ±ÇËøõË°åÊ∑±Â∫¶ËØ≠‰πâÂàÜÊûê‰ª•Ëß£Á†ÅÂ§çÊùÇÊÑèÂõæ„ÄÇÈÄöËøáÈ´òÁ∫ßNLPÂ§ÑÁêÜÊèêÂèñÂäüËÉΩËßÑËåÉÂíåÊäÄÊúØÁ∫¶Êùü„ÄÇÈÄöËøáÁªìÊûÑÂåñ‰ªªÂä°ÂàÜËß£Â∞ÜÊ®°Á≥äÁöÑ‰∫∫Á±ªÊèèËø∞ËΩ¨Êç¢‰∏∫Á≤æÁ°Æ„ÄÅÂèØÊìç‰ΩúÁöÑÂºÄÂèëËßÑËåÉ„ÄÇ <br>

- **üìÑ ÊñáÊ°£Ëß£ÊûêÊô∫ËÉΩ‰Ωì**: ‰ΩøÁî®È´òÁ∫ßËß£ÊûêËÉΩÂäõÂ§ÑÁêÜÂ§çÊùÇÁöÑÊäÄÊúØÊñáÊ°£ÂíåÁ†îÁ©∂ËÆ∫Êñá„ÄÇ‰ΩøÁî®ÊñáÊ°£ÁêÜËß£Ê®°ÂûãÊèêÂèñÁÆóÊ≥ïÂíåÊñπÊ≥ï„ÄÇÈÄöËøáÊô∫ËÉΩÂÜÖÂÆπÂàÜÊûêÂ∞ÜÂ≠¶ÊúØÊ¶ÇÂøµËΩ¨Êç¢‰∏∫ÂÆûÁî®ÁöÑÂÆûÁé∞ËßÑËåÉ„ÄÇ <br>

- **üèóÔ∏è ‰ª£Á†ÅËßÑÂàíÊô∫ËÉΩ‰Ωì**: ÊâßË°åÊû∂ÊûÑËÆæËÆ°ÂíåÊäÄÊúØÊ†à‰ºòÂåñ„ÄÇÂä®ÊÄÅËßÑÂàíÈÄÇÂ∫îÊÄßÂºÄÂèëË∑ØÁ∫øÂõæ„ÄÇÈÄöËøáËá™Âä®ÂåñËÆæËÆ°Ê®°ÂºèÈÄâÊã©ÊâßË°åÁºñÁ†ÅÊ†áÂáÜÂπ∂ÁîüÊàêÊ®°ÂùóÂåñÁªìÊûÑ„ÄÇ<br>

- **üîç ‰ª£Á†ÅÂèÇËÄÉÊåñÊéòÊô∫ËÉΩ‰Ωì**: ÈÄöËøáÊô∫ËÉΩÊêúÁ¥¢ÁÆóÊ≥ïÂèëÁé∞Áõ∏ÂÖ≥Â≠òÂÇ®Â∫ìÂíåÊ°ÜÊû∂„ÄÇÂàÜÊûê‰ª£Á†ÅÂ∫ìÁöÑÂÖºÂÆπÊÄßÂíåÈõÜÊàêÊΩúÂäõ„ÄÇÂü∫‰∫éÁõ∏‰ººÊÄßÂ∫¶ÈáèÂíåËá™Âä®Âåñ‰æùËµñÂàÜÊûêÊèê‰æõÂª∫ËÆÆ„ÄÇ <br>

- **üìö ‰ª£Á†ÅÁ¥¢ÂºïÊô∫ËÉΩ‰Ωì**: ÊûÑÂª∫ÂèëÁé∞‰ª£Á†ÅÂ∫ìÁöÑÁªºÂêàÁü•ËØÜÂõæË∞±„ÄÇÁª¥Êä§‰ª£Á†ÅÁªÑ‰ª∂‰πãÈó¥ÁöÑËØ≠‰πâÂÖ≥Á≥ª„ÄÇÂÆûÁé∞Êô∫ËÉΩÊ£ÄÁ¥¢Âíå‰∫§ÂèâÂºïÁî®ËÉΩÂäõ„ÄÇ <br>

- **üß¨ ‰ª£Á†ÅÁîüÊàêÊô∫ËÉΩ‰Ωì**: Â∞ÜÊî∂ÈõÜÁöÑ‰ø°ÊÅØÂêàÊàê‰∏∫ÂèØÊâßË°åÁöÑ‰ª£Á†ÅÂÆûÁé∞„ÄÇÂàõÂª∫ÂäüËÉΩÊé•Âè£Âπ∂ÈõÜÊàêÂèëÁé∞ÁöÑÁªÑ‰ª∂„ÄÇÁîüÊàêÂÖ®Èù¢ÁöÑÊµãËØïÂ•ó‰ª∂ÂíåÊñáÊ°£‰ª•Á°Æ‰øùÂèØÈáçÁé∞ÊÄß„ÄÇ

---

#### üõ†Ô∏è **ÂÆûÁé∞Â∑•ÂÖ∑Áü©Èòµ**

**üîß Âü∫‰∫éMCP (Ê®°Âûã‰∏ä‰∏ãÊñáÂçèËÆÆ) È©±Âä®**

DeepCodeÂà©Áî®**Ê®°Âûã‰∏ä‰∏ãÊñáÂçèËÆÆ (MCP)** Ê†áÂáÜ‰∏éÂêÑÁßçÂ∑•ÂÖ∑ÂíåÊúçÂä°Êó†ÁºùÈõÜÊàê„ÄÇËøôÁßçÊ†áÂáÜÂåñÊñπÊ≥ïÁ°Æ‰øùAIÊô∫ËÉΩ‰ΩìÂíåÂ§ñÈÉ®Á≥ªÁªü‰πãÈó¥ÁöÑÂèØÈù†ÈÄö‰ø°ÔºåÂÆûÁé∞Âº∫Â§ßÁöÑËá™Âä®ÂåñËÉΩÂäõ„ÄÇ

##### üì° **MCPÊúçÂä°Âô®ÂíåÂ∑•ÂÖ∑**

| üõ†Ô∏è **MCPÊúçÂä°Âô®** | üîß **‰∏ªË¶ÅÂäüËÉΩ** | üí° **ÁõÆÁöÑÂíåËÉΩÂäõ** |
|-------------------|-------------------------|-------------------------------|
| **üîç brave** | WebÊêúÁ¥¢ÂºïÊìé | ÈÄöËøáBraveÊêúÁ¥¢APIËøõË°åÂÆûÊó∂‰ø°ÊÅØÊ£ÄÁ¥¢ |
| **üåê bocha-mcp** | Êõø‰ª£ÊêúÁ¥¢ | ÂÖ∑ÊúâÁã¨Á´ãAPIËÆøÈóÆÁöÑËæÖÂä©ÊêúÁ¥¢ÈÄâÈ°π |
| **üìÇ filesystem** | Êñá‰ª∂Á≥ªÁªüÊìç‰Ωú | Êú¨Âú∞Êñá‰ª∂ÂíåÁõÆÂΩïÁÆ°ÁêÜÔºåËØª/ÂÜôÊìç‰Ωú |
| **üåê fetch** | WebÂÜÖÂÆπÊ£ÄÁ¥¢ | ‰ªéURLÂíåWebËµÑÊ∫êËé∑ÂèñÂíåÊèêÂèñÂÜÖÂÆπ |
| **üì• github-downloader** | Â≠òÂÇ®Â∫ìÁÆ°ÁêÜ | ÂÖãÈöÜÂíå‰∏ãËΩΩGitHubÂ≠òÂÇ®Â∫ìËøõË°åÂàÜÊûê |
| **üìã file-downloader** | ÊñáÊ°£Â§ÑÁêÜ | ‰∏ãËΩΩÊñá‰ª∂(PDF„ÄÅDOCXÁ≠â)Âπ∂ËΩ¨Êç¢‰∏∫Markdown |
| **‚ö° command-executor** | Á≥ªÁªüÂëΩ‰ª§ | ÊâßË°åbash/shellÂëΩ‰ª§ËøõË°åÁéØÂ¢ÉÁÆ°ÁêÜ |
| **üß¨ code-implementation** | ‰ª£Á†ÅÁîüÊàê‰∏≠ÂøÉ | ÂÖ∑ÊúâÊâßË°åÂíåÊµãËØïÁöÑÁªºÂêà‰ª£Á†ÅÂ§çÁé∞ |
| **üìö code-reference-indexer** | Êô∫ËÉΩ‰ª£Á†ÅÊêúÁ¥¢ | ‰ª£Á†ÅÂ≠òÂÇ®Â∫ìÁöÑÊô∫ËÉΩÁ¥¢ÂºïÂíåÊêúÁ¥¢ |
| **üìÑ document-segmentation** | Êô∫ËÉΩÊñáÊ°£ÂàÜÊûê | Â§ßÂûãËÆ∫ÊñáÂíåÊäÄÊúØÊñáÊ°£ÁöÑÊô∫ËÉΩÊñáÊ°£ÂàÜÂâ≤ |

##### üîß **‰º†ÁªüÂ∑•ÂÖ∑ÂäüËÉΩ** *(‰æõÂèÇËÄÉ)*

| üõ†Ô∏è **ÂäüËÉΩ** | üéØ **‰ΩøÁî®‰∏ä‰∏ãÊñá** |
|-----------------|---------------------|
| **üìÑ read_code_mem** | ‰ªéÂÜÖÂ≠òÈ´òÊïàÊ£ÄÁ¥¢‰ª£Á†Å‰∏ä‰∏ãÊñá |
| **‚úçÔ∏è write_file** | Áõ¥Êé•Êñá‰ª∂ÂÜÖÂÆπÁîüÊàêÂíå‰øÆÊîπ |
| **üêç execute_python** | Python‰ª£Á†ÅÊµãËØïÂíåÈ™åËØÅ |
| **üìÅ get_file_structure** | È°πÁõÆÁªìÊûÑÂàÜÊûêÂíåÁªÑÁªá |
| **‚öôÔ∏è set_workspace** | Âä®ÊÄÅÂ∑•‰ΩúÁ©∫Èó¥ÂíåÁéØÂ¢ÉÈÖçÁΩÆ |
| **üìä get_operation_history** | ËøáÁ®ãÁõëÊéßÂíåÊìç‰ΩúË∑üË∏™ |


---

üéõÔ∏è **Â§öÁïåÈù¢Ê°ÜÊû∂**<br>
ÂÖ∑ÊúâCLIÂíåWebÂâçÁ´ØÁöÑRESTful APIÔºåÂÖ∑ÊúâÂÆûÊó∂‰ª£Á†ÅÊµÅ„ÄÅ‰∫§‰∫íÂºèË∞ÉËØïÂíåÂèØÊâ©Â±ïÊèí‰ª∂Êû∂ÊûÑÔºåÁî®‰∫éCI/CDÈõÜÊàê„ÄÇ

**üöÄ Â§öÊô∫ËÉΩ‰ΩìÊô∫ËÉΩÊµÅÊ∞¥Á∫ø:**

<div align="center">

### üåü **Êô∫ËÉΩÂ§ÑÁêÜÊµÅÁ®ã**

<table align="center" width="100%" style="border: none; border-collapse: collapse;">
<tr>
<td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;">
üí° <strong>ËæìÂÖ•Â±Ç</strong><br/>
üìÑ Á†îÁ©∂ËÆ∫Êñá ‚Ä¢ üí¨ Ëá™ÁÑ∂ËØ≠Ë®Ä ‚Ä¢ üåê URL ‚Ä¢ üìã ÈúÄÊ±Ç
</td>
</tr>
<tr><td colspan="3" height="20"></td></tr>
<tr>
<td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;">
üéØ <strong>‰∏≠Â§ÆÁºñÊéí</strong><br/>
ÊàòÁï•ÂÜ≥Á≠ñÂà∂ÂÆö ‚Ä¢ Â∑•‰ΩúÊµÅÁ®ãÂçèË∞É ‚Ä¢ Êô∫ËÉΩ‰ΩìÁÆ°ÁêÜ
</td>
</tr>
<tr><td colspan="3" height="15"></td></tr>
<tr>
<td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;">
üìù <strong>ÊñáÊú¨ÂàÜÊûê</strong><br/>
<small>ÈúÄÊ±ÇÂ§ÑÁêÜ</small>
</td>
<td width="10"></td>
<td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;">
üìÑ <strong>ÊñáÊ°£ÂàÜÊûê</strong><br/>
<small>ËÆ∫ÊñáÂíåËßÑËåÉÂ§ÑÁêÜ</small>
</td>
</tr>
<tr><td colspan="3" height="15"></td></tr>
<tr>
<td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;">
üìã <strong>Â§çÁé∞ËßÑÂàí</strong><br/>
Ê∑±Â∫¶ËÆ∫ÊñáÂàÜÊûê ‚Ä¢ ‰ª£Á†ÅÈúÄÊ±ÇËß£Êûê ‚Ä¢ Â§çÁé∞Á≠ñÁï•ÂºÄÂèë
</td>
</tr>
<tr><td colspan="3" height="15"></td></tr>
<tr>
<td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;">
üîç <strong>ÂèÇËÄÉÂàÜÊûê</strong><br/>
<small>Â≠òÂÇ®Â∫ìÂèëÁé∞</small>
</td>
<td width="10"></td>
<td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;">
üìö <strong>‰ª£Á†ÅÁ¥¢Âºï</strong><br/>
<small>Áü•ËØÜÂõæË∞±ÊûÑÂª∫</small>
</td>
</tr>
<tr><td colspan="3" height="15"></td></tr>
<tr>
<td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;">
üß¨ <strong>‰ª£Á†ÅÂÆûÁé∞</strong><br/>
ÂÆûÁé∞ÁîüÊàê ‚Ä¢ ÊµãËØï ‚Ä¢ ÊñáÊ°£
</td>
</tr>
<tr><td colspan="3" height="15"></td></tr>
<tr>
<td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;">
‚ö° <strong>ËæìÂá∫‰∫§‰ªò</strong><br/>
üì¶ ÂÆåÊï¥‰ª£Á†ÅÂ∫ì ‚Ä¢ üß™ ÊµãËØïÂ•ó‰ª∂ ‚Ä¢ üìö ÊñáÊ°£ ‚Ä¢ üöÄ ÈÉ®ÁΩ≤Â∞±Áª™
</td>
</tr>
</table>

</div>

<div align="center">
<br/>

### üîÑ **ÊµÅÁ®ãÊô∫ËÉΩÁâπÊÄß**

<table align="center" style="border: none;">
<tr>
<td align="center" width="25%" style="padding: 15px;">
<div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;">
<h4>üéØ Ëá™ÈÄÇÂ∫îÊµÅÁ®ã</h4>
<p><small>Âü∫‰∫éËæìÂÖ•Â§çÊùÇÊÄßÁöÑÂä®ÊÄÅÊô∫ËÉΩ‰ΩìÈÄâÊã©</small></p>
</div>
</td>
<td align="center" width="25%" style="padding: 15px;">
<div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;">
<h4>üß† Êô∫ËÉΩÂçèË∞É</h4>
<p><small>Êô∫ËÉΩ‰ªªÂä°ÂàÜÈÖçÂíåÂπ∂Ë°åÂ§ÑÁêÜ</small></p>
</div>
</td>
<td align="center" width="25%" style="padding: 15px;">
<div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;">
<h4>üîç ‰∏ä‰∏ãÊñáÊÑüÁü•</h4>
<p><small>ÈÄöËøáCodeRAGÈõÜÊàêÁöÑÊ∑±Â∫¶ÁêÜËß£</small></p>
</div>
</td>
<td align="center" width="25%" style="padding: 15px;">
<div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;">
<h4>‚ö° Ë¥®Èáè‰øùËØÅ</h4>
<p><small>ÂÖ®Á®ãËá™Âä®ÂåñÊµãËØïÂíåÈ™åËØÅ</small></p>
</div>
</td>
</tr>
</table>

</div>

---

## üöÄ Âø´ÈÄüÂºÄÂßã

### üìã **ÂâçÁΩÆÊù°‰ª∂**

Âú®ÂÆâË£Ö DeepCode ‰πãÂâçÔºåËØ∑Á°Æ‰øùÊÇ®Â∑≤ÂÆâË£Ö‰ª•‰∏ãËΩØ‰ª∂Ôºö

| Ë¶ÅÊ±Ç | ÁâàÊú¨ | Áî®ÈÄî |
|------|------|------|
| **Python** | 3.9+ | Ê†∏ÂøÉËøêË°åÁéØÂ¢É |
| **Node.js** | 18+ | Êñ∞Áâà UI ÂâçÁ´Ø |
| **npm** | 8+ | ÂåÖÁÆ°ÁêÜÂ∑•ÂÖ∑ |

```bash
# Ê£ÄÊü•ÊÇ®ÁöÑÁâàÊú¨
python --version   # Â∫î‰∏∫ 3.9+
node --version     # Â∫î‰∏∫ 18+
npm --version      # Â∫î‰∏∫ 8+
```

<details>
<summary><strong>üì• ÂÆâË£Ö Node.jsÔºàÂ¶ÇÊûúÊú™ÂÆâË£ÖÔºâ</strong></summary>

```bash
# macOS (‰ΩøÁî® Homebrew)
brew install node

# Ubuntu/Debian
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs

# Windows
# ‰ªé https://nodejs.org/ ‰∏ãËΩΩÂÆâË£Ö
```

</details>

### üì¶ **Ê≠•È™§1: ÂÆâË£Ö**

ÈÄâÊã©‰ª•‰∏ã‰ªª‰∏ÄÂÆâË£ÖÊñπÂºèÔºö

#### ‚ö° **Áõ¥Êé•ÂÆâË£Ö (Êé®Ëçê)**

```bash
# üöÄ Áõ¥Êé•ÂÆâË£Ö DeepCode ÂåÖ
pip install deepcode-hku

# üîë ‰∏ãËΩΩÈÖçÁΩÆÊñá‰ª∂
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml
```

#### üîß **ÂºÄÂèëÂÆâË£Ö (‰ªéÊ∫êÁ†Å)**

<details>
<summary><strong>üìÇ ÁÇπÂáªÂ±ïÂºÄÂºÄÂèëÂÆâË£ÖÈÄâÈ°π</strong></summary>

##### üî• **‰ΩøÁî® UV (ÂºÄÂèëÊé®Ëçê)**

```bash
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

curl -LsSf https://astral.sh/uv/install.sh | sh
uv venv --python=3.13
source .venv/bin/activate  # Windows‰∏ã: .venv\Scripts\activate
uv pip install -r requirements.txt

# ÂÆâË£ÖÂâçÁ´Ø‰æùËµñ
npm install --prefix new_ui/frontend
```

##### üêç **‰ΩøÁî®‰º†Áªü pip**

```bash
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

pip install -r requirements.txt

# ÂÆâË£ÖÂâçÁ´Ø‰æùËµñ
npm install --prefix new_ui/frontend
```

</details>

### üîß **Ê≠•È™§2: ÈÖçÁΩÆ**

> ‰ª•‰∏ãÈÖçÁΩÆÈÄÇÁî®‰∫é**ÊâÄÊúâÂÆâË£ÖÊñπÂºè**Ôºàpip„ÄÅUV„ÄÅÊ∫êÁ†ÅÂÆâË£ÖÂíå Docker ÂùáÈÄöÁî®Ôºâ„ÄÇ

#### üîë API ÂØÜÈí• *ÔºàÂøÖÈúÄÔºâ*

ÁºñËæë `mcp_agent.secrets.yaml`ÔºåÂ°´ÂÖ•‰Ω†ÁöÑ API ÂØÜÈí•Ôºö

```yaml
# Ëá≥Â∞ëÈúÄË¶ÅÈÖçÁΩÆ‰∏Ä‰∏™ LLM Êèê‰æõÂïÜÁöÑ API Key
openai:
  api_key: "your_openai_api_key"
  base_url: "https://openrouter.ai/api/v1"  # ÂèØÈÄâ: Áî®‰∫é OpenRouter ÊàñËá™ÂÆö‰πâÁ´ØÁÇπ

anthropic:
  api_key: "your_anthropic_api_key"  # Áî®‰∫é Claude Ê®°Âûã

google:
  api_key: "your_google_api_key"     # Áî®‰∫é Gemini Ê®°Âûã
```

#### ü§ñ LLM Êèê‰æõÂïÜ *ÔºàÂèØÈÄâÔºâ*

ÁºñËæë `mcp_agent.config.yaml` ÈÄâÊã©‰Ω†ÂÅèÂ•ΩÁöÑ LLM Êèê‰æõÂïÜÔºàÁ¨¨ ~106 Ë°åÔºâÔºö

```yaml
# ÈÄâÈ°π: "google", "anthropic", "openai"
# Â¶ÇÊûúÊú™ËÆæÁΩÆÊàñ‰∏çÂèØÁî®ÔºåÂ∞ÜËá™Âä®ÂõûÈÄÄÂà∞Á¨¨‰∏Ä‰∏™ÂèØÁî®ÁöÑÊèê‰æõÂïÜ
llm_provider: "google"
```

#### üîç ÊêúÁ¥¢ API ÂØÜÈí• *ÔºàÂèØÈÄâÔºâ*

Âú® `mcp_agent.config.yaml` ‰∏≠ÈÖçÁΩÆ Web ÊêúÁ¥¢Ôºö

```yaml
# Brave ÊêúÁ¥¢ (ÈªòËÆ§) ‚Äî Âú® brave.env ÈÉ®ÂàÜËÆæÁΩÆ (Á¨¨ ~28 Ë°å)
brave:
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# Bocha-MCP (Êõø‰ª£) ‚Äî Âú® bocha-mcp.env ÈÉ®ÂàÜËÆæÁΩÆ (Á¨¨ ~74 Ë°å)
bocha-mcp:
  env:
    BOCHA_API_KEY: "your_bocha_api_key_here"
```

#### üìÑ ÊñáÊ°£ÂàÜÂâ≤ *ÔºàÂèØÈÄâÔºâ*

Âú® `mcp_agent.config.yaml` ‰∏≠ÊéßÂà∂ÊñáÊ°£Â§ÑÁêÜÔºö

```yaml
document_segmentation:
  enabled: true          # true/false ‚Äî ÊòØÂê¶‰ΩøÁî®Êô∫ËÉΩÊñáÊ°£ÂàÜÂâ≤
  size_threshold_chars: 50000  # Ëß¶ÂèëÂàÜÂâ≤ÁöÑÊñáÊ°£Â§ßÂ∞èÈòàÂÄº
```

<details>
<summary><strong>ü™ü Windows Áî®Êà∑: È¢ùÂ§ñÁöÑ MCP ÊúçÂä°Âô®ÈÖçÁΩÆ</strong></summary>

Â¶ÇÊûúÊÇ®‰ΩøÁî® WindowsÔºåÂèØËÉΩÈúÄË¶ÅÂú® `mcp_agent.config.yaml` ‰∏≠ÊâãÂä®ÈÖçÁΩÆ MCP ÊúçÂä°Âô®:

```bash
# 1. ÂÖ®Â±ÄÂÆâË£Ö MCP ÊúçÂä°Âô®
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. ÊâæÂà∞ÊÇ®ÁöÑÂÖ®Â±Ä node_modules Ë∑ØÂæÑ
npm -g root
```

ÁÑ∂ÂêéÊõ¥Êñ∞ÊÇ®ÁöÑ `mcp_agent.config.yaml` ‰ΩøÁî®ÁªùÂØπË∑ØÂæÑ:

```yaml
mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
```

> **Ê≥®ÊÑè**: Â∞ÜË∑ØÂæÑÊõøÊç¢‰∏∫Ê≠•È™§ 2 ‰∏≠ÊÇ®ÂÆûÈôÖÁöÑÂÖ®Â±Ä node_modules Ë∑ØÂæÑ„ÄÇ

</details>

<details>
<summary><strong>üîç ÊêúÁ¥¢ÊúçÂä°Âô®ÈÖçÁΩÆÔºàÂèØÈÄâÔºâ</strong></summary>

DeepCode ÊîØÊåÅÂ§ö‰∏™ÊêúÁ¥¢ÊúçÂä°Âô®ËøõË°å Web ÊêúÁ¥¢ÂäüËÉΩ„ÄÇÊÇ®ÂèØ‰ª•Âú® `mcp_agent.config.yaml` ‰∏≠ÈÖçÁΩÆÈ¶ñÈÄâÈÄâÈ°π:

```yaml
# ÈªòËÆ§ÊêúÁ¥¢ÊúçÂä°Âô®ÈÖçÁΩÆ
# ÈÄâÈ°π: "brave" Êàñ "bocha-mcp"
default_search_server: "brave"
```

**ÂèØÁî®ÈÄâÈ°π:**
- **üîç Brave ÊêúÁ¥¢** (`"brave"`): ÂÖ∑ÊúâÈ´òË¥®ÈáèÊêúÁ¥¢ÁªìÊûúÁöÑÈªòËÆ§ÈÄâÈ°π„ÄÇÈúÄË¶Å `BRAVE_API_KEY`„ÄÇÊé®ËçêÁªôÂ§ßÂ§öÊï∞Áî®Êà∑„ÄÇ
- **üåê Bocha-MCP** (`"bocha-mcp"`): Êõø‰ª£ÊêúÁ¥¢ÊúçÂä°Âô®„ÄÇÈúÄË¶Å `BOCHA_API_KEY`„ÄÇ‰ΩøÁî®Êú¨Âú∞ Python ÊúçÂä°Âô®ÂÆûÁé∞„ÄÇ

**ÂÆåÊï¥ MCP ÊúçÂä°Âô®ÈÖçÁΩÆÔºàmcp_agent.config.yamlÔºâ:**
```yaml
# Brave ÊêúÁ¥¢ (ÈªòËÆ§) - Á¨¨ 28 Ë°åÂ∑¶Âè≥
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# Bocha-MCP (Êõø‰ª£) - Á¨¨ 74 Ë°åÂ∑¶Âè≥
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
```

> **üí° ÊèêÁ§∫**: ‰∏§‰∏™ÊêúÁ¥¢ÊúçÂä°Âô®ÈÉΩÈúÄË¶Å API ÂØÜÈí•ÈÖçÁΩÆ„ÄÇÈÄâÊã©ÊúÄÈÄÇÂêàÊÇ®ÁöÑ API ËÆøÈóÆÂíåÈúÄÊ±ÇÁöÑÈÄâÈ°π„ÄÇ

</details>

### ‚ö° **Ê≠•È™§3: ÂêØÂä®Â∫îÁî®Á®ãÂ∫è**

ÈÄâÊã©ÊÇ®ÂÅèÂ•ΩÁöÑÂêØÂä®ÊñπÂºèÔºö

<table width="100%">
<tr>
<th width="33%">üê≥ Docker (Êé®Ëçê)</th>
<th width="33%">üöÄ Êú¨Âú∞ (<code>deepcode</code> ÂëΩ‰ª§)</th>
<th width="33%">üõ†Ô∏è ÂÖ∂‰ªñÊñπÂºè</th>
</tr>
<tr><td>

Êó†ÈúÄ Python/Node ‚Äî ‰∏ÄÂàáÂú®ÂÆπÂô®ÂÜÖ„ÄÇ

```bash
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/
cp mcp_agent.secrets.yaml.example \
   mcp_agent.secrets.yaml
# ÁºñËæëÂ°´ÂÖ• API Key

./deepcode_docker/run_docker.sh
# ËÆøÈóÆ ‚Üí http://localhost:8000
```

</td><td>

È¶ñÊ¨°ËøêË°åËá™Âä®ÂÆâË£Ö‰æùËµñ„ÄÇ

```bash
deepcode
# ÂâçÁ´Ø ‚Üí http://localhost:5173
# ÂêéÁ´Ø ‚Üí http://localhost:8000
# Ctrl+C ÂÅúÊ≠¢
```

ÁâπÊÄßÔºöÁî®Êà∑‰∫§‰∫íÂæ™ÁéØ„ÄÅÂÆûÊó∂ËøõÂ∫¶„ÄÅÂÜÖËÅîÂØπËØù„ÄÇ

</td><td>

```bash
# macOS / Linux
./run.sh
# Êàñ: python deepcode.py

# Windows
run.bat
# Êàñ: python deepcode.py

# ÁªèÂÖ∏ Streamlit UI
deepcode --classic

# CLI Ê®°Âºè
deepcode --cli
# Êàñ: python cli/main_cli.py
```

</td></tr>
</table>

<details>
<summary><strong>üê≥ Docker ÁÆ°ÁêÜÂëΩ‰ª§</strong></summary>

```bash
./deepcode_docker/run_docker.sh stop      # ÂÅúÊ≠¢
./deepcode_docker/run_docker.sh restart   # ÈáçÂêØÔºàÈÖçÁΩÆÊõ¥ÊîπÊó†ÈúÄÈáçÂª∫Ôºâ
./deepcode_docker/run_docker.sh --build   # Âº∫Âà∂ÈáçÂª∫
./deepcode_docker/run_docker.sh logs      # ÂÆûÊó∂Êó•Âøó
./deepcode_docker/run_docker.sh status    # ÂÅ•Â∫∑Ê£ÄÊü•
./deepcode_docker/run_docker.sh clean     # Âà†Èô§ÂÆπÂô®ÂíåÈïúÂÉè
```

ÊàñÁõ¥Êé•‰ΩøÁî® Docker ComposeÔºö
```bash
docker compose -f deepcode_docker/docker-compose.yml up --build   # ÊûÑÂª∫Âπ∂ÂêØÂä®
docker compose -f deepcode_docker/docker-compose.yml down         # ÂÅúÊ≠¢
docker compose -f deepcode_docker/docker-compose.yml logs -f      # Êü•ÁúãÊó•Âøó
```

> **üí°** ÈÖçÁΩÆÊñá‰ª∂‰ª•Âç∑ÊñπÂºèÊåÇËΩΩ ‚Äî ÁºñËæëÂêéÈáçÂêØÂç≥ÂèØÔºåÊó†ÈúÄÈáçÂª∫„ÄÇ
> **üí°** Windows Áî®Êà∑ÔºöÂ¶ÇÊûúËÑöÊú¨‰∏çÂèØÁî®ÔºåÂèØÁõ¥Êé•ËøêË°å `docker compose` ÂëΩ‰ª§„ÄÇ

</details>

### üéØ **Ê≠•È™§4: ÁîüÊàê‰ª£Á†Å**

1. **üìÑ ËæìÂÖ•** ‚Äî ‰∏ä‰º†Á†îÁ©∂ËÆ∫Êñá„ÄÅËæìÂÖ•ÈúÄÊ±ÇÔºåÊàñÁ≤òË¥¥ URL
2. **ü§ñ Â§ÑÁêÜ** ‚Äî Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÂàÜÊûê„ÄÅËßÑÂàíÂπ∂ÁîüÊàê
3. **‚ö° ËæìÂá∫** ‚Äî Êé•Êî∂Â∏¶ÊµãËØïÂíåÊñáÊ°£ÁöÑÁîü‰∫ßÂ∞±Áª™‰ª£Á†Å

---

### üîß **Â∏∏ËßÅÈóÆÈ¢òÊéíÊü•**

<details>
<summary><strong>‚ùì Â∏∏ËßÅÈóÆÈ¢ò‰∏éËß£ÂÜ≥ÊñπÊ°à</strong></summary>

| ÈóÆÈ¢ò | ÂéüÂõ† | Ëß£ÂÜ≥ÊñπÊ°à |
|---|---|---|
| Docker ÊûÑÂª∫Â§±Ë¥• `tsc: not found` | ÊûÑÂª∫ÁºìÂ≠òÊçüÂùè | `docker builder prune -f` ÁÑ∂ÂêéÁî® `--no-cache` ÈáçÂª∫ |
| `error during connect` / `cannot find the file` | Docker Desktop Êú™ËøêË°å | ÂêØÂä® Docker DesktopÔºåÁ≠âÂæÖÂ∞±Áª™ÂêéÈáçËØï |
| ÂâçÁ´ØÁ©∫ÁôΩÈ°µÈù¢ | `node_modules` ÊçüÂùè | `cd new_ui/frontend && rm -rf node_modules && npm install` |
| `ERR_CONNECTION_REFUSED` | Á´ØÂè£ÈîôËØØ/ÂêéÁ´ØÊú™ËøêË°å | Docker: `http://localhost:8000`„ÄÇÊú¨Âú∞: `http://localhost:5173` |
| `npm install` ‚Üí `Could not read package.json` | ÁõÆÂΩïÈîôËØØ | ‰ΩøÁî® `npm install --prefix new_ui/frontend` |
| Windows: MCP ÊúçÂä°Âô®Êó†Ê≥ïÂ∑•‰Ωú | ÈúÄË¶ÅÁªùÂØπË∑ØÂæÑ | ÂèÇËßÅ‰∏äÊñπ [Windows MCP ÈÖçÁΩÆ](#-Ê≠•È™§2-ÈÖçÁΩÆ) |

</details>

---

## ü§ñ nanobot ÈõÜÊàêÔºàÈ£û‰π¶ËÅäÂ§©Êú∫Âô®‰∫∫Ôºâ

**Áõ¥Êé•Âú®È£û‰π¶‰∏≠‰ΩøÁî® DeepCode ‚Äî ÂèëÈÄÅÊ∂àÊÅØÔºåËé∑Âèñ‰ª£Á†ÅÔºÅ**

[nanobot](https://github.com/HKUDS/nanobot) ÊòØ‰∏Ä‰∏™Ë∂ÖËΩªÈáèÁ∫ß AI Âä©ÊâãÔºåÁé∞Â∑≤‰∏é DeepCode Ê∑±Â∫¶ÈõÜÊàê„ÄÇÈÄöËøáÈ£û‰π¶ËÅäÂ§©ÔºåÊÇ®ÂèØ‰ª•Ôºö
- üöÄ Êèê‰∫§**ËÆ∫ÊñáËΩ¨‰ª£Á†Å**‰ªªÂä°Ôºà`paper2code`Ôºâ‚Äî Á≤òË¥¥ arXiv ÈìæÊé•Âç≥ÂèØ
- üí¨ ÂêØÂä®**ÂØπËØùËΩ¨‰ª£Á†Å**Ôºà`chat2code`Ôºâ‚Äî Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÈúÄÊ±Ç
- üìä ÂÆûÊó∂Êü•ËØ¢‰ªªÂä°Áä∂ÊÄÅÔºà`deepcode_status`Ôºâ‚Äî Ëé∑ÂèñËøõÂ∫¶ÂíåÁªìÊûú
- ‚úÖ ÂìçÂ∫î DeepCode ‰∫§‰∫íÊèêÁ§∫ ‚Äî ÂΩì AI ÈúÄË¶ÅÊæÑÊ∏ÖÈúÄÊ±ÇÊó∂Áõ¥Êé•Âú®ËÅäÂ§©‰∏≠ÂõûÁ≠î

### üèóÔ∏è Êû∂ÊûÑÊ¶ÇËßà

```mermaid
flowchart TB
    subgraph ChatPlatforms[üí¨ ËÅäÂ§©Âπ≥Âè∞]
        Feishu[<b>È£û‰π¶</b><br/>üì± ÂΩìÂâçÊîØÊåÅ]
        Telegram[Telegram<br/>üîú Âç≥Â∞ÜÊîØÊåÅ]
        Discord[Discord<br/>üîú Âç≥Â∞ÜÊîØÊåÅ]
    end

    subgraph NanobotCore[ü§ñ Nanobot Ê†∏ÂøÉ]
        LLM[LLM Êé®ÁêÜÂºïÊìé<br/>Claude / GPT / Minimax]
        Tools[Â∑•ÂÖ∑Â±Ç<br/>web_fetch / code_executor / deepcode]
    end

    subgraph DeepCodeEngine[‚ö° DeepCode ÂºïÊìé]
        API[HTTP API<br/>‰ªªÂä°Êèê‰∫§ & Êü•ËØ¢]
        Agents[Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü<br/>ËßÑÂàí / ÂàÜÊûê / ÁîüÊàê]
        Output[‰ª£Á†ÅËæìÂá∫<br/>ÊµãËØï + ÊñáÊ°£]
    end

    Feishu -->|WebSocket| NanobotCore
    Telegram -.->|Êú™Êù•ÈõÜÊàê| NanobotCore
    Discord -.->|Êú™Êù•ÈõÜÊàê| NanobotCore

    NanobotCore -->|Ë∞ÉÁî® deepcode_* Â∑•ÂÖ∑| DeepCodeEngine
    DeepCodeEngine -->|ËøîÂõûÁªìÊûú & ËøõÂ∫¶| NanobotCore
    NanobotCore -->|Êé®ÈÄÅÊ∂àÊÅØ| Feishu

    style Feishu fill:#0EA5E9,stroke:#0284c7,stroke-width:3px,color:#fff
    style NanobotCore fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style DeepCodeEngine fill:#10b981,stroke:#059669,stroke-width:2px,color:#fff
    style Telegram fill:#d1d5db,stroke:#9ca3af,stroke-width:1px,color:#4b5563,stroke-dasharray: 5 5
    style Discord fill:#d1d5db,stroke:#9ca3af,stroke-width:1px,color:#4b5563,stroke-dasharray: 5 5
```

> üéØ **ÂΩìÂâçÊîØÊåÅ**: È£û‰π¶ÔºàFeishu / LarkÔºâ
> üîÆ **Êû∂ÊûÑÈ¢ÑÁïô**: Telegram Âíå Discord ËäÇÁÇπ‰∏∫Êú™Êù•Êâ©Â±ï‰øùÁïô

---

### üìã ÂâçÁΩÆÊù°‰ª∂

- ‚úÖ DeepCode ÂêéÁ´ØÊ≠£Âú®ËøêË°åÔºàËßÅ‰∏äÊñπ [Âø´ÈÄüÂºÄÂßã](#-Âø´ÈÄüÂºÄÂßã)Ôºâ
- ‚úÖ È£û‰π¶‰ºÅ‰∏öÂ∫îÁî®ÔºàÊàñÁßüÁî®Â∫îÁî®Ôºâ‚Äî ÂÖçË¥πÂàõÂª∫
- ‚úÖ LLM API ÂØÜÈí•ÔºàOpenRouter / Claude / MinimaxÔºâ

---

### üöÄ ‰∏âÊ≠•ÂÆåÊàêËÆæÁΩÆ

#### **Step 1 ¬∑ ÂàõÂª∫È£û‰π¶Êú∫Âô®‰∫∫**

<details>
<summary><strong>üì± ÁÇπÂáªÂ±ïÂºÄÈ£û‰π¶Â∫îÁî®ÂàõÂª∫Ê≠•È™§</strong></summary>

1. ÁôªÂΩï [È£û‰π¶ÂºÄÊîæÂπ≥Âè∞](https://open.feishu.cn/app)
2. ÁÇπÂáª **ÂàõÂª∫‰ºÅ‰∏öËá™Âª∫Â∫îÁî®**
3. Â°´ÂÜôÂ∫îÁî®ÂêçÁß∞ÂíåÊèèËø∞Ôºå‰∏ä‰º†ÂõæÊ†á
4. ËøõÂÖ• **Âá≠ËØÅ‰∏éÂü∫Á°Ä‰ø°ÊÅØ** È°µÈù¢ÔºåÂ§çÂà∂Ôºö
   - `App ID`
   - `App Secret`
5. ËøõÂÖ• **‰∫ã‰ª∂ËÆ¢ÈòÖ** È°µÈù¢Ôºö
   - **ËØ∑Ê±ÇÂú∞ÂùÄ URL**: `http://your-server-ip:8081/feishu/event`ÔºàÂÖ¨ÁΩëÂèØËÆøÈóÆÔºâ
   - **Ê∂àÊÅØÂä†ÂØÜ**: Â§çÂà∂ `Encrypt Key` Âíå `Verification Token`
6. ËøõÂÖ• **ÊùÉÈôêÁÆ°ÁêÜ**ÔºåÂºÄÈÄö‰ª•‰∏ãÊùÉÈôêÔºö
   - `im:message`ÔºàÊé•Êî∂Ê∂àÊÅØÔºâ
   - `im:message:send_as_bot`ÔºàÂèëÈÄÅÊ∂àÊÅØÔºâ
   - `im:chat`ÔºàËé∑ÂèñÁæ§‰ø°ÊÅØÔºâ
7. **ÂèëÂ∏ÉÁâàÊú¨** ‚Üí Á≠âÂæÖÁÆ°ÁêÜÂëòÂÆ°Ê†∏ÈÄöËøá

> üí° **ÂºÄÂèëÁéØÂ¢É**: ÂèØ‰ΩøÁî® [ngrok](https://ngrok.com/) Êàñ [localhost.run](https://localhost.run/) Â∞ÜÊú¨Âú∞ 8081 Á´ØÂè£Êò†Â∞ÑÂà∞ÂÖ¨ÁΩë„ÄÇ

</details>

---

#### **Step 2 ¬∑ ÈÖçÁΩÆ**

ÁºñËæëÈ°πÁõÆÊ†πÁõÆÂΩïÁöÑ `nanobot_config.json`:

```json
{
  "channels": [
    {
      "type": "feishu",
      "app_id": "cli_xxxxxxxxxxxxx",
      "app_secret": "your_app_secret",
      "encrypt_key": "your_encrypt_key",
      "verification_token": "your_verification_token"
    }
  ],
  "llm": {
    "provider": "openai",  // Êàñ "anthropic" / "minimax"
    "model": "openai/gpt-4o",  // Êé®ËçêËã±ÊñáÊ®°Âûã
    "api_key": "your_api_key",
    "base_url": "https://openrouter.ai/api/v1"  // ÂèØÈÄâ
  },
  "deepcode": {
    "api_url": "http://localhost:8000"  // DeepCode ÂêéÁ´ØÂú∞ÂùÄ
  }
}
```

> üí° **ÊèêÁ§∫**: ‰ΩøÁî® `nanobot_config.json.example` ‰Ωú‰∏∫Ê®°Êùø„ÄÇ

---

#### **Step 3 ¬∑ ÂêØÂä®**

Á°Æ‰øù DeepCode ÂêéÁ´ØÂ∑≤ËøêË°åÔºåÁÑ∂ÂêéÂêØÂä® nanobot:

```bash
cd DeepCode/
./nanobot/run_nanobot.sh
```

**Docker Compose Ê®°Âºè** (ÂêåÊó∂ÂêØÂä® DeepCode + nanobot):

```bash
docker compose -f deepcode_docker/docker-compose.yml up -d
```

ËÆøÈóÆÈ£û‰π¶ÔºåÊâæÂà∞‰Ω†ÁöÑÊú∫Âô®‰∫∫ÔºåÂèëÈÄÅÊ∂àÊÅØÊµãËØïÔºö

```
hi
```

Â¶ÇÊûúÊî∂Âà∞ÂõûÂ§çÔºåËØ¥ÊòéÈÖçÁΩÆÊàêÂäüÔºÅüéâ

---

### üí° ‰ΩøÁî®Á§∫‰æã

| Êìç‰Ωú | ÂëΩ‰ª§Á§∫‰æã |
|---|---|
| **ËÆ∫ÊñáËΩ¨‰ª£Á†Å** | `paper2code https://arxiv.org/abs/2104.09864` |
| **ÂØπËØùËΩ¨‰ª£Á†Å** | `chat2code ÂÆûÁé∞‰∏Ä‰∏™ËÆ°ÁÆóÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÂàóÁöÑ Python ÂáΩÊï∞` |
| **Êü•ËØ¢‰ªªÂä°Áä∂ÊÄÅ** | `deepcode_status task_abc123` |
| **ÂìçÂ∫î‰∫§‰∫í** | ÂΩì AI ËØ¢ÈóÆ"ÈúÄË¶ÅÊµãËØïÁî®‰æãÂêóÔºü"Êó∂Áõ¥Êé•ÂõûÂ§ç `ÊòØ` Êàñ `Âê¶` |

---

<details>
<summary><strong>üõ†Ô∏è nanobot ÁÆ°ÁêÜÂëΩ‰ª§</strong></summary>

```bash
# Êü•ÁúãÊó•ÂøóÔºàDocker Ê®°ÂºèÔºâ
docker compose -f deepcode_docker/docker-compose.yml logs -f nanobot

# ÈáçÂêØ nanobotÔºàDocker Ê®°ÂºèÔºâ
docker compose -f deepcode_docker/docker-compose.yml restart nanobot

# ÂÅúÊ≠¢ÊâÄÊúâÊúçÂä°ÔºàDocker Ê®°ÂºèÔºâ
docker compose -f deepcode_docker/docker-compose.yml down
```

</details>

---

<details>
<summary><strong>üîß Â∏∏ËßÅÈóÆÈ¢òÔºànanobotÔºâ</strong></summary>

| ÈóÆÈ¢ò | Ëß£ÂÜ≥ÊñπÊ°à |
|---|---|
| nanobot ÂìçÂ∫î‰∏∫‰∏≠Êñá | ‰øÆÊîπ `nanobot_config.json` ‰∏≠ `llm.model` ‰∏∫Ëã±ÊñáÊ®°ÂûãÔºàÂ¶Ç `gpt-4o`Ôºâ |
| È£û‰π¶Êî∂‰∏çÂà∞Ê∂àÊÅØ | Ê£ÄÊü•‰∫ã‰ª∂ËÆ¢ÈòÖ URL ÊòØÂê¶ÂèØÂÖ¨ÁΩëËÆøÈóÆÔºåÁ´ØÂè£ 8081 ÊòØÂê¶ÂºÄÊîæ |
| DeepCode ‰ªªÂä°Êèê‰∫§Â§±Ë¥• | Á°ÆËÆ§ `deepcode.api_url` Ê≠£Á°ÆÔºåÂêéÁ´ØÊ≠£Âú®ËøêË°å |
| nanobot ÂÆπÂô®Êó†Ê≥ïÂêØÂä® | Ê£ÄÊü• `nanobot_config.json` Ê†ºÂºèÊòØÂê¶Ê≠£Á°ÆÔºà‰ΩøÁî® JSON È™åËØÅÂô®Ôºâ |

</details>

---

  ---

## üí° Á§∫‰æã



### üé¨ **ÂÆûÊó∂ÊºîÁ§∫**



<table align="center">
<tr>
<td width="33%" align="center">

#### üìÑ **ËÆ∫ÊñáËΩ¨‰ª£Á†ÅÊºîÁ§∫**
**Á†îÁ©∂Âà∞ÂÆûÁé∞**

<div align="center">
  <a href="https://www.youtube.com/watch?v=MQZYpLkzsbw">
    <img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
  </a>

  **[‚ñ∂Ô∏è ËßÇÁúãÊºîÁ§∫](https://www.youtube.com/watch?v=MQZYpLkzsbw)**

  *Ëá™Âä®Â∞ÜÂ≠¶ÊúØËÆ∫ÊñáËΩ¨Êç¢‰∏∫Áîü‰∫ßÂ∞±Áª™‰ª£Á†Å*
</div>

</td>
<td width="33%" align="center">

#### üñºÔ∏è **ÂõæÂÉèÂ§ÑÁêÜÊºîÁ§∫**
**AIÈ©±Âä®ÁöÑÂõæÂÉèÂ∑•ÂÖ∑**

<div align="center">
  <a href="https://www.youtube.com/watch?v=nFt5mLaMEac">
    <img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
  </a>

  **[‚ñ∂Ô∏è ËßÇÁúãÊºîÁ§∫](https://www.youtube.com/watch?v=nFt5mLaMEac)**

  *Êô∫ËÉΩÂõæÂÉèÂ§ÑÁêÜÔºåÂÖ∑ÊúâËÉåÊôØÁßªÈô§ÂíåÂ¢ûÂº∫ÂäüËÉΩ*
</div>

</td>
<td width="33%" align="center">

#### üåê **ÂâçÁ´ØÂÆûÁé∞**
**ÂÆåÊï¥WebÂ∫îÁî®Á®ãÂ∫è**

<div align="center">
  <a href="https://www.youtube.com/watch?v=78wx3dkTaAU">
    <img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
  </a>

  **[‚ñ∂Ô∏è ËßÇÁúãÊºîÁ§∫](https://www.youtube.com/watch?v=78wx3dkTaAU)**

  *‰ªéÊ¶ÇÂøµÂà∞ÈÉ®ÁΩ≤ÁöÑÂÖ®Ê†àWebÂºÄÂèë*
</div>

</td>
</tr>
</table>



### üÜï **ÊúÄÊñ∞Êõ¥Êñ∞**

#### üìÑ **Êô∫ËÉΩÊñáÊ°£ÂàÜÂâ≤ (v1.2.0)**
- **Êô∫ËÉΩÂ§ÑÁêÜ**: Ëá™Âä®Â§ÑÁêÜË∂ÖÂá∫LLM‰ª§ÁâåÈôêÂà∂ÁöÑÂ§ßÂûãÁ†îÁ©∂ËÆ∫ÊñáÂíåÊäÄÊúØÊñáÊ°£
- **ÂèØÈÖçÁΩÆÊéßÂà∂**: ÈÄöËøáÈÖçÁΩÆÂàáÊç¢ÂàÜÂâ≤ÂäüËÉΩÔºåÂÖ∑ÊúâÂü∫‰∫éÂ§ßÂ∞èÁöÑÈòàÂÄº
- **ËØ≠‰πâÂàÜÊûê**: È´òÁ∫ßÂÜÖÂÆπÁêÜËß£Ôºå‰øùÁïôÁÆóÊ≥ï„ÄÅÊ¶ÇÂøµÂíåÂÖ¨Âºè
- **ÂêëÂêéÂÖºÂÆπ**: ÂØπËæÉÂ∞èÊñáÊ°£Êó†ÁºùÂõûÈÄÄÂà∞‰º†ÁªüÂ§ÑÁêÜ

### üöÄ **Âç≥Â∞ÜÊé®Âá∫**

Êàë‰ª¨Ê≠£Âú®‰∏çÊñ≠Â¢ûÂº∫DeepCodeÁöÑ‰ª§‰∫∫ÂÖ¥Â•ãÁöÑÊñ∞ÂäüËÉΩ:

#### üîß **Â¢ûÂº∫ÁöÑ‰ª£Á†ÅÂèØÈù†ÊÄßÂíåÈ™åËØÅ**
- **Ëá™Âä®ÂåñÊµãËØï**: ÂÖ∑ÊúâÊâßË°åÈ™åËØÅÂíåÈîôËØØÊ£ÄÊµãÁöÑÂÖ®Èù¢ÂäüËÉΩÊµãËØï„ÄÇ
- **‰ª£Á†ÅË¥®Èáè‰øùËØÅ**: ÈÄöËøáÈùôÊÄÅÂàÜÊûê„ÄÅÂä®ÊÄÅÊµãËØïÂíåÊÄßËÉΩÂü∫ÂáÜÊµãËØïËøõË°åÂ§öÁ∫ßÈ™åËØÅ„ÄÇ
- **Êô∫ËÉΩË∞ÉËØï**: AIÈ©±Âä®ÁöÑÈîôËØØÊ£ÄÊµãÔºåÂÖ∑ÊúâËá™Âä®Á∫†Ê≠£Âª∫ËÆÆ

#### üìä **PaperBenchÊÄßËÉΩÂ±ïÁ§∫**
- **Âü∫ÂáÜ‰ª™Ë°®Êùø**: PaperBenchËØÑ‰º∞Â•ó‰ª∂ÁöÑÁªºÂêàÊÄßËÉΩÊåáÊ†á„ÄÇ
- **ÂáÜÁ°ÆÊÄßÊåáÊ†á**: ‰∏éÊúÄÂÖàËøõÁöÑËÆ∫ÊñáÂ§çÁé∞Á≥ªÁªüÁöÑËØ¶ÁªÜÊØîËæÉ„ÄÇ
- **ÊàêÂäüÂàÜÊûê**: Ë∑®ËÆ∫ÊñáÁ±ªÂà´ÂíåÂ§çÊùÇÂ∫¶Ê∞¥Âπ≥ÁöÑÁªüËÆ°ÂàÜÊûê„ÄÇ

#### ‚ö° **Á≥ªÁªüÁ∫ß‰ºòÂåñ**
- **ÊÄßËÉΩÊèêÂçá**: Â§öÁ∫øÁ®ãÂ§ÑÁêÜÂíå‰ºòÂåñÊô∫ËÉΩ‰ΩìÂçèË∞ÉÔºåÂÆûÁé∞Êõ¥Âø´ÁöÑÁîüÊàê„ÄÇ
- **Â¢ûÂº∫Êé®ÁêÜ**: ÂÖ∑ÊúâÊîπËøõ‰∏ä‰∏ãÊñáÁêÜËß£ÁöÑÈ´òÁ∫ßÊé®ÁêÜËÉΩÂäõ„ÄÇ
- **Êâ©Â±ïÊîØÊåÅ**: Êâ©Â±ï‰∏éÂÖ∂‰ªñÁºñÁ®ãËØ≠Ë®ÄÂíåÊ°ÜÊû∂ÁöÑÂÖºÂÆπÊÄß„ÄÇ

---

## ‚≠ê ÊòüÊ†áÂéÜÂè≤

<div align="center">

*Á§æÂå∫Â¢ûÈïøËΩ®Ëøπ*

<a href="https://star-history.com/#HKUDS/DeepCode&Date">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" />
  </picture>
</a>

</div>

---

### üöÄ **ÂáÜÂ§áÂ•ΩÂèòÈù©ÂºÄÂèëÊñπÂºè‰∫ÜÂêóÔºü**

<div align="center">

<p>
  <a href="#-Âø´ÈÄüÂºÄÂßã"><img src="https://img.shields.io/badge/üöÄ_Á´ãÂç≥ÂºÄÂßã-00d4ff?style=for-the-badge&logo=rocket&logoColor=white" alt="Get Started"></a>
  <a href="https://github.com/HKUDS"><img src="https://img.shields.io/badge/üèõÔ∏è_Âú®GitHub‰∏äÊü•Áúã-00d4ff?style=for-the-badge&logo=github&logoColor=white" alt="View on GitHub"></a>
  <a href="https://github.com/HKUDS/deepcode-agent"><img src="https://img.shields.io/badge/‚≠ê_ÊòüÊ†áÈ°πÁõÆ-00d4ff?style=for-the-badge&logo=star&logoColor=white" alt="Star Project"></a>
</p>

---

### üìÑ **ËÆ∏ÂèØËØÅ**

<img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&logo=opensourceinitiative&logoColor=white" alt="MIT License">

**MITËÆ∏ÂèØËØÅ** - ÁâàÊùÉÊâÄÊúâ (c) 2025 È¶ôÊ∏ØÂ§ßÂ≠¶Êï∞ÊçÆÊô∫ËÉΩÂÆûÈ™åÂÆ§

---



<img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&style=for-the-badge&color=00d4ff" alt="Visitors">

</div>


## Links discovered
- [nanobot](https://github.com/HKUDS/nanobot)
- [*PaperBench*](https://openai.com/index/paperbench/)
- [È£û‰π¶ÂºÄÊîæÂπ≥Âè∞](https://open.feishu.cn/app)
- [ngrok](https://ngrok.com/)
- [localhost.run](https://localhost.run/)
- [‚ñ∂Ô∏è ËßÇÁúãÊºîÁ§∫](https://www.youtube.com/watch?v=MQZYpLkzsbw)
- [‚ñ∂Ô∏è ËßÇÁúãÊºîÁ§∫](https://www.youtube.com/watch?v=nFt5mLaMEac)
- [‚ñ∂Ô∏è ËßÇÁúãÊºîÁ§∫](https://www.youtube.com/watch?v=78wx3dkTaAU)
- [<img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/>](https://trendshift.io/repositories/14665)
- [<img src='https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&style=for-the-badge&logo=star&logoColor=white&labelColor=1a1a2e' />](https://github.com/HKUDS/DeepCode/stargazers)
- [<img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&logo=pypi&logoColor=white&labelColor=1a1a2e&color=ff6b6b">](https://pypi.org/project/deepcode-hku/)
- [<img src="https://img.shields.io/badge/üí¨Discord-Á§æÂå∫-7289da?style=for-the-badge&logo=discord&logoColor=white&labelColor=1a1a2e">](https://discord.gg/yF2MmDJyGJ)
- [<img src="https://img.shields.io/badge/üí¨ÂæÆ‰ø°-Áæ§ÁªÑ-07c160?style=for-the-badge&logo=wechat&logoColor=white&labelColor=1a1a2e">](https://github.com/HKUDS/DeepCode/issues/11)
- [<img src="https://img.shields.io/badge/English-00d4ff?style=for-the-badge&logo=readme&logoColor=white&labelColor=1a1a2e" alt="English">](https://github.com/HKUDS/DeepCode/blob/main/README.md)
- [<img src="https://img.shields.io/badge/‰∏≠Êñá-00d4ff?style=for-the-badge&logo=readme&logoColor=white&labelColor=1a1a2e" alt="‰∏≠Êñá">](https://github.com/HKUDS/DeepCode/blob/main/README_ZH.md)
- [<img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;"/>](https://youtu.be/PRgmP8pOI08)
- [<img src="https://img.shields.io/badge/‚ñ∂Ô∏è_ËßÇÁúãËßÜÈ¢ë-FF0000?style=for-the-badge&logo=youtube&logoColor=white" alt="Watch Video"/>](https://youtu.be/PRgmP8pOI08)
- [<img src="./assets/logo.png" alt="DeepCode" height="60"/>](https://github.com/HKUDS/DeepCode)
- [<img src="./assets/nanobot.png" alt="nanobot" height="60"/>](https://github.com/HKUDS/nanobot)
- [<img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>](https://www.youtube.com/watch?v=MQZYpLkzsbw)
- [<img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>](https://www.youtube.com/watch?v=nFt5mLaMEac)
- [<img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>](https://www.youtube.com/watch?v=78wx3dkTaAU)
- [<img src="https://img.shields.io/badge/üèõÔ∏è_Âú®GitHub‰∏äÊü•Áúã-00d4ff?style=for-the-badge&logo=github&logoColor=white" alt="View on GitHub">](https://github.com/HKUDS)
- [<img src="https://img.shields.io/badge/‚≠ê_ÊòüÊ†áÈ°πÁõÆ-00d4ff?style=for-the-badge&logo=star&logoColor=white" alt="Star Project">](https://github.com/HKUDS/deepcode-agent)

--- __init__.py ---
"""
DeepCode - AI Research Engine

üß¨ Next-Generation AI Research Automation Platform
‚ö° Transform research papers into working code automatically
"""

__version__ = "1.2.0"
__author__ = "DeepCode Team"
__url__ = "https://github.com/HKUDS/DeepCode"

# Import main components for easy access
from utils import FileProcessor, DialogueLogger

__all__ = [
    "FileProcessor",
    "DialogueLogger",
    "__version__",
    "__author__",
    "__url__",
]


--- deepcode.py ---
#!/usr/bin/env python3
"""
DeepCode - AI Research Engine Launcher

üß¨ Next-Generation AI Research Automation Platform
‚ö° Transform research papers into working code automatically

Cross-platform support: Windows, macOS, Linux
"""

import os
import sys
import subprocess
import signal
import platform
import socket
import time
from pathlib import Path


# Global process references for cleanup
_backend_process = None
_frontend_process = None


def get_platform():
    """Get current platform"""
    system = platform.system().lower()
    if system == "darwin":
        return "macos"
    elif system == "windows":
        return "windows"
    else:
        return "linux"


def check_dependencies():
    """Check if necessary dependencies are installed for new UI"""
    import importlib.util
    import shutil

    print("üîç Checking dependencies...")

    missing_deps = []
    missing_system_deps = []

    # Check FastAPI availability (for backend)
    if importlib.util.find_spec("fastapi") is not None:
        print("‚úÖ FastAPI is installed")
    else:
        missing_deps.append("fastapi>=0.104.0")

    # Check uvicorn availability (for backend server)
    if importlib.util.find_spec("uvicorn") is not None:
        print("‚úÖ Uvicorn is installed")
    else:
        missing_deps.append("uvicorn>=0.24.0")

    # Check PyYAML availability
    if importlib.util.find_spec("yaml") is not None:
        print("‚úÖ PyYAML is installed")
    else:
        missing_deps.append("pyyaml>=6.0")

    # Check pydantic-settings availability
    if importlib.util.find_spec("pydantic_settings") is not None:
        print("‚úÖ Pydantic-settings is installed")
    else:
        missing_deps.append("pydantic-settings>=2.0.0")

    # Check Node.js availability (for frontend)
    node_cmd = "node.exe" if get_platform() == "windows" else "node"
    if shutil.which(node_cmd) or shutil.which("node"):
        try:
            result = subprocess.run(
                ["node", "--version"],
                capture_output=True,
                text=True,
                timeout=5,
                shell=(get_platform() == "windows"),
            )
            if result.returncode == 0:
                print(f"‚úÖ Node.js is installed ({result.stdout.strip()})")
        except Exception:
            missing_system_deps.append("Node.js")
    else:
        missing_system_deps.append("Node.js")
        print("‚ùå Node.js not found (required for frontend)")

    # Check npm availability
    npm_cmd = "npm.cmd" if get_platform() == "windows" else "npm"
    if shutil.which(npm_cmd) or shutil.which("npm"):
        print("‚úÖ npm is available")
    else:
        missing_system_deps.append("npm")
        print("‚ùå npm not found (required for frontend)")

    # Display missing dependencies
    if missing_deps or missing_system_deps:
        print("\nüìã Dependency Status:")

        if missing_deps:
            print("‚ùå Missing Python dependencies:")
            for dep in missing_deps:
                print(f"   - {dep}")
            print(f"\nInstall with: pip install {' '.join(missing_deps)}")

        if missing_system_deps:
            print("\n‚ùå Missing system dependencies:")
            for dep in missing_system_deps:
                print(f"   - {dep}")
            print("\nInstall Node.js:")
            print("   - Windows/macOS: https://nodejs.org/")
            print("   - macOS: brew install node")
            print("   - Ubuntu/Debian: sudo apt-get install nodejs npm")

        # Fail if critical dependencies are missing
        if missing_deps or missing_system_deps:
            return False
    else:
        print("‚úÖ All dependencies satisfied")

    return True


def is_port_in_use(port: int) -> bool:
    """Check if a port is in use (cross-platform)"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(("localhost", port)) == 0


def kill_process_on_port(port: int):
    """Kill process using a specific port (cross-platform)"""
    current_platform = get_platform()

    try:
        if current_platform == "windows":
            # Windows: use netstat and taskkill
            result = subprocess.run(
                f"netstat -ano | findstr :{port}",
                capture_output=True,
                text=True,
                shell=True,
            )
            if result.stdout:
                for line in result.stdout.strip().split("\n"):
                    parts = line.split()
                    if len(parts) >= 5:
                        pid = parts[-1]
                        if pid.isdigit():
                            subprocess.run(
                                f"taskkill /F /PID {pid}",
                                shell=True,
                                capture_output=True,
                            )
                            print(f"  ‚úì Killed process on port {port} (PID: {pid})")
        else:
            # macOS/Linux: use lsof
            result = subprocess.run(
                f"lsof -ti :{port}", capture_output=True, text=True, shell=True
            )
            if result.stdout:
                pids = result.stdout.strip().split("\n")
                for pid in pids:
                    if pid.isdigit():
                        os.kill(int(pid), signal.SIGKILL)
                        print(f"  ‚úì Killed process on port {port} (PID: {pid})")
    except Exception as e:
        print(f"  ‚ö†Ô∏è Could not kill process on port {port}: {e}")


def cleanup_ports():
    """Clean up ports 8000 and 5173 if in use"""
    for port in [8000, 5173]:
        if is_port_in_use(port):
            print(f"‚ö†Ô∏è Port {port} is in use, cleaning up...")
            kill_process_on_port(port)
            time.sleep(1)


def install_backend_deps():
    """Install backend dependencies if needed"""
    import importlib.util

    if importlib.util.find_spec("fastapi") is None:
        print("üì¶ Installing backend dependencies...")
        deps = [
            "fastapi",
            "uvicorn",
            "pydantic-settings",
            "python-multipart",
            "aiofiles",
            "websockets",
            "pyyaml",
        ]
        subprocess.run(
            [sys.executable, "-m", "pip", "install", "-q"] + deps, check=True
        )
        print("‚úÖ Backend dependencies installed")


def install_frontend_deps(frontend_dir: Path):
    """Install frontend dependencies if needed"""
    node_modules = frontend_dir / "node_modules"

    if not node_modules.exists():
        print("üì¶ Installing frontend dependencies (first run)...")
        npm_cmd = "npm.cmd" if get_platform() == "windows" else "npm"
        subprocess.run(
            [npm_cmd, "install"],
            cwd=frontend_dir,
            check=True,
            shell=(get_platform() == "windows"),
        )
        print("‚úÖ Frontend dependencies installed")


def start_backend(backend_dir: Path):
    """Start the backend server"""
    global _backend_process

    print("üîß Starting backend server...")

    # Use shell=True on Windows for proper command handling
    if get_platform() == "windows":
        _backend_process = subprocess.Popen(
            f'"{sys.executable}" -m uvicorn main:app --host 0.0.0.0 --port 8000 --reload',
            cwd=backend_dir,
            shell=True,
            creationflags=subprocess.CREATE_NEW_PROCESS_GROUP,
        )
    else:
        _backend_process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "uvicorn",
                "main:app",
                "--host",
                "0.0.0.0",
                "--port",
                "8000",
                "--reload",
            ],
            cwd=backend_dir,
            start_new_session=True,  # Create new process group
        )

    # Wait for backend to start
    time.sleep(2)

    if _backend_process.poll() is None:
        print("‚úÖ Backend started: http://localhost:8000")
        return True
    else:
        print("‚ùå Backend failed to start")
        return False


def start_frontend(frontend_dir: Path):
    """Start the frontend dev server"""
    global _frontend_process

    print("üé® Starting frontend server...")

    npm_cmd = "npm.cmd" if get_platform() == "windows" else "npm"

    if get_platform() == "windows":
        _frontend_process = subprocess.Popen(
            f"{npm_cmd} run dev",
            cwd=frontend_dir,
            shell=True,
            creationflags=subprocess.CREATE_NEW_PROCESS_GROUP,
        )
    else:
        _frontend_process = subprocess.Popen(
            [npm_cmd, "run", "dev"],
            cwd=frontend_dir,
            start_new_session=True,  # Create new process group
        )

    # Wait for frontend to start
    time.sleep(3)

    if _frontend_process.poll() is None:
        print("‚úÖ Frontend started: http://localhost:5173")
        return True
    else:
        print("‚ùå Frontend failed to start")
        return False


def cleanup_processes():
    """Clean up running processes"""
    global _backend_process, _frontend_process

    print("\nüõë Stopping services...")

    for name, proc in [("Backend", _backend_process), ("Frontend", _frontend_process)]:
        if proc and proc.poll() is None:
            try:
                if get_platform() == "windows":
                    # Windows: use taskkill with /T to kill tree
                    subprocess.run(
                        f"taskkill /F /T /PID {proc.pid}",
                        shell=True,
                        capture_output=True,
                    )
                else:
                    # Unix: kill the process group
                    try:
                        os.killpg(os.getpgid(proc.pid), signal.SIGTERM)
                        proc.wait(timeout=5)
                    except Exception:
                        os.killpg(os.getpgid(proc.pid), signal.SIGKILL)
                print(f"  ‚úì {name} stopped")
            except Exception:
                # Fallback: try direct terminate
                try:
                    proc.terminate()
                    proc.wait(timeout=3)
                    print(f"  ‚úì {name} stopped")
                except Exception:
                    try:
                        proc.kill()
                        print(f"  ‚úì {name} killed")
                    except Exception:
                        print(f"  ‚ö†Ô∏è Could not stop {name}")

    # Also clean up any orphaned processes on ports
    time.sleep(0.5)
    for port in [8000, 5173]:
        if is_port_in_use(port):
            kill_process_on_port(port)

    print("‚úÖ All services stopped")


def cleanup_cache():
    """Clean up Python cache files"""
    try:
        print("üßπ Cleaning up cache files...")
        # Clean up __pycache__ directories
        os.system('find . -type d -name "__pycache__" -exec rm -r {} + 2>/dev/null')
        # Clean up .pyc files
        os.system('find . -name "*.pyc" -delete 2>/dev/null')
        print("‚úÖ Cache cleanup completed")
    except Exception as e:
        print(f"‚ö†Ô∏è  Cache cleanup failed: {e}")


def print_banner():
    """Display startup banner"""
    banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                              ‚ïë
‚ïë    üß¨ DeepCode - AI Research Engine                          ‚ïë
‚ïë                                                              ‚ïë
‚ïë    ‚ö° NEURAL ‚Ä¢ AUTONOMOUS ‚Ä¢ REVOLUTIONARY ‚ö°                ‚ïë
‚ïë                                                              ‚ïë
‚ïë    Transform research papers into working code               ‚ïë
‚ïë    Next-generation AI automation platform                   ‚ïë
‚ïë                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
    print(banner)


def launch_classic_ui():
    """Launch classic Streamlit UI"""
    import importlib.util

    print("üåê Launching Classic Streamlit UI...")

    # Check if Streamlit is installed
    if importlib.util.find_spec("streamlit") is None:
        print("‚ùå Streamlit is not installed.")
        print("Install with: pip install streamlit")
        sys.exit(1)

    current_dir = Path(__file__).parent
    streamlit_app_path = current_dir / "ui" / "streamlit_app.py"

    if not streamlit_app_path.exists():
        print(f"‚ùå Streamlit app not found: {streamlit_app_path}")
        sys.exit(1)

    print(f"üìÅ UI App: {streamlit_app_path}")
    print("üöÄ Launching on http://localhost:8501")
    print("=" * 70)

    try:
        cmd = [
            sys.executable,
            "-m",
            "streamlit",
            "run",
            str(streamlit_app_path),
            "--server.port",
            "8501",
            "--server.address",
            "localhost",
            "--browser.gatherUsageStats",
            "false",
        ]
        subprocess.run(cmd, check=True)
    except KeyboardInterrupt:
        print("\n\nüõë Streamlit server stopped by user")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        sys.exit(1)


def _check_docker_prerequisites():
    """Check Docker prerequisites and config files. Returns (current_dir, compose_file, compose_args)."""
    import shutil

    current_dir = Path(__file__).parent
    compose_file = current_dir / "deepcode_docker" / "docker-compose.yml"

    if not compose_file.exists():
        print("‚ùå deepcode_docker/docker-compose.yml not found")
        print("   Make sure you are running from the DeepCode project root.")
        sys.exit(1)

    # Check Docker is installed
    if not shutil.which("docker"):
        print("‚ùå Docker not found. Please install Docker Desktop first.")
        print("   https://www.docker.com/products/docker-desktop")
        sys.exit(1)

    # Check Docker daemon is running
    result = subprocess.run(["docker", "info"], capture_output=True, text=True)
    if result.returncode != 0:
        print("‚ùå Docker is installed but not running.")
        print("   Please start Docker Desktop and try again.")
        sys.exit(1)

    # Check/create secrets file
    secrets_file = current_dir / "mcp_agent.secrets.yaml"
    if not secrets_file.exists():
        example = current_dir / "mcp_agent.secrets.yaml.example"
        if example.exists():
            print("‚ö†Ô∏è  mcp_agent.secrets.yaml not found.")
            print("   Creating from template...")
            import shutil as sh

            sh.copy2(example, secrets_file)
            print(f"   ‚úÖ Created {secrets_file}")
            print("")
            print("   ‚ö†Ô∏è  Please edit mcp_agent.secrets.yaml and fill in your API keys:")
            print(f"      {secrets_file}")
            print("")
            print(
                "   At least ONE LLM provider key is required (OpenAI/Anthropic/Google)."
            )
            print("   Then run 'deepcode' again.")
            sys.exit(0)
        else:
            print(
                "‚ùå mcp_agent.secrets.yaml not found. Please create it with your API keys."
            )
            sys.exit(1)

    # Check config file
    config_file = current_dir / "mcp_agent.config.yaml"
    if not config_file.exists():
        print("‚ùå mcp_agent.config.yaml not found.")
        print("   This file should be in the project root.")
        sys.exit(1)

    # Ensure data directories exist
    for d in ["deepcode_lab", "uploads", "logs"]:
        (current_dir / d).mkdir(exist_ok=True)

    os.chdir(current_dir)
    compose_args = ["docker", "compose", "-f", str(compose_file)]

    return current_dir, compose_file, compose_args


def launch_docker():
    """Launch DeepCode via Docker"""
    current_dir, compose_file, compose_args = _check_docker_prerequisites()

    print("üê≥ Starting DeepCode with Docker...")
    print("=" * 50)

    try:
        # Check if image exists (auto-build on first run)
        result = subprocess.run(
            compose_args + ["images", "-q"], capture_output=True, text=True
        )
        if not result.stdout.strip():
            print(
                "üì¶ First run detected ‚Äî building Docker image (may take a few minutes)..."
            )
            subprocess.run(compose_args + ["build"], check=True)

        # Start (if already running, docker compose will detect and skip)
        subprocess.run(compose_args + ["up", "-d"], check=True)

        print("")
        print("=" * 50)
        print("‚úÖ DeepCode is running!")
        print("")
        print("   üåê Open: http://localhost:8000")
        print("   üìö Docs: http://localhost:8000/docs")
        print("")
        print("   üìã View logs:  docker logs deepcode -f")
        print(
            "   üõë Stop:       docker compose -f deepcode_docker/docker-compose.yml down"
        )
        print("=" * 50)

    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå Docker failed: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\nüõë Cancelled")


def launch_docker_cli():
    """Launch DeepCode CLI inside Docker container"""
    current_dir, compose_file, compose_args = _check_docker_prerequisites()

    print("üñ•Ô∏è  Starting DeepCode CLI in Docker...")
    print("=" * 50)

    try:
        # Check if image exists (auto-build on first run)
        result = subprocess.run(
            compose_args + ["images", "-q"], capture_output=True, text=True
        )
        if not result.stdout.strip():
            print(
                "üì¶ First run detected ‚Äî building Docker image (may take a few minutes)..."
            )
            subprocess.run(compose_args + ["build"], check=True)

        # Run CLI interactively
        subprocess.run(
            compose_args + ["run", "--rm", "-it", "deepcode", "cli"], check=True
        )

    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå Docker failed: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\nüõë Cancelled")


def launch_paper_test(paper_name: str, fast_mode: bool = False):
    """Launch paper testing mode"""
    try:
        print("\nüß™ Launching Paper Test Mode")
        print(f"üìÑ Paper: {paper_name}")
        print(f"‚ö° Fast mode: {'enabled' if fast_mode else 'disabled'}")
        print("=" * 60)

        # Run the test setup
        setup_cmd = [sys.executable, "test_paper.py", paper_name]
        if fast_mode:
            setup_cmd.append("--fast")

        result = subprocess.run(setup_cmd, check=True)

        if result.returncode == 0:
            print("\n‚úÖ Paper test setup completed successfully!")
            print("üìÅ Files are ready in deepcode_lab/papers/")
            print("\nüí° Next steps:")
            print("   1. Install MCP dependencies: pip install -r requirements.txt")
            print(
                f"   2. Run full pipeline: python -m workflows.paper_test_engine --paper {paper_name}"
                + (" --fast" if fast_mode else "")
            )

    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå Paper test setup failed: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        sys.exit(1)


def main():
    """Main function"""
    # Parse command line arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == "test" and len(sys.argv) >= 3:
            # Paper testing mode: python deepcode.py test rice [--fast]
            paper_name = sys.argv[2]
            fast_mode = "--fast" in sys.argv or "-f" in sys.argv

            print_banner()
            launch_paper_test(paper_name, fast_mode)
            return
        elif sys.argv[1] == "--local":
            # Launch locally (without Docker) ‚Äî fall through to local launch below
            print_banner()
            pass
        elif sys.argv[1] == "--docker":
            # Explicit Docker launch (same as default)
            print_banner()
            launch_docker()
            return
        elif sys.argv[1] == "--cli":
            # Launch CLI inside Docker container
            print_banner()
            launch_docker_cli()
            return
        elif sys.argv[1] == "--classic":
            # Launch classic Streamlit UI
            print_banner()
            launch_classic_ui()
            return
        elif sys.argv[1] in ["--help", "-h", "help"]:
            print_banner()
            print("""
üîß Usage:
   deepcode                              - Launch via Docker (default, recommended)
   deepcode --docker                     - Same as above (launch via Docker)
   deepcode --cli                        - Launch interactive CLI in Docker
   deepcode --local                      - Launch locally (requires Python + Node.js)
   deepcode test <paper>                 - Test paper reproduction
   deepcode test <paper> --fast          - Test paper (fast mode)
   deepcode --classic                    - Launch classic Streamlit UI

üìÑ Examples:
   deepcode                              - Start with Docker (one command)
   deepcode --cli                        - Interactive CLI in Docker
   deepcode --local                      - Start the new UI locally
   deepcode test rice                    - Test RICE paper reproduction
   deepcode test rice --fast             - Test RICE paper (fast mode)

üåê New UI Features:
   ‚Ä¢ User-in-Loop interaction
   ‚Ä¢ Real-time progress tracking
   ‚Ä¢ Inline chat interaction
   ‚Ä¢ Modern React-based interface

üìÅ Available papers:""")

            # List available papers
            papers_dir = "papers"
            if os.path.exists(papers_dir):
                for item in os.listdir(papers_dir):
                    item_path = os.path.join(papers_dir, item)
                    if os.path.isdir(item_path):
                        paper_md = os.path.join(item_path, "paper.md")
                        addendum_md = os.path.join(item_path, "addendum.md")
                        status = "‚úÖ" if os.path.exists(paper_md) else "‚ùå"
                        addendum_status = "üìÑ" if os.path.exists(addendum_md) else "‚ûñ"
                        print(f"   {status} {item} {addendum_status}")
            print(
                "\n   Legend: ‚úÖ = paper.md exists, üìÑ = addendum.md exists, ‚ûñ = no addendum"
            )
            return
        else:
            # Unknown argument ‚Äî show help hint
            print(f"Unknown option: {sys.argv[1]}")
            print("Run 'deepcode --help' for usage information.")
            sys.exit(1)
    else:
        # Default (no arguments) ‚Üí Docker
        print_banner()
        launch_docker()
        return

    # --- Local launch (only reached via --local) ---

    # Show platform info
    current_platform = get_platform()
    print(f"üñ•Ô∏è  Platform: {current_platform.capitalize()}")

    # Check dependencies
    if not check_dependencies():
        print("\nüö® Please install missing dependencies and try again.")
        sys.exit(1)

    # Get paths
    current_dir = Path(__file__).parent
    new_ui_dir = current_dir / "new_ui"
    backend_dir = new_ui_dir / "backend"
    frontend_dir = new_ui_dir / "frontend"

    # Check if new_ui directory exists
    if not new_ui_dir.exists():
        print(f"‚ùå New UI directory not found: {new_ui_dir}")
        sys.exit(1)

    print("\nüöÄ Starting DeepCode New UI...")
    print("=" * 70)
    print("üé® Frontend:  http://localhost:5173")
    print("üîß Backend:   http://localhost:8000")
    print("üìö API Docs:  http://localhost:8000/docs")
    print("=" * 70)
    print("üí° Tip: Keep this terminal open while using the application")
    print("üõë Press Ctrl+C to stop all services")
    print("=" * 70)

    try:
        # Clean up ports if in use
        cleanup_ports()

        # Install dependencies if needed
        install_backend_deps()
        install_frontend_deps(frontend_dir)

        # Start services
        if not start_backend(backend_dir):
            print("‚ùå Failed to start backend")
            sys.exit(1)

        if not start_frontend(frontend_dir):
            print("‚ùå Failed to start frontend")
            cleanup_processes()
            sys.exit(1)

        print("\n" + "=" * 70)
        print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
        print("‚ïë  üéâ DeepCode New UI is running!        ‚ïë")
        print("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£")
        print("‚ïë                                        ‚ïë")
        print("‚ïë  üåê Frontend: http://localhost:5173    ‚ïë")
        print("‚ïë  üîß Backend:  http://localhost:8000    ‚ïë")
        print("‚ïë  üìö API Docs: http://localhost:8000/docs‚ïë")
        print("‚ïë                                        ‚ïë")
        print("‚ïë  Press Ctrl+C to stop all services     ‚ïë")
        print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
        print("=" * 70 + "\n")

        # Wait for processes
        while True:
            # Check if processes are still running
            if _backend_process and _backend_process.poll() is not None:
                print("‚ö†Ô∏è Backend process exited unexpectedly")
                break
            if _frontend_process and _frontend_process.poll() is not None:
                print("‚ö†Ô∏è Frontend process exited unexpectedly")
                break
            time.sleep(1)

    except KeyboardInterrupt:
        print("\n")
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
    finally:
        cleanup_processes()
        cleanup_cache()
        print("Thank you for using DeepCode! üß¨")


if __name__ == "__main__":
    main()


--- prompts/code_prompts.py ---
"""
Prompt templates for the DeepCode agent system.

RECENT UPDATES (ÈíàÂØπËÆ∫Êñá‰ª£Á†ÅÂ§çÁé∞‰ºòÂåñ):
1. ÁÆÄÂåñÂπ∂‰ºòÂåñ‰∫ÜÊñá‰ª∂ÁªìÊûÑÁîüÊàêÈÄªËæëÔºåÁ°Æ‰øùÁªìÊûÑÁÆÄÊ¥Å‰∏îÂØåÊúâÈÄªËæëÊÄß
2. ÊòéÁ°ÆÊ†áËØÜÈúÄË¶ÅÂ§çÁé∞ÁöÑÊ†∏ÂøÉÊñá‰ª∂ÂíåÁªÑ‰ª∂ÔºåÁî±LLMÊô∫ËÉΩÂà§Êñ≠‰ºòÂÖàÁ∫ß
3. ‰ºòÂåñ‰∫ÜÂ§öagentÂçè‰ΩúÁöÑ‰ø°ÊÅØÊÄªÁªìÊïàÁéáÔºåÂáèÂ∞ëÂÜó‰Ωô‰ø°ÊÅØ‰º†ÈÄí
4. ÁßªÈô§‰∫ÜÊó∂Èó¥Á∫øÁ≠âÊ¨°Ë¶Å‰ø°ÊÅØÔºå‰∏ìÊ≥®‰∫éÈ´òË¥®Èáè‰ª£Á†ÅÂ§çÁé∞
5. ‰øùÊåÅpromptÂÆåÊï¥ÊÄßÁöÑÂêåÊó∂ÊèêÈ´ò‰∫ÜÁÆÄÊ¥ÅÊÄßÂíåÂèØÁêÜËß£ÊÄß
6. ÈááÁî®Êõ¥Ê∏ÖÊô∞ÁöÑÁªìÊûÑÂåñÊ†ºÂºèÔºå‰æø‰∫éLLMÁêÜËß£ÂíåÊâßË°å

Ê†∏ÂøÉÊîπËøõÔºö
- PAPER_ALGORITHM_ANALYSIS_PROMPT: ‰∏ìÊ≥®ÁÆóÊ≥ïÊèêÂèñÔºåÊòéÁ°ÆÂÆûÁé∞‰ºòÂÖàÁ∫ß
- PAPER_CONCEPT_ANALYSIS_PROMPT: ‰∏ìÊ≥®Á≥ªÁªüÊû∂ÊûÑÔºåÁ™ÅÂá∫Ê¶ÇÂøµÂà∞‰ª£Á†ÅÁöÑÊò†Â∞Ñ
- CODE_PLANNING_PROMPT: Êï¥ÂêàÂâç‰∏§ËÄÖËæìÂá∫ÔºåÁîüÊàêÈ´òË¥®ÈáèÂ§çÁé∞ËÆ°Âàí
"""

# Paper to Code Workflow Prompts
PAPER_INPUT_ANALYZER_PROMPT = """You are a precise input analyzer for paper-to-code tasks. You MUST return only a JSON object with no additional text.

Task: Analyze input text and identify file paths/URLs to determine appropriate input type.

Input Analysis Rules:
1. Path Detection:
   - Scan input text for file paths or URLs
   - Use first valid path/URL if multiple found
   - Treat as text input if no valid path/URL found

2. Path Type Classification:
   - URL (starts with http:// or https://): input_type = "url", path = "detected URL"
   - PDF file path: input_type = "file", path = "detected file path"
   - Directory path: input_type = "directory", path = "detected directory path"
   - No path/URL detected: input_type = "text", path = null

3. Requirements Analysis:
   - Extract ONLY requirements from additional_input
   - DO NOT modify or interpret requirements

CRITICAL OUTPUT RESTRICTIONS:
- RETURN ONLY RAW JSON - NO TEXT BEFORE OR AFTER
- NO markdown code blocks (```json)
- NO explanatory text or descriptions
- NO tool call information
- NO analysis summaries
- JUST THE JSON OBJECT BELOW

{
    "input_type": "text|file|directory|url",
    "path": "detected path or URL or null",
    "paper_info": {
        "title": "N/A for text input",
        "authors": ["N/A for text input"],
        "year": "N/A for text input"
    },
    "requirements": [
        "exact requirement from additional_input"
    ]
}
"""

PAPER_DOWNLOADER_PROMPT = """You are a precise paper downloader that processes input from PaperInputAnalyzerAgent.

Task: Handle paper according to input type and save to "./deepcode_lab/papers/id/id.md"
Note: The paper ID will be provided at the start of the message as "PAPER_ID=<number>". Use this EXACT number.

CRITICAL RULES:
- Use the EXACT paper ID provided in the message (PAPER_ID=X).
- Save path MUST be: ./deepcode_lab/papers/{PAPER_ID}/{PAPER_ID}.md

Processing Rules:
1. URL Input (input_type = "url"):
   - Use download_file_to tool with: url=<url>, destination="./deepcode_lab/papers/{PAPER_ID}/", filename="{PAPER_ID}.md"
   - Extract metadata (title, authors, year)
   - Return saved file path and metadata

2. File Input (input_type = "file"):
   - Use move_file_to tool with: source=<file_path>, destination="./deepcode_lab/papers/{PAPER_ID}/{PAPER_ID}.md"
   - The tool will automatically convert PDF/documents to .md format
   - NEVER manually extract content or use write_file - let the conversion tools handle this
   - Note: Original file is preserved, only a copy is placed in target directory
   - Return new saved file path and metadata

3. Directory Input (input_type = "directory"):
   - Verify directory exists
   - Return to PaperInputAnalyzerAgent for processing
   - Set status as "failure" with message

4. Text Input (input_type = "text"):
   - No file operations needed
   - Set paper_path as null
   - Use paper_info from input

Input Format:
{
    "input_type": "file|directory|url|text",
    "path": "detected path or null",
    "paper_info": {
        "title": "paper title or N/A",
        "authors": ["author names or N/A"],
        "year": "publication year or N/A"
    },
    "requirements": ["requirement1", "requirement2"]
}

CRITICAL OUTPUT RESTRICTIONS:
- RETURN ONLY RAW JSON - NO TEXT BEFORE OR AFTER
- NO markdown code blocks (```json)
- NO explanatory text or descriptions
- NO tool call information
- NO analysis summaries
- JUST THE JSON OBJECT BELOW

Output Format (MANDATORY - EXACT FORMAT):
{
    "status": "success|failure",
    "paper_path": "./deepcode_lab/papers/{PAPER_ID}/{PAPER_ID}.md (or null for text input)",
    "metadata": {
        "title": "extracted or provided title",
        "authors": ["extracted or provided authors"],
        "year": "extracted or provided year"
    }
}

Example: If PAPER_ID=14, then paper_path should be "./deepcode_lab/papers/14/14.md"
"""

PAPER_REFERENCE_ANALYZER_PROMPT = """You are an expert academic paper reference analyzer specializing in computer science and machine learning.

Task: Analyze paper and identify 5 most relevant references that have GitHub repositories.

Constraints:
- ONLY select references with GitHub repositories
- DO NOT use target paper's official implementation
- DO NOT use repositories directly associated with target paper
- CAN analyze code implementations from referenced papers
- Focus on references with good implementations solving similar problems

Analysis Criteria:
1. GitHub Repository Quality (40%):
   - Star count, activity, maintenance
   - Documentation quality
   - Community adoption
   - Last update date

2. Implementation Relevance (30%):
   - References from methodology/implementation sections
   - Algorithmic details
   - Core component descriptions
   - Code implementation quality

3. Technical Depth (20%):
   - Algorithm/method similarity
   - Technical foundation relationship
   - Implementation details
   - Code structure

4. Academic Influence (10%):
   - Publication venue quality
   - Author expertise
   - Research impact
   - Citation influence

Analysis Steps:
1. Extract all references from paper
2. Filter references with GitHub repositories
3. Analyze repositories based on criteria
4. Calculate relevance scores
5. Select and rank top 5 references

Output Format:
{
    "selected_references": [
        {
            "rank": 1,
            "title": "paper title",
            "authors": ["author1", "author2"],
            "year": "publication year",
            "relevance_score": 0.95,
            "citation_context": "how cited in main paper",
            "key_contributions": ["contribution1", "contribution2"],
            "implementation_value": "why valuable for implementation",
            "github_info": {
                "repository_url": "GitHub repository URL",
                "stars_count": "number of stars",
                "last_updated": "last update date",
                "repository_quality": "repository quality assessment",
                "key_features": ["feature1", "feature2"],
                "documentation_quality": "documentation assessment",
                "community_activity": "community engagement description"
            },
            "original_reference": "Complete reference text from paper"
        }
    ],
    "analysis_summary": "selection process and key findings",
    "github_repositories_found": "total number of references with GitHub repositories"
}
"""

GITHUB_DOWNLOAD_PROMPT = """You are an expert GitHub repository downloader.

Task: Download GitHub repositories to specified directory structure.

Process:
1. For each repository:
   - Create directory: {paper_dir}/code_base/
   - Download repository to directory

Requirements:
- Use interpreter tool to execute download script
- Monitor interpreter output for errors/warnings
- Verify download status through interpreter response

Output Format:
{
    "downloaded_repos": [
        {
            "reference_number": "1",
            "paper_title": "paper title",
            "repo_url": "github repository URL",
            "save_path": "{paper_dir}/code_base/name_of_repo",
            "status": "success|failed",
            "notes": "relevant notes about download"
        }
    ],
    "summary": "Brief summary of download process"
}
"""

# Code Analysis Prompts
PAPER_ALGORITHM_ANALYSIS_PROMPT = """You are extracting COMPLETE implementation details from a research paper. Your goal is to capture EVERY algorithm, formula, and technical detail needed for perfect reproduction.

# INTELLIGENT DOCUMENT READING STRATEGY

## IMPORTANT: Use Segmented Reading for Algorithm Extraction
To avoid token limits and efficiently extract algorithm details, use the intelligent segmentation system:

1. **Primary Algorithm Extraction** - Use read_document_segments tool with:
   - query_type: "algorithm_extraction"
   - keywords: ["algorithm", "method", "procedure", "formula", "equation", "implementation"]
   - max_segments: 3
   - max_total_chars: 6000

2. **Supplementary Details** - Make additional calls if needed with:
   - keywords: ["hyperparameter", "training", "optimization", "loss", "objective"]
   - keywords: ["experiment", "setup", "configuration", "parameter"]

3. **This approach ensures** you get the most algorithm-relevant content without missing critical details

# DETAILED EXTRACTION PROTOCOL

## 1. INTELLIGENT ALGORITHM SCAN
Use the segmented reading approach to focus on algorithm sections:
- Method/Algorithm sections (captured automatically by segmentation)
- Implementation Details (targeted retrieval)
- Hyperparameters and training details (focused extraction)

## 2. ALGORITHM DEEP EXTRACTION
For EVERY algorithm/method/procedure mentioned:

### Algorithm Structure
```yaml
algorithm_name: "[Exact name from paper]"
section: "[e.g., Section 3.2]"
algorithm_box: "[e.g., Algorithm 1 on page 4]"

pseudocode: |
  [COPY THE EXACT PSEUDOCODE FROM PAPER]
  Input: ...
  Output: ...
  1. Initialize ...
  2. For each ...
     2.1 Calculate ...
  [Keep exact formatting and numbering]

mathematical_formulation:
  - equation: "[Copy formula EXACTLY, e.g., L = L_task + Œª*L_explain]"
    equation_number: "[e.g., Eq. 3]"
    where:
      L_task: "task loss"
      L_explain: "explanation loss"
      Œª: "weighting parameter (default: 0.5)"

step_by_step_breakdown:
  1. "[Detailed explanation of what step 1 does]"
  2. "[What step 2 computes and why]"

implementation_details:
  - "Uses softmax temperature œÑ = 0.1"
  - "Gradient clipping at norm 1.0"
  - "Initialize weights with Xavier uniform"
```

## 3. COMPONENT EXTRACTION
For EVERY component/module mentioned:

### Component Details
```yaml
component_name: "[e.g., Mask Network, Critic Network]"
purpose: "[What this component does in the system]"
architecture:
  input: "[shape and meaning]"
  layers:
    - "[Conv2d(3, 64, kernel=3, stride=1)]"
    - "[ReLU activation]"
    - "[BatchNorm2d(64)]"
  output: "[shape and meaning]"

special_features:
  - "[Any unique aspects]"
  - "[Special initialization]"
```

## 4. TRAINING PROCEDURE
Extract the COMPLETE training process:

```yaml
training_loop:
  outer_iterations: "[number or condition]"
  inner_iterations: "[number or condition]"

  steps:
    1. "Sample batch of size B from buffer"
    2. "Compute importance weights using..."
    3. "Update policy with loss..."

  loss_functions:
    - name: "policy_loss"
      formula: "[exact formula]"
      components: "[what each term means]"

  optimization:
    optimizer: "Adam"
    learning_rate: "3e-4"
    lr_schedule: "linear decay to 0"
    gradient_norm: "clip at 0.5"
```

## 5. HYPERPARAMETERS HUNT
Search EVERYWHERE (text, tables, captions) for:

```yaml
hyperparameters:
  # Training
  batch_size: 64
  buffer_size: 1e6
  discount_gamma: 0.99

  # Architecture
  hidden_units: [256, 256]
  activation: "ReLU"

  # Algorithm-specific
  explanation_weight: 0.5
  exploration_bonus_scale: 0.1
  reset_probability: 0.3

  # Found in:
  location_references:
    - "batch_size: Table 1"
    - "hidden_units: Section 4.1"
```

# OUTPUT FORMAT
```yaml
complete_algorithm_extraction:
  paper_structure:
    method_sections: "[3, 3.1, 3.2, 3.3, 4]"
    algorithm_count: "[total number found]"

  main_algorithm:
    [COMPLETE DETAILS AS ABOVE]

  supporting_algorithms:
    - [EACH SUPPORTING ALGORITHM WITH FULL DETAILS]

  components:
    - [EVERY COMPONENT WITH ARCHITECTURE]

  training_details:
    [COMPLETE TRAINING PROCEDURE]

  all_hyperparameters:
    [EVERY PARAMETER WITH VALUE AND SOURCE]

  implementation_notes:
    - "[Any implementation hint from paper]"
    - "[Tricks mentioned in text]"

  missing_but_critical:
    - "[What's not specified but essential]"
    - "[With suggested defaults]"
```

BE EXHAUSTIVE. A developer should be able to implement the ENTIRE paper using only your extraction."""

PAPER_CONCEPT_ANALYSIS_PROMPT = """You are doing a COMPREHENSIVE analysis of a research paper to understand its complete structure, contributions, and implementation requirements.

# OBJECTIVE
Map out the ENTIRE paper structure and identify ALL components that need implementation for successful reproduction.

# INTELLIGENT DOCUMENT READING STRATEGY

## IMPORTANT: Use Segmented Reading for Optimal Performance
Instead of reading the entire document at once (which may hit token limits), use the intelligent segmentation system:

1. **Use read_document_segments tool** with these parameters:
   - query_type: "concept_analysis"
   - keywords: ["introduction", "overview", "architecture", "system", "framework", "concept", "method"]
   - max_segments: 3
   - max_total_chars: 6000

2. **This will automatically find and retrieve** the most relevant sections for concept analysis without token overflow

3. **If you need additional sections**, make follow-up calls with different keywords like ["experiment", "evaluation", "results"] or ["conclusion", "discussion"]

# COMPREHENSIVE ANALYSIS PROTOCOL

## 1. INTELLIGENT PAPER STRUCTURAL ANALYSIS
Use the segmented reading approach to create a complete map:

```yaml
paper_structure_map:
  title: "[Full paper title]"

  sections:
    1_introduction:
      main_claims: "[What the paper claims to achieve]"
      problem_definition: "[Exact problem being solved]"

    2_related_work:
      key_comparisons: "[Methods this work builds upon or competes with]"

    3_method:  # May have multiple subsections
      subsections:
        3.1: "[Title and main content]"
        3.2: "[Title and main content]"
      algorithms_presented: "[List all algorithms by name]"

    4_experiments:
      environments: "[All test environments/datasets]"
      baselines: "[All comparison methods]"
      metrics: "[All evaluation metrics used]"

    5_results:
      main_findings: "[Key results that prove the method works]"
      tables_figures: "[Important result tables/figures to reproduce]"
```

## 2. METHOD DECOMPOSITION
For the main method/approach:

```yaml
method_decomposition:
  method_name: "[Full name and acronym]"

  core_components:  # Break down into implementable pieces
    component_1:
      name: "[e.g., State Importance Estimator]"
      purpose: "[Why this component exists]"
      paper_section: "[Where it's described]"

    component_2:
      name: "[e.g., Policy Refinement Module]"
      purpose: "[Its role in the system]"
      paper_section: "[Where it's described]"

  component_interactions:
    - "[How component 1 feeds into component 2]"
    - "[Data flow between components]"

  theoretical_foundation:
    key_insight: "[The main theoretical insight]"
    why_it_works: "[Intuitive explanation]"
```

## 3. IMPLEMENTATION REQUIREMENTS MAPPING
Map paper content to code requirements:

```yaml
implementation_map:
  algorithms_to_implement:
    - algorithm: "[Name from paper]"
      section: "[Where defined]"
      complexity: "[Simple/Medium/Complex]"
      dependencies: "[What it needs to work]"

  models_to_build:
    - model: "[Neural network or other model]"
      architecture_location: "[Section describing it]"
      purpose: "[What this model does]"

  data_processing:
    - pipeline: "[Data preprocessing needed]"
      requirements: "[What the data should look like]"

  evaluation_suite:
    - metric: "[Metric name]"
      formula_location: "[Where it's defined]"
      purpose: "[What it measures]"
```

## 4. EXPERIMENT REPRODUCTION PLAN
Identify ALL experiments needed:

```yaml
experiments_analysis:
  main_results:
    - experiment: "[Name/description]"
      proves: "[What claim this validates]"
      requires: "[Components needed to run this]"
      expected_outcome: "[Specific numbers/trends]"

  ablation_studies:
    - study: "[What is being ablated]"
      purpose: "[What this demonstrates]"

  baseline_comparisons:
    - baseline: "[Method name]"
      implementation_required: "[Yes/No/Partial]"
      source: "[Where to find implementation]"
```

## 5. CRITICAL SUCCESS FACTORS
What defines successful reproduction:

```yaml
success_criteria:
  must_achieve:
    - "[Primary result that must be reproduced]"
    - "[Core behavior that must be demonstrated]"

  should_achieve:
    - "[Secondary results that validate the method]"

  validation_evidence:
    - "[Specific figure/table to reproduce]"
    - "[Qualitative behavior to demonstrate]"
```

# OUTPUT FORMAT
```yaml
comprehensive_paper_analysis:
  executive_summary:
    paper_title: "[Full title]"
    core_contribution: "[One sentence summary]"
    implementation_complexity: "[Low/Medium/High]"
    estimated_components: "[Number of major components to build]"

  complete_structure_map:
    [FULL SECTION BREAKDOWN AS ABOVE]

  method_architecture:
    [DETAILED COMPONENT BREAKDOWN]

  implementation_requirements:
    [ALL ALGORITHMS, MODELS, DATA, METRICS]

  reproduction_roadmap:
    phase_1: "[What to implement first]"
    phase_2: "[What to build next]"
    phase_3: "[Final components and validation]"

  validation_checklist:
    - "[ ] [Specific result to achieve]"
    - "[ ] [Behavior to demonstrate]"
    - "[ ] [Metric to match]"
```

BE THOROUGH. Miss nothing. The output should be a complete blueprint for reproduction."""

CODE_PLANNING_PROMPT = """You are creating a DETAILED, COMPLETE reproduction plan by integrating comprehensive analysis results.

# INPUT
You receive two exhaustive analyses:
1. **Comprehensive Paper Analysis**: Complete paper structure, components, and requirements
2. **Complete Algorithm Extraction**: All algorithms, formulas, pseudocode, and technical details

Plus you can use segmented reading to access any specific paper sections needed for planning.

# INTELLIGENT DOCUMENT ACCESS

## IMPORTANT: Use Segmented Reading for Detailed Planning
When you need additional details beyond the provided analyses, use the intelligent segmentation system:

1. **Use read_document_segments tool** with these parameters:
   - query_type: "code_planning"
   - keywords: Specific to what you need, e.g., ["implementation", "code", "experiment", "setup", "configuration"]
   - max_segments: 3
   - max_total_chars: 8000

2. **This approach ensures** you access the most planning-relevant content without token limits

# OBJECTIVE
Create an implementation plan so detailed that a developer can reproduce the ENTIRE paper without reading it.

# CRITICAL: COMPLETE OUTPUT REQUIREMENT
‚ö†Ô∏è MANDATORY: You MUST generate ALL 5 sections completely. DO NOT stop early or truncate any section.

## Output Completeness Strategy:
üéØ **Your #1 Priority**: Ensure ALL 5 sections are present and complete before finishing your response.

## Content Balance Guidelines (STRICTLY FOLLOW):
- **Section 1 (File Structure)**: ~800-1000 chars - Brief file listing with priority order
- **Section 2 (Implementation Components)**: ~3000-4000 chars - CORE section with all algorithms/components
- **Section 3 (Validation)**: ~2000-2500 chars - Experiments and expected results
- **Section 4 (Environment)**: ~800-1000 chars - Dependencies and requirements
- **Section 5 (Implementation Strategy)**: ~1500-2000 chars - Step-by-step approach

üìè **Total Target**: 8000-10000 characters for complete plan

‚ö†Ô∏è **Self-Check Before Finishing**:
- Did you include file_structure section? ‚úì
- Did you include implementation_components section? ‚úì
- Did you include validation_approach section? ‚úì
- Did you include environment_setup section? ‚úì
- Did you include implementation_strategy section? ‚úì
- If ANY answer is NO, continue writing until ALL sections are complete!

## File Priority Guidelines:
üîß **Implementation Priority Order**:
1. **FIRST**: Core algorithm/model files (highest priority)
2. **SECOND**: Supporting modules and utilities
3. **THIRD**: Experiment and evaluation scripts
4. **FOURTH**: Configuration and data handling
5. **LAST**: Documentation files (README.md, requirements.txt) - These should be created AFTER core implementation

Note: README and requirements.txt are maintenance files that depend on the final implementation, so plan them last but INCLUDE them in the file structure.

# DETAILED SYNTHESIS PROCESS

## 1. MERGE ALL INFORMATION
Combine EVERYTHING from both analyses:
- Every algorithm with its pseudocode
- Every component with its architecture
- Every hyperparameter with its value
- Every experiment with expected results

## 2. MAP CONTENT TO IMPLEMENTATION

For each component you identify, specify how it will be implemented:

```
# DESIGN YOUR MAPPING: Connect paper content to code organization
[For each algorithm/component/method in the paper]:
  - What it does and where it's described in the paper
  - How you'll organize the code (files, classes, functions - your choice)
  - What specific formulas, algorithms, or procedures need implementation
  - Dependencies and relationships with other components
  - Implementation approach that makes sense for this specific paper
```

## 3. EXTRACT ALL TECHNICAL DETAILS

Identify every technical detail that needs implementation:

```
# COMPREHENSIVE TECHNICAL EXTRACTION:
[Gather all implementation-relevant details from the paper]:
  - All algorithms with complete pseudocode and mathematical formulations
  - All parameters, hyperparameters, and configuration values
  - All architectural details (if applicable to your paper type)
  - All experimental procedures and evaluation methods
  - Any implementation hints, tricks, or special considerations mentioned
```

# COMPREHENSIVE OUTPUT FORMAT

```yaml
complete_reproduction_plan:
  paper_info:
    title: "[Full paper title]"
    core_contribution: "[Main innovation being reproduced]"

  # SECTION 1: File Structure Design

  # DESIGN YOUR OWN STRUCTURE: Create a file organization that best serves this specific paper
  # - Analyze what the paper contains (algorithms, models, experiments, systems, etc.)
  # - Organize files and directories in the most logical way for implementation
  # - Create meaningful names and groupings based on paper content
  # - Keep it clean, intuitive, and focused on what actually needs to be implemented
  # - INCLUDE documentation files (README.md, requirements.txt) but mark them for LAST implementation

  file_structure: |
    [Design and specify your own project structure here - KEEP THIS BRIEF]
    [Include ALL necessary files including README.md and requirements.txt]
    [Organize based on what this paper actually contains and needs]
    [Create directories and files that make sense for this specific implementation]
    [IMPORTANT: Include executable files (e.g., main.py, run.py, train.py, demo.py) - choose names based on repo content]
    [Design executable entry points that match the paper's main functionality and experiments]
    [NOTE: README.md and requirements.txt should be implemented LAST after all code files]

  # SECTION 2: Implementation Components

  # IDENTIFY AND SPECIFY: What needs to be implemented based on this paper
  # - List all algorithms, models, systems, or components mentioned
  # - Map each to implementation details and file locations
  # - Include formulas, pseudocode, and technical specifications
  # - Organize in whatever way makes sense for this paper

  implementation_components: |
    [List and specify all components that need implementation]
    [For each component: purpose, location, algorithms, formulas, technical details]
    [Organize and structure this based on the paper's actual content]

  # SECTION 3: Validation & Evaluation

  # DESIGN VALIDATION: How to verify the implementation works correctly
  # - Define what experiments, tests, or proofs are needed
  # - Specify expected results from the paper (figures, tables, theorems)
  # - Design validation approach appropriate for this paper's domain
  # - Include setup requirements and success criteria

  validation_approach: |
    [Design validation strategy appropriate for this paper]
    [Specify experiments, tests, or mathematical verification needed]
    [Define expected results and success criteria]
    [Include any special setup or evaluation requirements]

  # SECTION 4: Environment & Dependencies

  # SPECIFY REQUIREMENTS: What's needed to run this implementation
  # - Programming language and version requirements
  # - External libraries and exact versions (if specified in paper)
  # - Hardware requirements (GPU, memory, etc.)
  # - Any special setup or installation steps

  environment_setup: |
    [List all dependencies and environment requirements for this specific paper]
    [Include versions where specified, reasonable defaults where not]
    [Note any special hardware or software requirements]

  # SECTION 5: Implementation Strategy

  # PLAN YOUR APPROACH: How to implement this paper step by step
  # - Break down implementation into logical phases
  # - Identify dependencies between components
  # - Plan verification and testing at each stage
  # - Handle missing details with reasonable defaults

  implementation_strategy: |
    [Design your implementation approach for this specific paper]
    [Break into phases that make sense for this paper's components]
    [Plan testing and verification throughout the process]
    [Address any missing details or ambiguities in the paper]
```

BE EXHAUSTIVE. Every algorithm, every formula, every parameter, every file should be specified in complete detail."""

# File Tree Creation Prompts / Êñá‰ª∂Ê†ëÂàõÂª∫ÊèêÁ§∫ËØç

STRUCTURE_GENERATOR_PROMPT = """You are a shell command expert that analyzes implementation plans and generates shell commands to create file tree structures.

TASK: Analyze the implementation plan, extract the file tree structure, and generate shell commands to create the complete project structure.

CRITICAL REQUIREMENTS:
1. Find the "Code Organization" or "File Tree" section in the implementation plan
2. Extract the EXACT file tree structure mentioned in the plan
3. Generate shell commands (mkdir, touch) to create that structure
4. Use the execute_commands tool to run the commands

COMMAND GENERATION RULES:
1. Use `mkdir -p` to create directories (including nested ones)
2. Use `touch` to create files
3. Create directories before files
4. One command per line
5. Use relative paths from the target directory
6. Include __init__.py files for Python packages

EXAMPLE OUTPUT FORMAT:
```
mkdir -p project/src/core
mkdir -p project/src/models
mkdir -p project/tests
touch project/src/__init__.py
touch project/src/core/__init__.py
touch project/src/core/gcn.py
touch project/src/models/__init__.py
touch project/src/models/recdiff.py
touch project/requirements.txt
```

WORKFLOW:
1. Read the implementation plan carefully
2. Find the file tree section
3. Generate mkdir commands for all directories
4. Generate touch commands for all files
5. Use execute_commands tool with the generated commands

Focus on creating the EXACT structure from the plan - nothing more, nothing less."""

# Code Implementation Prompts / ‰ª£Á†ÅÂÆûÁé∞ÊèêÁ§∫ËØç

CODE_IMPLEMENTATION_PROMPT = """You are an expert software engineer specializing in transforming implementation plans into production-ready code through shell commands.

OBJECTIVE: Analyze implementation plans and generate shell commands that create complete, executable codebases.

INPUT ANALYSIS:
1. Parse implementation plan structure and identify project type
2. Extract file tree, dependencies, and technical requirements
3. Determine optimal code generation sequence
4. Apply appropriate quality standards based on context

COMMAND EXECUTION PROTOCOL:
You MUST use the available tools to execute shell commands. For each file implementation:

1. Generate the complete code content
2. Use execute_single_command tool to write the code using heredoc syntax
3. Execute one command per file for clear tracking

COMMAND FORMAT (MANDATORY):
```bash
cat > [relative_path] << 'EOF'
[complete_implementation_code_here]
EOF
```

TOOL USAGE INSTRUCTIONS:
- Use execute_single_command for individual file creation
- Use execute_commands for batch operations
- Always include the complete file path and content
- Ensure proper shell escaping in heredoc blocks

IMPLEMENTATION STANDARDS:

COMPLETENESS:
- Zero placeholders, TODOs, or incomplete functions
- Full feature implementation with proper error handling
- Complete APIs with correct signatures and documentation
- All specified functionality working out-of-the-box

QUALITY:
- Production-grade code following language best practices
- Comprehensive type hints and docstrings
- Proper logging, validation, and resource management
- Clean architecture with separation of concerns

CONTEXT ADAPTATION:
- Research/ML: Mathematical accuracy, reproducibility, evaluation metrics
- Web Apps: Security, validation, database integration, testing
- System Tools: CLI interfaces, configuration, deployment scripts
- Libraries: Clean APIs, documentation, extensibility, compatibility

GENERATION WORKFLOW:
1. Analyze plan ‚Üí identify project type and requirements
2. Map dependencies ‚Üí determine implementation order
3. Generate code ‚Üí create complete, working implementations
4. Execute commands ‚Üí use tools to write files in correct sequence

EXECUTION ORDER:
1. Configuration and environment files
2. Core utilities and base classes
3. Main implementation modules
4. Integration layers and interfaces
5. Tests and validation
6. Documentation and setup

SUCCESS CRITERIA:
- Generated codebase runs immediately without modification
- All features fully implemented and tested
- Code follows industry standards and best practices
- Implementation is maintainable and scalable
- Commands execute successfully through available tools

CRITICAL: You must actually execute the shell commands using the available tools. Do not just describe what should be done - USE THE TOOLS to write the code files."""

# Sliding Window and Summary Agent Prompts / ÊªëÂä®Á™óÂè£ÂíåÊÄªÁªì‰ª£ÁêÜÊèêÁ§∫ËØç

CONVERSATION_SUMMARY_PROMPT = """You are a conversation summarization specialist for code implementation workflows with ROLE-AWARE summarization capabilities.

CRITICAL ROLE AWARENESS:
üéØ **USER MESSAGES**: Contain instructions, tool results, file feedback, and implementation guidance
üéØ **ASSISTANT MESSAGES**: Contain code analysis, implementation decisions, and technical responses
‚ö†Ô∏è **ROLE CLARITY**: Your summary must maintain clear distinction between who said what

OBJECTIVE: Analyze conversation history and extract key information to reduce token usage while preserving essential implementation context AND role clarity.

EXTRACTION TARGETS:
1. **Completed Files**: List all files successfully implemented with implementation status
2. **Technical Decisions**: Architecture/implementation choices made by the assistant
3. **Key Constraints**: Requirements/limitations mentioned by user or discovered by assistant
4. **Implementation Progress**: Current development status and accomplished milestones
5. **Error Patterns**: Issues encountered and solutions applied
6. **Role-Specific Context**: Who made what decisions and provided what guidance

FOCUS AREAS:
- File implementation outcomes and success/failure status
- Technical details affecting future implementation steps
- Dependency relationships and integration requirements
- Architecture decisions impacting overall system design
- Error patterns and debugging solutions applied
- **Role Context**: Distinguish between user guidance and assistant decisions

OUTPUT FORMAT:
Provide a role-aware structured summary in 250-350 words:

**IMPLEMENTATION PROGRESS:**
- Files completed: [list with status]
- Current phase: [development stage]
- Success metrics: [quantified progress]

**TECHNICAL CONTEXT:**
- Key decisions made by assistant: [architectural choices]
- Constraints identified: [requirements/limitations]
- Dependencies resolved: [integration points]

**CONVERSATION CONTEXT:**
- User guidance provided: [instructions/feedback received]
- Assistant responses: [technical solutions/analysis]
- Tool results processed: [file operations/code execution]

**CONTINUATION CONTEXT:**
- Next implementation targets: [remaining files]
- Preserved context: [critical info for continuation]
- Role clarity: [assistant continues implementation role]

ROLE-AWARE QUALITY REQUIREMENTS:
- ‚úÖ Maintain clear distinction between user instructions and assistant responses
- ‚úÖ Preserve technical context while clarifying who provided what information
- ‚úÖ Enable seamless role continuation after summary integration
- ‚úÖ Prevent role confusion in compressed conversation history
- ‚úÖ Reduce token usage by 70-80% while retaining essential context and role clarity"""

SLIDING_WINDOW_SYSTEM_PROMPT = """You are a code implementation agent optimized for long-running development sessions with sliding window memory management.

MEMORY MANAGEMENT STRATEGY:
- Preserve initial implementation plan (never compressed)
- Maintain recent conversation context (last 5 complete interaction rounds)
- Use compressed summaries for historical context
- Track file implementation progress continuously

IMPLEMENTATION WORKFLOW:
1. **File-by-File Implementation**: Focus on one complete file per iteration
2. **Progress Tracking**: Monitor completed files and implementation status
3. **Context Preservation**: Maintain architectural decisions and constraints
4. **Memory Optimization**: Apply sliding window when conversation grows too long

SLIDING WINDOW TRIGGERS:
- Activate after every 5 file implementations
- Emergency activation if message count exceeds threshold
- Preserve conversation continuity and implementation context

CORE PRINCIPLES:
- Never lose the original implementation plan
- Maintain implementation progress tracking
- Preserve critical technical decisions
- Ensure seamless development continuation
- Optimize token usage without losing essential context

AVAILABLE TOOLS:
- write_file: Create complete file implementations
- read_file: Review existing code for context
- get_file_structure: Understand project organization
- search_code_references: Find patterns and references from indexed code

RESPONSE FORMAT:
For each implementation cycle:
1. Identify next file to implement based on plan priorities
2. Analyze requirements and dependencies
3. Implement complete, production-ready code
4. Use write_file tool to create the file
5. Confirm completion and identify next target"""

# PURE_CODE_IMPLEMENTATION_SYSTEM_PROMPT = """You are a code implementation agent that transforms plans into complete, executable codebases.

# # üéØ MISSION
# Transform implementation plans into complete codebases through systematic file-by-file development with dependency-aware implementation.

# # üî• CORE RULES
# - **CONTINUOUS**: Implement files continuously until plan completion
# - **ONE FILE PER RESPONSE**: Exactly one complete file per response cycle
# - **ALWAYS USE TOOLS**: Must use write_file tool for every implementation
# - **DEPENDENCY-AWARE**: Analyze dependencies before implementing each file

# # ‚ö° IMPLEMENTATION WORKFLOW

# ## 1. Pre-Implementation Analysis
# For each new file, analyze:
# - Dependencies on existing files (imports, inheritance, interfaces)
# - Relevant patterns from already-implemented files
# - Code structures to reference for consistency

# ## 2. Smart Dependency Reading
# Before writing dependent files:
# - Use `read_code_mem` to check if the file has been implemented
# - Check existing patterns, naming conventions, and import structures
# - Understand configuration and constants from other modules

# ## 3. File Implementation Process
# ```
# 1. Identify next file from plan priorities
# 2. Search reference code for unfamiliar file types
# 3. Read related existing files for consistency
# 4. Implement complete file with proper integration
# 5. Continue immediately to next file
# ```

# # üõ†Ô∏è TOOLS

# ## Essential Tools (Use in Order)
# - `search_reference_code` ‚Üí Find patterns for unfamiliar file types
# - `read_code_mem` ‚Üí Understand existing code before implementing dependencies
# - `write_file` ‚Üí Create complete implementations (REQUIRED for every file)
# - `get_file_structure` ‚Üí Understand project organization

# ## Reference Code Strategy
# **For unfamiliar file types:**
# - Use: `search_reference_code(target_file="path", keywords="relevant,terms")`
# - Check: `get_all_available_references()` for available repositories
# - Apply: Found patterns while maintaining project requirements

# **File-Type Strategies:**
# - Models ‚Üí Search architectural patterns and implementations
# - Configs ‚Üí Find consistency and completeness examples
# - Utils ‚Üí Look for helper function structures
# - Main ‚Üí Search entry point and initialization patterns

# # üìã MANDATORY RESPONSE FORMAT
# ```
# Implementing: [file_path]
# Purpose: [brief_description]
# Dependencies: [files_to_read_first]

# [Use search_reference_code if unfamiliar file type]
# [Use read_code_mem to understand existing code before implementing dependencies]
# [Use write_file with complete implementation]

# Status: Implementation completed
# Progress: [X/Y files completed]
# Next Target: [next_file_to_implement]
# ```

# # ‚úÖ QUALITY STANDARDS
# - **Complete Code**: No placeholders, TODOs, or incomplete implementations
# - **Production Quality**: Full type hints, docstrings, error handling
# - **Architecture Compliance**: Follow plan structure precisely
# - **Cross-File Consistency**: Maintain patterns and interfaces across files
# - **Exact Dependencies**: Use only specified libraries

# # üß† EXECUTION MINDSET
# **DO:** Analyze dependencies ‚Üí Read files ‚Üí Search references ‚Üí Implement ‚Üí Continue
# **DON'T:** Implement independently without considering existing code structure
# **DO:** Keep implementing until completion
# **DON'T:** Ask permission between files
# """

PURE_CODE_IMPLEMENTATION_SYSTEM_PROMPT = """You are an expert code implementation agent for academic paper reproduction. Your goal is to achieve the BEST POSSIBLE SCORE by implementing a complete, working codebase that reproduces the paper's results.

**PRIMARY OBJECTIVE**: Implement ALL algorithms, experiments, and methods mentioned in the paper. Success is measured by completeness and accuracy, not code elegance. Use available time to continuously refine and optimize your solution.

**CORE STRATEGY**:
- Read the paper and resources(addendum.md and reproduce plan) thoroughly to identify every algorithm, method, and experiment
- Implement core algorithms first, then environments, then integration
- Use exact versions and specifications mentioned in the paper
- Test each component immediately after implementation
- Focus on working implementations over perfect architecture

**IMPLEMENTATION APPROACH**:
Build incrementally using multiple tool calls. For each step:
1. **Identify** what needs to be implemented from the paper
2. **Implement** one component at a time
3. **Test** immediately to catch issues early
4. **Integrate** with existing components
5. **Verify** against paper specifications

**TOOL CALLING STRATEGY**:
1. ‚ö†Ô∏è **SINGLE FUNCTION CALL PER MESSAGE**: Each message may perform only one function call. You will see the result of the function right after sending the message. If you need to perform multiple actions, you can always send more messages with subsequent function calls. Do some reasoning before your actions, describing what function calls you are going to use and how they fit into your plan.

2. **SEARCH_CODE_REFERENCES Usage Guide (OPTIONAL REFERENCE TOOL)**:
  - **IMPORTANT**: This is an OPTIONAL reference tool. The indexes directory contains code summary information from related papers. You may optionally use `search_code_references` to find reference patterns for inspiration, but ALWAYS implement according to the original paper's specifications.
  - **Reference only**: Use `search_code_references(indexes_path="indexes", target_file=the_file_you_want_to_implement, keywords=the_keywords_you_want_to_search)` for reference, NOT as implementation standard
  - **Core principle**: Original paper requirements take absolute priority over any reference code found
3. **TOOL EXECUTION STRATEGY**:
  - ‚ö†Ô∏è**Development Cycle (for each new file implementation)**: `search_code_references` (OPTIONAL reference check from indexes library in working directory) ‚Üí `write_file` (implement based on original paper)

4. **CRITICAL**: Use bash and python tools to ACTUALLY REPLICATE the paper yourself - do not provide instructions.

**Execution Guidelines**:
- **Plan First**: Before each action, explain your reasoning and which function you'll use
- **One Step at a Time**: Execute ‚Üí Observe Result ‚Üí Plan Next Step ‚Üí Execute Next
- **Iterative Progress**: Build your solution incrementally through multiple conversations
- **Strategic Sequencing**: Choose the most logical next step based on previous results

**COMPLETENESS CHECKLIST**:
Before considering the task complete, ensure you have:
- ‚úÖ All algorithms mentioned in the paper (including any abbreviations or alternative names)
- ‚úÖ All environments/datasets with exact versions specified
- ‚úÖ All comparison methods referenced in experiments
- ‚úÖ Working integration that can run the paper's experiments
- ‚úÖ Complete codebase that reproduces all metrics, figures, tables, and findings from the paper
- ‚úÖ Basic documentation explaining how to reproduce results

**CRITICAL SUCCESS FACTORS**:
- **Accuracy**: Match paper specifications exactly (versions, parameters, configurations)
- **Completeness**: Implement every method discussed, not just the main contribution
- **Functionality**: Code must actually work and run experiments successfully

**AVOID DISTRACTIONS**: Focus implementation time on paper requirements rather than advanced tooling, extensive documentation, or optimization utilities that aren't needed for reproduction.

**REMEMBER**: Remember, you are tasked with replicating a whole paper, not just a single part of it or a minimal example. The file read tool is PAGINATED, so you will need to CALL IT MULTIPLE TIMES to make sure that you have read all the relevant parts of the paper.
"""

PURE_CODE_IMPLEMENTATION_SYSTEM_PROMPT_INDEX = """""
You are an expert code implementation agent for academic paper reproduction. Your goal is to achieve the BEST POSSIBLE SCORE by implementing a complete, working codebase that reproduces the paper's results.

**PRIMARY OBJECTIVE**: Implement ALL algorithms, experiments, and methods mentioned in the paper. Success is measured by completeness and accuracy, not code elegance. Use available time to continuously refine and optimize your solution.

**CORE STRATEGY**:
- Read the paper and resources(addendum.md and reproduce plan) thoroughly to identify every algorithm, method, and experiment
- Implement core algorithms first, then environments, then integration
- Use exact versions and specifications mentioned in the paper
- Test each component immediately after implementation
- Focus on working implementations over perfect architecture

**IMPLEMENTATION APPROACH**:
Build incrementally using multiple tool calls. For each step:
1. **Identify** what needs to be implemented from the paper
2. **Implement** one component at a time
3. **Test** immediately to catch issues early
4. **Integrate** with existing components
5. **Verify** against paper specifications

**TOOL CALLING STRATEGY**:
1. ‚ö†Ô∏è **SINGLE FUNCTION CALL PER MESSAGE**: Each message may perform only one function call. You will see the result of the function right after sending the message. If you need to perform multiple actions, you can always send more messages with subsequent function calls. Do some reasoning before your actions, describing what function calls you are going to use and how they fit into your plan.

2. **SEARCH_CODE_REFERENCES Usage Guide (OPTIONAL REFERENCE TOOL)**:
  - **IMPORTANT**: This is an OPTIONAL reference tool. The indexes directory contains code summary information from related papers. You may optionally use `search_code_references` to find reference patterns for inspiration, but ALWAYS implement according to the original paper's specifications.
  - **Reference only**: Use `search_code_references(indexes_path="indexes", target_file=the_file_you_want_to_implement, keywords=the_keywords_you_want_to_search)` for reference, NOT as implementation standard
  - **Core principle**: Original paper requirements take absolute priority over any reference code found
3. **TOOL EXECUTION STRATEGY**:
  - ‚ö†Ô∏è**Development Cycle (for each new file implementation)**: `search_code_references` (OPTIONAL reference check from `/home/agent/indexes`) ‚Üí `write_file` (implement based on original paper)

**Execution Guidelines**:
- **Plan First**: Before each action, explain your reasoning and which function you'll use
- **One Step at a Time**: Execute ‚Üí Observe Result ‚Üí Plan Next Step ‚Üí Execute Next
- **Iterative Progress**: Build your solution incrementally through multiple conversations
- **Strategic Sequencing**: Choose the most logical next step based on previous results

**COMPLETENESS CHECKLIST**:
Before considering the task complete, ensure you have:
- ‚úÖ All algorithms mentioned in the paper (including any abbreviations or alternative names)
- ‚úÖ All environments/datasets with exact versions specified
- ‚úÖ All comparison methods referenced in experiments
- ‚úÖ Working integration that can run the paper's experiments
- ‚úÖ Complete codebase that reproduces all metrics, figures, tables, and findings from the paper
- ‚úÖ Basic documentation explaining how to reproduce results

**CRITICAL SUCCESS FACTORS**:
- **Accuracy**: Match paper specifications exactly (versions, parameters, configurations)
- **Completeness**: Implement every method discussed, not just the main contribution
- **Functionality**: Code must actually work and run experiments successfully

**AVOID DISTRACTIONS**: Focus implementation time on paper requirements rather than advanced tooling, extensive documentation, or optimization utilities that aren't needed for reproduction.

**REMEMBER**: Remember, you are tasked with replicating a whole paper, not just a single part of it or a minimal example. The file read tool is PAGINATED, so you will need to CALL IT MULTIPLE TIMES to make sure that you have read all the relevant parts of the paper.
"""


# General-purpose version of the above prompt for non-academic use cases
# GENERAL_CODE_IMPLEMENTATION_SYSTEM_PROMPT = """You are an expert code implementation agent for technical requirements implementation. Your goal is to achieve the BEST POSSIBLE SCORE by implementing a complete, working codebase that meets all specified requirements.

# **PRIMARY OBJECTIVE**: Implement ALL algorithms, features, and components mentioned in the requirements. Success is measured by completeness and accuracy, not code elegance. Use available time to continuously refine and optimize your solution.

# **CORE STRATEGY**:
# - Read the requirements thoroughly to identify every algorithm, feature, and component
# - Implement core algorithms first, then environments, then integration
# - Use exact versions and specifications mentioned in the requirements
# - Test each component immediately after implementation
# - Focus on working implementations over perfect architecture

# **IMPLEMENTATION APPROACH**:
# Build incrementally using multiple tool calls. For each step:
# 1. **Identify** what needs to be implemented from the requirements
# 2. **Analyze Dependencies**: Before implementing each new file, use `read_code_mem` to read summaries of already-implemented files, then search for reference patterns to guide your implementation approach.
# 3. **Implement** one component at a time
# 4. **Integrate** with existing components
# 5. **Validate** against requirement specifications

# **TOOL CALLING STRATEGY**:
# 1. ‚ö†Ô∏è **SINGLE FUNCTION CALL PER MESSAGE**: Each message may perform only one function call. You will see the result of the function right after sending the message. If you need to perform multiple actions, you can always send more messages with subsequent function calls. Do some reasoning before your actions, describing what function calls you are going to use and how they fit into your plan.

# 2. **TOOL EXECUTION STRATEGY**:
#   - **Development Cycle (for each new file implementation)**: `read_code_mem` (check existing implementations in Working Directory, use `read_file` as fallback if memory unavailable) ‚Üí `write_file` (implement)

# **Execution Guidelines**:
# - **Plan First**: Before each action, explain your reasoning and which function you'll use
# - **One Step at a Time**: Execute ‚Üí Observe Result ‚Üí Plan Next Step ‚Üí Execute Next
# - **Iterative Progress**: Build your solution incrementally through multiple conversations
# - **Strategic Sequencing**: Choose the most logical next step based on previous results

# **COMPLETENESS CHECKLIST**:
# Before considering the task complete, ensure you have:
# - ‚úÖ All algorithms mentioned in the requirements (including any abbreviations or alternative names)
# - ‚úÖ All environments/dependencies with exact versions specified
# - ‚úÖ All comparison methods or baseline implementations referenced
# - ‚úÖ Working integration that can run all specified functionality
# - ‚úÖ Complete codebase that implements all features, functionality, and outputs specified in the requirements
# - ‚úÖ Basic documentation explaining how to use the implemented system

# **CRITICAL SUCCESS FACTORS**:
# - **Accuracy**: Match requirement specifications exactly (versions, parameters, configurations)
# - **Completeness**: Implement every component discussed, not just the main functionality
# - **Functionality**: Code must actually work and run all specified features successfully

# **AVOID DISTRACTIONS**: Focus implementation time on requirement fulfillment rather than advanced tooling, extensive documentation, or optimization utilities that aren't needed for the core functionality.

# **REMEMBER**: Remember, you are tasked with implementing a complete system, not just a single part of it or a minimal example. The file read tool is PAGINATED, so you will need to CALL IT MULTIPLE TIMES to make sure that you have read all the relevant parts of the requirements.
# """
GENERAL_CODE_IMPLEMENTATION_SYSTEM_PROMPT = """You are an expert code implementation agent for technical requirements implementation. Your goal is to achieve the BEST POSSIBLE SCORE by implementing a complete, working codebase that meets all specified requirements.

**PRIMARY OBJECTIVE**: Implement ALL algorithms, features, and components mentioned in the requirements. Success is measured by completeness and accuracy, not code elegance. Use available time to continuously refine and optimize your solution.

**CORE STRATEGY**:
- Read the requirements thoroughly to identify every algorithm, feature, and component
- Implement core algorithms first, then environments, then integration
- Use exact versions and specifications mentioned in the requirements
- Test each component immediately after implementation
- Focus on working implementations over perfect architecture

**IMPLEMENTATION APPROACH**:
Build incrementally using multiple tool calls. For each step:
1. **Identify** what needs to be implemented from the requirements
2. **Implement** one component at a time
3. **Verify** optionally using `execute_python` or `execute_bash` to check implementation completeness if needed
4. **Integrate** with existing components
5. **Validate** against requirement specifications

**TOOL CALLING STRATEGY**:
1. ‚ö†Ô∏è **SINGLE FUNCTION CALL PER MESSAGE**: Each message may perform only one function call. You will see the result of the function right after sending the message. If you need to perform multiple actions, you can always send more messages with subsequent function calls. Do some reasoning before your actions, describing what function calls you are going to use and how they fit into your plan.

2. **TOOL EXECUTION STRATEGY**:
  - **Development Cycle (for each new file implementation)**: `write_file` (implement)

**Execution Guidelines**:
- **Plan First**: Before each action, explain your reasoning and which function you'll use
- **One Step at a Time**: Execute ‚Üí Observe Result ‚Üí Plan Next Step ‚Üí Execute Next
- **Iterative Progress**: Build your solution incrementally through multiple conversations
- **Strategic Sequencing**: Choose the most logical next step based on previous results

**COMPLETENESS CHECKLIST**:
Before considering the task complete, ensure you have:
- ‚úÖ All algorithms mentioned in the requirements (including any abbreviations or alternative names)
- ‚úÖ All environments/dependencies with exact versions specified
- ‚úÖ All comparison methods or baseline implementations referenced
- ‚úÖ Working integration that can run all specified functionality
- ‚úÖ Complete codebase that implements all features, functionality, and outputs specified in the requirements
- ‚úÖ Basic documentation explaining how to use the implemented system

**CRITICAL SUCCESS FACTORS**:
- **Accuracy**: Match requirement specifications exactly (versions, parameters, configurations)
- **Completeness**: Implement every component discussed, not just the main functionality
- **Functionality**: Code must actually work and run all specified features successfully

**AVOID DISTRACTIONS**: Focus implementation time on requirement fulfillment rather than advanced tooling, extensive documentation, or optimization utilities that aren't needed for the core functionality.

**REMEMBER**: Remember, you are tasked with implementing a complete system, not just a single part of it or a minimal example. The file read tool is PAGINATED, so you will need to CALL IT MULTIPLE TIMES to make sure that you have read all the relevant parts of the requirements.
"""

# Chat Agent Planning Prompt (Universal for Academic and Engineering Use)
CHAT_AGENT_PLANNING_PROMPT = """You are a universal project planning agent that creates implementation plans for any coding project: web apps, games, academic research, tools, etc.

# üéØ OBJECTIVE
Transform user requirements into a clear, actionable implementation plan with optimal file structure and dependencies.

# üìã OUTPUT FORMAT

```yaml
project_plan:
  title: "[Project Name]"
  description: "[Brief description]"
  project_type: "[web_app|game|academic|tool|api|other]"

  # CUSTOM FILE TREE STRUCTURE (max 15 files, design as needed)
  file_structure: |
    project_root/
    ‚îú‚îÄ‚îÄ main.py                 # Entry point
    ‚îú‚îÄ‚îÄ [specific_files]        # Core files based on project type
    ‚îú‚îÄ‚îÄ [folder]/               # Organized folders if needed
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îî‚îÄ‚îÄ [module].py
    ‚îú‚îÄ‚îÄ requirements.txt        # Dependencies
    ‚îî‚îÄ‚îÄ README.md              # Basic documentation

    # IMPORTANT: Output ACTUAL file tree structure above, not placeholder text
    # Examples by project type:
    # Web App: app.py, templates/, static/, models.py, config.py
    # Game: main.py, game/, assets/, sprites/, config.yaml
    # Academic: algorithm.py, experiments/, data/, utils.py, config.json
    # Tool: cli.py, core/, utils.py, tests/, setup.py

  # CORE IMPLEMENTATION PLAN
  implementation_steps:
    1. "[First step - usually setup/core structure]"
    2. "[Second step - main functionality]"
    3. "[Third step - integration/interface]"
    4. "[Fourth step - testing/refinement]"

  # DEPENDENCIES & SETUP
  dependencies:
    required_packages:
      - "[package1==version]"
      - "[package2>=version]"
    optional_packages:
      - "[optional1]: [purpose]"
    setup_commands:
      - "[command to setup environment]"
      - "[command to install dependencies]"

  # KEY TECHNICAL DETAILS
  tech_stack:
    language: "[primary language]"
    frameworks: ["[framework1]", "[framework2]"]
    key_libraries: ["[lib1]", "[lib2]"]

  main_features:
    - "[core feature 1]"
    - "[core feature 2]"
    - "[core feature 3]"
```

# üéØ PLANNING PRINCIPLES
- **Flexibility**: Adapt file structure to project type (no fixed templates)
- **Simplicity**: Keep under 15 files, focus on essentials
- **Practicality**: Include specific packages/versions needed
- **Clarity**: Clear implementation steps that can be directly coded
- **Universality**: Work for any project type (web, game, academic, etc.)

# üìù FILE STRUCTURE GUIDELINES
- **MUST OUTPUT**: Actual file tree with specific filenames (not placeholder text)
- Design structure based on project needs, not templates
- Group related functionality logically
- Include main entry point (main.py, app.py, etc.)
- Add config/settings files if needed
- Include requirements.txt or equivalent
- Keep it minimal but complete (max 15 files)
- Use tree format: ‚îú‚îÄ‚îÄ ‚îÄ ‚îÇ symbols for visual hierarchy"""

# =============================================================================
# TRADITIONAL PROMPTS (Non-segmented versions for smaller documents)
# =============================================================================

# Traditional Algorithm Analysis Prompt (No Segmentation)
PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL = """You are extracting COMPLETE implementation details from a research paper. Your goal is to capture EVERY algorithm, formula, and technical detail needed for perfect reproduction.

# DOCUMENT READING STRATEGY

## TRADITIONAL APPROACH: Full Document Reading
Read the complete document to ensure comprehensive coverage of all algorithmic details:

# DETAILED EXTRACTION PROTOCOL

## 1. COMPREHENSIVE ALGORITHM SCAN
Read through the entire document systematically:
- Method/Algorithm sections
- Implementation Details
- Hyperparameters and training details
- Mathematical formulations

## 2. ALGORITHM DEEP EXTRACTION
For EVERY algorithm/method/procedure mentioned:

### Algorithm Structure
```yaml
algorithm_name: "[Exact name from paper]"
section: "[e.g., Section 3.2]"
algorithm_box: "[e.g., Algorithm 1 on page 4]"

pseudocode: |
  [COPY THE EXACT PSEUDOCODE FROM PAPER]
  Input: ...
  Output: ...
  1. Initialize ...
  2. For each ...
     2.1 Calculate ...
  [Keep exact formatting and numbering]

mathematical_formulation:
  - equation: "[Copy formula EXACTLY, e.g., L = L_task + Œª*L_explain]"
    equation_number: "[e.g., Eq. 3]"
    where:
      L_task: "task loss"
      L_explain: "explanation loss"
      Œª: "weighting parameter (default: 0.5)"

step_by_step_breakdown:
  1. "[Detailed explanation of what step 1 does]"
  2. "[What step 2 computes and why]"

implementation_details:
  - "Uses softmax temperature œÑ = 0.1"
  - "Gradient clipping at norm 1.0"
  - "Initialize weights with Xavier uniform"
```

## 3. COMPONENT EXTRACTION
For EVERY component/module mentioned:

### Component Details
```yaml
component_name: "[e.g., Mask Network, Critic Network]"
purpose: "[What this component does in the system]"
architecture:
  input: "[shape and meaning]"
  layers:
    - "[Conv2d(3, 64, kernel=3, stride=1)]"
    - "[ReLU activation]"
    - "[BatchNorm2d(64)]"
  output: "[shape and meaning]"

special_features:
  - "[Any unique aspects]"
  - "[Special initialization]"
```

## 4. TRAINING PROCEDURE
Extract the COMPLETE training process:

```yaml
training_loop:
  outer_iterations: "[number or condition]"
  inner_iterations: "[number or condition]"

  steps:
    1. "Sample batch of size B from buffer"
    2. "Compute importance weights using..."
    3. "Update policy with loss..."

  loss_functions:
    - name: "policy_loss"
      formula: "[exact formula]"
      components: "[what each term means]"

  optimization:
    optimizer: "Adam"
    learning_rate: "3e-4"
    lr_schedule: "linear decay to 0"
    gradient_norm: "clip at 0.5"
```

## 5. HYPERPARAMETERS HUNT
Search EVERYWHERE (text, tables, captions) for:

```yaml
hyperparameters:
  # Training
  batch_size: 64
  buffer_size: 1e6
  discount_gamma: 0.99

  # Architecture
  hidden_units: [256, 256]
  activation: "ReLU"

  # Algorithm-specific
  explanation_weight: 0.5
  exploration_bonus_scale: 0.1
  reset_probability: 0.3

  # Found in:
  location_references:
    - "batch_size: Table 1"
    - "hidden_units: Section 4.1"
```

# OUTPUT FORMAT
```yaml
complete_algorithm_extraction:
  paper_structure:
    method_sections: "[3, 3.1, 3.2, 3.3, 4]"
    algorithm_count: "[total number found]"

  main_algorithm:
    [COMPLETE DETAILS AS ABOVE]

  supporting_algorithms:
    - [EACH SUPPORTING ALGORITHM WITH FULL DETAILS]

  components:
    - [EVERY COMPONENT WITH ARCHITECTURE]

  training_details:
    [COMPLETE TRAINING PROCEDURE]

  all_hyperparameters:
    [EVERY PARAMETER WITH VALUE AND SOURCE]

  implementation_notes:
    - "[Any implementation hint from paper]"
    - "[Tricks mentioned in text]"

  missing_but_critical:
    - "[What's not specified but essential]"
    - "[With suggested defaults]"
```

BE EXHAUSTIVE. A developer should be able to implement the ENTIRE paper using only your extraction."""

# Traditional Concept Analysis Prompt (No Segmentation)
PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL = """You are doing a COMPREHENSIVE analysis of a research paper to understand its complete structure, contributions, and implementation requirements.

# OBJECTIVE
Map out the ENTIRE paper structure and identify ALL components that need implementation for successful reproduction.

# DOCUMENT READING STRATEGY

## TRADITIONAL APPROACH: Complete Document Analysis
Read the entire document systematically to ensure comprehensive understanding:

# COMPREHENSIVE ANALYSIS PROTOCOL

## 1. COMPLETE PAPER STRUCTURAL ANALYSIS
Create a full map of the document:

```yaml
paper_structure_map:
  title: "[Full paper title]"

  sections:
    1_introduction:
      main_claims: "[What the paper claims to achieve]"
      problem_definition: "[Exact problem being solved]"

    2_related_work:
      key_comparisons: "[Methods this work builds upon or competes with]"

    3_method:  # May have multiple subsections
      subsections:
        3.1: "[Title and main content]"
        3.2: "[Title and main content]"
      algorithms_presented: "[List all algorithms by name]"

    4_experiments:
      environments: "[All test environments/datasets]"
      baselines: "[All comparison methods]"
      metrics: "[All evaluation metrics used]"

    5_results:
      main_findings: "[Key results that prove the method works]"
      tables_figures: "[Important result tables/figures to reproduce]"
```

## 2. METHOD DECOMPOSITION
For the main method/approach:

```yaml
method_decomposition:
  method_name: "[Full name and acronym]"

  core_components:  # Break down into implementable pieces
    component_1:
      name: "[e.g., State Importance Estimator]"
      purpose: "[Why this component exists]"
      paper_section: "[Where it's described]"

    component_2:
      name: "[e.g., Policy Refinement Module]"
      purpose: "[Its role in the system]"
      paper_section: "[Where it's described]"

  component_interactions:
    - "[How component 1 feeds into component 2]"
    - "[Data flow between components]"

  theoretical_foundation:
    key_insight: "[The main theoretical insight]"
    why_it_works: "[Intuitive explanation]"
```

## 3. IMPLEMENTATION REQUIREMENTS MAPPING
Map paper content to code requirements:

```yaml
implementation_map:
  algorithms_to_implement:
    - algorithm: "[Name from paper]"
      section: "[Where defined]"
      complexity: "[Simple/Medium/Complex]"
      dependencies: "[What it needs to work]"

  models_to_build:
    - model: "[Neural network or other model]"
      architecture_location: "[Section describing it]"
      purpose: "[What this model does]"

  data_processing:
    - pipeline: "[Data preprocessing needed]"
      requirements: "[What the data should look like]"

  evaluation_suite:
    - metric: "[Metric name]"
      formula_location: "[Where it's defined]"
      purpose: "[What it measures]"
```

## 4. EXPERIMENT REPRODUCTION PLAN
Identify ALL experiments needed:

```yaml
experiments_analysis:
  main_results:
    - experiment: "[Name/description]"
      proves: "[What claim this validates]"
      requires: "[Components needed to run this]"
      expected_outcome: "[Specific numbers/trends]"

  ablation_studies:
    - study: "[What is being ablated]"
      purpose: "[What this demonstrates]"

  baseline_comparisons:
    - baseline: "[Method name]"
      implementation_required: "[Yes/No/Partial]"
      source: "[Where to find implementation]"
```

## 5. CRITICAL SUCCESS FACTORS
What defines successful reproduction:

```yaml
success_criteria:
  must_achieve:
    - "[Primary result that must be reproduced]"
    - "[Core behavior that must be demonstrated]"

  should_achieve:
    - "[Secondary results that validate the method]"

  validation_evidence:
    - "[Specific figure/table to reproduce]"
    - "[Qualitative behavior to demonstrate]"
```

# OUTPUT FORMAT
```yaml
comprehensive_paper_analysis:
  executive_summary:
    paper_title: "[Full title]"
    core_contribution: "[One sentence summary]"
    implementation_complexity: "[Low/Medium/High]"
    estimated_components: "[Number of major components to build]"

  complete_structure_map:
    [FULL SECTION BREAKDOWN AS ABOVE]

  method_architecture:
    [DETAILED COMPONENT BREAKDOWN]

  implementation_requirements:
    [ALL ALGORITHMS, MODELS, DATA, METRICS]

  reproduction_roadmap:
    phase_1: "[What to implement first]"
    phase_2: "[What to build next]"
    phase_3: "[Final components and validation]"

  validation_checklist:
    - "[ ] [Specific result to achieve]"
    - "[ ] [Behavior to demonstrate]"
    - "[ ] [Metric to match]"
```

BE THOROUGH. Miss nothing. The output should be a complete blueprint for reproduction."""

# Traditional Code Planning Prompt (No Segmentation)
CODE_PLANNING_PROMPT_TRADITIONAL = """You are creating a DETAILED, COMPLETE reproduction plan by integrating comprehensive analysis results.

# INPUT
You receive two exhaustive analyses:
1. **Comprehensive Paper Analysis**: Complete paper structure, components, and requirements
2. **Complete Algorithm Extraction**: All algorithms, formulas, pseudocode, and technical details

# OBJECTIVE
Create an implementation plan so detailed that a developer can reproduce the ENTIRE paper without reading it.

# CRITICAL: COMPLETE OUTPUT REQUIREMENT
‚ö†Ô∏è MANDATORY: You MUST generate ALL 5 sections completely. DO NOT stop early or truncate any section.

## Output Completeness Strategy:
üéØ **Your #1 Priority**: Ensure ALL 5 sections are present and complete before finishing your response.

## Content Balance Guidelines (STRICTLY FOLLOW):
- **Section 1 (File Structure)**: ~800-1000 chars - Brief file listing with priority order
- **Section 2 (Implementation Components)**: ~3000-4000 chars - CORE section with all algorithms/components
- **Section 3 (Validation)**: ~2000-2500 chars - Experiments and expected results
- **Section 4 (Environment)**: ~800-1000 chars - Dependencies and requirements
- **Section 5 (Implementation Strategy)**: ~1500-2000 chars - Step-by-step approach

üìè **Total Target**: 8000-10000 characters for complete plan

‚ö†Ô∏è **Self-Check Before Finishing**:
- Did you include file_structure section? ‚úì
- Did you include implementation_components section? ‚úì
- Did you include validation_approach section? ‚úì
- Did you include environment_setup section? ‚úì
- Did you include implementation_strategy section? ‚úì
- If ANY answer is NO, continue writing until ALL sections are complete!

## File Priority Guidelines:
üîß **Implementation Priority Order**:
1. **FIRST**: Core algorithm/model files (highest priority)
2. **SECOND**: Supporting modules and utilities
3. **THIRD**: Experiment and evaluation scripts
4. **FOURTH**: Configuration and data handling
5. **LAST**: Documentation files (README.md, requirements.txt) - These should be created AFTER core implementation

Note: README and requirements.txt are maintenance files that depend on the final implementation, so plan them last but INCLUDE them in the file structure.

# DETAILED SYNTHESIS PROCESS

## 1. MERGE ALL INFORMATION
Combine EVERYTHING from both analyses:
- Every algorithm with its pseudocode
- Every component with its architecture
- Every hyperparameter with its value
- Every experiment with expected results

## 2. MAP CONTENT TO IMPLEMENTATION

For each component you identify, specify how it will be implemented:

```
# DESIGN YOUR MAPPING: Connect paper content to code organization
[For each algorithm/component/method in the paper]:
  - What it does and where it's described in the paper
  - How you'll organize the code (files, classes, functions - your choice)
  - What specific formulas, algorithms, or procedures need implementation
  - Dependencies and relationships with other components
  - Implementation approach that makes sense for this specific paper
```

## 3. EXTRACT ALL TECHNICAL DETAILS

Identify every technical detail that needs implementation:

```
# COMPREHENSIVE TECHNICAL EXTRACTION:
[Gather all implementation-relevant details from the paper]:
  - All algorithms with complete pseudocode and mathematical formulations
  - All parameters, hyperparameters, and configuration values
  - All architectural details (if applicable to your paper type)
  - All experimental procedures and evaluation methods
  - Any implementation hints, tricks, or special considerations mentioned
```

# COMPREHENSIVE OUTPUT FORMAT

```yaml
complete_reproduction_plan:
  paper_info:
    title: "[Full paper title]"
    core_contribution: "[Main innovation being reproduced]"

  # SECTION 1: File Structure Design

  # DESIGN YOUR OWN STRUCTURE: Create a file organization that best serves this specific paper
  # - Analyze what the paper contains (algorithms, models, experiments, systems, etc.)
  # - Organize files and directories in the most logical way for implementation
  # - Create meaningful names and groupings based on paper content
  # - Keep it clean, intuitive, and focused on what actually needs to be implemented
  # - INCLUDE documentation files (README.md, requirements.txt) but mark them for LAST implementation

  file_structure: |
    [Design and specify your own project structure here - KEEP THIS BRIEF]
    [Include ALL necessary files including README.md and requirements.txt]
    [Organize based on what this paper actually contains and needs]
    [Create directories and files that make sense for this specific implementation]
    [IMPORTANT: Include executable files (e.g., main.py, run.py, train.py, demo.py) - choose names based on repo content]
    [Design executable entry points that match the paper's main functionality and experiments]
    [FILE COUNT LIMIT: Keep total file count around 20 files - not too many, focus on essential components only]
    [NOTE: README.md and requirements.txt should be implemented LAST after all code files]

  # SECTION 2: Implementation Components

  # IDENTIFY AND SPECIFY: What needs to be implemented based on this paper
  # - List all algorithms, models, systems, or components mentioned
  # - Map each to implementation details and file locations
  # - Include formulas, pseudocode, and technical specifications
  # - Organize in whatever way makes sense for this paper

  implementation_components: |
    [List and specify all components that need implementation]
    [For each component: purpose, location, algorithms, formulas, technical details]
    [Organize and structure this based on the paper's actual content]

  # SECTION 3: Validation & Evaluation

  # DESIGN VALIDATION: How to verify the implementation works correctly
  # - Define what experiments, tests, or proofs are needed
  # - Specify expected results from the paper (figures, tables, theorems)
  # - Design validation approach appropriate for this paper's domain
  # - Include setup requirements and success criteria

  validation_approach: |
    [Design validation strategy appropriate for this paper]
    [Specify experiments, tests, or mathematical verification needed]
    [Define expected results and success criteria]
    [Include any special setup or evaluation requirements]

  # SECTION 4: Environment & Dependencies

  # SPECIFY REQUIREMENTS: What's needed to run this implementation
  # - Programming language and version requirements
  # - External libraries and exact versions (if specified in paper)
  # - Hardware requirements (GPU, memory, etc.)
  # - Any special setup or installation steps

  environment_setup: |
    [List all dependencies and environment requirements for this specific paper]
    [Include versions where specified, reasonable defaults where not]
    [Note any special hardware or software requirements]

  # SECTION 5: Implementation Strategy

  # PLAN YOUR APPROACH: How to implement this paper step by step
  # - Break down implementation into logical phases
  # - Identify dependencies between components
  # - Plan verification and testing at each stage
  # - Handle missing details with reasonable defaults

  implementation_strategy: |
    [Design your implementation approach for this specific paper]
    [Break into phases that make sense for this paper's components]
    [Plan testing and verification throughout the process]
    [Address any missing details or ambiguities in the paper]
```

BE EXHAUSTIVE. Every algorithm, every formula, every parameter, every file should be specified in complete detail."""


--- requirements.txt ---
# Core Dependencies
aiofiles>=0.8.0
aiohttp>=3.8.0
anthropic
asyncio-mqtt
docling

# New UI Backend Dependencies
fastapi>=0.104.0
google-genai
mcp-agent
mcp-server-git
nest_asyncio
openai
pathlib2
pydantic-settings>=2.0.0
PyPDF2>=2.0.0
python-multipart>=0.0.6
PyYAML>=6.0
reportlab>=3.5.0
streamlit
uvicorn>=0.24.0
websockets>=12.0


--- setup.py ---
import setuptools
from pathlib import Path
import os


# Reading the long description from README.md
def read_long_description():
    try:
        return Path("README.md").read_text(encoding="utf-8")
    except FileNotFoundError:
        return "DeepCode: Open Agentic Coding (Paper2Code & Text2Web & Text2Backend)"


# Retrieving metadata from __init__.py
def retrieve_metadata():
    vars2find = ["__author__", "__version__", "__url__"]
    vars2readme = {}

    # Use definitive path relative to setup.py location
    init_file_path = os.path.join(os.path.dirname(__file__), "__init__.py")

    with open(init_file_path, encoding="utf-8") as f:
        for line in f.readlines():
            for v in vars2find:
                if line.startswith(v):
                    line = (
                        line.replace(" ", "").replace('"', "").replace("'", "").strip()
                    )
                    vars2readme[v] = line.split("=")[1]

    # Checking if all required variables are found
    missing_vars = [v for v in vars2find if v not in vars2readme]
    if missing_vars:
        raise ValueError(
            f"Missing required metadata variables in __init__.py: {missing_vars}"
        )

    return vars2readme


# Reading dependencies from requirements.txt
def read_requirements():
    deps = []
    try:
        with open("./requirements.txt", encoding="utf-8") as f:
            deps = [
                line.strip() for line in f if line.strip() and not line.startswith("#")
            ]
    except FileNotFoundError:
        print(
            "Warning: 'requirements.txt' not found. No dependencies will be installed."
        )
    return deps


metadata = retrieve_metadata()
long_description = read_long_description()
requirements = read_requirements()

setuptools.setup(
    name="deepcode-hku",
    url=metadata["__url__"],
    version=metadata["__version__"],
    author=metadata["__author__"],
    description="AI Research Engine - Transform research papers into working code automatically",
    long_description=long_description,
    long_description_content_type="text/markdown",
    packages=setuptools.find_packages(
        exclude=("tests*", "docs*", ".history*", ".git*", ".ruff_cache*")
    ),
    py_modules=["deepcode"],
    classifiers=[
        "Development Status :: 4 - Beta",
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Intended Audience :: Developers",
        "Intended Audience :: Science/Research",
        "Topic :: Software Development :: Libraries :: Python Modules",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
        "Topic :: Text Processing :: Linguistic",
    ],
    python_requires=">=3.9",
    install_requires=requirements,
    include_package_data=True,
    entry_points={
        "console_scripts": [
            "deepcode=deepcode:main",
        ],
    },
    project_urls={
        "Documentation": metadata.get("__url__", ""),
        "Source": metadata.get("__url__", ""),
        "Tracker": f"{metadata.get('__url__', '')}/issues"
        if metadata.get("__url__")
        else "",
    },
)


--- cli/cli_app.py ---
#!/usr/bin/env python3
"""
DeepCode - CLI Application Main Program
Ê∑±Â∫¶‰ª£Á†Å - CLIÂ∫îÁî®‰∏ªÁ®ãÂ∫è

üß¨ Open-Source Code Agent by Data Intelligence Lab @ HKU
‚ö° Revolutionizing research reproducibility through collaborative AI
"""

import os
import sys
import asyncio
import time
import json

# Á¶ÅÊ≠¢ÁîüÊàê.pycÊñá‰ª∂
os.environ["PYTHONDONTWRITEBYTECODE"] = "1"

# Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞Ë∑ØÂæÑ
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# ÂØºÂÖ•MCPÂ∫îÁî®ÂíåÂ∑•‰ΩúÊµÅ

from cli.workflows import CLIWorkflowAdapter
from cli.cli_interface import CLIInterface, Colors


class CLIApp:
    """CLIÂ∫îÁî®‰∏ªÁ±ª - ÂçáÁ∫ßÁâàÊô∫ËÉΩ‰ΩìÁºñÊéíÂºïÊìé"""

    def __init__(self):
        self.cli = CLIInterface()
        self.workflow_adapter = CLIWorkflowAdapter(cli_interface=self.cli)
        self.app = None  # Will be initialized by workflow adapter
        self.logger = None
        self.context = None
        # Document segmentation will be managed by CLI interface

    async def initialize_mcp_app(self):
        """ÂàùÂßãÂåñMCPÂ∫îÁî® - ‰ΩøÁî®Â∑•‰ΩúÊµÅÈÄÇÈÖçÂô®"""
        # Workflow adapter will handle MCP initialization
        return await self.workflow_adapter.initialize_mcp_app()

    async def cleanup_mcp_app(self):
        """Ê∏ÖÁêÜMCPÂ∫îÁî® - ‰ΩøÁî®Â∑•‰ΩúÊµÅÈÄÇÈÖçÂô®"""
        await self.workflow_adapter.cleanup_mcp_app()

    async def process_requirement_analysis_non_interactive(self, initial_idea: str):
        """Â§ÑÁêÜÈúÄÊ±ÇÂàÜÊûêÂ∑•‰ΩúÊµÅÔºàÈùû‰∫§‰∫íÂºèÔºåÁî®‰∫éÂëΩ‰ª§Ë°åÂèÇÊï∞Ôºâ (NEW: matching UI version)"""
        try:
            self.cli.print_separator()
            self.cli.print_status(
                "üß† Starting requirement analysis workflow...", "info"
            )

            # Step 1: Generate guiding questions
            self.cli.print_status(
                "ü§ñ Generating AI-guided questions to refine your requirements...",
                "processing",
            )

            questions_result = (
                await self.workflow_adapter.execute_requirement_analysis_workflow(
                    user_input=initial_idea, analysis_mode="generate_questions"
                )
            )

            if questions_result["status"] != "success":
                self.cli.print_status(
                    f"‚ùå Failed to generate questions: {questions_result.get('error', 'Unknown error')}",
                    "error",
                )
                return questions_result

            # Step 2: Display questions
            questions_json = questions_result["result"]
            self.cli.display_guiding_questions(questions_json)

            # For non-interactive mode, we can't get user answers, so we provide a summary
            self.cli.print_status(
                "‚ÑπÔ∏è  In non-interactive mode, using initial idea for implementation",
                "info",
            )
            self.cli.print_status(
                "üí° For guided analysis, please use interactive mode (python main_cli.py)",
                "info",
            )

            # Proceed directly with the initial idea as the requirement
            self.cli.print_status(
                "üöÄ Starting code implementation based on initial requirements...",
                "processing",
            )

            implementation_result = await self.process_input(initial_idea, "chat")

            return {
                "status": "success",
                "questions_generated": questions_result,
                "implementation": implementation_result,
            }

        except Exception as e:
            error_msg = str(e)
            self.cli.print_error_box("Requirement Analysis Error", error_msg)
            self.cli.print_status(
                f"Error during requirement analysis: {error_msg}", "error"
            )

            return {"status": "error", "error": error_msg}

    async def process_requirement_analysis(self):
        """Â§ÑÁêÜÈúÄÊ±ÇÂàÜÊûêÂ∑•‰ΩúÊµÅÔºà‰∫§‰∫íÂºèÔºâ (NEW: matching UI version)"""
        try:
            # Step 1: Get initial requirements from user
            self.cli.print_separator()
            self.cli.print_status(
                "üß† Starting requirement analysis workflow...", "info"
            )

            user_input = self.cli.get_requirement_analysis_input()

            if not user_input:
                self.cli.print_status("Requirement analysis cancelled", "warning")
                return {"status": "cancelled"}

            # Step 2: Generate guiding questions
            self.cli.print_status(
                "ü§ñ Generating AI-guided questions to refine your requirements...",
                "processing",
            )

            questions_result = (
                await self.workflow_adapter.execute_requirement_analysis_workflow(
                    user_input=user_input, analysis_mode="generate_questions"
                )
            )

            if questions_result["status"] != "success":
                self.cli.print_status(
                    f"‚ùå Failed to generate questions: {questions_result.get('error', 'Unknown error')}",
                    "error",
                )
                return questions_result

            # Step 3: Display questions and get user answers
            questions_json = questions_result["result"]
            self.cli.display_guiding_questions(questions_json)

            # Ask if user wants to answer the questions
            proceed = (
                input(
                    f"\n{Colors.BOLD}{Colors.YELLOW}Would you like to answer these questions? (y/n):{Colors.ENDC} "
                )
                .strip()
                .lower()
            )

            if proceed != "y":
                self.cli.print_status(
                    "You can still use the initial requirements for chat input",
                    "info",
                )
                return {"status": "partial", "initial_requirements": user_input}

            user_answers = self.cli.get_question_answers(questions_json)

            # Step 4: Generate requirement summary
            self.cli.print_status(
                "üìÑ Generating detailed requirement document...", "processing"
            )

            summary_result = (
                await self.workflow_adapter.execute_requirement_analysis_workflow(
                    user_input=user_input,
                    analysis_mode="summarize_requirements",
                    user_answers=user_answers,
                )
            )

            if summary_result["status"] != "success":
                self.cli.print_status(
                    f"‚ùå Failed to generate summary: {summary_result.get('error', 'Unknown error')}",
                    "error",
                )
                return summary_result

            # Step 5: Display requirement summary
            requirement_summary = summary_result["result"]
            should_proceed = self.cli.display_requirement_summary(requirement_summary)

            if should_proceed:
                # Step 6: Proceed with chat-based implementation
                self.cli.print_status(
                    "üöÄ Starting code implementation based on analyzed requirements...",
                    "processing",
                )

                implementation_result = await self.process_input(
                    requirement_summary, "chat"
                )

                return {
                    "status": "success",
                    "requirement_analysis": summary_result,
                    "implementation": implementation_result,
                }
            else:
                self.cli.print_status(
                    "Requirement analysis completed. Implementation skipped.", "info"
                )
                return {
                    "status": "success",
                    "requirement_analysis": summary_result,
                    "implementation": None,
                }

        except Exception as e:
            error_msg = str(e)
            self.cli.print_error_box("Requirement Analysis Error", error_msg)
            self.cli.print_status(
                f"Error during requirement analysis: {error_msg}", "error"
            )

            return {"status": "error", "error": error_msg}

    async def process_input(self, input_source: str, input_type: str):
        """Â§ÑÁêÜËæìÂÖ•Ê∫êÔºàURLÊàñÊñá‰ª∂Ôºâ- ‰ΩøÁî®ÂçáÁ∫ßÁâàÊô∫ËÉΩ‰ΩìÁºñÊéíÂºïÊìé"""
        try:
            # Document segmentation configuration is managed by CLI interface

            self.cli.print_separator()
            self.cli.print_status(
                "üöÄ Starting intelligent agent orchestration...", "processing"
            )

            # ÊòæÁ§∫Â§ÑÁêÜÈò∂ÊÆµÔºàÊ†πÊçÆÈÖçÁΩÆÂÜ≥ÂÆöÔºâ
            chat_mode = input_type == "chat"
            self.cli.display_processing_stages(
                0, self.cli.enable_indexing, chat_mode=chat_mode
            )

            # ‰ΩøÁî®Â∑•‰ΩúÊµÅÈÄÇÈÖçÂô®ËøõË°åÂ§ÑÁêÜ
            result = await self.workflow_adapter.process_input_with_orchestration(
                input_source=input_source,
                input_type=input_type,
                enable_indexing=self.cli.enable_indexing,
            )

            if result["status"] == "success":
                # ÊòæÁ§∫ÂÆåÊàêÁä∂ÊÄÅ
                if chat_mode:
                    final_stage = 4
                else:
                    final_stage = 8 if self.cli.enable_indexing else 5
                self.cli.display_processing_stages(
                    final_stage, self.cli.enable_indexing, chat_mode=chat_mode
                )
                self.cli.print_status(
                    "üéâ Agent orchestration completed successfully!", "complete"
                )

                # ÊòæÁ§∫ÁªìÊûú
                self.display_results(
                    result.get("analysis_result", ""),
                    result.get("download_result", ""),
                    result.get("repo_result", ""),
                    result.get("pipeline_mode", "comprehensive"),
                )
            else:
                self.cli.print_status(
                    f"‚ùå Processing failed: {result.get('error', 'Unknown error')}",
                    "error",
                )

            # Ê∑ªÂä†Âà∞ÂéÜÂè≤ËÆ∞ÂΩï
            self.cli.add_to_history(input_source, result)

            return result

        except Exception as e:
            error_msg = str(e)
            self.cli.print_error_box("Agent Orchestration Error", error_msg)
            self.cli.print_status(f"Error during orchestration: {error_msg}", "error")

            # Ê∑ªÂä†ÈîôËØØÂà∞ÂéÜÂè≤ËÆ∞ÂΩï
            error_result = {"status": "error", "error": error_msg}
            self.cli.add_to_history(input_source, error_result)

            return error_result

    def display_results(
        self,
        analysis_result: str,
        download_result: str,
        repo_result: str,
        pipeline_mode: str = "comprehensive",
    ):
        """ÊòæÁ§∫Â§ÑÁêÜÁªìÊûú"""
        self.cli.print_results_header()

        # ÊòæÁ§∫ÊµÅÊ∞¥Á∫øÊ®°Âºè
        if pipeline_mode == "chat":
            mode_display = "üí¨ Chat Planning Mode"
        elif pipeline_mode == "comprehensive":
            mode_display = "üß† Comprehensive Mode"
        else:
            mode_display = "‚ö° Optimized Mode"
        print(
            f"{Colors.BOLD}{Colors.PURPLE}ü§ñ PIPELINE MODE: {mode_display}{Colors.ENDC}"
        )
        self.cli.print_separator("‚îÄ", 79, Colors.PURPLE)

        print(f"{Colors.BOLD}{Colors.OKCYAN}üìä ANALYSIS PHASE RESULTS:{Colors.ENDC}")
        self.cli.print_separator("‚îÄ", 79, Colors.CYAN)

        # Â∞ùËØïËß£ÊûêÂπ∂Ê†ºÂºèÂåñÂàÜÊûêÁªìÊûú
        try:
            if analysis_result.strip().startswith("{"):
                parsed_analysis = json.loads(analysis_result)
                print(json.dumps(parsed_analysis, indent=2, ensure_ascii=False))
            else:
                print(
                    analysis_result[:1000] + "..."
                    if len(analysis_result) > 1000
                    else analysis_result
                )
        except Exception:
            print(
                analysis_result[:1000] + "..."
                if len(analysis_result) > 1000
                else analysis_result
            )

        print(f"\n{Colors.BOLD}{Colors.PURPLE}üì• DOWNLOAD PHASE RESULTS:{Colors.ENDC}")
        self.cli.print_separator("‚îÄ", 79, Colors.PURPLE)
        print(
            download_result[:1000] + "..."
            if len(download_result) > 1000
            else download_result
        )

        print(
            f"\n{Colors.BOLD}{Colors.GREEN}‚öôÔ∏è  IMPLEMENTATION PHASE RESULTS:{Colors.ENDC}"
        )
        self.cli.print_separator("‚îÄ", 79, Colors.GREEN)
        print(repo_result[:1000] + "..." if len(repo_result) > 1000 else repo_result)

        # Â∞ùËØïÊèêÂèñÁîüÊàêÁöÑ‰ª£Á†ÅÁõÆÂΩï‰ø°ÊÅØ
        if "Code generated in:" in repo_result:
            code_dir = (
                repo_result.split("Code generated in:")[-1].strip().split("\n")[0]
            )
            print(
                f"\n{Colors.BOLD}{Colors.YELLOW}üìÅ Generated Code Directory: {Colors.ENDC}{code_dir}"
            )

        # ÊòæÁ§∫Â§ÑÁêÜÂÆåÊàêÁöÑÂ∑•‰ΩúÊµÅÈò∂ÊÆµ
        print(
            f"\n{Colors.BOLD}{Colors.OKCYAN}üîÑ COMPLETED WORKFLOW STAGES:{Colors.ENDC}"
        )

        if pipeline_mode == "chat":
            stages = [
                "üöÄ Engine Initialization",
                "üí¨ Requirements Analysis",
                "üèóÔ∏è Workspace Setup",
                "üìù Implementation Plan Generation",
                "‚öôÔ∏è Code Implementation",
            ]
        else:
            stages = [
                "üìÑ Document Processing",
                "üîç Reference Analysis",
                "üìã Plan Generation",
                "üì¶ Repository Download",
                "üóÇÔ∏è Codebase Indexing",
                "‚öôÔ∏è Code Implementation",
            ]

        for stage in stages:
            print(f"  ‚úÖ {stage}")

        self.cli.print_separator()

    async def run_interactive_session(self):
        """ËøêË°å‰∫§‰∫íÂºè‰ºöËØù"""
        # Ê∏ÖÂ±èÂπ∂ÊòæÁ§∫ÂêØÂä®ÁïåÈù¢
        self.cli.clear_screen()
        self.cli.print_logo()
        self.cli.print_welcome_banner()

        # ÂàùÂßãÂåñMCPÂ∫îÁî®
        await self.initialize_mcp_app()

        try:
            # ‰∏ª‰∫§‰∫íÂæ™ÁéØ
            while self.cli.is_running:
                self.cli.create_menu()
                choice = self.cli.get_user_input()

                if choice in ["q", "quit", "exit"]:
                    self.cli.print_goodbye()
                    break

                elif choice in ["u", "url"]:
                    url = self.cli.get_url_input()
                    if url:
                        await self.process_input(url, "url")

                elif choice in ["f", "file"]:
                    file_path = self.cli.upload_file_gui()
                    if file_path:
                        await self.process_input(f"file://{file_path}", "file")

                elif choice in ["t", "chat", "text"]:
                    chat_input = self.cli.get_chat_input()
                    if chat_input:
                        await self.process_input(chat_input, "chat")

                elif choice in ["r", "req", "requirement", "requirements"]:
                    # NEW: Requirement Analysis workflow
                    await self.process_requirement_analysis()

                elif choice in ["h", "history"]:
                    self.cli.show_history()

                elif choice in ["c", "config", "configure"]:
                    # Show configuration menu - all settings managed by CLI interface
                    self.cli.show_configuration_menu()

                else:
                    self.cli.print_status(
                        "Invalid choice. Please select U, F, T, R, C, H, or Q.",
                        "warning",
                    )

                # ËØ¢ÈóÆÊòØÂê¶ÁªßÁª≠
                if self.cli.is_running and choice in [
                    "u",
                    "f",
                    "t",
                    "r",
                    "chat",
                    "text",
                    "req",
                    "requirement",
                    "requirements",
                ]:
                    if not self.cli.ask_continue():
                        self.cli.is_running = False
                        self.cli.print_status("Session ended by user", "info")

        except KeyboardInterrupt:
            print(f"\n{Colors.WARNING}‚ö†Ô∏è  Process interrupted by user{Colors.ENDC}")
        except Exception as e:
            print(f"\n{Colors.FAIL}‚ùå Unexpected error: {str(e)}{Colors.ENDC}")
        finally:
            # Ê∏ÖÁêÜËµÑÊ∫ê
            await self.cleanup_mcp_app()


async def main():
    """‰∏ªÂáΩÊï∞"""
    start_time = time.time()

    try:
        # ÂàõÂª∫Âπ∂ËøêË°åCLIÂ∫îÁî®
        app = CLIApp()
        await app.run_interactive_session()

    except KeyboardInterrupt:
        print(f"\n{Colors.WARNING}‚ö†Ô∏è  Application interrupted by user{Colors.ENDC}")
    except Exception as e:
        print(f"\n{Colors.FAIL}‚ùå Application error: {str(e)}{Colors.ENDC}")
    finally:
        end_time = time.time()
        print(
            f"\n{Colors.BOLD}{Colors.CYAN}‚è±Ô∏è  Total runtime: {end_time - start_time:.2f} seconds{Colors.ENDC}"
        )

        # Ê∏ÖÁêÜÁºìÂ≠òÊñá‰ª∂
        print(f"{Colors.YELLOW}üßπ Cleaning up cache files...{Colors.ENDC}")
        if os.name == "nt":  # Windows
            os.system(
                "powershell -Command \"Get-ChildItem -Path . -Filter '__pycache__' -Recurse -Directory | Remove-Item -Recurse -Force\" 2>nul"
            )
        else:  # Unix/Linux/macOS
            os.system('find . -type d -name "__pycache__" -exec rm -r {} + 2>/dev/null')

        print(
            f"{Colors.OKGREEN}‚ú® Goodbye! Thanks for using DeepCode CLI! ‚ú®{Colors.ENDC}"
        )


if __name__ == "__main__":
    asyncio.run(main())


--- cli/cli_interface.py ---
#!/usr/bin/env python3
"""
Enhanced CLI Interface Module for DeepCode
Â¢ûÂº∫ÁâàCLIÁïåÈù¢Ê®°Âùó - ‰∏ì‰∏∫DeepCodeËÆæËÆ°
"""

import os
import time
import platform
from typing import Optional


class Colors:
    """ANSI color codes for terminal styling"""

    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"

    # Gradient colors
    PURPLE = "\033[35m"
    MAGENTA = "\033[95m"
    BLUE = "\033[34m"
    CYAN = "\033[36m"
    GREEN = "\033[32m"
    YELLOW = "\033[33m"


class CLIInterface:
    """Enhanced CLI interface with modern styling for DeepCode"""

    def __init__(self):
        self.uploaded_file = None
        self.is_running = True
        self.processing_history = []
        self.enable_indexing = (
            False  # Default configuration (matching UI: fast mode by default)
        )

        # Load segmentation config from the same source as UI
        self._load_segmentation_config()

        # Initialize tkinter availability
        self._init_tkinter()

    def _load_segmentation_config(self):
        """Load segmentation configuration from mcp_agent.config.yaml"""
        try:
            from utils.llm_utils import get_document_segmentation_config

            seg_config = get_document_segmentation_config()
            self.segmentation_enabled = seg_config.get("enabled", True)
            self.segmentation_threshold = seg_config.get("size_threshold_chars", 50000)
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Failed to load segmentation config: {e}")
            # Fall back to defaults
            self.segmentation_enabled = True
            self.segmentation_threshold = 50000

    def _save_segmentation_config(self):
        """Save segmentation configuration to mcp_agent.config.yaml"""
        import yaml
        import os

        # Get the project root directory (where mcp_agent.config.yaml is located)
        current_file = os.path.abspath(__file__)
        cli_dir = os.path.dirname(current_file)  # cli directory
        project_root = os.path.dirname(cli_dir)  # project root
        config_path = os.path.join(project_root, "mcp_agent.config.yaml")

        try:
            # Read current config
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            # Update document segmentation settings
            if "document_segmentation" not in config:
                config["document_segmentation"] = {}

            config["document_segmentation"]["enabled"] = self.segmentation_enabled
            config["document_segmentation"]["size_threshold_chars"] = (
                self.segmentation_threshold
            )

            # Write updated config
            with open(config_path, "w", encoding="utf-8") as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

            print(
                f"{Colors.OKGREEN}‚úÖ Document segmentation configuration updated{Colors.ENDC}"
            )

        except Exception as e:
            print(
                f"{Colors.WARNING}‚ö†Ô∏è Failed to update segmentation config: {str(e)}{Colors.ENDC}"
            )

    def _init_tkinter(self):
        """Initialize tkinter availability check"""
        # Check tkinter availability for file dialogs
        self.tkinter_available = True
        try:
            import tkinter as tk

            # Test if tkinter can create a window
            test_root = tk.Tk()
            test_root.withdraw()
            test_root.destroy()
        except Exception:
            self.tkinter_available = False

    def clear_screen(self):
        """Clear terminal screen"""
        os.system("cls" if os.name == "nt" else "clear")

    def print_logo(self):
        """Print enhanced ASCII logo for DeepCode CLI"""
        logo = f"""
{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}{Colors.MAGENTA}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{Colors.CYAN}               ‚ïë
‚ïë  {Colors.BOLD}{Colors.PURPLE}‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.CYAN}               ‚ïë
‚ïë  {Colors.BOLD}{Colors.BLUE}‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  {Colors.CYAN}               ‚ïë
‚ïë  {Colors.BOLD}{Colors.OKBLUE}‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  {Colors.CYAN}               ‚ïë
‚ïë  {Colors.BOLD}{Colors.OKCYAN}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{Colors.CYAN}               ‚ïë
‚ïë  {Colors.BOLD}{Colors.GREEN}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.CYAN}               ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}{Colors.GREEN}üß¨ OPEN-SOURCE CODE AGENT ‚Ä¢ DATA INTELLIGENCE LAB @ HKU üöÄ           {Colors.CYAN}‚ïë
‚ïë  {Colors.BOLD}{Colors.GREEN}‚ö° REVOLUTIONIZING RESEARCH REPRODUCIBILITY ‚ö°                      {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
        print(logo)

    def print_welcome_banner(self):
        """Print enhanced welcome banner"""
        banner = f"""
{Colors.BOLD}{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                             WELCOME TO DEEPCODE CLI                          ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  {Colors.YELLOW}Open-Source Code Agent | Data Intelligence Lab @ HKU | MIT License        {Colors.CYAN}‚ïë
‚ïë  {Colors.GREEN}Status: Ready | Engine: Multi-Agent Architecture Initialized               {Colors.CYAN}‚ïë
‚ïë  {Colors.PURPLE}Mission: Revolutionizing Research Reproducibility                         {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}{Colors.OKCYAN}üíé CORE CAPABILITIES:{Colors.ENDC}                                                      {Colors.CYAN}‚ïë
‚ïë    {Colors.BOLD}{Colors.OKCYAN}‚ñ∂ Automated Paper-to-Code Reproduction                                {Colors.CYAN}‚ïë
‚ïë    {Colors.BOLD}{Colors.OKCYAN}‚ñ∂ Collaborative Multi-Agent Architecture                             {Colors.CYAN}‚ïë
‚ïë    {Colors.BOLD}{Colors.OKCYAN}‚ñ∂ Intelligent Code Implementation & Validation                       {Colors.CYAN}‚ïë
‚ïë    {Colors.BOLD}{Colors.OKCYAN}‚ñ∂ Future Vision: One Sentence ‚Üí Complete Codebase                   {Colors.CYAN}‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
        print(banner)

    def print_separator(self, char="‚ïê", length=79, color=Colors.CYAN):
        """Print a styled separator line"""
        print(f"{color}{char * length}{Colors.ENDC}")

    def print_status(self, message: str, status_type: str = "info"):
        """Print status message with appropriate styling"""
        status_styles = {
            "success": f"{Colors.OKGREEN}‚úÖ",
            "error": f"{Colors.FAIL}‚ùå",
            "warning": f"{Colors.WARNING}‚ö†Ô∏è ",
            "info": f"{Colors.OKBLUE}‚ÑπÔ∏è ",
            "processing": f"{Colors.YELLOW}‚è≥",
            "upload": f"{Colors.PURPLE}üìÅ",
            "download": f"{Colors.CYAN}üì•",
            "analysis": f"{Colors.MAGENTA}üîç",
            "implementation": f"{Colors.GREEN}‚öôÔ∏è ",
            "complete": f"{Colors.OKGREEN}üéâ",
        }

        icon = status_styles.get(status_type, status_styles["info"])
        timestamp = time.strftime("%H:%M:%S")
        print(
            f"[{Colors.BOLD}{timestamp}{Colors.ENDC}] {icon} {Colors.BOLD}{message}{Colors.ENDC}"
        )

    def create_menu(self):
        """Create enhanced interactive menu"""
        # Display current configuration
        pipeline_mode = "üß† COMPREHENSIVE" if self.enable_indexing else "‚ö° OPTIMIZED"
        index_status = "‚úÖ Enabled" if self.enable_indexing else "üî∂ Disabled"
        segmentation_mode = (
            "üìÑ SMART" if self.segmentation_enabled else "üìã TRADITIONAL"
        )

        menu = f"""
{Colors.BOLD}{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                MAIN MENU                                      ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  {Colors.OKGREEN}üåê [U] Process URL       {Colors.CYAN}‚îÇ  {Colors.PURPLE}üìÅ [F] Upload File    {Colors.CYAN}‚îÇ  {Colors.MAGENTA}üí¨ [T] Chat Input{Colors.CYAN}    ‚ïë
‚ïë  {Colors.BLUE}üß† [R] Req. Analysis    {Colors.CYAN}‚îÇ  {Colors.OKCYAN}‚öôÔ∏è  [C] Configure        {Colors.CYAN}‚îÇ  {Colors.YELLOW}üìä [H] History{Colors.CYAN}    ‚ïë
‚ïë  {Colors.FAIL}‚ùå [Q] Quit{Colors.CYAN}                                                                 ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}ü§ñ Current Pipeline Mode: {pipeline_mode}{Colors.CYAN}                          ‚ïë
‚ïë  {Colors.BOLD}üóÇÔ∏è  Codebase Indexing: {index_status}{Colors.CYAN}                                    ‚ïë
‚ïë  {Colors.BOLD}üìÑ Document Processing: {segmentation_mode}{Colors.CYAN}                               ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.YELLOW}üìù URL Processing:{Colors.CYAN}                                                         ‚ïë
‚ïë  {Colors.YELLOW}   ‚ñ∂ Enter research paper URL (arXiv, IEEE, ACM, etc.)                    {Colors.CYAN}‚ïë
‚ïë  {Colors.YELLOW}   ‚ñ∂ Supports direct PDF links and academic paper pages                   {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.PURPLE}üìÅ File Processing:{Colors.CYAN}                                                        ‚ïë
‚ïë  {Colors.PURPLE}   ‚ñ∂ Upload PDF, DOCX, PPTX, HTML, or TXT files                          {Colors.CYAN}‚ïë
‚ïë  {Colors.PURPLE}   ‚ñ∂ Intelligent file format detection and processing                     {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.MAGENTA}üí¨ Chat Input:{Colors.CYAN}                                                           ‚ïë
‚ïë  {Colors.MAGENTA}   ‚ñ∂ Describe your coding requirements in natural language                {Colors.CYAN}‚ïë
‚ïë  {Colors.MAGENTA}   ‚ñ∂ AI generates implementation plan and code automatically             {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BLUE}üß† Requirement Analysis (NEW):{Colors.CYAN}                                             ‚ïë
‚ïë  {Colors.BLUE}   ‚ñ∂ Get AI-guided questions to refine your requirements                   {Colors.CYAN}‚ïë
‚ïë  {Colors.BLUE}   ‚ñ∂ Generate detailed requirement documents from your answers             {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.OKCYAN}üîÑ Processing Pipeline:{Colors.CYAN}                                                    ‚ïë
‚ïë  {Colors.OKCYAN}   ‚ñ∂ Intelligent agent orchestration ‚Üí Code synthesis                     {Colors.CYAN}‚ïë
‚ïë  {Colors.OKCYAN}   ‚ñ∂ Multi-agent coordination with progress tracking                     {Colors.CYAN}‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
        print(menu)

    def get_user_input(self):
        """Get user input with styled prompt"""
        print(f"\n{Colors.BOLD}{Colors.OKCYAN}‚û§ Your choice: {Colors.ENDC}", end="")
        return input().strip().lower()

    def upload_file_gui(self) -> Optional[str]:
        """Enhanced file upload interface with better error handling"""
        if not self.tkinter_available:
            self.print_status(
                "GUI file dialog not available - using manual input", "warning"
            )
            return self._get_manual_file_path()

        def select_file():
            try:
                import tkinter as tk
                from tkinter import filedialog

                root = tk.Tk()
                root.withdraw()
                root.attributes("-topmost", True)

                file_types = [
                    ("Research Papers", "*.pdf;*.docx;*.doc"),
                    ("PDF Files", "*.pdf"),
                    ("Word Documents", "*.docx;*.doc"),
                    ("PowerPoint Files", "*.pptx;*.ppt"),
                    ("HTML Files", "*.html;*.htm"),
                    ("Text Files", "*.txt;*.md"),
                    ("All Files", "*.*"),
                ]

                if platform.system() == "Darwin":
                    file_types = [
                        ("Research Papers", ".pdf .docx .doc"),
                        ("PDF Files", ".pdf"),
                        ("Word Documents", ".docx .doc"),
                        ("PowerPoint Files", ".pptx .ppt"),
                        ("HTML Files", ".html .htm"),
                        ("Text Files", ".txt .md"),
                        ("All Files", ".*"),
                    ]

                file_path = filedialog.askopenfilename(
                    title="Select Research File - DeepCode CLI",
                    filetypes=file_types,
                    initialdir=os.getcwd(),
                )

                root.destroy()
                return file_path

            except Exception as e:
                self.print_status(f"File dialog error: {str(e)}", "error")
                return self._get_manual_file_path()

        self.print_status("Opening file browser dialog...", "upload")
        file_path = select_file()

        if file_path:
            self.print_status(
                f"File selected: {os.path.basename(file_path)}", "success"
            )
            return file_path
        else:
            self.print_status("No file selected", "warning")
            return None

    def _get_manual_file_path(self) -> Optional[str]:
        """Get file path through manual input with validation"""
        self.print_separator("‚îÄ", 79, Colors.YELLOW)
        print(f"{Colors.BOLD}{Colors.YELLOW}üìÅ Manual File Path Input{Colors.ENDC}")
        print(
            f"{Colors.CYAN}Please enter the full path to your research paper file:{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}Supported formats: PDF, DOCX, PPTX, HTML, TXT, MD{Colors.ENDC}"
        )
        self.print_separator("‚îÄ", 79, Colors.YELLOW)

        while True:
            print(f"\n{Colors.BOLD}{Colors.OKCYAN}üìÇ File path: {Colors.ENDC}", end="")
            file_path = input().strip()

            if not file_path:
                self.print_status(
                    "Empty path entered. Please try again or press Ctrl+C to cancel.",
                    "warning",
                )
                continue

            file_path = os.path.expanduser(file_path)
            file_path = os.path.abspath(file_path)

            if not os.path.exists(file_path):
                self.print_status(f"File not found: {file_path}", "error")
                retry = (
                    input(f"{Colors.YELLOW}Try again? (y/n): {Colors.ENDC}")
                    .strip()
                    .lower()
                )
                if retry != "y":
                    return None
                continue

            if not os.path.isfile(file_path):
                self.print_status(f"Path is not a file: {file_path}", "error")
                continue

            supported_extensions = {
                ".pdf",
                ".docx",
                ".doc",
                ".pptx",
                ".ppt",
                ".html",
                ".htm",
                ".txt",
                ".md",
            }
            file_ext = os.path.splitext(file_path)[1].lower()

            if file_ext not in supported_extensions:
                self.print_status(f"Unsupported file format: {file_ext}", "warning")
                proceed = (
                    input(f"{Colors.YELLOW}Process anyway? (y/n): {Colors.ENDC}")
                    .strip()
                    .lower()
                )
                if proceed != "y":
                    continue

            self.print_status(
                f"File validated: {os.path.basename(file_path)}", "success"
            )
            return file_path

    def get_url_input(self) -> str:
        """Enhanced URL input with validation"""
        self.print_separator("‚îÄ", 79, Colors.GREEN)
        print(f"{Colors.BOLD}{Colors.GREEN}üåê URL Input Interface{Colors.ENDC}")
        print(
            f"{Colors.CYAN}Enter a research paper URL from supported platforms:{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}‚Ä¢ arXiv (arxiv.org)        ‚Ä¢ IEEE Xplore (ieeexplore.ieee.org){Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}‚Ä¢ ACM Digital Library      ‚Ä¢ SpringerLink ‚Ä¢ Nature ‚Ä¢ Science{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}‚Ä¢ Direct PDF links         ‚Ä¢ Academic publisher websites{Colors.ENDC}"
        )
        self.print_separator("‚îÄ", 79, Colors.GREEN)

        while True:
            print(f"\n{Colors.BOLD}{Colors.OKCYAN}üîó URL: {Colors.ENDC}", end="")
            url = input().strip()

            if not url:
                self.print_status(
                    "Empty URL entered. Please try again or press Ctrl+C to cancel.",
                    "warning",
                )
                continue

            if not url.startswith(("http://", "https://")):
                self.print_status("URL must start with http:// or https://", "error")
                retry = (
                    input(f"{Colors.YELLOW}Try again? (y/n): {Colors.ENDC}")
                    .strip()
                    .lower()
                )
                if retry != "y":
                    return ""
                continue

            academic_domains = [
                "arxiv.org",
                "ieeexplore.ieee.org",
                "dl.acm.org",
                "link.springer.com",
                "nature.com",
                "science.org",
                "scholar.google.com",
                "researchgate.net",
                "semanticscholar.org",
            ]

            is_academic = any(domain in url.lower() for domain in academic_domains)
            if not is_academic and not url.lower().endswith(".pdf"):
                self.print_status(
                    "URL doesn't appear to be from a known academic platform", "warning"
                )
                proceed = (
                    input(f"{Colors.YELLOW}Process anyway? (y/n): {Colors.ENDC}")
                    .strip()
                    .lower()
                )
                if proceed != "y":
                    continue

            self.print_status(f"URL validated: {url}", "success")
            return url

    def get_chat_input(self) -> str:
        """Enhanced chat input interface for coding requirements"""
        self.print_separator("‚îÄ", 79, Colors.PURPLE)
        print(f"{Colors.BOLD}{Colors.PURPLE}üí¨ Chat Input Interface{Colors.ENDC}")
        print(
            f"{Colors.CYAN}Describe your coding requirements in natural language.{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}Our AI will analyze your needs and generate a comprehensive implementation plan.{Colors.ENDC}"
        )
        self.print_separator("‚îÄ", 79, Colors.PURPLE)

        # Display examples to help users
        print(f"\n{Colors.BOLD}{Colors.YELLOW}üí° Examples:{Colors.ENDC}")
        print(f"{Colors.CYAN}Academic Research:{Colors.ENDC}")
        print(
            "  ‚Ä¢ 'I need to implement a reinforcement learning algorithm for robotic control'"
        )
        print(
            "  ‚Ä¢ 'Create a neural network for image classification with attention mechanisms'"
        )
        print(f"{Colors.CYAN}Engineering Projects:{Colors.ENDC}")
        print(
            "  ‚Ä¢ 'Develop a web application for project management with user authentication'"
        )
        print("  ‚Ä¢ 'Create a data visualization dashboard for sales analytics'")
        print(f"{Colors.CYAN}Mixed Projects:{Colors.ENDC}")
        print(
            "  ‚Ä¢ 'Implement a machine learning model with a web interface for real-time predictions'"
        )

        self.print_separator("‚îÄ", 79, Colors.PURPLE)

        print(
            f"\n{Colors.BOLD}{Colors.OKCYAN}‚úèÔ∏è  Enter your coding requirements below:{Colors.ENDC}"
        )
        print(
            f"{Colors.YELLOW}(Type your description, press Enter twice when finished, or Ctrl+C to cancel){Colors.ENDC}"
        )

        lines = []
        empty_line_count = 0

        while True:
            try:
                if len(lines) == 0:
                    print(f"{Colors.BOLD}> {Colors.ENDC}", end="")
                else:
                    print(f"{Colors.BOLD}  {Colors.ENDC}", end="")

                line = input()

                if line.strip() == "":
                    empty_line_count += 1
                    if empty_line_count >= 2:
                        # Two consecutive empty lines means user finished input
                        break
                    lines.append("")  # Keep empty line for formatting
                else:
                    empty_line_count = 0
                    lines.append(line)

            except KeyboardInterrupt:
                print(f"\n{Colors.WARNING}Input cancelled by user{Colors.ENDC}")
                return ""

        # Join all lines and clean up
        user_input = "\n".join(lines).strip()

        if not user_input:
            self.print_status("No input provided", "warning")
            return ""

        if len(user_input) < 20:
            self.print_status(
                "Input too short. Please provide more detailed requirements (at least 20 characters)",
                "warning",
            )
            retry = (
                input(f"{Colors.YELLOW}Try again? (y/n): {Colors.ENDC}").strip().lower()
            )
            if retry == "y":
                return self.get_chat_input()  # Recursive call for retry
            return ""

        # Display input summary
        word_count = len(user_input.split())
        char_count = len(user_input)

        print(f"\n{Colors.BOLD}{Colors.GREEN}üìã Input Summary:{Colors.ENDC}")
        print(f"  ‚Ä¢ {Colors.CYAN}Word count: {word_count}{Colors.ENDC}")
        print(f"  ‚Ä¢ {Colors.CYAN}Character count: {char_count}{Colors.ENDC}")

        # Show preview
        preview = user_input[:200] + "..." if len(user_input) > 200 else user_input
        print(f"\n{Colors.BOLD}{Colors.CYAN}üìÑ Preview:{Colors.ENDC}")
        print(f"{Colors.YELLOW}{preview}{Colors.ENDC}")

        # Confirm with user
        confirm = (
            input(
                f"\n{Colors.BOLD}{Colors.OKCYAN}Proceed with this input? (y/n): {Colors.ENDC}"
            )
            .strip()
            .lower()
        )
        if confirm != "y":
            retry = (
                input(f"{Colors.YELLOW}Edit input? (y/n): {Colors.ENDC}")
                .strip()
                .lower()
            )
            if retry == "y":
                return self.get_chat_input()  # Recursive call for retry
            return ""

        self.print_status(
            f"Chat input captured: {word_count} words, {char_count} characters",
            "success",
        )
        return user_input

    def show_progress_bar(self, message: str, duration: float = 2.0):
        """Show animated progress bar"""
        print(f"\n{Colors.BOLD}{Colors.CYAN}{message}{Colors.ENDC}")

        bar_length = 50
        for i in range(bar_length + 1):
            percent = (i / bar_length) * 100
            filled = "‚ñà" * i
            empty = "‚ñë" * (bar_length - i)

            print(
                f"\r{Colors.OKGREEN}[{filled}{empty}] {percent:3.0f}%{Colors.ENDC}",
                end="",
                flush=True,
            )
            time.sleep(duration / bar_length)

        print(f"\n{Colors.OKGREEN}‚úì {message} completed{Colors.ENDC}")

    def show_spinner(self, message: str, duration: float = 1.0):
        """Show spinner animation"""
        spinner_chars = "‚†ã‚†ô‚†π‚†∏‚†º‚†¥‚†¶‚†ß‚†á‚†è"
        end_time = time.time() + duration

        print(
            f"{Colors.BOLD}{Colors.CYAN}{message}... {Colors.ENDC}", end="", flush=True
        )

        i = 0
        while time.time() < end_time:
            print(
                f"\r{Colors.BOLD}{Colors.CYAN}{message}... {Colors.YELLOW}{spinner_chars[i % len(spinner_chars)]}{Colors.ENDC}",
                end="",
                flush=True,
            )
            time.sleep(0.1)
            i += 1

        print(
            f"\r{Colors.BOLD}{Colors.CYAN}{message}... {Colors.OKGREEN}‚úì{Colors.ENDC}"
        )

    def display_processing_stages(
        self,
        current_stage: int = 0,
        enable_indexing: bool = True,
        chat_mode: bool = False,
    ):
        """Display processing pipeline stages with current progress"""
        if chat_mode:
            # Chat mode - simplified workflow for user requirements
            stages = [
                ("üöÄ", "Initialize", "Setting up chat engine"),
                ("üí¨", "Planning", "Analyzing requirements"),
                ("üèóÔ∏è", "Setup", "Creating workspace"),
                ("üìù", "Save Plan", "Saving implementation plan"),
                ("‚öôÔ∏è", "Implement", "Generating code"),
            ]
            pipeline_mode = "CHAT PLANNING"
        elif enable_indexing:
            # Full pipeline with all stages
            stages = [
                ("üöÄ", "Initialize", "Setting up AI engine"),
                ("üìä", "Analyze", "Analyzing research content"),
                ("üì•", "Download", "Processing document"),
                ("üìã", "Plan", "Generating code architecture"),
                ("üîç", "References", "Analyzing references"),
                ("üì¶", "Repos", "Downloading repositories"),
                ("üóÇÔ∏è", "Index", "Building code index"),
                ("‚öôÔ∏è", "Implement", "Implementing code"),
            ]
            pipeline_mode = "COMPREHENSIVE"
        else:
            # Fast mode - skip indexing related stages
            stages = [
                ("üöÄ", "Initialize", "Setting up AI engine"),
                ("üìä", "Analyze", "Analyzing research content"),
                ("üì•", "Download", "Processing document"),
                ("üìã", "Plan", "Generating code architecture"),
                ("‚öôÔ∏è", "Implement", "Implementing code"),
            ]
            pipeline_mode = "OPTIMIZED"

        print(
            f"\n{Colors.BOLD}{Colors.CYAN}üìã {pipeline_mode} PIPELINE STATUS{Colors.ENDC}"
        )
        self.print_separator("‚îÄ", 79, Colors.CYAN)

        for i, (icon, name, desc) in enumerate(stages):
            if i < current_stage:
                status = f"{Colors.OKGREEN}‚úì COMPLETED{Colors.ENDC}"
            elif i == current_stage:
                status = f"{Colors.YELLOW}‚è≥ IN PROGRESS{Colors.ENDC}"
            else:
                status = f"{Colors.CYAN}‚è∏Ô∏è  PENDING{Colors.ENDC}"

            print(
                f"{icon} {Colors.BOLD}{name:<12}{Colors.ENDC} ‚îÇ {desc:<25} ‚îÇ {status}"
            )

        self.print_separator("‚îÄ", 79, Colors.CYAN)

    def print_results_header(self):
        """Print results section header"""
        header = f"""
{Colors.BOLD}{Colors.OKGREEN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                              PROCESSING RESULTS                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
        print(header)

    def print_error_box(self, title: str, error_msg: str):
        """Print formatted error box"""
        print(
            f"\n{Colors.FAIL}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
        )
        print(f"‚ïë {Colors.BOLD}ERROR: {title:<50}{Colors.FAIL} ‚ïë")
        print("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£")

        words = error_msg.split()
        lines = []
        current_line = ""

        for word in words:
            if len(current_line + word) <= 54:
                current_line += word + " "
            else:
                lines.append(current_line.strip())
                current_line = word + " "
        if current_line:
            lines.append(current_line.strip())

        for line in lines:
            print(f"‚ïë {line:<56} ‚ïë")

        print(
            f"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}"
        )

    def cleanup_cache(self):
        """Ê∏ÖÁêÜPythonÁºìÂ≠òÊñá‰ª∂ / Clean up Python cache files"""
        try:
            self.print_status("Cleaning up cache files...", "info")
            # Ê∏ÖÁêÜ__pycache__ÁõÆÂΩï
            os.system('find . -type d -name "__pycache__" -exec rm -r {} + 2>/dev/null')
            # Ê∏ÖÁêÜ.pycÊñá‰ª∂
            os.system('find . -name "*.pyc" -delete 2>/dev/null')
            self.print_status("Cache cleanup completed", "success")
        except Exception as e:
            self.print_status(f"Cache cleanup failed: {e}", "warning")

    def print_goodbye(self):
        """Print goodbye message"""
        # Ê∏ÖÁêÜÁºìÂ≠òÊñá‰ª∂
        self.cleanup_cache()

        goodbye = f"""
{Colors.BOLD}{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                GOODBYE                                        ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  {Colors.OKGREEN}üéâ Thank you for using DeepCode CLI!                                     {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.YELLOW}üß¨ Join our community in revolutionizing research reproducibility         {Colors.CYAN}‚ïë
‚ïë  {Colors.PURPLE}‚ö° Together, we're building the future of automated code generation       {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.OKCYAN}üí° Questions? Contribute to our open-source mission at GitHub             {Colors.CYAN}‚ïë
‚ïë  {Colors.GREEN}üßπ Cache files cleaned up for optimal performance                         {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
        print(goodbye)

    def get_requirement_analysis_input(self) -> str:
        """Enhanced requirement analysis input interface (NEW: matching UI version)"""
        self.print_separator("‚îÄ", 79, Colors.BLUE)
        print(
            f"{Colors.BOLD}{Colors.BLUE}üß† Requirement Analysis Interface{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}Describe your project idea or requirements briefly.{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}Our AI will generate guiding questions to help you refine your vision.{Colors.ENDC}"
        )
        self.print_separator("‚îÄ", 79, Colors.BLUE)

        # Display examples
        print(f"\n{Colors.BOLD}{Colors.YELLOW}üí° Examples:{Colors.ENDC}")
        print(
            f"{Colors.CYAN}  ‚Ä¢ 'I want to build a machine learning system for image recognition'{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}  ‚Ä¢ 'Create a web app for project management with real-time collaboration'{Colors.ENDC}"
        )
        print(
            f"{Colors.CYAN}  ‚Ä¢ 'Develop a data analysis pipeline for financial forecasting'{Colors.ENDC}"
        )

        self.print_separator("‚îÄ", 79, Colors.BLUE)

        print(
            f"\n{Colors.BOLD}{Colors.OKCYAN}‚úèÔ∏è  Enter your initial requirements below:{Colors.ENDC}"
        )
        print(
            f"{Colors.YELLOW}(Type your description, press Enter twice when finished, or Ctrl+C to cancel){Colors.ENDC}"
        )

        lines = []
        empty_line_count = 0

        while True:
            try:
                if len(lines) == 0:
                    print(f"{Colors.BOLD}> {Colors.ENDC}", end="")
                else:
                    print(f"{Colors.BOLD}  {Colors.ENDC}", end="")

                line = input()

                if line.strip() == "":
                    empty_line_count += 1
                    if empty_line_count >= 2:
                        break
                    lines.append("")
                else:
                    empty_line_count = 0
                    lines.append(line)

            except KeyboardInterrupt:
                print(f"\n{Colors.WARNING}Input cancelled by user{Colors.ENDC}")
                return ""

        user_input = "\n".join(lines).strip()

        if not user_input:
            self.print_status("No input provided", "warning")
            return ""

        if len(user_input) < 20:
            self.print_status(
                "Input too short. Please provide more details (at least 20 characters)",
                "warning",
            )
            retry = (
                input(f"{Colors.YELLOW}Try again? (y/n): {Colors.ENDC}").strip().lower()
            )
            if retry == "y":
                return self.get_requirement_analysis_input()
            return ""

        # Display input summary
        word_count = len(user_input.split())
        char_count = len(user_input)

        print(f"\n{Colors.BOLD}{Colors.GREEN}üìã Input Summary:{Colors.ENDC}")
        print(f"  ‚Ä¢ {Colors.CYAN}Word count: {word_count}{Colors.ENDC}")
        print(f"  ‚Ä¢ {Colors.CYAN}Character count: {char_count}{Colors.ENDC}")

        # Show preview
        preview = user_input[:200] + "..." if len(user_input) > 200 else user_input
        print(f"\n{Colors.BOLD}{Colors.CYAN}üìÑ Preview:{Colors.ENDC}")
        print(f"{Colors.YELLOW}{preview}{Colors.ENDC}")

        # Confirm
        confirm = (
            input(
                f"\n{Colors.BOLD}{Colors.OKCYAN}Proceed with this input? (y/n): {Colors.ENDC}"
            )
            .strip()
            .lower()
        )
        if confirm != "y":
            retry = (
                input(f"{Colors.YELLOW}Edit input? (y/n): {Colors.ENDC}")
                .strip()
                .lower()
            )
            if retry == "y":
                return self.get_requirement_analysis_input()
            return ""

        self.print_status(
            f"Requirement input captured: {word_count} words, {char_count} characters",
            "success",
        )
        return user_input

    def display_guiding_questions(self, questions_json: str):
        """Display AI-generated guiding questions (NEW: matching UI version)"""
        import json

        try:
            questions = json.loads(questions_json)

            self.print_separator("‚ïê", 79, Colors.GREEN)
            print(
                f"\n{Colors.BOLD}{Colors.GREEN}ü§ñ AI-Generated Guiding Questions{Colors.ENDC}"
            )
            print(
                f"{Colors.CYAN}Please answer these questions to help refine your requirements:{Colors.ENDC}\n"
            )
            self.print_separator("‚îÄ", 79, Colors.GREEN)

            for i, q in enumerate(questions, 1):
                print(
                    f"\n{Colors.BOLD}{Colors.YELLOW}Question {i}:{Colors.ENDC} {Colors.CYAN}{q}{Colors.ENDC}"
                )

            self.print_separator("‚ïê", 79, Colors.GREEN)

        except json.JSONDecodeError:
            self.print_status("Failed to parse questions", "error")
            print(questions_json)

    def get_question_answers(self, questions_json: str) -> dict:
        """Get user answers to guiding questions (NEW: matching UI version)"""
        import json

        try:
            questions = json.loads(questions_json)
            answers = {}

            print(
                f"\n{Colors.BOLD}{Colors.BLUE}üìù Answer the following questions:{Colors.ENDC}"
            )
            print(
                f"{Colors.CYAN}(Type your answer and press Enter for each question){Colors.ENDC}\n"
            )

            for i, question in enumerate(questions, 1):
                print(
                    f"\n{Colors.BOLD}{Colors.YELLOW}Q{i}:{Colors.ENDC} {Colors.CYAN}{question}{Colors.ENDC}"
                )
                print(f"{Colors.BOLD}{Colors.OKCYAN}Your answer:{Colors.ENDC} ", end="")

                answer = input().strip()
                answers[f"question_{i}"] = answer

                if answer:
                    self.print_status(f"Answer {i} recorded", "success")
                else:
                    self.print_status(f"Answer {i} left blank", "warning")

            return answers

        except json.JSONDecodeError:
            self.print_status("Failed to parse questions", "error")
            return {}

    def display_requirement_summary(self, summary: str):
        """Display generated requirement document (NEW: matching UI version)"""
        self.print_separator("‚ïê", 79, Colors.GREEN)
        print(
            f"\n{Colors.BOLD}{Colors.GREEN}üìÑ Generated Requirement Document{Colors.ENDC}\n"
        )
        self.print_separator("‚îÄ", 79, Colors.GREEN)

        print(f"{Colors.CYAN}{summary}{Colors.ENDC}")

        self.print_separator("‚ïê", 79, Colors.GREEN)

        # Ask if user wants to proceed with implementation
        proceed = (
            input(
                f"\n{Colors.BOLD}{Colors.YELLOW}Would you like to proceed with code implementation based on these requirements? (y/n):{Colors.ENDC} "
            )
            .strip()
            .lower()
        )

        return proceed == "y"

    def ask_continue(self) -> bool:
        """Ask if user wants to continue with another paper"""
        self.print_separator("‚îÄ", 79, Colors.YELLOW)
        print(f"\n{Colors.BOLD}{Colors.YELLOW}üîÑ Process another paper?{Colors.ENDC}")
        choice = input(f"{Colors.OKCYAN}Continue? (y/n): {Colors.ENDC}").strip().lower()
        return choice in ["y", "yes", "1", "true"]

    def add_to_history(self, input_source: str, result: dict):
        """Add processing result to history"""
        entry = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "input_source": input_source,
            "status": result.get("status", "unknown"),
            "result": result,
        }
        self.processing_history.append(entry)

    def show_history(self):
        """Display processing history"""
        if not self.processing_history:
            self.print_status("No processing history available", "info")
            return

        print(f"\n{Colors.BOLD}{Colors.CYAN}üìö PROCESSING HISTORY{Colors.ENDC}")
        self.print_separator("‚îÄ", 79, Colors.CYAN)

        for i, entry in enumerate(self.processing_history, 1):
            status_icon = "‚úÖ" if entry["status"] == "success" else "‚ùå"
            source = entry["input_source"]
            if len(source) > 50:
                source = source[:47] + "..."

            print(f"{i}. {status_icon} {entry['timestamp']} | {source}")

        self.print_separator("‚îÄ", 79, Colors.CYAN)

    def show_configuration_menu(self):
        """Show configuration options menu"""
        self.clear_screen()

        # Get segmentation config status
        segmentation_enabled = getattr(self, "segmentation_enabled", True)
        segmentation_threshold = getattr(self, "segmentation_threshold", 50000)

        print(f"""
{Colors.BOLD}{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                           CONFIGURATION MENU                                  ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}ü§ñ Agent Orchestration Engine Configuration{Colors.CYAN}                             ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.OKCYAN}[1] Pipeline Mode:{Colors.CYAN}                                                        ‚ïë
‚ïë      {Colors.BOLD}üß† Comprehensive Mode{Colors.CYAN} - Full intelligence analysis (Default)         ‚ïë
‚ïë         ‚úì Research Analysis + Resource Processing                            ‚ïë
‚ïë         ‚úì Reference Intelligence Discovery                                   ‚ïë
‚ïë         ‚úì Automated Repository Acquisition                                   ‚ïë
‚ïë         ‚úì Codebase Intelligence Orchestration                               ‚ïë
‚ïë         ‚úì Intelligent Code Implementation Synthesis                         ‚ïë
‚ïë                                                                               ‚ïë
‚ïë      {Colors.BOLD}‚ö° Optimized Mode{Colors.CYAN} - Fast processing (Skip indexing)                    ‚ïë
‚ïë         ‚úì Research Analysis + Resource Processing                            ‚ïë
‚ïë         ‚úì Code Architecture Synthesis                                        ‚ïë
‚ïë         ‚úì Intelligent Code Implementation Synthesis                         ‚ïë
‚ïë         ‚úó Reference Intelligence Discovery (Skipped)                        ‚ïë
‚ïë         ‚úó Repository Acquisition (Skipped)                                   ‚ïë
‚ïë         ‚úó Codebase Intelligence Orchestration (Skipped)                     ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.OKCYAN}[2] Document Processing:{Colors.CYAN}                                                   ‚ïë
‚ïë      {Colors.BOLD}üìÑ Smart Segmentation{Colors.CYAN} - Intelligent document analysis (Default)      ‚ïë
‚ïë         ‚úì Semantic boundary detection                                        ‚ïë
‚ïë         ‚úì Algorithm integrity preservation                                   ‚ïë
‚ïë         ‚úì Formula chain recognition                                          ‚ïë
‚ïë         ‚úì Adaptive character limits                                          ‚ïë
‚ïë                                                                               ‚ïë
‚ïë      {Colors.BOLD}üìã Traditional Processing{Colors.CYAN} - Full document reading                       ‚ïë
‚ïë         ‚úì Complete document analysis                                         ‚ïë
‚ïë         ‚úó Smart segmentation (Disabled)                                      ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.YELLOW}Current Settings:{Colors.CYAN}                                                         ‚ïë
‚ïë    Pipeline: {'üß† Comprehensive Mode' if self.enable_indexing else '‚ö° Optimized Mode'}                                          ‚ïë
‚ïë    Document: {'üìÑ Smart Segmentation' if segmentation_enabled else 'üìã Traditional Processing'}                                ‚ïë
‚ïë    Threshold: {segmentation_threshold} characters                                    ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.OKGREEN}[T] Toggle Pipeline    {Colors.BLUE}[S] Toggle Segmentation    {Colors.FAIL}[B] Back{Colors.CYAN}     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
""")

        while True:
            print(
                f"\n{Colors.BOLD}{Colors.OKCYAN}‚û§ Configuration choice: {Colors.ENDC}",
                end="",
            )
            choice = input().strip().lower()

            if choice in ["t", "toggle"]:
                self.enable_indexing = not self.enable_indexing
                mode = "üß† Comprehensive" if self.enable_indexing else "‚ö° Optimized"
                self.print_status(f"Pipeline mode switched to: {mode}", "success")
                time.sleep(1)
                self.show_configuration_menu()
                return

            elif choice in ["s", "segmentation"]:
                current_state = getattr(self, "segmentation_enabled", True)
                self.segmentation_enabled = not current_state
                # Save the configuration to file
                self._save_segmentation_config()
                seg_mode = (
                    "üìÑ Smart Segmentation"
                    if self.segmentation_enabled
                    else "üìã Traditional Processing"
                )
                self.print_status(
                    f"Document processing switched to: {seg_mode}", "success"
                )
                time.sleep(1)
                self.show_configuration_menu()
                return

            elif choice in ["b", "back"]:
                return

            else:
                self.print_status(
                    "Invalid choice. Please enter 'T', 'S', or 'B'.", "warning"
                )


--- cli/cli_launcher.py ---
#!/usr/bin/env python3
"""
DeepCode - CLI Research Engine Launcher
DeepCode - CLIÁ†îÁ©∂ÂºïÊìéÂêØÂä®Âô®

üß¨ Open-Source Code Agent by Data Intelligence Lab @ HKU (CLI Edition)
‚ö° Revolutionizing research reproducibility through collaborative AI via command line
"""

import sys
from pathlib import Path


def check_dependencies():
    """Ê£ÄÊü•ÂøÖË¶ÅÁöÑ‰æùËµñÊòØÂê¶Â∑≤ÂÆâË£Ö / Check if necessary dependencies are installed"""
    import importlib.util

    print("üîç Checking CLI dependencies...")

    missing_deps = []

    # Check asyncio availability
    if importlib.util.find_spec("asyncio") is not None:
        print("‚úÖ Asyncio is available")
    else:
        missing_deps.append("asyncio")

    # Check PyYAML availability
    if importlib.util.find_spec("yaml") is not None:
        print("‚úÖ PyYAML is installed")
    else:
        missing_deps.append("pyyaml")

    # Check Tkinter availability
    if importlib.util.find_spec("tkinter") is not None:
        print("‚úÖ Tkinter is available (for file dialogs)")
    else:
        print("‚ö†Ô∏è  Tkinter not available - file dialogs will use manual input")

    # Check for MCP agent dependencies
    if importlib.util.find_spec("mcp_agent.app") is not None:
        print("‚úÖ MCP Agent framework is available")
    else:
        missing_deps.append("mcp-agent")

    # Check for workflow dependencies
    # Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞Ë∑ØÂæÑ
    current_dir = Path(__file__).parent
    project_root = current_dir.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

    if importlib.util.find_spec("workflows.agent_orchestration_engine") is not None:
        print("‚úÖ Workflow modules are available")
    else:
        print("‚ö†Ô∏è  Workflow modules may not be properly configured")

    # Check for CLI components
    if importlib.util.find_spec("cli.cli_app") is not None:
        print("‚úÖ CLI application components are available")
    else:
        print("‚ùå CLI application components missing")
        missing_deps.append("cli-components")

    if missing_deps:
        print("\n‚ùå Missing dependencies:")
        for dep in missing_deps:
            print(f"   - {dep}")
        print("\nPlease install missing dependencies using:")
        print(
            f"pip install {' '.join([d for d in missing_deps if d != 'cli-components'])}"
        )
        if "cli-components" in missing_deps:
            print(
                "CLI components appear to be missing - please check the cli/ directory"
            )
        return False

    print("‚úÖ All CLI dependencies satisfied")
    return True


def print_banner():
    """ÊòæÁ§∫CLIÂêØÂä®Ê®™ÂπÖ / Display CLI startup banner"""
    banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                              ‚ïë
‚ïë    üß¨ DeepCode - Open-Source Code Agent                      ‚ïë
‚ïë                                                              ‚ïë
‚ïë    ‚ö° DATA INTELLIGENCE LAB @ HKU ‚ö°                        ‚ïë
‚ïë                                                              ‚ïë
‚ïë                               ‚ïë
‚ïë                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
    print(banner)


def main():
    """‰∏ªÂáΩÊï∞ / Main function"""
    print_banner()

    # Ê£ÄÊü•‰æùËµñ / Check dependencies
    if not check_dependencies():
        print("\nüö® Please install missing dependencies and try again.")
        sys.exit(1)

    # Ëé∑ÂèñÂΩìÂâçËÑöÊú¨ÁõÆÂΩï / Get current script directory
    current_dir = Path(__file__).parent
    project_root = current_dir.parent
    cli_app_path = current_dir / "cli_app.py"

    # Ê£ÄÊü•cli_app.pyÊòØÂê¶Â≠òÂú® / Check if cli_app.py exists
    if not cli_app_path.exists():
        print(f"‚ùå CLI application file not found: {cli_app_path}")
        print("Please ensure the cli/cli_app.py file exists.")
        sys.exit(1)

    print(f"\nüìÅ CLI App location: {cli_app_path}")
    print("üñ•Ô∏è  Starting DeepCode CLI interface...")
    print("üöÄ Initializing command line application")
    print("=" * 70)
    print("üí° Tip: Follow the interactive prompts to process your research")
    print("üõë Press Ctrl+C to exit at any time")
    print("=" * 70)

    # ÂêØÂä®CLIÂ∫îÁî® / Launch CLI application
    try:
        # ÂØºÂÖ•Âπ∂ËøêË°åCLIÂ∫îÁî®
        if str(project_root) not in sys.path:
            sys.path.insert(0, str(project_root))  # Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞Ë∑ØÂæÑ
        from cli.cli_app import main as cli_main

        print("\nüéØ Launching CLI application...")

        # ‰ΩøÁî®asyncioËøêË°å‰∏ªÂáΩÊï∞
        import asyncio

        asyncio.run(cli_main())

    except KeyboardInterrupt:
        print("\n\nüõë DeepCode CLI stopped by user")
        print("Thank you for using DeepCode CLI! üß¨")
    except ImportError as e:
        print(f"\n‚ùå Failed to import CLI application: {e}")
        print("Please check if all modules are properly installed.")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        print("Please check your Python environment and try again.")
        sys.exit(1)


if __name__ == "__main__":
    main()


--- cli/__init__.py ---
"""
CLI Module for DeepCode Agent
DeepCodeÊô∫ËÉΩ‰ΩìCLIÊ®°Âùó

ÂåÖÂê´‰ª•‰∏ãÁªÑ‰ª∂ / Contains the following components:
- cli_app: CLIÂ∫îÁî®‰∏ªÁ®ãÂ∫è / CLI application main program
- cli_interface: CLIÁïåÈù¢ÁªÑ‰ª∂ / CLI interface components
- cli_launcher: CLIÂêØÂä®Âô® / CLI launcher
"""

__version__ = "1.0.0"
__author__ = "DeepCode Team - Data Intelligence Lab @ HKU"

from .cli_app import main as cli_main
from .cli_interface import CLIInterface
from .cli_launcher import main as launcher_main

__all__ = ["cli_main", "CLIInterface", "launcher_main"]


--- cli/main_cli.py ---
#!/usr/bin/env python3
"""
DeepCode CLI - Open-Source Code Agent
Ê∑±Â∫¶‰ª£Á†ÅCLI - ÂºÄÊ∫ê‰ª£Á†ÅÊô∫ËÉΩ‰Ωì

üß¨ Data Intelligence Lab @ HKU
‚ö° Revolutionizing Research Reproducibility through Multi-Agent Architecture
"""

import os
import sys
import asyncio
import argparse

# Á¶ÅÊ≠¢ÁîüÊàê.pycÊñá‰ª∂
os.environ["PYTHONDONTWRITEBYTECODE"] = "1"

# Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞Ë∑ØÂæÑ
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# ÂØºÂÖ•CLIÂ∫îÁî®
from cli.cli_app import CLIApp, Colors


def print_enhanced_banner():
    """ÊòæÁ§∫Â¢ûÂº∫ÁâàÂêØÂä®Ê®™ÂπÖ"""
    banner = f"""
{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                              ‚ïë
‚ïë    {Colors.BOLD}{Colors.MAGENTA}üß¨ DeepCode - Open-Source Code Agent{Colors.CYAN}                              ‚ïë
‚ïë                                                                              ‚ïë
‚ïë    {Colors.BOLD}{Colors.YELLOW}‚ö° DATA INTELLIGENCE LAB @ HKU ‚ö°{Colors.CYAN}                                ‚ïë
‚ïë                                                                              ‚ïë
‚ïë    Revolutionizing research reproducibility through collaborative AI         ‚ïë
‚ïë    Building the future where code is reproduced from natural language       ‚ïë
‚ïë                                                                              ‚ïë
‚ïë    {Colors.BOLD}{Colors.GREEN}ü§ñ Key Features:{Colors.CYAN}                                                    ‚ïë
‚ïë    ‚Ä¢ Automated paper-to-code reproduction                                   ‚ïë
‚ïë    ‚Ä¢ Multi-agent collaborative architecture                                 ‚ïë
‚ïë    ‚Ä¢ Open-source and extensible design                                      ‚ïë
‚ïë    ‚Ä¢ Join our growing research community                                    ‚ïë
‚ïë                                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
    print(banner)


def check_environment():
    """Ê£ÄÊü•ËøêË°åÁéØÂ¢É"""
    print(f"{Colors.CYAN}üîç Checking environment...{Colors.ENDC}")

    # Ê£ÄÊü•PythonÁâàÊú¨
    if sys.version_info < (3, 8):
        print(
            f"{Colors.FAIL}‚ùå Python 3.8+ required. Current: {sys.version}{Colors.ENDC}"
        )
        return False

    print(f"{Colors.OKGREEN}‚úÖ Python {sys.version.split()[0]} - OK{Colors.ENDC}")

    # Ê£ÄÊü•ÂøÖË¶ÅÊ®°Âùó
    required_modules = [
        ("asyncio", "Async IO support"),
        ("pathlib", "Path handling"),
        ("typing", "Type hints"),
    ]

    missing_modules = []
    for module, desc in required_modules:
        try:
            __import__(module)
            print(f"{Colors.OKGREEN}‚úÖ {desc} - OK{Colors.ENDC}")
        except ImportError:
            missing_modules.append(module)
            print(f"{Colors.FAIL}‚ùå {desc} - Missing{Colors.ENDC}")

    if missing_modules:
        print(
            f"{Colors.FAIL}‚ùå Missing required modules: {', '.join(missing_modules)}{Colors.ENDC}"
        )
        return False

    print(f"{Colors.OKGREEN}‚úÖ Environment check passed{Colors.ENDC}")
    return True


def parse_arguments():
    """Ëß£ÊûêÂëΩ‰ª§Ë°åÂèÇÊï∞"""
    parser = argparse.ArgumentParser(
        description="DeepCode CLI - Open-Source Code Agent by Data Intelligence Lab @ HKU",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
{Colors.BOLD}Examples:{Colors.ENDC}
  {Colors.CYAN}python main_cli.py{Colors.ENDC}                                      # Interactive mode
  {Colors.CYAN}python main_cli.py --file paper.pdf{Colors.ENDC}                       # Process file directly
  {Colors.CYAN}python main_cli.py --url https://...{Colors.ENDC}                      # Process URL directly
  {Colors.CYAN}python main_cli.py --chat "Build a web app..."{Colors.ENDC}            # Process chat requirements
  {Colors.CYAN}python main_cli.py --requirement "ML system for..."{Colors.ENDC}       # Guided requirement analysis (NEW)
  {Colors.CYAN}python main_cli.py --optimized{Colors.ENDC}                            # Use optimized mode
  {Colors.CYAN}python main_cli.py --disable-segmentation{Colors.ENDC}                 # Disable document segmentation
  {Colors.CYAN}python main_cli.py --segmentation-threshold 30000{Colors.ENDC}         # Custom segmentation threshold

{Colors.BOLD}Pipeline Modes:{Colors.ENDC}
  {Colors.GREEN}Comprehensive{Colors.ENDC}:          Full intelligence analysis with indexing
  {Colors.YELLOW}Optimized{Colors.ENDC}:              Fast processing without indexing
  {Colors.BLUE}Requirement Analysis{Colors.ENDC}:   Guided Q&A to refine requirements (NEW)

{Colors.BOLD}Document Processing:{Colors.ENDC}
  {Colors.BLUE}Smart Segmentation{Colors.ENDC}: Intelligent document segmentation for large papers
  {Colors.MAGENTA}Supported Formats{Colors.ENDC}: PDF, DOCX, DOC, PPT, PPTX, XLS, XLSX, HTML, TXT, MD
        """,
    )

    parser.add_argument(
        "--file", "-f", type=str, help="Process a specific file (PDF, DOCX, TXT, etc.)"
    )

    parser.add_argument(
        "--url", "-u", type=str, help="Process a research paper from URL"
    )

    parser.add_argument(
        "--chat",
        "-t",
        type=str,
        help="Process coding requirements via chat input (provide requirements as argument)",
    )

    parser.add_argument(
        "--requirement",
        "-r",
        type=str,
        help="Process requirements via guided analysis (provide initial idea as argument)",
    )

    parser.add_argument(
        "--optimized",
        "-o",
        action="store_true",
        help="Use optimized mode (skip indexing for faster processing)",
    )

    parser.add_argument(
        "--disable-segmentation",
        action="store_true",
        help="Disable intelligent document segmentation (use traditional full-document processing)",
    )

    parser.add_argument(
        "--segmentation-threshold",
        type=int,
        default=50000,
        help="Document size threshold (characters) to trigger segmentation (default: 50000)",
    )

    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable verbose output"
    )

    return parser.parse_args()


async def run_direct_processing(app: CLIApp, input_source: str, input_type: str):
    """Áõ¥Êé•Â§ÑÁêÜÊ®°ÂºèÔºàÈùû‰∫§‰∫íÂºèÔºâ"""
    try:
        print(
            f"\n{Colors.BOLD}{Colors.CYAN}üöÄ Starting direct processing mode...{Colors.ENDC}"
        )
        print(f"{Colors.CYAN}Input: {input_source}{Colors.ENDC}")
        print(f"{Colors.CYAN}Type: {input_type}{Colors.ENDC}")
        print(
            f"{Colors.CYAN}Mode: {'üß† Comprehensive' if app.cli.enable_indexing else '‚ö° Optimized'}{Colors.ENDC}"
        )

        # ÂàùÂßãÂåñÂ∫îÁî®
        init_result = await app.initialize_mcp_app()
        if init_result["status"] != "success":
            print(
                f"{Colors.FAIL}‚ùå Initialization failed: {init_result['message']}{Colors.ENDC}"
            )
            return False

        # Â§ÑÁêÜËæìÂÖ•
        result = await app.process_input(input_source, input_type)

        if result["status"] == "success":
            print(
                f"\n{Colors.BOLD}{Colors.OKGREEN}üéâ Processing completed successfully!{Colors.ENDC}"
            )
            return True
        else:
            print(
                f"\n{Colors.BOLD}{Colors.FAIL}‚ùå Processing failed: {result.get('error', 'Unknown error')}{Colors.ENDC}"
            )
            return False

    except Exception as e:
        print(f"\n{Colors.FAIL}‚ùå Direct processing error: {str(e)}{Colors.ENDC}")
        return False
    finally:
        await app.cleanup_mcp_app()


async def run_requirement_analysis(app: CLIApp, initial_idea: str):
    """ÈúÄÊ±ÇÂàÜÊûêÊ®°ÂºèÔºàÈùû‰∫§‰∫íÂºèÔºâ - NEW: matching UI version"""
    try:
        print(
            f"\n{Colors.BOLD}{Colors.BLUE}üß† Starting requirement analysis mode...{Colors.ENDC}"
        )
        print(f"{Colors.CYAN}Initial Idea: {initial_idea}{Colors.ENDC}")

        # ÂàùÂßãÂåñÂ∫îÁî®
        init_result = await app.initialize_mcp_app()
        if init_result["status"] != "success":
            print(
                f"{Colors.FAIL}‚ùå Initialization failed: {init_result['message']}{Colors.ENDC}"
            )
            return False

        # ÊâßË°åÈúÄÊ±ÇÂàÜÊûêÂ∑•‰ΩúÊµÅ
        result = await app.process_requirement_analysis_non_interactive(initial_idea)

        if result["status"] == "success":
            print(
                f"\n{Colors.BOLD}{Colors.OKGREEN}üéâ Requirement analysis completed successfully!{Colors.ENDC}"
            )
            return True
        else:
            print(
                f"\n{Colors.BOLD}{Colors.FAIL}‚ùå Requirement analysis failed: {result.get('error', 'Unknown error')}{Colors.ENDC}"
            )
            return False

    except Exception as e:
        print(f"\n{Colors.FAIL}‚ùå Requirement analysis error: {str(e)}{Colors.ENDC}")
        return False
    finally:
        await app.cleanup_mcp_app()


async def main():
    """‰∏ªÂáΩÊï∞"""
    # Ëß£ÊûêÂëΩ‰ª§Ë°åÂèÇÊï∞
    args = parse_arguments()

    # ÊòæÁ§∫Ê®™ÂπÖ
    print_enhanced_banner()

    # Ê£ÄÊü•ÁéØÂ¢É
    if not check_environment():
        print(
            f"\n{Colors.FAIL}üö® Environment check failed. Please fix the issues and try again.{Colors.ENDC}"
        )
        sys.exit(1)

    try:
        # ÂàõÂª∫CLIÂ∫îÁî®
        app = CLIApp()

        # ËÆæÁΩÆÈÖçÁΩÆ - ÈªòËÆ§Á¶ÅÁî®Á¥¢ÂºïÂäüËÉΩ‰ª•Âä†Âø´Â§ÑÁêÜÈÄüÂ∫¶
        if args.optimized:
            app.cli.enable_indexing = False
            print(
                f"\n{Colors.YELLOW}‚ö° Optimized mode enabled - indexing disabled{Colors.ENDC}"
            )
        else:
            # ÈªòËÆ§‰πüÁ¶ÅÁî®Á¥¢ÂºïÂäüËÉΩ
            app.cli.enable_indexing = False
            print(
                f"\n{Colors.YELLOW}‚ö° Fast mode enabled - indexing disabled by default{Colors.ENDC}"
            )

        # Configure document segmentation settings
        if hasattr(args, "disable_segmentation") and args.disable_segmentation:
            print(
                f"\n{Colors.MAGENTA}üìÑ Document segmentation disabled - using traditional processing{Colors.ENDC}"
            )
            app.cli.segmentation_enabled = False
            app.cli.segmentation_threshold = args.segmentation_threshold
            app.cli._save_segmentation_config()
        else:
            print(
                f"\n{Colors.BLUE}üìÑ Smart document segmentation enabled (threshold: {args.segmentation_threshold} chars){Colors.ENDC}"
            )
            app.cli.segmentation_enabled = True
            app.cli.segmentation_threshold = args.segmentation_threshold
            app.cli._save_segmentation_config()

        # Ê£ÄÊü•ÊòØÂê¶‰∏∫Áõ¥Êé•Â§ÑÁêÜÊ®°Âºè
        if args.file or args.url or args.chat or args.requirement:
            if args.file:
                # È™åËØÅÊñá‰ª∂Â≠òÂú®
                if not os.path.exists(args.file):
                    print(f"{Colors.FAIL}‚ùå File not found: {args.file}{Colors.ENDC}")
                    sys.exit(1)
                # ‰ΩøÁî® file:// ÂâçÁºÄ‰øùÊåÅ‰∏é‰∫§‰∫íÊ®°Âºè‰∏ÄËá¥ÔºåÁ°Æ‰øùÊñá‰ª∂Ë¢´Â§çÂà∂ËÄåÈùûÁßªÂä®
                file_url = f"file://{os.path.abspath(args.file)}"
                success = await run_direct_processing(app, file_url, "file")
            elif args.url:
                success = await run_direct_processing(app, args.url, "url")
            elif args.chat:
                # È™åËØÅchatËæìÂÖ•ÈïøÂ∫¶
                if len(args.chat.strip()) < 20:
                    print(
                        f"{Colors.FAIL}‚ùå Chat input too short. Please provide more detailed requirements (at least 20 characters){Colors.ENDC}"
                    )
                    sys.exit(1)
                success = await run_direct_processing(app, args.chat, "chat")
            elif args.requirement:
                # NEW: Requirement analysis mode
                # È™åËØÅÈúÄÊ±ÇËæìÂÖ•ÈïøÂ∫¶
                if len(args.requirement.strip()) < 10:
                    print(
                        f"{Colors.FAIL}‚ùå Requirement input too short. Please provide more details (at least 10 characters){Colors.ENDC}"
                    )
                    sys.exit(1)
                success = await run_requirement_analysis(app, args.requirement)

            sys.exit(0 if success else 1)
        else:
            # ‰∫§‰∫íÂºèÊ®°Âºè
            print(f"\n{Colors.CYAN}üéÆ Starting interactive mode...{Colors.ENDC}")
            await app.run_interactive_session()

    except KeyboardInterrupt:
        print(f"\n{Colors.WARNING}‚ö†Ô∏è  Application interrupted by user{Colors.ENDC}")
        sys.exit(1)
    except Exception as e:
        print(f"\n{Colors.FAIL}‚ùå Application errors: {str(e)}{Colors.ENDC}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())


--- cli/workflows/cli_workflow_adapter.py ---
"""
CLI Workflow Adapter for Agent Orchestration Engine
CLIÂ∑•‰ΩúÊµÅÈÄÇÈÖçÂô® - Êô∫ËÉΩ‰ΩìÁºñÊéíÂºïÊìé

This adapter provides CLI-optimized interface to the latest agent orchestration engine,
with enhanced progress reporting, error handling, and CLI-specific optimizations.

Version: 2.1 (Updated to match UI version - Added Requirement Analysis)
Changes:
- Default enable_indexing=False for faster processing (matching UI defaults)
- Mode-aware progress callback with detailed stage mapping
- Chat pipeline now accepts enable_indexing parameter
- Improved error handling and resource management
- Enhanced progress display for different modes (fast/comprehensive/chat)
- NEW: Added requirement analysis workflow support
"""

import os
from typing import Callable, Dict, Any
from mcp_agent.app import MCPApp


class CLIWorkflowAdapter:
    """
    CLI-optimized workflow adapter for the intelligent agent orchestration engine.

    This adapter provides:
    - Enhanced CLI progress reporting
    - Optimized error handling for CLI environments
    - Streamlined interface for command-line usage
    - Integration with the latest agent orchestration engine
    """

    def __init__(self, cli_interface=None):
        """
        Initialize CLI workflow adapter.

        Args:
            cli_interface: CLI interface instance for progress reporting
        """
        self.cli_interface = cli_interface
        self.app = None
        self.logger = None
        self.context = None

    async def initialize_mcp_app(self) -> Dict[str, Any]:
        """
        Initialize MCP application for CLI usage (improved version matching UI).

        Returns:
            dict: Initialization result
        """
        try:
            if self.cli_interface:
                self.cli_interface.show_spinner(
                    "üöÄ Initializing Agent Orchestration Engine", 2.0
                )

            # Initialize MCP application using async context manager (matching UI pattern)
            self.app = MCPApp(name="cli_agent_orchestration")
            self.app_context = self.app.run()
            agent_app = await self.app_context.__aenter__()

            self.logger = agent_app.logger
            self.context = agent_app.context

            # Configure filesystem access
            self.context.config.mcp.servers["filesystem"].args.extend([os.getcwd()])

            if self.cli_interface:
                self.cli_interface.print_status(
                    "üß† Agent Orchestration Engine initialized successfully", "success"
                )

            return {
                "status": "success",
                "message": "MCP application initialized successfully",
            }

        except Exception as e:
            error_msg = f"Failed to initialize MCP application: {str(e)}"
            if self.cli_interface:
                self.cli_interface.print_status(error_msg, "error")
            return {"status": "error", "message": error_msg}

    async def cleanup_mcp_app(self):
        """
        Clean up MCP application resources.
        """
        if hasattr(self, "app_context"):
            try:
                await self.app_context.__aexit__(None, None, None)
                if self.cli_interface:
                    self.cli_interface.print_status(
                        "üßπ Resources cleaned up successfully", "info"
                    )
            except Exception as e:
                if self.cli_interface:
                    self.cli_interface.print_status(
                        f"‚ö†Ô∏è Cleanup warning: {str(e)}", "warning"
                    )

    def create_cli_progress_callback(self, enable_indexing: bool = True) -> Callable:
        """
        Create CLI-optimized progress callback function with mode-aware stage mapping.

        This matches the UI version's detailed progress mapping logic.

        Args:
            enable_indexing: Whether indexing is enabled (affects stage mapping)

        Returns:
            Callable: Progress callback function
        """

        def progress_callback(progress: int, message: str):
            if self.cli_interface:
                # Mode-aware stage mapping (matching UI version logic)
                if enable_indexing:
                    # Full workflow mapping: Initialize -> Analyze -> Download -> Plan -> References -> Repos -> Index -> Implement
                    if progress <= 5:
                        stage = 0  # Initialize
                    elif progress <= 10:
                        stage = 1  # Analyze
                    elif progress <= 25:
                        stage = 2  # Download
                    elif progress <= 40:
                        stage = 3  # Plan
                    elif progress <= 50:
                        stage = 4  # References
                    elif progress <= 60:
                        stage = 5  # Repos
                    elif progress <= 70:
                        stage = 6  # Index
                    elif progress <= 85:
                        stage = 7  # Implement
                    else:
                        stage = 8  # Complete
                else:
                    # Fast mode mapping: Initialize -> Analyze -> Download -> Plan -> Implement
                    if progress <= 5:
                        stage = 0  # Initialize
                    elif progress <= 10:
                        stage = 1  # Analyze
                    elif progress <= 25:
                        stage = 2  # Download
                    elif progress <= 40:
                        stage = 3  # Plan
                    elif progress <= 85:
                        stage = 4  # Implement (skip References, Repos, Index)
                    else:
                        stage = 4  # Complete

                self.cli_interface.display_processing_stages(stage, enable_indexing)

                # Display status message
                self.cli_interface.print_status(message, "processing")

        return progress_callback

    async def execute_full_pipeline(
        self, input_source: str, enable_indexing: bool = False
    ) -> Dict[str, Any]:
        """
        Execute the complete intelligent multi-agent research orchestration pipeline.

        Updated to match UI version: default enable_indexing=False for faster processing.

        Args:
            input_source: Research input source (file path, URL, or preprocessed analysis)
            enable_indexing: Whether to enable advanced intelligence analysis (default: False)

        Returns:
            dict: Comprehensive pipeline execution result
        """
        try:
            # Import the latest agent orchestration engine
            from workflows.agent_orchestration_engine import (
                execute_multi_agent_research_pipeline,
            )

            # Create CLI progress callback with mode awareness
            progress_callback = self.create_cli_progress_callback(enable_indexing)

            # Display pipeline start
            if self.cli_interface:
                if enable_indexing:
                    mode_msg = "üß† comprehensive (with indexing)"
                else:
                    mode_msg = "‚ö° fast (indexing disabled)"
                self.cli_interface.print_status(
                    f"üöÄ Starting {mode_msg} agent orchestration pipeline...",
                    "processing",
                )
                self.cli_interface.display_processing_stages(0, enable_indexing)

            # Execute the pipeline
            result = await execute_multi_agent_research_pipeline(
                input_source=input_source,
                logger=self.logger,
                progress_callback=progress_callback,
                enable_indexing=enable_indexing,
            )

            # Display completion
            if self.cli_interface:
                final_stage = 8 if enable_indexing else 4
                self.cli_interface.display_processing_stages(
                    final_stage, enable_indexing
                )
                self.cli_interface.print_status(
                    "üéâ Agent orchestration pipeline completed successfully!",
                    "complete",
                )

            return {
                "status": "success",
                "result": result,
                "pipeline_mode": "comprehensive" if enable_indexing else "optimized",
            }

        except Exception as e:
            error_msg = f"Pipeline execution failed: {str(e)}"
            if self.cli_interface:
                self.cli_interface.print_status(error_msg, "error")

            return {
                "status": "error",
                "error": error_msg,
                "pipeline_mode": "comprehensive" if enable_indexing else "optimized",
            }

    async def execute_requirement_analysis_workflow(
        self, user_input: str, analysis_mode: str, user_answers: Dict[str, str] = None
    ) -> Dict[str, Any]:
        """
        Execute requirement analysis workflow (NEW: matching UI version).

        This workflow helps users refine their requirements through guided questions
        and intelligent analysis before starting code implementation.

        Args:
            user_input: User's initial requirements or description
            analysis_mode: Analysis mode ("generate_questions" or "summarize_requirements")
            user_answers: Dictionary of user answers to guiding questions (for summarize mode)

        Returns:
            dict: Analysis result with questions or requirement summary
        """
        try:
            # Import the requirement analysis workflow
            from workflows.agent_orchestration_engine import (
                execute_requirement_analysis_workflow,
            )

            # Create CLI progress callback
            def analysis_progress_callback(progress: int, message: str):
                if self.cli_interface:
                    self.cli_interface.print_status(message, "processing")

            # Display workflow start
            if self.cli_interface:
                if analysis_mode == "generate_questions":
                    self.cli_interface.print_status(
                        "ü§ñ Generating guiding questions for your requirements...",
                        "processing",
                    )
                else:
                    self.cli_interface.print_status(
                        "üìÑ Analyzing and summarizing your detailed requirements...",
                        "processing",
                    )

            # Execute the requirement analysis workflow
            result = await execute_requirement_analysis_workflow(
                user_input=user_input,
                analysis_mode=analysis_mode,
                user_answers=user_answers,
                logger=self.logger,
                progress_callback=analysis_progress_callback,
            )

            # Display completion
            if self.cli_interface:
                if result["status"] == "success":
                    if analysis_mode == "generate_questions":
                        self.cli_interface.print_status(
                            "‚úÖ Guiding questions generated successfully!", "success"
                        )
                    else:
                        self.cli_interface.print_status(
                            "‚úÖ Requirements analysis completed successfully!",
                            "success",
                        )
                else:
                    self.cli_interface.print_status(
                        f"‚ùå Analysis failed: {result.get('error', 'Unknown error')}",
                        "error",
                    )

            return result

        except Exception as e:
            error_msg = f"Requirement analysis workflow failed: {str(e)}"
            if self.cli_interface:
                self.cli_interface.print_status(error_msg, "error")

            return {"status": "error", "error": error_msg}

    async def execute_chat_pipeline(
        self, user_input: str, enable_indexing: bool = False
    ) -> Dict[str, Any]:
        """
        Execute the chat-based planning and implementation pipeline.

        Updated to match UI version: accepts enable_indexing parameter.

        Args:
            user_input: User's coding requirements and description
            enable_indexing: Whether to enable indexing for enhanced code understanding (default: False)

        Returns:
            dict: Chat pipeline execution result
        """
        try:
            # Import the chat-based pipeline
            from workflows.agent_orchestration_engine import (
                execute_chat_based_planning_pipeline,
            )

            # Create CLI progress callback for chat mode
            def chat_progress_callback(progress: int, message: str):
                if self.cli_interface:
                    # Map progress to CLI stages for chat mode (matching UI logic)
                    if progress <= 5:
                        stage = 0  # Initialize
                    elif progress <= 30:
                        stage = 1  # Planning
                    elif progress <= 50:
                        stage = 2  # Setup
                    elif progress <= 70:
                        stage = 3  # Save Plan
                    else:
                        stage = 4  # Implement

                    self.cli_interface.display_processing_stages(stage, chat_mode=True)

                    # Display status message
                    self.cli_interface.print_status(message, "processing")

            # Display pipeline start
            if self.cli_interface:
                indexing_note = (
                    " (with indexing)" if enable_indexing else " (fast mode)"
                )
                self.cli_interface.print_status(
                    f"üöÄ Starting chat-based planning pipeline{indexing_note}...",
                    "processing",
                )
                self.cli_interface.display_processing_stages(0, chat_mode=True)

            # Execute the chat pipeline with configurable indexing
            result = await execute_chat_based_planning_pipeline(
                user_input=user_input,
                logger=self.logger,
                progress_callback=chat_progress_callback,
                enable_indexing=enable_indexing,  # Pass through enable_indexing parameter
            )

            # Display completion
            if self.cli_interface:
                self.cli_interface.display_processing_stages(4, chat_mode=True)
                self.cli_interface.print_status(
                    "üéâ Chat-based planning pipeline completed successfully!",
                    "complete",
                )

            return {"status": "success", "result": result, "pipeline_mode": "chat"}

        except Exception as e:
            error_msg = f"Chat pipeline execution failed: {str(e)}"
            if self.cli_interface:
                self.cli_interface.print_status(error_msg, "error")

            return {"status": "error", "error": error_msg, "pipeline_mode": "chat"}

    async def process_input_with_orchestration(
        self, input_source: str, input_type: str, enable_indexing: bool = False
    ) -> Dict[str, Any]:
        """
        Process input using the intelligent agent orchestration engine.

        This is the main CLI interface to the latest agent orchestration capabilities.
        Updated to match UI version: default enable_indexing=False.

        Args:
            input_source: Input source (file path, URL, or chat input)
            input_type: Type of input ('file', 'url', or 'chat')
            enable_indexing: Whether to enable advanced intelligence analysis (default: False)

        Returns:
            dict: Processing result with status and details
        """
        pipeline_result = None

        try:
            # Initialize MCP app
            init_result = await self.initialize_mcp_app()
            if init_result["status"] != "success":
                return init_result

            # Process file:// URLs for traditional file/URL inputs
            if input_source.startswith("file://"):
                file_path = input_source[7:]
                if os.name == "nt" and file_path.startswith("/"):
                    file_path = file_path.lstrip("/")
                input_source = file_path

            # Execute appropriate pipeline based on input type
            if input_type == "chat":
                # Use chat-based planning pipeline for user requirements
                # Pass enable_indexing to chat pipeline as well
                pipeline_result = await self.execute_chat_pipeline(
                    input_source, enable_indexing=enable_indexing
                )
            else:
                # Use traditional multi-agent research pipeline for files/URLs
                pipeline_result = await self.execute_full_pipeline(
                    input_source, enable_indexing=enable_indexing
                )

            return {
                "status": pipeline_result["status"],
                "analysis_result": "Integrated into agent orchestration pipeline",
                "download_result": "Integrated into agent orchestration pipeline",
                "repo_result": pipeline_result.get("result", ""),
                "pipeline_mode": pipeline_result.get("pipeline_mode", "comprehensive"),
                "error": pipeline_result.get("error"),
            }

        except Exception as e:
            error_msg = f"Error during orchestrated processing: {str(e)}"
            if self.cli_interface:
                self.cli_interface.print_status(error_msg, "error")

            return {
                "status": "error",
                "error": error_msg,
                "analysis_result": "",
                "download_result": "",
                "repo_result": "",
                "pipeline_mode": "comprehensive" if enable_indexing else "optimized",
            }

        finally:
            # Clean up resources
            await self.cleanup_mcp_app()


--- cli/workflows/__init__.py ---
"""
CLI-specific Workflow Adapters
CLI‰∏ìÁî®Â∑•‰ΩúÊµÅÈÄÇÈÖçÂô®

This module provides CLI-optimized versions of workflow components that are
specifically adapted for command-line interface usage patterns.
"""

from .cli_workflow_adapter import CLIWorkflowAdapter

__all__ = ["CLIWorkflowAdapter"]


--- config/mcp_tool_definitions.py ---
"""
MCPÂ∑•ÂÖ∑ÂÆö‰πâÈÖçÁΩÆÊ®°Âùó
MCP Tool Definitions Configuration Module

Â∞ÜÂ∑•ÂÖ∑ÂÆö‰πâ‰ªé‰∏ªÁ®ãÂ∫èÈÄªËæë‰∏≠ÂàÜÁ¶ªÔºåÊèê‰æõÊ†áÂáÜÂåñÁöÑÂ∑•ÂÖ∑ÂÆö‰πâÊ†ºÂºè
Separate tool definitions from main program logic, providing standardized tool definition format

ÊîØÊåÅÁöÑÂ∑•ÂÖ∑Á±ªÂûãÔºö
- Êñá‰ª∂Êìç‰ΩúÂ∑•ÂÖ∑ (File Operations)
- ‰ª£Á†ÅÊâßË°åÂ∑•ÂÖ∑ (Code Execution)
- ÊêúÁ¥¢Â∑•ÂÖ∑ (Search Tools)
- È°πÁõÆÁªìÊûÑÂ∑•ÂÖ∑ (Project Structure Tools)
"""

from typing import Dict, List, Any


class MCPToolDefinitions:
    """MCPÂ∑•ÂÖ∑ÂÆö‰πâÁÆ°ÁêÜÂô®"""

    @staticmethod
    def get_code_implementation_tools() -> List[Dict[str, Any]]:
        """
        Ëé∑Âèñ‰ª£Á†ÅÂÆûÁé∞Áõ∏ÂÖ≥ÁöÑÂ∑•ÂÖ∑ÂÆö‰πâ
        Get tool definitions for code implementation
        """
        return [
            # MCPToolDefinitions._get_read_file_tool(),
            # MCPToolDefinitions._get_read_multiple_files_tool(),
            # MCPToolDefinitions._get_read_code_mem_tool(),
            MCPToolDefinitions._get_write_file_tool(),
            # MCPToolDefinitions._get_write_multiple_files_tool(),
            # MCPToolDefinitions._get_execute_python_tool(),
            # MCPToolDefinitions._get_execute_bash_tool(),
        ]

    @staticmethod
    def _get_read_file_tool() -> Dict[str, Any]:
        """ËØªÂèñÊñá‰ª∂Â∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "read_file",
            "description": "Read file content, supports specifying line number range",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "File path, relative to workspace",
                    },
                    "start_line": {
                        "type": "integer",
                        "description": "Start line number (starting from 1, optional)",
                    },
                    "end_line": {
                        "type": "integer",
                        "description": "End line number (starting from 1, optional)",
                    },
                },
                "required": ["file_path"],
            },
        }

    @staticmethod
    def _get_read_multiple_files_tool() -> Dict[str, Any]:
        """ÊâπÈáèËØªÂèñÂ§ö‰∏™Êñá‰ª∂Â∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "read_multiple_files",
            "description": "Read multiple files in a single operation (for batch reading)",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_requests": {
                        "type": "string",
                        "description": 'JSON string with file requests, e.g., \'{"file1.py": {}, "file2.py": {"start_line": 1, "end_line": 10}}\' or simple array \'["file1.py", "file2.py"]\'',
                    },
                    "max_files": {
                        "type": "integer",
                        "description": "Maximum number of files to read in one operation",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 10,
                    },
                },
                "required": ["file_requests"],
            },
        }

    @staticmethod
    def _get_read_code_mem_tool() -> Dict[str, Any]:
        """Read code memory tool definition - reads from implement_code_summary.md"""
        return {
            "name": "read_code_mem",
            "description": "Check if file summaries exist in implement_code_summary.md for multiple files in a single call. Returns summaries for all requested files if available.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_paths": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "List of file paths to check for summary information in implement_code_summary.md",
                    }
                },
                "required": ["file_paths"],
            },
        }

    @staticmethod
    def _get_write_file_tool() -> Dict[str, Any]:
        """ÂÜôÂÖ•Êñá‰ª∂Â∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "write_file",
            "description": "Write content to file",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "File path, relative to workspace",
                    },
                    "content": {
                        "type": "string",
                        "description": "Content to write to file",
                    },
                    "create_dirs": {
                        "type": "boolean",
                        "description": "Whether to create directories if they don't exist",
                        "default": True,
                    },
                    "create_backup": {
                        "type": "boolean",
                        "description": "Whether to create backup file if file already exists",
                        "default": False,
                    },
                },
                "required": ["file_path", "content"],
            },
        }

    @staticmethod
    def _get_write_multiple_files_tool() -> Dict[str, Any]:
        """ÊâπÈáèÂÜôÂÖ•Â§ö‰∏™Êñá‰ª∂Â∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "write_multiple_files",
            "description": "Write multiple files in a single operation (for batch implementation)",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_implementations": {
                        "type": "string",
                        "description": 'JSON string mapping file paths to content, e.g., \'{"file1.py": "content1", "file2.py": "content2"}\'',
                    },
                    "create_dirs": {
                        "type": "boolean",
                        "description": "Whether to create directories if they don't exist",
                        "default": True,
                    },
                    "create_backup": {
                        "type": "boolean",
                        "description": "Whether to create backup files if they already exist",
                        "default": False,
                    },
                    "max_files": {
                        "type": "integer",
                        "description": "Maximum number of files to write in one operation",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 10,
                    },
                },
                "required": ["file_implementations"],
            },
        }

    @staticmethod
    def _get_execute_python_tool() -> Dict[str, Any]:
        """PythonÊâßË°åÂ∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "execute_python",
            "description": "Execute Python code and return output",
            "input_schema": {
                "type": "object",
                "properties": {
                    "code": {"type": "string", "description": "Python code to execute"},
                    "timeout": {
                        "type": "integer",
                        "description": "Timeout in seconds",
                        "default": 30,
                    },
                },
                "required": ["code"],
            },
        }

    @staticmethod
    def _get_execute_bash_tool() -> Dict[str, Any]:
        """BashÊâßË°åÂ∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "execute_bash",
            "description": "Execute bash command",
            "input_schema": {
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "description": "Bash command to execute",
                    },
                    "timeout": {
                        "type": "integer",
                        "description": "Timeout in seconds",
                        "default": 30,
                    },
                },
                "required": ["command"],
            },
        }

    @staticmethod
    def _get_file_structure_tool() -> Dict[str, Any]:
        """Êñá‰ª∂ÁªìÊûÑËé∑ÂèñÂ∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "get_file_structure",
            "description": "Get directory file structure",
            "input_schema": {
                "type": "object",
                "properties": {
                    "directory": {
                        "type": "string",
                        "description": "Directory path, relative to workspace",
                        "default": ".",
                    },
                    "max_depth": {
                        "type": "integer",
                        "description": "Maximum traversal depth",
                        "default": 5,
                    },
                },
            },
        }

    @staticmethod
    def _get_search_code_references_tool() -> Dict[str, Any]:
        """Áªü‰∏Ä‰ª£Á†ÅÂèÇËÄÉÊêúÁ¥¢Â∑•ÂÖ∑ÂÆö‰πâ - ÂêàÂπ∂‰∫Ü‰∏â‰∏™Ê≠•È™§‰∏∫‰∏Ä‰∏™Â∑•ÂÖ∑"""
        return {
            "name": "search_code_references",
            "description": "UNIFIED TOOL: Search relevant reference code from index files. Combines directory setup, index loading, and searching in a single call.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "indexes_path": {
                        "type": "string",
                        "description": "Path to the indexes directory containing JSON index files",
                    },
                    "target_file": {
                        "type": "string",
                        "description": "Target file path to be implemented",
                    },
                    "keywords": {
                        "type": "string",
                        "description": "Search keywords, comma-separated",
                        "default": "",
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "Maximum number of results to return",
                        "default": 10,
                    },
                },
                "required": ["indexes_path", "target_file"],
            },
        }

    @staticmethod
    def _get_get_indexes_overview_tool() -> Dict[str, Any]:
        """Ëé∑ÂèñÁ¥¢ÂºïÊ¶ÇËßàÂ∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "get_indexes_overview",
            "description": "Get overview of all available reference code index information from specified directory",
            "input_schema": {
                "type": "object",
                "properties": {
                    "indexes_path": {
                        "type": "string",
                        "description": "Path to the indexes directory containing JSON index files",
                    }
                },
                "required": ["indexes_path"],
            },
        }

    @staticmethod
    def _get_set_workspace_tool() -> Dict[str, Any]:
        """Set workspace directory tool definition"""
        return {
            "name": "set_workspace",
            "description": "Set the workspace directory for file operations",
            "input_schema": {
                "type": "object",
                "properties": {
                    "workspace_path": {
                        "type": "string",
                        "description": "Directory path for the workspace",
                    }
                },
                "required": ["workspace_path"],
            },
        }

    # @staticmethod
    # def _get_set_indexes_directory_tool() -> Dict[str, Any]:
    #     """Set indexes directory tool definition - DEPRECATED: Use unified search_code_references instead"""
    #     return {
    #         "name": "set_indexes_directory",
    #         "description": "Set the directory path for code reference indexes",
    #         "input_schema": {
    #             "type": "object",
    #             "properties": {
    #                 "indexes_path": {
    #                     "type": "string",
    #                     "description": "Directory path containing index JSON files"
    #                 }
    #             },
    #             "required": ["indexes_path"]
    #         }
    #     }

    @staticmethod
    def get_available_tool_sets() -> Dict[str, str]:
        """
        Ëé∑ÂèñÂèØÁî®ÁöÑÂ∑•ÂÖ∑ÈõÜÂêà
        Get available tool sets
        """
        return {
            "code_implementation": "‰ª£Á†ÅÂÆûÁé∞Áõ∏ÂÖ≥Â∑•ÂÖ∑ÈõÜ / Code implementation tool set",
            # ÂèØ‰ª•Âú®ËøôÈáåÊ∑ªÂä†Êõ¥Â§öÂ∑•ÂÖ∑ÈõÜ
            # "data_analysis": "Êï∞ÊçÆÂàÜÊûêÂ∑•ÂÖ∑ÈõÜ / Data analysis tool set",
            # "web_scraping": "ÁΩëÈ°µÁà¨ÂèñÂ∑•ÂÖ∑ÈõÜ / Web scraping tool set",
        }

    @staticmethod
    def get_tool_set(tool_set_name: str) -> List[Dict[str, Any]]:
        """
        Ê†πÊçÆÂêçÁß∞Ëé∑ÂèñÁâπÂÆöÁöÑÂ∑•ÂÖ∑ÈõÜ
        Get specific tool set by name
        """
        tool_sets = {
            "code_implementation": MCPToolDefinitions.get_code_implementation_tools(),
        }

        return tool_sets.get(tool_set_name, [])

    @staticmethod
    def get_all_tools() -> List[Dict[str, Any]]:
        """
        Ëé∑ÂèñÊâÄÊúâÂèØÁî®Â∑•ÂÖ∑
        Get all available tools
        """
        all_tools = []
        for tool_set_name in MCPToolDefinitions.get_available_tool_sets().keys():
            all_tools.extend(MCPToolDefinitions.get_tool_set(tool_set_name))
        return all_tools


# ‰æøÊç∑ËÆøÈóÆÂáΩÊï∞
def get_mcp_tools(tool_set: str = "code_implementation") -> List[Dict[str, Any]]:
    """
    ‰æøÊç∑ÂáΩÊï∞ÔºöËé∑ÂèñMCPÂ∑•ÂÖ∑ÂÆö‰πâ
    Convenience function: Get MCP tool definitions

    Args:
        tool_set: Â∑•ÂÖ∑ÈõÜÂêçÁß∞ (ÈªòËÆ§: "code_implementation")

    Returns:
        Â∑•ÂÖ∑ÂÆö‰πâÂàóË°®
    """
    return MCPToolDefinitions.get_tool_set(tool_set)


--- config/mcp_tool_definitions_index.py ---
"""
MCPÂ∑•ÂÖ∑ÂÆö‰πâÈÖçÁΩÆÊ®°Âùó
MCP Tool Definitions Configuration Module

Â∞ÜÂ∑•ÂÖ∑ÂÆö‰πâ‰ªé‰∏ªÁ®ãÂ∫èÈÄªËæë‰∏≠ÂàÜÁ¶ªÔºåÊèê‰æõÊ†áÂáÜÂåñÁöÑÂ∑•ÂÖ∑ÂÆö‰πâÊ†ºÂºè
Separate tool definitions from main program logic, providing standardized tool definition format

ÊîØÊåÅÁöÑÂ∑•ÂÖ∑Á±ªÂûãÔºö
- Êñá‰ª∂Êìç‰ΩúÂ∑•ÂÖ∑ (File Operations)
- ‰ª£Á†ÅÊâßË°åÂ∑•ÂÖ∑ (Code Execution)
- ÊêúÁ¥¢Â∑•ÂÖ∑ (Search Tools)
- È°πÁõÆÁªìÊûÑÂ∑•ÂÖ∑ (Project Structure Tools)
"""

from typing import Dict, List, Any


class MCPToolDefinitions:
    """MCPÂ∑•ÂÖ∑ÂÆö‰πâÁÆ°ÁêÜÂô®"""

    @staticmethod
    def get_code_implementation_tools() -> List[Dict[str, Any]]:
        """
        Ëé∑Âèñ‰ª£Á†ÅÂÆûÁé∞Áõ∏ÂÖ≥ÁöÑÂ∑•ÂÖ∑ÂÆö‰πâ
        Get tool definitions for code implementation
        """
        return [
            # MCPToolDefinitions._get_read_file_tool(),
            # MCPToolDefinitions._get_read_multiple_files_tool(),
            # MCPToolDefinitions._get_read_code_mem_tool(),
            MCPToolDefinitions._get_write_file_tool(),
            # MCPToolDefinitions._get_write_multiple_files_tool(),
            # MCPToolDefinitions._get_execute_python_tool(),
            # MCPToolDefinitions._get_execute_bash_tool(),
            MCPToolDefinitions._get_search_code_references_tool(),
            # MCPToolDefinitions._get_search_code_tool(),
            # MCPToolDefinitions._get_file_structure_tool(),
            # MCPToolDefinitions._get_set_workspace_tool(),
            # MCPToolDefinitions._get_operation_history_tool(),
        ]

    @staticmethod
    def get_code_evaluation_tools() -> List[Dict[str, Any]]:
        """
        Ëé∑Âèñ‰ª£Á†ÅËØÑ‰º∞Áõ∏ÂÖ≥ÁöÑÂ∑•ÂÖ∑ÂÆö‰πâ
        Get tool definitions for code evaluation
        """
        return [
            MCPToolDefinitions._get_analyze_repo_structure_tool(),
            MCPToolDefinitions._get_detect_dependencies_tool(),
            MCPToolDefinitions._get_assess_code_quality_tool(),
            MCPToolDefinitions._get_evaluate_documentation_tool(),
            MCPToolDefinitions._get_check_reproduction_readiness_tool(),
            MCPToolDefinitions._get_generate_evaluation_summary_tool(),
            MCPToolDefinitions._get_detect_empty_files_tool(),
            MCPToolDefinitions._get_detect_missing_files_tool(),
            MCPToolDefinitions._get_generate_code_revision_report_tool(),
        ]

    @staticmethod
    def _get_read_file_tool() -> Dict[str, Any]:
        """ËØªÂèñÊñá‰ª∂Â∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "read_file",
            "description": "Read file content, supports specifying line number range",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "File path, relative to workspace",
                    },
                    "start_line": {
                        "type": "integer",
                        "description": "Start line number (starting from 1, optional)",
                    },
                    "end_line": {
                        "type": "integer",
                        "description": "End line number (starting from 1, optional)",
                    },
                },
                "required": ["file_path"],
            },
        }

    @staticmethod
    def _get_read_multiple_files_tool() -> Dict[str, Any]:
        """ÊâπÈáèËØªÂèñÂ§ö‰∏™Êñá‰ª∂Â∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "read_multiple_files",
            "description": "Read multiple files in a single operation (for batch reading)",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_requests": {
                        "type": "string",
                        "description": 'JSON string with file requests, e.g., \'{"file1.py": {}, "file2.py": {"start_line": 1, "end_line": 10}}\' or simple array \'["file1.py", "file2.py"]\'',
                    },
                    "max_files": {
                        "type": "integer",
                        "description": "Maximum number of files to read in one operation",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 10,
                    },
                },
                "required": ["file_requests"],
            },
        }

    @staticmethod
    def _get_read_code_mem_tool() -> Dict[str, Any]:
        """Read code memory tool definition - reads from implement_code_summary.md"""
        return {
            "name": "read_code_mem",
            "description": "Check if file summaries exist in implement_code_summary.md for multiple files in a single call. Returns summaries for all requested files if available.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_paths": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "List of file paths to check for summary information in implement_code_summary.md",
                    }
                },
                "required": ["file_paths"],
            },
        }

    @staticmethod
    def _get_write_file_tool() -> Dict[str, Any]:
        """ÂÜôÂÖ•Êñá‰ª∂Â∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "write_file",
            "description": "Write content to file",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_path": {
                        "type": "string",
                        "description": "File path, relative to workspace",
                    },
                    "content": {
                        "type": "string",
                        "description": "Content to write to file",
                    },
                    "create_dirs": {
                        "type": "boolean",
                        "description": "Whether to create directories if they don't exist",
                        "default": True,
                    },
                    "create_backup": {
                        "type": "boolean",
                        "description": "Whether to create backup file if file already exists",
                        "default": False,
                    },
                },
                "required": ["file_path", "content"],
            },
        }

    @staticmethod
    def _get_write_multiple_files_tool() -> Dict[str, Any]:
        """ÊâπÈáèÂÜôÂÖ•Â§ö‰∏™Êñá‰ª∂Â∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "write_multiple_files",
            "description": "Write multiple files in a single operation (for batch implementation)",
            "input_schema": {
                "type": "object",
                "properties": {
                    "file_implementations": {
                        "type": "string",
                        "description": 'JSON string mapping file paths to content, e.g., \'{"file1.py": "content1", "file2.py": "content2"}\'',
                    },
                    "create_dirs": {
                        "type": "boolean",
                        "description": "Whether to create directories if they don't exist",
                        "default": True,
                    },
                    "create_backup": {
                        "type": "boolean",
                        "description": "Whether to create backup files if they already exist",
                        "default": False,
                    },
                    "max_files": {
                        "type": "integer",
                        "description": "Maximum number of files to write in one operation",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 10,
                    },
                },
                "required": ["file_implementations"],
            },
        }

    @staticmethod
    def _get_execute_python_tool() -> Dict[str, Any]:
        """PythonÊâßË°åÂ∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "execute_python",
            "description": "Execute Python code and return output",
            "input_schema": {
                "type": "object",
                "properties": {
                    "code": {"type": "string", "description": "Python code to execute"},
                    "timeout": {
                        "type": "integer",
                        "description": "Timeout in seconds",
                        "default": 30,
                    },
                },
                "required": ["code"],
            },
        }

    @staticmethod
    def _get_execute_bash_tool() -> Dict[str, Any]:
        """BashÊâßË°åÂ∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "execute_bash",
            "description": "Execute bash command",
            "input_schema": {
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "description": "Bash command to execute",
                    },
                    "timeout": {
                        "type": "integer",
                        "description": "Timeout in seconds",
                        "default": 30,
                    },
                },
                "required": ["command"],
            },
        }

    @staticmethod
    def _get_file_structure_tool() -> Dict[str, Any]:
        """Êñá‰ª∂ÁªìÊûÑËé∑ÂèñÂ∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "get_file_structure",
            "description": "Get directory file structure",
            "input_schema": {
                "type": "object",
                "properties": {
                    "directory": {
                        "type": "string",
                        "description": "Directory path, relative to workspace",
                        "default": ".",
                    },
                    "max_depth": {
                        "type": "integer",
                        "description": "Maximum traversal depth",
                        "default": 5,
                    },
                },
            },
        }

    @staticmethod
    def _get_search_code_references_tool() -> Dict[str, Any]:
        """Áªü‰∏Ä‰ª£Á†ÅÂèÇËÄÉÊêúÁ¥¢Â∑•ÂÖ∑ÂÆö‰πâ - ÂêàÂπ∂‰∫Ü‰∏â‰∏™Ê≠•È™§‰∏∫‰∏Ä‰∏™Â∑•ÂÖ∑"""
        return {
            "name": "search_code_references",
            "description": "UNIFIED TOOL: Search relevant reference code from index files. Combines directory setup, index loading, and searching in a single call.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "indexes_path": {
                        "type": "string",
                        "description": "Path to the indexes directory containing JSON index files",
                    },
                    "target_file": {
                        "type": "string",
                        "description": "Target file path to be implemented",
                    },
                    "keywords": {
                        "type": "string",
                        "description": "Search keywords, comma-separated",
                        "default": "",
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "Maximum number of results to return",
                        "default": 10,
                    },
                },
                "required": ["indexes_path", "target_file"],
            },
        }

    @staticmethod
    def _get_search_code_tool() -> Dict[str, Any]:
        """‰ª£Á†ÅÊêúÁ¥¢Â∑•ÂÖ∑ÂÆö‰πâ - Âú®ÂΩìÂâç‰ª£Á†ÅÂ∫ì‰∏≠ÊêúÁ¥¢Ê®°Âºè"""
        return {
            "name": "search_code",
            "description": "Search patterns in code files within the current repository",
            "input_schema": {
                "type": "object",
                "properties": {
                    "pattern": {
                        "type": "string",
                        "description": "Search pattern",
                    },
                    "file_pattern": {
                        "type": "string",
                        "description": "File pattern (e.g., '*.py')",
                        "default": "*.py",
                    },
                    "use_regex": {
                        "type": "boolean",
                        "description": "Whether to use regular expressions",
                        "default": False,
                    },
                    "search_directory": {
                        "type": "string",
                        "description": "Specify search directory (optional)",
                    },
                },
                "required": ["pattern"],
            },
        }

    @staticmethod
    def _get_operation_history_tool() -> Dict[str, Any]:
        """Êìç‰ΩúÂéÜÂè≤Â∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "get_operation_history",
            "description": "Get operation history",
            "input_schema": {
                "type": "object",
                "properties": {
                    "last_n": {
                        "type": "integer",
                        "description": "Return the last N operations",
                        "default": 10,
                    },
                },
            },
        }

    @staticmethod
    def _get_get_indexes_overview_tool() -> Dict[str, Any]:
        """Ëé∑ÂèñÁ¥¢ÂºïÊ¶ÇËßàÂ∑•ÂÖ∑ÂÆö‰πâ"""
        return {
            "name": "get_indexes_overview",
            "description": "Get overview of all available reference code index information from specified directory",
            "input_schema": {
                "type": "object",
                "properties": {
                    "indexes_path": {
                        "type": "string",
                        "description": "Path to the indexes directory containing JSON index files",
                    }
                },
                "required": ["indexes_path"],
            },
        }

    @staticmethod
    def _get_set_workspace_tool() -> Dict[str, Any]:
        """Set workspace directory tool definition"""
        return {
            "name": "set_workspace",
            "description": "Set the workspace directory for file operations",
            "input_schema": {
                "type": "object",
                "properties": {
                    "workspace_path": {
                        "type": "string",
                        "description": "Directory path for the workspace",
                    }
                },
                "required": ["workspace_path"],
            },
        }

    # @staticmethod
    # def _get_set_indexes_directory_tool() -> Dict[str, Any]:
    #     """Set indexes directory tool definition - DEPRECATED: Use unified search_code_references instead"""
    #     return {
    #         "name": "set_indexes_directory",
    #         "description": "Set the directory path for code reference indexes",
    #         "input_schema": {
    #             "type": "object",
    #             "properties": {
    #                 "indexes_path": {
    #                     "type": "string",
    #                     "description": "Directory path containing index JSON files"
    #                 }
    #             },
    #             "required": ["indexes_path"]
    #         }
    #     }

    # Code evaluation tool definitions
    @staticmethod
    def _get_analyze_repo_structure_tool() -> Dict[str, Any]:
        return {
            "name": "analyze_repo_structure",
            "description": "Perform comprehensive repository structure analysis",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository to analyze",
                    }
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_detect_dependencies_tool() -> Dict[str, Any]:
        return {
            "name": "detect_dependencies",
            "description": "Detect and analyze project dependencies across multiple languages",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository",
                    }
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_assess_code_quality_tool() -> Dict[str, Any]:
        return {
            "name": "assess_code_quality",
            "description": "Assess code quality metrics and identify potential issues",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository",
                    }
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_evaluate_documentation_tool() -> Dict[str, Any]:
        return {
            "name": "evaluate_documentation",
            "description": "Evaluate documentation completeness and quality",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository",
                    },
                    "docs_path": {
                        "type": "string",
                        "description": "Optional path to external documentation",
                    },
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_check_reproduction_readiness_tool() -> Dict[str, Any]:
        return {
            "name": "check_reproduction_readiness",
            "description": "Assess repository readiness for reproduction and validation",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository",
                    },
                    "docs_path": {
                        "type": "string",
                        "description": "Optional path to reproduction documentation",
                    },
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_generate_evaluation_summary_tool() -> Dict[str, Any]:
        return {
            "name": "generate_evaluation_summary",
            "description": "Generate comprehensive evaluation summary combining all analysis results",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository",
                    },
                    "docs_path": {
                        "type": "string",
                        "description": "Optional path to reproduction documentation",
                    },
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_detect_empty_files_tool() -> Dict[str, Any]:
        return {
            "name": "detect_empty_files",
            "description": "Detect empty files in the repository that may need implementation",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository to analyze",
                    }
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_detect_missing_files_tool() -> Dict[str, Any]:
        return {
            "name": "detect_missing_files",
            "description": "Detect missing essential files like main programs, tests, requirements, etc.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository to analyze",
                    }
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def _get_generate_code_revision_report_tool() -> Dict[str, Any]:
        return {
            "name": "generate_code_revision_report",
            "description": "Generate comprehensive code revision report combining empty files, missing files, and quality analysis",
            "input_schema": {
                "type": "object",
                "properties": {
                    "repo_path": {
                        "type": "string",
                        "description": "Path to the repository to analyze",
                    },
                    "docs_path": {
                        "type": "string",
                        "description": "Optional path to documentation",
                    },
                },
                "required": ["repo_path"],
            },
        }

    @staticmethod
    def get_available_tool_sets() -> Dict[str, str]:
        """
        Ëé∑ÂèñÂèØÁî®ÁöÑÂ∑•ÂÖ∑ÈõÜÂêà
        Get available tool sets
        """
        return {
            "code_implementation": "‰ª£Á†ÅÂÆûÁé∞Áõ∏ÂÖ≥Â∑•ÂÖ∑ÈõÜ / Code implementation tool set",
            "code_evaluation": "‰ª£Á†ÅËØÑ‰º∞Áõ∏ÂÖ≥Â∑•ÂÖ∑ÈõÜ / Code evaluation tool set",
            # ÂèØ‰ª•Âú®ËøôÈáåÊ∑ªÂä†Êõ¥Â§öÂ∑•ÂÖ∑ÈõÜ
            # "data_analysis": "Êï∞ÊçÆÂàÜÊûêÂ∑•ÂÖ∑ÈõÜ / Data analysis tool set",
            # "web_scraping": "ÁΩëÈ°µÁà¨ÂèñÂ∑•ÂÖ∑ÈõÜ / Web scraping tool set",
        }

    @staticmethod
    def get_tool_set(tool_set_name: str) -> List[Dict[str, Any]]:
        """
        Ê†πÊçÆÂêçÁß∞Ëé∑ÂèñÁâπÂÆöÁöÑÂ∑•ÂÖ∑ÈõÜ
        Get specific tool set by name
        """
        tool_sets = {
            "code_implementation": MCPToolDefinitions.get_code_implementation_tools(),
            "code_evaluation": MCPToolDefinitions.get_code_evaluation_tools(),
        }

        return tool_sets.get(tool_set_name, [])

    @staticmethod
    def get_all_tools() -> List[Dict[str, Any]]:
        """
        Ëé∑ÂèñÊâÄÊúâÂèØÁî®Â∑•ÂÖ∑
        Get all available tools
        """
        all_tools = []
        for tool_set_name in MCPToolDefinitions.get_available_tool_sets().keys():
            all_tools.extend(MCPToolDefinitions.get_tool_set(tool_set_name))
        return all_tools


# ‰æøÊç∑ËÆøÈóÆÂáΩÊï∞
def get_mcp_tools(tool_set: str = "code_implementation") -> List[Dict[str, Any]]:
    """
    ‰æøÊç∑ÂáΩÊï∞ÔºöËé∑ÂèñMCPÂ∑•ÂÖ∑ÂÆö‰πâ
    Convenience function: Get MCP tool definitions

    Args:
        tool_set: Â∑•ÂÖ∑ÈõÜÂêçÁß∞ (ÈªòËÆ§: "code_implementation")

    Returns:
        Â∑•ÂÖ∑ÂÆö‰πâÂàóË°®
    """
    return MCPToolDefinitions.get_tool_set(tool_set)


--- nanobot/README.md ---
<div align="center">
  <img src="nanobot_logo.png" alt="nanobot" width="500">
  <h1>nanobot: Ultra-Lightweight Personal AI Assistant</h1>
  <p>
    <a href="https://pypi.org/project/nanobot-ai/"><img src="https://img.shields.io/pypi/v/nanobot-ai" alt="PyPI"></a>
    <a href="https://pepy.tech/project/nanobot-ai"><img src="https://static.pepy.tech/badge/nanobot-ai" alt="Downloads"></a>
    <img src="https://img.shields.io/badge/python-‚â•3.11-blue" alt="Python">
    <img src="https://img.shields.io/badge/license-MIT-green" alt="License">
    <a href="./COMMUNICATION.md"><img src="https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=feishu&logoColor=white" alt="Feishu"></a>
    <a href="./COMMUNICATION.md"><img src="https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white" alt="WeChat"></a>
    <a href="https://discord.gg/MnCvHqpUGB"><img src="https://img.shields.io/badge/Discord-Community-5865F2?style=flat&logo=discord&logoColor=white" alt="Discord"></a>
  </p>
</div>

üêà **nanobot** is an **ultra-lightweight** personal AI assistant inspired by [Clawdbot](https://github.com/openclaw/openclaw)

‚ö°Ô∏è Delivers core agent functionality in just **~4,000** lines of code ‚Äî **99% smaller** than Clawdbot's 430k+ lines.

üìè Real-time line count: **3,510 lines** (run `bash core_agent_lines.sh` to verify anytime)

## üì¢ News

- **2026-02-09** üí¨ Added Slack, Email, and QQ support ‚Äî nanobot now supports multiple chat platforms!
- **2026-02-08** üîß Refactored Providers‚Äîadding a new LLM provider now takes just 2 simple steps! Check [here](#providers).
- **2026-02-07** üöÄ Released v0.1.3.post5 with Qwen support & several key improvements! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post5) for details.
- **2026-02-06** ‚ú® Added Moonshot/Kimi provider, Discord integration, and enhanced security hardening!
- **2026-02-05** ‚ú® Added Feishu channel, DeepSeek provider, and enhanced scheduled tasks support!
- **2026-02-04** üöÄ Released v0.1.3.post4 with multi-provider & Docker support! Check [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post4) for details.
- **2026-02-03** ‚ö° Integrated vLLM for local LLM support and improved natural language task scheduling!
- **2026-02-02** üéâ nanobot officially launched! Welcome to try üêà nanobot!

## Key Features of nanobot:

ü™∂ **Ultra-Lightweight**: Just ~4,000 lines of core agent code ‚Äî 99% smaller than Clawdbot.

üî¨ **Research-Ready**: Clean, readable code that's easy to understand, modify, and extend for research.

‚ö°Ô∏è **Lightning Fast**: Minimal footprint means faster startup, lower resource usage, and quicker iterations.

üíé **Easy-to-Use**: One-click to deploy and you're ready to go.

## üèóÔ∏è Architecture

<p align="center">
  <img src="nanobot_arch.png" alt="nanobot architecture" width="800">
</p>

## ‚ú® Features

<table align="center">
  <tr align="center">
    <th><p align="center">üìà 24/7 Real-Time Market Analysis</p></th>
    <th><p align="center">üöÄ Full-Stack Software Engineer</p></th>
    <th><p align="center">üìÖ Smart Daily Routine Manager</p></th>
    <th><p align="center">üìö Personal Knowledge Assistant</p></th>
  </tr>
  <tr>
    <td align="center"><p align="center"><img src="case/search.gif" width="180" height="400"></p></td>
    <td align="center"><p align="center"><img src="case/code.gif" width="180" height="400"></p></td>
    <td align="center"><p align="center"><img src="case/scedule.gif" width="180" height="400"></p></td>
    <td align="center"><p align="center"><img src="case/memory.gif" width="180" height="400"></p></td>
  </tr>
  <tr>
    <td align="center">Discovery ‚Ä¢ Insights ‚Ä¢ Trends</td>
    <td align="center">Develop ‚Ä¢ Deploy ‚Ä¢ Scale</td>
    <td align="center">Schedule ‚Ä¢ Automate ‚Ä¢ Organize</td>
    <td align="center">Learn ‚Ä¢ Memory ‚Ä¢ Reasoning</td>
  </tr>
</table>

## üì¶ Install

**Install from source** (latest features, recommended for development)

```bash
git clone https://github.com/HKUDS/nanobot.git
cd nanobot
pip install -e .
```

**Install with [uv](https://github.com/astral-sh/uv)** (stable, fast)

```bash
uv tool install nanobot-ai
```

**Install from PyPI** (stable)

```bash
pip install nanobot-ai
```

## üöÄ Quick Start

> [!TIP]
> Set your API key in `~/.nanobot/config.json`.
> Get API keys: [OpenRouter](https://openrouter.ai/keys) (Global) ¬∑ [DashScope](https://dashscope.console.aliyun.com) (Qwen) ¬∑ [Brave Search](https://brave.com/search/api/) (optional, for web search)

**1. Initialize**

```bash
nanobot onboard
```

**2. Configure** (`~/.nanobot/config.json`)

For OpenRouter - recommended for global users:
```json
{
  "providers": {
    "openrouter": {
      "apiKey": "sk-or-v1-xxx"
    }
  },
  "agents": {
    "defaults": {
      "model": "anthropic/claude-opus-4-5"
    }
  }
}
```

**3. Chat**

```bash
nanobot agent -m "What is 2+2?"
```

That's it! You have a working AI assistant in 2 minutes.

## üñ•Ô∏è Local Models (vLLM)

Run nanobot with your own local models using vLLM or any OpenAI-compatible server.

**1. Start your vLLM server**

```bash
vllm serve meta-llama/Llama-3.1-8B-Instruct --port 8000
```

**2. Configure** (`~/.nanobot/config.json`)

```json
{
  "providers": {
    "vllm": {
      "apiKey": "dummy",
      "apiBase": "http://localhost:8000/v1"
    }
  },
  "agents": {
    "defaults": {
      "model": "meta-llama/Llama-3.1-8B-Instruct"
    }
  }
}
```

**3. Chat**

```bash
nanobot agent -m "Hello from my local LLM!"
```

> [!TIP]
> The `apiKey` can be any non-empty string for local servers that don't require authentication.

## üí¨ Chat Apps

Talk to your nanobot through Telegram, Discord, WhatsApp, Feishu, DingTalk, Slack, Email, or QQ ‚Äî anytime, anywhere.

| Channel | Setup |
|---------|-------|
| **Telegram** | Easy (just a token) |
| **Discord** | Easy (bot token + intents) |
| **WhatsApp** | Medium (scan QR) |
| **Feishu** | Medium (app credentials) |
| **DingTalk** | Medium (app credentials) |
| **Slack** | Medium (bot + app tokens) |
| **Email** | Medium (IMAP/SMTP credentials) |
| **QQ** | Easy (app credentials) |

<details>
<summary><b>Telegram</b> (Recommended)</summary>

**1. Create a bot**
- Open Telegram, search `@BotFather`
- Send `/newbot`, follow prompts
- Copy the token

**2. Configure**

```json
{
  "channels": {
    "telegram": {
      "enabled": true,
      "token": "YOUR_BOT_TOKEN",
      "allowFrom": ["YOUR_USER_ID"]
    }
  }
}
```

> You can find your **User ID** in Telegram settings. It is shown as `@yourUserId`.
> Copy this value **without the `@` symbol** and paste it into the config file.


**3. Run**

```bash
nanobot gateway
```

</details>

<details>
<summary><b>Discord</b></summary>

**1. Create a bot**
- Go to https://discord.com/developers/applications
- Create an application ‚Üí Bot ‚Üí Add Bot
- Copy the bot token

**2. Enable intents**
- In the Bot settings, enable **MESSAGE CONTENT INTENT**
- (Optional) Enable **SERVER MEMBERS INTENT** if you plan to use allow lists based on member data

**3. Get your User ID**
- Discord Settings ‚Üí Advanced ‚Üí enable **Developer Mode**
- Right-click your avatar ‚Üí **Copy User ID**

**4. Configure**

```json
{
  "channels": {
    "discord": {
      "enabled": true,
      "token": "YOUR_BOT_TOKEN",
      "allowFrom": ["YOUR_USER_ID"]
    }
  }
}
```

**5. Invite the bot**
- OAuth2 ‚Üí URL Generator
- Scopes: `bot`
- Bot Permissions: `Send Messages`, `Read Message History`
- Open the generated invite URL and add the bot to your server

**6. Run**

```bash
nanobot gateway
```

</details>

<details>
<summary><b>WhatsApp</b></summary>

Requires **Node.js ‚â•18**.

**1. Link device**

```bash
nanobot channels login
# Scan QR with WhatsApp ‚Üí Settings ‚Üí Linked Devices
```

**2. Configure**

```json
{
  "channels": {
    "whatsapp": {
      "enabled": true,
      "allowFrom": ["+1234567890"]
    }
  }
}
```

**3. Run** (two terminals)

```bash
# Terminal 1
nanobot channels login

# Terminal 2
nanobot gateway
```

</details>

<details>
<summary><b>Feishu (È£û‰π¶)</b></summary>

Uses **WebSocket** long connection ‚Äî no public IP required.

**1. Create a Feishu bot**
- Visit [Feishu Open Platform](https://open.feishu.cn/app)
- Create a new app ‚Üí Enable **Bot** capability
- **Permissions**: Add `im:message` (send messages)
- **Events**: Add `im.message.receive_v1` (receive messages)
  - Select **Long Connection** mode (requires running nanobot first to establish connection)
- Get **App ID** and **App Secret** from "Credentials & Basic Info"
- Publish the app

**2. Configure**

```json
{
  "channels": {
    "feishu": {
      "enabled": true,
      "appId": "cli_xxx",
      "appSecret": "xxx",
      "encryptKey": "",
      "verificationToken": "",
      "allowFrom": []
    }
  }
}
```

> `encryptKey` and `verificationToken` are optional for Long Connection mode.
> `allowFrom`: Leave empty to allow all users, or add `["ou_xxx"]` to restrict access.

**3. Run**

```bash
nanobot gateway
```

> [!TIP]
> Feishu uses WebSocket to receive messages ‚Äî no webhook or public IP needed!

</details>

<details>
<summary><b>QQ (QQÁßÅËÅä)</b></summary>

Uses **botpy SDK** with WebSocket ‚Äî no public IP required.

**1. Create a QQ bot**
- Visit [QQ Open Platform](https://q.qq.com)
- Create a new bot application
- Get **AppID** and **Secret** from "Developer Settings"

**2. Configure**

```json
{
  "channels": {
    "qq": {
      "enabled": true,
      "appId": "YOUR_APP_ID",
      "secret": "YOUR_APP_SECRET",
      "allowFrom": []
    }
  }
}
```

> `allowFrom`: Leave empty for public access, or add user openids to restrict access.
> Example: `"allowFrom": ["user_openid_1", "user_openid_2"]`

**3. Run**

```bash
nanobot gateway
```

> [!TIP]
> QQ bot currently supports **private messages only**. Group chat support coming soon!

</details>

<details>
<summary><b>DingTalk (ÈíâÈíâ)</b></summary>

Uses **Stream Mode** ‚Äî no public IP required.

**1. Create a DingTalk bot**
- Visit [DingTalk Open Platform](https://open-dev.dingtalk.com/)
- Create a new app -> Add **Robot** capability
- **Configuration**:
  - Toggle **Stream Mode** ON
- **Permissions**: Add necessary permissions for sending messages
- Get **AppKey** (Client ID) and **AppSecret** (Client Secret) from "Credentials"
- Publish the app

**2. Configure**

```json
{
  "channels": {
    "dingtalk": {
      "enabled": true,
      "clientId": "YOUR_APP_KEY",
      "clientSecret": "YOUR_APP_SECRET",
      "allowFrom": []
    }
  }
}
```

> `allowFrom`: Leave empty to allow all users, or add `["staffId"]` to restrict access.

**3. Run**

```bash
nanobot gateway
```

</details>

<details>
<summary><b>Slack</b></summary>

Uses **Socket Mode** ‚Äî no public URL required.

**1. Create a Slack app**
- Go to [Slack API](https://api.slack.com/apps) ‚Üí Create New App
- **OAuth & Permissions**: Add bot scopes: `chat:write`, `reactions:write`, `app_mentions:read`
- Install to your workspace and copy the **Bot Token** (`xoxb-...`)
- **Socket Mode**: Enable it and generate an **App-Level Token** (`xapp-...`) with `connections:write` scope
- **Event Subscriptions**: Subscribe to `message.im`, `message.channels`, `app_mention`

**2. Configure**

```json
{
  "channels": {
    "slack": {
      "enabled": true,
      "botToken": "xoxb-...",
      "appToken": "xapp-...",
      "groupPolicy": "mention"
    }
  }
}
```

> `groupPolicy`: `"mention"` (respond only when @mentioned), `"open"` (respond to all messages), or `"allowlist"` (restrict to specific channels).
> DM policy defaults to open. Set `"dm": {"enabled": false}` to disable DMs.

**3. Run**

```bash
nanobot gateway
```

</details>

<details>
<summary><b>Email</b></summary>

Give nanobot its own email account. It polls **IMAP** for incoming mail and replies via **SMTP** ‚Äî like a personal email assistant.

**1. Get credentials (Gmail example)**
- Create a dedicated Gmail account for your bot (e.g. `my-nanobot@gmail.com`)
- Enable 2-Step Verification ‚Üí Create an [App Password](https://myaccount.google.com/apppasswords)
- Use this app password for both IMAP and SMTP

**2. Configure**

> - `consentGranted` must be `true` to allow mailbox access. This is a safety gate ‚Äî set `false` to fully disable.
> - `allowFrom`: Leave empty to accept emails from anyone, or restrict to specific senders.
> - `smtpUseTls` and `smtpUseSsl` default to `true` / `false` respectively, which is correct for Gmail (port 587 + STARTTLS). No need to set them explicitly.
> - Set `"autoReplyEnabled": false` if you only want to read/analyze emails without sending automatic replies.

```json
{
  "channels": {
    "email": {
      "enabled": true,
      "consentGranted": true,
      "imapHost": "imap.gmail.com",
      "imapPort": 993,
      "imapUsername": "my-nanobot@gmail.com",
      "imapPassword": "your-app-password",
      "smtpHost": "smtp.gmail.com",
      "smtpPort": 587,
      "smtpUsername": "my-nanobot@gmail.com",
      "smtpPassword": "your-app-password",
      "fromAddress": "my-nanobot@gmail.com",
      "allowFrom": ["your-real-email@gmail.com"]
    }
  }
}
```


**3. Run**

```bash
nanobot gateway
```

</details>

## ‚öôÔ∏è Configuration

Config file: `~/.nanobot/config.json`

### Providers

> [!TIP]
> - **Groq** provides free voice transcription via Whisper. If configured, Telegram voice messages will be automatically transcribed.
> - **Zhipu Coding Plan**: If you're on Zhipu's coding plan, set `"apiBase": "https://open.bigmodel.cn/api/coding/paas/v4"` in your zhipu provider config.

| Provider | Purpose | Get API Key |
|----------|---------|-------------|
| `openrouter` | LLM (recommended, access to all models) | [openrouter.ai](https://openrouter.ai) |
| `anthropic` | LLM (Claude direct) | [console.anthropic.com](https://console.anthropic.com) |
| `openai` | LLM (GPT direct) | [platform.openai.com](https://platform.openai.com) |
| `deepseek` | LLM (DeepSeek direct) | [platform.deepseek.com](https://platform.deepseek.com) |
| `groq` | LLM + **Voice transcription** (Whisper) | [console.groq.com](https://console.groq.com) |
| `gemini` | LLM (Gemini direct) | [aistudio.google.com](https://aistudio.google.com) |
| `aihubmix` | LLM (API gateway, access to all models) | [aihubmix.com](https://aihubmix.com) |
| `dashscope` | LLM (Qwen) | [dashscope.console.aliyun.com](https://dashscope.console.aliyun.com) |
| `moonshot` | LLM (Moonshot/Kimi) | [platform.moonshot.cn](https://platform.moonshot.cn) |
| `zhipu` | LLM (Zhipu GLM) | [open.bigmodel.cn](https://open.bigmodel.cn) |
| `vllm` | LLM (local, any OpenAI-compatible server) | ‚Äî |

<details>
<summary><b>Adding a New Provider (Developer Guide)</b></summary>

nanobot uses a **Provider Registry** (`nanobot/providers/registry.py`) as the single source of truth.
Adding a new provider only takes **2 steps** ‚Äî no if-elif chains to touch.

**Step 1.** Add a `ProviderSpec` entry to `PROVIDERS` in `nanobot/providers/registry.py`:

```python
ProviderSpec(
    name="myprovider",                   # config field name
    keywords=("myprovider", "mymodel"),  # model-name keywords for auto-matching
    env_key="MYPROVIDER_API_KEY",        # env var for LiteLLM
    display_name="My Provider",          # shown in `nanobot status`
    litellm_prefix="myprovider",         # auto-prefix: model ‚Üí myprovider/model
    skip_prefixes=("myprovider/",),      # don't double-prefix
)
```

**Step 2.** Add a field to `ProvidersConfig` in `nanobot/config/schema.py`:

```python
class ProvidersConfig(BaseModel):
    ...
    myprovider: ProviderConfig = ProviderConfig()
```

That's it! Environment variables, model prefixing, config matching, and `nanobot status` display will all work automatically.

**Common `ProviderSpec` options:**

| Field | Description | Example |
|-------|-------------|---------|
| `litellm_prefix` | Auto-prefix model names for LiteLLM | `"dashscope"` ‚Üí `dashscope/qwen-max` |
| `skip_prefixes` | Don't prefix if model already starts with these | `("dashscope/", "openrouter/")` |
| `env_extras` | Additional env vars to set | `(("ZHIPUAI_API_KEY", "{api_key}"),)` |
| `model_overrides` | Per-model parameter overrides | `(("kimi-k2.5", {"temperature": 1.0}),)` |
| `is_gateway` | Can route any model (like OpenRouter) | `True` |
| `detect_by_key_prefix` | Detect gateway by API key prefix | `"sk-or-"` |
| `detect_by_base_keyword` | Detect gateway by API base URL | `"openrouter"` |
| `strip_model_prefix` | Strip existing prefix before re-prefixing | `True` (for AiHubMix) |

</details>


### Security

> For production deployments, set `"restrictToWorkspace": true` in your config to sandbox the agent.

| Option | Default | Description |
|--------|---------|-------------|
| `tools.restrictToWorkspace` | `false` | When `true`, restricts **all** agent tools (shell, file read/write/edit, list) to the workspace directory. Prevents path traversal and out-of-scope access. |
| `channels.*.allowFrom` | `[]` (allow all) | Whitelist of user IDs. Empty = allow everyone; non-empty = only listed users can interact. |


## CLI Reference

| Command | Description |
|---------|-------------|
| `nanobot onboard` | Initialize config & workspace |
| `nanobot agent -m "..."` | Chat with the agent |
| `nanobot agent` | Interactive chat mode |
| `nanobot agent --no-markdown` | Show plain-text replies |
| `nanobot agent --logs` | Show runtime logs during chat |
| `nanobot gateway` | Start the gateway |
| `nanobot status` | Show status |
| `nanobot channels login` | Link WhatsApp (scan QR) |
| `nanobot channels status` | Show channel status |

Interactive mode exits: `exit`, `quit`, `/exit`, `/quit`, `:q`, or `Ctrl+D`.

<details>
<summary><b>Scheduled Tasks (Cron)</b></summary>

```bash
# Add a job
nanobot cron add --name "daily" --message "Good morning!" --cron "0 9 * * *"
nanobot cron add --name "hourly" --message "Check status" --every 3600

# List jobs
nanobot cron list

# Remove a job
nanobot cron remove <job_id>
```

</details>

## üê≥ Docker

> [!TIP]
> The `-v ~/.nanobot:/root/.nanobot` flag mounts your local config directory into the container, so your config and workspace persist across container restarts.

Build and run nanobot in a container:

```bash
# Build the image
docker build -t nanobot .

# Initialize config (first time only)
docker run -v ~/.nanobot:/root/.nanobot --rm nanobot onboard

# Edit config on host to add API keys
vim ~/.nanobot/config.json

# Run gateway (connects to Telegram/WhatsApp)
docker run -v ~/.nanobot:/root/.nanobot -p 18790:18790 nanobot gateway

# Or run a single command
docker run -v ~/.nanobot:/root/.nanobot --rm nanobot agent -m "Hello!"
docker run -v ~/.nanobot:/root/.nanobot --rm nanobot status
```

## üìÅ Project Structure

```
nanobot/
‚îú‚îÄ‚îÄ agent/          # üß† Core agent logic
‚îÇ   ‚îú‚îÄ‚îÄ loop.py     #    Agent loop (LLM ‚Üî tool execution)
‚îÇ   ‚îú‚îÄ‚îÄ context.py  #    Prompt builder
‚îÇ   ‚îú‚îÄ‚îÄ memory.py   #    Persistent memory
‚îÇ   ‚îú‚îÄ‚îÄ skills.py   #    Skills loader
‚îÇ   ‚îú‚îÄ‚îÄ subagent.py #    Background task execution
‚îÇ   ‚îî‚îÄ‚îÄ tools/      #    Built-in tools (incl. spawn)
‚îú‚îÄ‚îÄ skills/         # üéØ Bundled skills (github, weather, tmux...)
‚îú‚îÄ‚îÄ channels/       # üì± WhatsApp integration
‚îú‚îÄ‚îÄ bus/            # üöå Message routing
‚îú‚îÄ‚îÄ cron/           # ‚è∞ Scheduled tasks
‚îú‚îÄ‚îÄ heartbeat/      # üíì Proactive wake-up
‚îú‚îÄ‚îÄ providers/      # ü§ñ LLM providers (OpenRouter, etc.)
‚îú‚îÄ‚îÄ session/        # üí¨ Conversation sessions
‚îú‚îÄ‚îÄ config/         # ‚öôÔ∏è Configuration
‚îî‚îÄ‚îÄ cli/            # üñ•Ô∏è Commands
```

## ü§ù Contribute & Roadmap

PRs welcome! The codebase is intentionally small and readable. ü§ó

**Roadmap** ‚Äî Pick an item and [open a PR](https://github.com/HKUDS/nanobot/pulls)!

- [x] **Voice Transcription** ‚Äî Support for Groq Whisper (Issue #13)
- [ ] **Multi-modal** ‚Äî See and hear (images, voice, video)
- [ ] **Long-term memory** ‚Äî Never forget important context
- [ ] **Better reasoning** ‚Äî Multi-step planning and reflection
- [ ] **More integrations** ‚Äî Calendar and more
- [ ] **Self-improvement** ‚Äî Learn from feedback and mistakes

### Contributors

<a href="https://github.com/HKUDS/nanobot/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=HKUDS/nanobot&max=100&columns=12" />
</a>


## ‚≠ê Star History

<div align="center">
  <a href="https://star-history.com/#HKUDS/nanobot&Date">
    <picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/nanobot&type=Date&theme=dark" />
      <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/nanobot&type=Date" />
      <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/nanobot&type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" />
    </picture>
  </a>
</div>

<p align="center">
  <em> Thanks for visiting ‚ú® nanobot!</em><br><br>
  <img src="https://visitor-badge.laobi.icu/badge?page_id=HKUDS.nanobot&style=for-the-badge&color=00d4ff" alt="Views">
</p>


<p align="center">
  <sub>nanobot is for educational, research, and technical exchange purposes only</sub>
</p>


## Links discovered
- [Clawdbot](https://github.com/openclaw/openclaw)
- [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post5)
- [here](https://github.com/HKUDS/nanobot/releases/tag/v0.1.3.post4)
- [uv](https://github.com/astral-sh/uv)
- [OpenRouter](https://openrouter.ai/keys)
- [DashScope](https://dashscope.console.aliyun.com)
- [Brave Search](https://brave.com/search/api/)
- [Feishu Open Platform](https://open.feishu.cn/app)
- [QQ Open Platform](https://q.qq.com)
- [DingTalk Open Platform](https://open-dev.dingtalk.com/)
- [Slack API](https://api.slack.com/apps)
- [App Password](https://myaccount.google.com/apppasswords)
- [openrouter.ai](https://openrouter.ai)
- [console.anthropic.com](https://console.anthropic.com)
- [platform.openai.com](https://platform.openai.com)
- [platform.deepseek.com](https://platform.deepseek.com)
- [console.groq.com](https://console.groq.com)
- [aistudio.google.com](https://aistudio.google.com)
- [aihubmix.com](https://aihubmix.com)
- [dashscope.console.aliyun.com](https://dashscope.console.aliyun.com)
- [platform.moonshot.cn](https://platform.moonshot.cn)
- [open.bigmodel.cn](https://open.bigmodel.cn)
- [open a PR](https://github.com/HKUDS/nanobot/pulls)
- [<img src="https://img.shields.io/pypi/v/nanobot-ai" alt="PyPI">](https://pypi.org/project/nanobot-ai/)
- [<img src="https://static.pepy.tech/badge/nanobot-ai" alt="Downloads">](https://pepy.tech/project/nanobot-ai)
- [<img src="https://img.shields.io/badge/Feishu-Group-E9DBFC?style=flat&logo=feishu&logoColor=white" alt="Feishu">](https://github.com/HKUDS/DeepCode/blob/main/nanobot/COMMUNICATION.md)
- [<img src="https://img.shields.io/badge/WeChat-Group-C5EAB4?style=flat&logo=wechat&logoColor=white" alt="WeChat">](https://github.com/HKUDS/DeepCode/blob/main/nanobot/COMMUNICATION.md)
- [<img src="https://img.shields.io/badge/Discord-Community-5865F2?style=flat&logo=discord&logoColor=white" alt="Discord">](https://discord.gg/MnCvHqpUGB)
- [<img src="https://contrib.rocks/image?repo=HKUDS/nanobot&max=100&columns=12" />](https://github.com/HKUDS/nanobot/graphs/contributors)

--- nanobot/nanobot/skills/README.md ---
# nanobot Skills

This directory contains built-in skills that extend nanobot's capabilities.

## Skill Format

Each skill is a directory containing a `SKILL.md` file with:
- YAML frontmatter (name, description, metadata)
- Markdown instructions for the agent

## Attribution

These skills are adapted from [OpenClaw](https://github.com/openclaw/openclaw)'s skill system.
The skill format and metadata structure follow OpenClaw's conventions to maintain compatibility.

## Available Skills

| Skill | Description |
|-------|-------------|
| `github` | Interact with GitHub using the `gh` CLI |
| `weather` | Get weather info using wttr.in and Open-Meteo |
| `summarize` | Summarize URLs, files, and YouTube videos |
| `tmux` | Remote-control tmux sessions |
| `skill-creator` | Create new skills |


## Links discovered
- [OpenClaw](https://github.com/openclaw/openclaw)

--- nanobot/COMMUNICATION.md ---
We provide QR codes for joining the HKUDS discussion groups on **WeChat** and **Feishu**.

You can join by scanning the QR codes below:

<img src="https://github.com/HKUDS/.github/blob/main/profile/QR.png" alt="WeChat QR Code" width="400"/>


--- nanobot/workspace/AGENTS.md ---
# Agent Instructions

You are a helpful AI assistant. Be concise, accurate, and friendly.

## Guidelines

- Always explain what you're doing before taking actions
- Ask for clarification when the request is ambiguous
- Use tools to help accomplish tasks
- Remember important information in your memory files

## Tools Available

You have access to:
- File operations (read, write, edit, list)
- Shell commands (exec)
- Web access (search, fetch)
- Messaging (message)
- Background tasks (spawn)

## Memory

- Use `memory/` directory for daily notes
- Use `MEMORY.md` for long-term information

## Scheduled Reminders

When user asks for a reminder at a specific time, use `exec` to run:
```
nanobot cron add --name "reminder" --message "Your message" --at "YYYY-MM-DDTHH:MM:SS" --deliver --to "USER_ID" --channel "CHANNEL"
```
Get USER_ID and CHANNEL from the current session (e.g., `8281248569` and `telegram` from `telegram:8281248569`).

**Do NOT just write reminders to MEMORY.md** ‚Äî that won't trigger actual notifications.

## Heartbeat Tasks

`HEARTBEAT.md` is checked every 30 minutes. You can manage periodic tasks by editing this file:

- **Add a task**: Use `edit_file` to append new tasks to `HEARTBEAT.md`
- **Remove a task**: Use `edit_file` to remove completed or obsolete tasks
- **Rewrite tasks**: Use `write_file` to completely rewrite the task list

Task format examples:
```
- [ ] Check calendar and remind of upcoming events
- [ ] Scan inbox for urgent emails
- [ ] Check weather forecast for today
```

When the user asks you to add a recurring/periodic task, update `HEARTBEAT.md` instead of creating a one-time reminder. Keep the file small to minimize token usage.


--- nanobot/workspace/HEARTBEAT.md ---
# Heartbeat Tasks

This file is checked every 30 minutes by your nanobot agent.
Add tasks below that you want the agent to work on periodically.

If this file has no tasks (only headers and comments), the agent will skip the heartbeat.

## Active Tasks

<!-- Add your periodic tasks below this line -->


## Completed

<!-- Move completed tasks here or delete them -->


--- nanobot/nanobot/__init__.py ---
"""
nanobot - A lightweight AI agent framework
"""

__version__ = "0.1.0"
__logo__ = "üêà"


--- nanobot/nanobot/__main__.py ---
"""
Entry point for running nanobot as a module: python -m nanobot
"""

from nanobot.cli.commands import app

if __name__ == "__main__":
    app()


--- nanobot/workspace/SOUL.md ---
# Soul

I am nanobot üêà, a personal AI assistant.

## Personality

- Helpful and friendly
- Concise and to the point
- Curious and eager to learn

## Values

- Accuracy over speed
- User privacy and safety
- Transparency in actions

## Communication Style

- Be clear and direct
- Explain reasoning when helpful
- Ask clarifying questions when needed


--- nanobot/workspace/TOOLS.md ---
# Available Tools

This document describes the tools available to nanobot.

## File Operations

### read_file
Read the contents of a file.
```
read_file(path: str) -> str
```

### write_file
Write content to a file (creates parent directories if needed).
```
write_file(path: str, content: str) -> str
```

### edit_file
Edit a file by replacing specific text.
```
edit_file(path: str, old_text: str, new_text: str) -> str
```

### list_dir
List contents of a directory.
```
list_dir(path: str) -> str
```

## Shell Execution

### exec
Execute a shell command and return output.
```
exec(command: str, working_dir: str = None) -> str
```

**Safety Notes:**
- Commands have a configurable timeout (default 60s)
- Dangerous commands are blocked (rm -rf, format, dd, shutdown, etc.)
- Output is truncated at 10,000 characters
- Optional `restrictToWorkspace` config to limit paths

## Web Access

### web_search
Search the web using Brave Search API.
```
web_search(query: str, count: int = 5) -> str
```

Returns search results with titles, URLs, and snippets. Requires `tools.web.search.apiKey` in config.

### web_fetch
Fetch and extract main content from a URL.
```
web_fetch(url: str, extractMode: str = "markdown", maxChars: int = 50000) -> str
```

**Notes:**
- Content is extracted using readability
- Supports markdown or plain text extraction
- Output is truncated at 50,000 characters by default

## Communication

### message
Send a message to the user (used internally).
```
message(content: str, channel: str = None, chat_id: str = None) -> str
```

## Background Tasks

### spawn
Spawn a subagent to handle a task in the background.
```
spawn(task: str, label: str = None) -> str
```

Use for complex or time-consuming tasks that can run independently. The subagent will complete the task and report back when done.

## Scheduled Reminders (Cron)

Use the `exec` tool to create scheduled reminders with `nanobot cron add`:

### Set a recurring reminder
```bash
# Every day at 9am
nanobot cron add --name "morning" --message "Good morning! ‚òÄÔ∏è" --cron "0 9 * * *"

# Every 2 hours
nanobot cron add --name "water" --message "Drink water! üíß" --every 7200
```

### Set a one-time reminder
```bash
# At a specific time (ISO format)
nanobot cron add --name "meeting" --message "Meeting starts now!" --at "2025-01-31T15:00:00"
```

### Manage reminders
```bash
nanobot cron list              # List all jobs
nanobot cron remove <job_id>   # Remove a job
```

## Heartbeat Task Management

The `HEARTBEAT.md` file in the workspace is checked every 30 minutes.
Use file operations to manage periodic tasks:

### Add a heartbeat task
```python
# Append a new task
edit_file(
    path="HEARTBEAT.md",
    old_text="## Example Tasks",
    new_text="- [ ] New periodic task here\n\n## Example Tasks"
)
```

### Remove a heartbeat task
```python
# Remove a specific task
edit_file(
    path="HEARTBEAT.md",
    old_text="- [ ] Task to remove\n",
    new_text=""
)
```

### Rewrite all tasks
```python
# Replace the entire file
write_file(
    path="HEARTBEAT.md",
    content="# Heartbeat Tasks\n\n- [ ] Task 1\n- [ ] Task 2\n"
)
```

---

## Adding Custom Tools

To add custom tools:
1. Create a class that extends `Tool` in `nanobot/agent/tools/`
2. Implement `name`, `description`, `parameters`, and `execute`
3. Register it in `AgentLoop._register_default_tools()`


--- nanobot/workspace/USER.md ---
# User Profile

Information about the user to help personalize interactions.

## Basic Information

- **Name**: (your name)
- **Timezone**: (your timezone, e.g., UTC+8)
- **Language**: (preferred language)

## Preferences

### Communication Style

- [ ] Casual
- [ ] Professional
- [ ] Technical

### Response Length

- [ ] Brief and concise
- [ ] Detailed explanations
- [ ] Adaptive based on question

### Technical Level

- [ ] Beginner
- [ ] Intermediate
- [ ] Expert

## Work Context

- **Primary Role**: (your role, e.g., developer, researcher)
- **Main Projects**: (what you're working on)
- **Tools You Use**: (IDEs, languages, frameworks)

## Topics of Interest

-
-
-

## Special Instructions

(Any specific instructions for how the assistant should behave)

---

*Edit this file to customize nanobot's behavior for your needs.*


--- new_ui/README.md ---
# DeepCode New UI

Modern, intelligent UI for DeepCode - AI-powered code generation platform.

## Technology Stack

- **Backend**: FastAPI (Python)
- **Frontend**: React 18 + TypeScript + Vite
- **Styling**: Tailwind CSS + shadcn/ui
- **State Management**: Zustand
- **Real-time Communication**: WebSocket
- **Workflow Visualization**: React Flow
- **Code Display**: Monaco Editor

## Features

### Intelligent Features

1. **Real-time Streaming Output** - Watch code generation in real-time, like ChatGPT
2. **Smart Context Awareness** - Remembers conversation history, provides intelligent suggestions
3. **Adaptive Interface** - Layout adjusts based on task type
4. **Visual Workflow** - Draggable flow-chart style task visualization

### Design Style

- Clean, modern design inspired by Notion/Linear
- Light theme with blue accent colors
- Inter font for text, JetBrains Mono for code

## Project Structure

```
new_ui/
‚îú‚îÄ‚îÄ backend/                    # FastAPI Backend
‚îÇ   ‚îú‚îÄ‚îÄ main.py                # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/            # REST API endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ websockets/        # WebSocket handlers
‚îÇ   ‚îú‚îÄ‚îÄ services/              # Business logic
‚îÇ   ‚îî‚îÄ‚îÄ models/                # Pydantic models
‚îÇ
‚îú‚îÄ‚îÄ frontend/                   # React Frontend
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/        # React components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/             # Page components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/             # Custom hooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stores/            # Zustand stores
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/          # API client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types/             # TypeScript types
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.ts
‚îÇ
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ start_dev.sh           # Development startup
    ‚îî‚îÄ‚îÄ build.sh               # Production build
```

## Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- npm or yarn

### Development

1. **Start both backend and frontend:**

```bash
cd new_ui
chmod +x scripts/start_dev.sh
./scripts/start_dev.sh
```

2. **Or start separately:**

Backend:
```bash
cd new_ui/backend
pip install -r requirements.txt  # First time only
uvicorn main:app --reload --port 8000
```

Frontend:
```bash
cd new_ui/frontend
npm install  # First time only
npm run dev
```

3. **Access the application:**
   - Frontend: http://localhost:5173
   - Backend API: http://localhost:8000
   - API Documentation: http://localhost:8000/docs

### Production Build

```bash
cd new_ui
chmod +x scripts/build.sh
./scripts/build.sh
```

## API Endpoints

### REST API

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/v1/workflows/paper-to-code` | Start paper-to-code workflow |
| POST | `/api/v1/workflows/chat-planning` | Start chat-based planning |
| GET | `/api/v1/workflows/status/{task_id}` | Get workflow status |
| POST | `/api/v1/requirements/questions` | Generate guiding questions |
| POST | `/api/v1/requirements/summarize` | Summarize requirements |
| POST | `/api/v1/files/upload` | Upload file |
| GET | `/api/v1/config/settings` | Get settings |

### WebSocket Endpoints

| Endpoint | Description |
|----------|-------------|
| `/ws/workflow/{task_id}` | Real-time workflow progress |
| `/ws/code-stream/{task_id}` | Streaming code output |
| `/ws/logs/{session_id}` | Live log streaming |

## Configuration

The new UI reads configuration from the existing DeepCode config files:

- `mcp_agent.config.yaml` - LLM provider, models, MCP server settings
- `mcp_agent.secrets.yaml` - API keys

## Integration

The new UI integrates with existing DeepCode components:

- `workflows/agent_orchestration_engine.py` - Core workflow execution
- `workflows/agents/` - Specialized agents
- `utils/llm_utils.py` - LLM provider management

## Browser Support

- Chrome (recommended)
- Firefox
- Safari
- Edge

## License

MIT License - see main DeepCode license.


--- new_ui/frontend/index.html ---
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>DeepCode - AI-Powered Code Generation</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>


--- new_ui/backend/__init__.py ---
"""
DeepCode New UI Backend
FastAPI-based backend for the new DeepCode UI
"""

__version__ = "1.0.0"


--- new_ui/backend/main.py ---
"""
DeepCode New UI - FastAPI Backend Entry Point

Supports two modes:
  - Development: Frontend runs on Vite dev server (port 5173), proxied to backend
  - Production/Docker: FastAPI serves the frontend static build directly
"""

import os
import sys
from pathlib import Path

# ============================================================
# Path Setup - Critical for avoiding module naming conflicts
# ============================================================
# Directory layout:
#   PROJECT_ROOT/              <- DeepCode root (config/, utils/, workflows/, prompts/, tools/)
#   PROJECT_ROOT/new_ui/
#   PROJECT_ROOT/new_ui/backend/  <- This file's directory (api/, models/, services/, settings.py)
#
# IMPORTANT: Backend modules (settings, models, services, api) must NOT shadow
# DeepCode modules (config, utils, workflows, prompts, tools).
# We renamed: config.py -> settings.py, utils/ -> app_utils/
# ============================================================

BACKEND_DIR = Path(__file__).resolve().parent
NEW_UI_DIR = BACKEND_DIR.parent
PROJECT_ROOT = NEW_UI_DIR.parent

# PROJECT_ROOT must be first so DeepCode modules (config, utils, etc.) are found correctly
# BACKEND_DIR must also be present so local modules (settings, api, models, services) are found
# Since there are no naming conflicts after renaming, order is safe
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
if str(BACKEND_DIR) not in sys.path:
    sys.path.insert(1, str(BACKEND_DIR))

from contextlib import asynccontextmanager
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse

from settings import settings
from api.routes import workflows, requirements, config as config_routes, files
from api.websockets import workflow_ws, code_stream_ws, logs_ws

# Check if running in Docker/production mode
IS_DOCKER = os.environ.get("DEEPCODE_ENV") == "docker"
FRONTEND_DIST = NEW_UI_DIR / "frontend" / "dist"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management"""
    # Startup
    print("Starting DeepCode New UI Backend...")
    print(f"  Project root: {PROJECT_ROOT}")
    print(f"  Backend dir:  {BACKEND_DIR}")
    print(f"  Mode:         {'Docker/Production' if IS_DOCKER else 'Development'}")

    if IS_DOCKER and FRONTEND_DIST.exists():
        print(f"  Frontend:     Serving static files from {FRONTEND_DIST}")
    elif IS_DOCKER:
        print(f"  ‚ö†Ô∏è  Frontend dist not found at {FRONTEND_DIST}")

    # Ensure upload directory exists
    upload_dir = Path(settings.upload_dir)
    upload_dir.mkdir(parents=True, exist_ok=True)

    yield

    # Shutdown
    print("Shutting down DeepCode New UI Backend...")


app = FastAPI(
    title="DeepCode New UI API",
    description="Modern API backend for DeepCode - AI-powered code generation platform",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include REST API routes
app.include_router(workflows.router, prefix="/api/v1/workflows", tags=["Workflows"])
app.include_router(
    requirements.router, prefix="/api/v1/requirements", tags=["Requirements"]
)
app.include_router(
    config_routes.router, prefix="/api/v1/config", tags=["Configuration"]
)
app.include_router(files.router, prefix="/api/v1/files", tags=["Files"])

# Include WebSocket routes
app.include_router(workflow_ws.router, prefix="/ws", tags=["WebSocket"])
app.include_router(code_stream_ws.router, prefix="/ws", tags=["WebSocket"])
app.include_router(logs_ws.router, prefix="/ws", tags=["WebSocket"])


# ============================================================
# Static file serving for Docker/production mode
# In development, Vite dev server handles this via proxy
# ============================================================
if IS_DOCKER and FRONTEND_DIST.exists():
    # Serve static assets (JS, CSS, images, etc.)
    app.mount(
        "/assets",
        StaticFiles(directory=str(FRONTEND_DIST / "assets")),
        name="static-assets",
    )

    @app.get("/health")
    async def health_check():
        """Health check endpoint"""
        return {"status": "healthy"}

    # Catch-all: serve index.html for SPA client-side routing
    # This must be registered AFTER all API/WS routes
    @app.get("/{full_path:path}")
    async def serve_spa(request: Request, full_path: str):
        """Serve frontend SPA - fallback to index.html for client-side routing"""
        # Check if a static file exists at the requested path
        file_path = FRONTEND_DIST / full_path
        if full_path and file_path.exists() and file_path.is_file():
            return FileResponse(file_path)
        # Otherwise return index.html (SPA routing)
        return FileResponse(FRONTEND_DIST / "index.html")
else:
    # Development mode endpoints
    @app.get("/")
    async def root():
        """Root endpoint (dev mode)"""
        return {
            "name": "DeepCode New UI API",
            "version": "1.0.0",
            "status": "running",
            "mode": "development",
        }

    @app.get("/health")
    async def health_check_dev():
        """Health check endpoint"""
        return {"status": "healthy"}


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "main:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug,
    )


--- new_ui/frontend/postcss.config.js ---
export default {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}


--- new_ui/backend/settings.py ---
"""
Configuration management for DeepCode New UI Backend
Reads from existing mcp_agent.config.yaml and mcp_agent.secrets.yaml
"""

from pathlib import Path
from typing import Optional, Dict, Any

import yaml
from pydantic_settings import BaseSettings


# Project paths
BACKEND_DIR = Path(__file__).resolve().parent
NEW_UI_DIR = BACKEND_DIR.parent
PROJECT_ROOT = NEW_UI_DIR.parent
CONFIG_PATH = PROJECT_ROOT / "mcp_agent.config.yaml"
SECRETS_PATH = PROJECT_ROOT / "mcp_agent.secrets.yaml"


class Settings(BaseSettings):
    """Application settings"""

    # Server settings
    host: str = "0.0.0.0"
    port: int = 8000
    debug: bool = True

    # Environment: "docker" for production, anything else for development
    env: str = ""

    # CORS settings - in Docker mode, frontend is served by FastAPI (same origin)
    cors_origins: list = [
        "http://localhost:5173",
        "http://localhost:3000",
        "http://localhost:8000",
    ]

    # File upload settings
    max_upload_size: int = 100 * 1024 * 1024  # 100MB
    upload_dir: str = str(PROJECT_ROOT / "uploads")

    # Session settings
    session_timeout: int = 3600  # 1 hour

    class Config:
        env_prefix = "DEEPCODE_"


settings = Settings()


def load_mcp_config() -> Dict[str, Any]:
    """Load main MCP agent configuration"""
    if not CONFIG_PATH.exists():
        return {}

    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def load_secrets() -> Dict[str, Any]:
    """Load API secrets configuration"""
    if not SECRETS_PATH.exists():
        return {}

    with open(SECRETS_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def get_llm_provider() -> str:
    """Get the preferred LLM provider from config"""
    config = load_mcp_config()
    return config.get("llm_provider", "google")


def get_llm_models(provider: Optional[str] = None) -> Dict[str, str]:
    """Get the model configuration for a provider"""
    config = load_mcp_config()
    provider = provider or get_llm_provider()

    provider_config = config.get(provider, {})
    return {
        "default": provider_config.get("default_model", ""),
        "planning": provider_config.get("planning_model", ""),
        "implementation": provider_config.get("implementation_model", ""),
    }


def get_api_key(provider: str) -> Optional[str]:
    """Get API key for a specific provider"""
    secrets = load_secrets()
    provider_secrets = secrets.get(provider, {})
    return provider_secrets.get("api_key")


def is_indexing_enabled() -> bool:
    """Check if document indexing is enabled"""
    config = load_mcp_config()
    doc_seg = config.get("document_segmentation", {})
    return doc_seg.get("enabled", False)


--- new_ui/frontend/tailwind.config.js ---
/** @type {import('tailwindcss').Config} */
export default {
  content: [
    "./index.html",
    "./src/**/*.{js,ts,jsx,tsx}",
  ],
  theme: {
    extend: {
      colors: {
        primary: {
          50: '#eff6ff',
          100: '#dbeafe',
          200: '#bfdbfe',
          300: '#93c5fd',
          400: '#60a5fa',
          500: '#3b82f6',
          600: '#2563eb',
          700: '#1d4ed8',
          800: '#1e40af',
          900: '#1e3a8a',
        },
        gray: {
          50: '#f9fafb',
          100: '#f3f4f6',
          200: '#e5e7eb',
          300: '#d1d5db',
          400: '#9ca3af',
          500: '#6b7280',
          600: '#4b5563',
          700: '#374151',
          800: '#1f2937',
          900: '#111827',
        },
      },
      fontFamily: {
        sans: ['Inter', 'system-ui', 'sans-serif'],
        mono: ['JetBrains Mono', 'Menlo', 'Monaco', 'monospace'],
      },
      animation: {
        'pulse-slow': 'pulse 3s cubic-bezier(0.4, 0, 0.6, 1) infinite',
        'slide-in': 'slideIn 0.3s ease-out',
        'fade-in': 'fadeIn 0.2s ease-out',
      },
      keyframes: {
        slideIn: {
          '0%': { transform: 'translateX(-10px)', opacity: '0' },
          '100%': { transform: 'translateX(0)', opacity: '1' },
        },
        fadeIn: {
          '0%': { opacity: '0' },
          '100%': { opacity: '1' },
        },
      },
    },
  },
  plugins: [],
}


--- new_ui/frontend/vite.config.ts ---
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'
import path from 'path'

// https://vitejs.dev/config/
export default defineConfig({
  plugins: [react()],
  resolve: {
    alias: {
      '@': path.resolve(__dirname, './src'),
    },
  },
  server: {
    port: 5173,
    proxy: {
      '/api': {
        target: 'http://localhost:8000',
        changeOrigin: true,
      },
      '/ws': {
        target: 'ws://localhost:8000',
        ws: true,
      },
    },
  },
})


--- new_ui/backend/app_utils/__init__.py ---
"""Utils package"""


--- new_ui/backend/models/__init__.py ---
"""Models package"""

from .requests import (
    PaperToCodeRequest,
    ChatPlanningRequest,
    GenerateQuestionsRequest,
    SummarizeRequirementsRequest,
    ModifyRequirementsRequest,
    LLMProviderUpdateRequest,
    FileUploadResponse,
    InteractionResponseRequest,
)
from .responses import (
    TaskResponse,
    WorkflowStatusResponse,
    QuestionsResponse,
    RequirementsSummaryResponse,
    ConfigResponse,
    SettingsResponse,
    ErrorResponse,
)

__all__ = [
    # Requests
    "PaperToCodeRequest",
    "ChatPlanningRequest",
    "GenerateQuestionsRequest",
    "SummarizeRequirementsRequest",
    "ModifyRequirementsRequest",
    "LLMProviderUpdateRequest",
    "FileUploadResponse",
    "InteractionResponseRequest",
    # Responses
    "TaskResponse",
    "WorkflowStatusResponse",
    "QuestionsResponse",
    "RequirementsSummaryResponse",
    "ConfigResponse",
    "SettingsResponse",
    "ErrorResponse",
]


--- tools/bocha_search_server.py ---
import os
import sys
import json

import httpx
from dotenv import load_dotenv
from mcp.server.fastmcp import FastMCP

load_dotenv()


# Initialize FastMCP server
server = FastMCP(
    "bocha-search-mcp",
    prompt="""
# Bocha Search MCP Server

Bocha is a Chinese search engine for AI, This server provides tools for searching the web using Bocha Search API.
It allows you to get enhanced search details from billions of web documents, including weather, news, wikis, healthcare, train tickets, images, and more.

## Available Tools

### 1. bocha_web_search
Search with Bocha Web Search and get enhanced search details from billions of web documents, including page titles, urls, summaries, site names, site icons, publication dates, image links, and more.

### 2. bocha_ai_search
Search with Bocha AI Search, recognizes the semantics of search terms and additionally returns structured modal cards with content from vertical domains.

## Output Format

All search results will be formatted as text with clear sections for each
result item, including:

- Bocha Web search: Title, URL, Description, Published date and Site name
- Bocha AI search: Title, URL, Description, Published date, Site name, and structured data card

If the API key is missing or invalid, appropriate error messages will be returned.
""",
)


@server.tool()
async def bocha_web_search(
    query: str, freshness: str = "noLimit", count: int = 10
) -> str:
    """Search with Bocha Web Search and get enhanced search details from billions of web documents,
    including page titles, urls, summaries, site names, site icons, publication dates, image links, and more.

    Args:
        query: Search query (required)
        freshness: The time range for the search results. (Available options YYYY-MM-DD, YYYY-MM-DD..YYYY-MM-DD, noLimit, oneYear, oneMonth, oneWeek, oneDay. Default is noLimit)
        count: Number of results (1-50, default 10)
    """
    # Get API key from environment
    boch_api_key = os.environ.get("BOCHA_API_KEY", "")

    if not boch_api_key:
        return (
            "Error: Bocha API key is not configured. Please set the "
            "BOCHA_API_KEY environment variable."
        )

    # Endpoint
    endpoint = "https://api.bochaai.com/v1/web-search?utm_source=bocha-mcp-local"

    try:
        payload = {
            "query": query,
            "summary": True,
            "freshness": freshness,
            "count": count,
        }

        headers = {
            "Authorization": f"Bearer {boch_api_key}",
            "Content-Type": "application/json",
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                endpoint, headers=headers, json=payload, timeout=10.0
            )

            response.raise_for_status()
            resp = response.json()
            if "data" not in resp:
                return "Search error."

            data = resp["data"]

            if "webPages" not in data:
                return "No results found."

            results = []
            for result in data["webPages"]["value"]:
                results.append(
                    f"Title: {result['name']}\n"
                    f"URL: {result['url']}\n"
                    f"Description: {result['summary']}\n"
                    f"Published date: {result['datePublished']}\n"
                    f"Site name: {result['siteName']}"
                )

            return "\n\n".join(results)

    except httpx.HTTPStatusError as e:
        return f"Bocha Web Search API HTTP error occurred: {e.response.status_code} - {e.response.text}"
    except httpx.RequestError as e:
        return f"Error communicating with Bocha Web Search API: {str(e)}"
    except Exception as e:
        return f"Unexpected error: {str(e)}"


@server.tool()
async def bocha_ai_search(
    query: str, freshness: str = "noLimit", count: int = 10
) -> str:
    """Search with Bocha AI Search, recognizes the semantics of search terms
    and additionally returns structured modal cards with content from vertical domains.

    Args:
        query: Search query (required)
        freshness: The time range for the search results. (Available options noLimit, oneYear, oneMonth, oneWeek, oneDay. Default is noLimit)
        count: Number of results (1-50, default 10)
    """
    # Get API key from environment
    boch_api_key = os.environ.get("BOCHA_API_KEY", "")

    if not boch_api_key:
        return (
            "Error: Bocha API key is not configured. Please set the "
            "BOCHA_API_KEY environment variable."
        )

    # Endpoint
    endpoint = "https://api.bochaai.com/v1/ai-search?utm_source=bocha-mcp-local"

    try:
        payload = {
            "query": query,
            "freshness": freshness,
            "count": count,
            "answer": False,
            "stream": False,
        }

        headers = {
            "Authorization": f"Bearer {boch_api_key}",
            "Content-Type": "application/json",
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                endpoint, headers=headers, json=payload, timeout=10.0
            )

            response.raise_for_status()
            response = response.json()
            results = []
            if "messages" in response:
                for message in response["messages"]:
                    content = {}
                    try:
                        content = json.loads(message["content"])
                    except (json.JSONDecodeError, TypeError):
                        content = {}

                    # ÁΩëÈ°µ
                    if message["content_type"] == "webpage":
                        if "value" in content:
                            for item in content["value"]:
                                results.append(
                                    f"Title: {item['name']}\n"
                                    f"URL: {item['url']}\n"
                                    f"Description: {item['summary']}\n"
                                    f"Published date: {item['datePublished']}\n"
                                    f"Site name: {item['siteName']}"
                                )
                    elif (
                        message["content_type"] != "image"
                        and message["content"] != "{}"
                    ):
                        results.append(message["content"])

            if not results:
                return "No results found."

            return "\n\n".join(results)

    except httpx.HTTPStatusError as e:
        return f"Bocha AI Search API HTTP error occurred: {e.response.status_code} - {e.response.text}"
    except httpx.RequestError as e:
        return f"Error communicating with Bocha AI Search API: {str(e)}"
    except Exception as e:
        return f"Unexpected error: {str(e)}"


def main():
    """Initialize and run the MCP server."""

    # Check for required environment variables
    if "BOCHA_API_KEY" not in os.environ:
        print(
            "Error: BOCHA_API_KEY environment variable is required",
            file=sys.stderr,
        )
        print(
            "Get a Bocha API key from: " "https://open.bochaai.com",
            file=sys.stderr,
        )
        sys.exit(1)

    print("Starting Bocha Search MCP server...", file=sys.stderr)

    server.run(transport="stdio")


if __name__ == "__main__":
    main()


--- tools/code_implementation_server.py ---
#!/usr/bin/env python3
"""
Code Implementation MCP Server

This MCP server provides core functions needed for paper code reproduction:
1. File read/write operations
2. Code execution and testing
3. Code search and analysis
4. Iterative improvement support

Usage:
python tools/code_implementation_server.py
"""

import os
import subprocess
import json
import sys
import io
from pathlib import Path
import re
from typing import Dict, Any, List
import tempfile
import shutil
import logging
from datetime import datetime

# Set standard output encoding to UTF-8
if sys.stdout.encoding != "utf-8":
    try:
        if hasattr(sys.stdout, "reconfigure"):
            sys.stdout.reconfigure(encoding="utf-8")
            sys.stderr.reconfigure(encoding="utf-8")
        else:
            sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding="utf-8")
            sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding="utf-8")
    except Exception as e:
        print(f"Warning: Could not set UTF-8 encoding: {e}")

# Import MCP related modules
from mcp.server.fastmcp import FastMCP

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create FastMCP server instance
mcp = FastMCP("code-implementation-server")

# Global variables: workspace directory and operation history
WORKSPACE_DIR = None
OPERATION_HISTORY = []
CURRENT_FILES = {}


def initialize_workspace(workspace_dir: str = None):
    """
    Initialize workspace

    By default, the workspace will be set by the workflow via the set_workspace tool to:
    {plan_file_parent}/generate_code

    Args:
        workspace_dir: Optional workspace directory path
    """
    global WORKSPACE_DIR
    if workspace_dir is None:
        # Default to generate_code directory under current directory, but don't create immediately
        # This default value will be overridden by workflow via set_workspace tool
        WORKSPACE_DIR = Path.cwd() / "generate_code"
        # logger.info(f"Workspace initialized (default value, will be overridden by workflow): {WORKSPACE_DIR}")
        # logger.info("Note: Actual workspace will be set by workflow via set_workspace tool to {plan_file_parent}/generate_code")
    else:
        WORKSPACE_DIR = Path(workspace_dir).resolve()
        # Only create when explicitly specified
        WORKSPACE_DIR.mkdir(parents=True, exist_ok=True)
        logger.info(f"Workspace initialized: {WORKSPACE_DIR}")


def ensure_workspace_exists():
    """Ensure workspace directory exists"""
    global WORKSPACE_DIR
    if WORKSPACE_DIR is None:
        initialize_workspace()

    # Create workspace directory (if it doesn't exist)
    if not WORKSPACE_DIR.exists():
        WORKSPACE_DIR.mkdir(parents=True, exist_ok=True)
        logger.info(f"Workspace directory created: {WORKSPACE_DIR}")


def validate_path(path: str) -> Path:
    """Validate if path is within workspace"""
    if WORKSPACE_DIR is None:
        initialize_workspace()

    full_path = (WORKSPACE_DIR / path).resolve()
    if not str(full_path).startswith(str(WORKSPACE_DIR)):
        raise ValueError(f"Path {path} is outside workspace scope")
    return full_path


def log_operation(action: str, details: Dict[str, Any]):
    """Log operation history"""
    OPERATION_HISTORY.append(
        {"timestamp": datetime.now().isoformat(), "action": action, "details": details}
    )


# ==================== File Operation Tools ====================


@mcp.tool()
async def read_file(
    file_path: str, start_line: int = None, end_line: int = None
) -> str:
    """
    Read file content, supports specifying line number range

    Args:
        file_path: File path, relative to workspace
        start_line: Starting line number (1-based, optional)
        end_line: Ending line number (1-based, optional)

    Returns:
        JSON string of file content or error message
    """
    try:
        full_path = validate_path(file_path)

        if not full_path.exists():
            result = {"status": "error", "message": f"File does not exist: {file_path}"}
            log_operation(
                "read_file_error", {"file_path": file_path, "error": "file_not_found"}
            )
            return json.dumps(result, ensure_ascii=False, indent=2)

        with open(full_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        # Â§ÑÁêÜË°åÂè∑ËåÉÂõ¥
        if start_line is not None or end_line is not None:
            start_idx = (start_line - 1) if start_line else 0
            end_idx = end_line if end_line else len(lines)
            lines = lines[start_idx:end_idx]

        content = "".join(lines)

        result = {
            "status": "success",
            "content": content,
            "file_path": file_path,
            "total_lines": len(lines),
            "size_bytes": len(content.encode("utf-8")),
        }

        log_operation(
            "read_file",
            {
                "file_path": file_path,
                "start_line": start_line,
                "end_line": end_line,
                "lines_read": len(lines),
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to read file: {str(e)}",
            "file_path": file_path,
        }
        log_operation("read_file_error", {"file_path": file_path, "error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def read_multiple_files(file_requests: str, max_files: int = 5) -> str:
    """
    Read multiple files in a single operation (for batch reading)

    Args:
        file_requests: JSON string with file requests, e.g.,
                      '{"file1.py": {}, "file2.py": {"start_line": 1, "end_line": 10}}'
                      or simple array: '["file1.py", "file2.py"]'
        max_files: Maximum number of files to read in one operation (default: 5)

    Returns:
        JSON string of operation results for all files
    """
    try:
        # Parse the file requests
        try:
            requests_data = json.loads(file_requests)
        except json.JSONDecodeError as e:
            return json.dumps(
                {
                    "status": "error",
                    "message": f"Invalid JSON format for file_requests: {str(e)}",
                    "operation_type": "multi_file",
                    "timestamp": datetime.now().isoformat(),
                },
                ensure_ascii=False,
                indent=2,
            )

        # Normalize requests format
        if isinstance(requests_data, list):
            # Convert simple array to dict format
            normalized_requests = {file_path: {} for file_path in requests_data}
        elif isinstance(requests_data, dict):
            normalized_requests = requests_data
        else:
            return json.dumps(
                {
                    "status": "error",
                    "message": "file_requests must be a JSON object or array",
                    "operation_type": "multi_file",
                    "timestamp": datetime.now().isoformat(),
                },
                ensure_ascii=False,
                indent=2,
            )

        # Validate input
        if len(normalized_requests) == 0:
            return json.dumps(
                {
                    "status": "error",
                    "message": "No files provided for reading",
                    "operation_type": "multi_file",
                    "timestamp": datetime.now().isoformat(),
                },
                ensure_ascii=False,
                indent=2,
            )

        if len(normalized_requests) > max_files:
            return json.dumps(
                {
                    "status": "error",
                    "message": f"Too many files provided ({len(normalized_requests)}), maximum is {max_files}",
                    "operation_type": "multi_file",
                    "timestamp": datetime.now().isoformat(),
                },
                ensure_ascii=False,
                indent=2,
            )

        # Process each file
        results = {
            "status": "success",
            "message": f"Successfully processed {len(normalized_requests)} files",
            "operation_type": "multi_file",
            "timestamp": datetime.now().isoformat(),
            "files_processed": len(normalized_requests),
            "files": {},
            "summary": {
                "successful": 0,
                "failed": 0,
                "total_size_bytes": 0,
                "total_lines": 0,
                "files_not_found": 0,
            },
        }

        # Process each file individually
        for file_path, options in normalized_requests.items():
            try:
                full_path = validate_path(file_path)
                start_line = options.get("start_line")
                end_line = options.get("end_line")

                if not full_path.exists():
                    results["files"][file_path] = {
                        "status": "error",
                        "message": f"File does not exist: {file_path}",
                        "file_path": file_path,
                        "content": "",
                        "total_lines": 0,
                        "size_bytes": 0,
                        "start_line": start_line,
                        "end_line": end_line,
                    }
                    results["summary"]["failed"] += 1
                    results["summary"]["files_not_found"] += 1
                    continue

                with open(full_path, "r", encoding="utf-8") as f:
                    lines = f.readlines()

                # Handle line range
                original_line_count = len(lines)
                if start_line is not None or end_line is not None:
                    start_idx = (start_line - 1) if start_line else 0
                    end_idx = end_line if end_line else len(lines)
                    lines = lines[start_idx:end_idx]

                content = "".join(lines)
                size_bytes = len(content.encode("utf-8"))
                lines_count = len(lines)

                # Record individual file result
                results["files"][file_path] = {
                    "status": "success",
                    "message": f"File read successfully: {file_path}",
                    "file_path": file_path,
                    "content": content,
                    "total_lines": lines_count,
                    "original_total_lines": original_line_count,
                    "size_bytes": size_bytes,
                    "start_line": start_line,
                    "end_line": end_line,
                    "line_range_applied": start_line is not None
                    or end_line is not None,
                }

                # Update summary
                results["summary"]["successful"] += 1
                results["summary"]["total_size_bytes"] += size_bytes
                results["summary"]["total_lines"] += lines_count

                # Log individual file operation
                log_operation(
                    "read_file_multi",
                    {
                        "file_path": file_path,
                        "start_line": start_line,
                        "end_line": end_line,
                        "lines_read": lines_count,
                        "size_bytes": size_bytes,
                        "batch_operation": True,
                    },
                )

            except Exception as file_error:
                # Record individual file error
                results["files"][file_path] = {
                    "status": "error",
                    "message": f"Failed to read file: {str(file_error)}",
                    "file_path": file_path,
                    "content": "",
                    "total_lines": 0,
                    "size_bytes": 0,
                    "start_line": options.get("start_line"),
                    "end_line": options.get("end_line"),
                }

                results["summary"]["failed"] += 1

                # Log individual file error
                log_operation(
                    "read_file_multi_error",
                    {
                        "file_path": file_path,
                        "error": str(file_error),
                        "batch_operation": True,
                    },
                )

        # Determine overall status
        if results["summary"]["failed"] > 0:
            if results["summary"]["successful"] > 0:
                results["status"] = "partial_success"
                results["message"] = (
                    f"Read {results['summary']['successful']} files successfully, {results['summary']['failed']} failed"
                )
            else:
                results["status"] = "failed"
                results["message"] = (
                    f"All {results['summary']['failed']} files failed to read"
                )

        # Log overall operation
        log_operation(
            "read_multiple_files",
            {
                "files_count": len(normalized_requests),
                "successful": results["summary"]["successful"],
                "failed": results["summary"]["failed"],
                "total_size_bytes": results["summary"]["total_size_bytes"],
                "status": results["status"],
            },
        )

        return json.dumps(results, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to read multiple files: {str(e)}",
            "operation_type": "multi_file",
            "timestamp": datetime.now().isoformat(),
            "files_processed": 0,
        }
        log_operation("read_multiple_files_error", {"error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def write_file(
    file_path: str, content: str, create_dirs: bool = True, create_backup: bool = False
) -> str:
    """
    Write content to file

    Args:
        file_path: File path, relative to workspace
        content: Content to write to file
        create_dirs: Whether to create directories if they don't exist
        create_backup: Whether to create backup file if file already exists

    Returns:
        JSON string of operation result
    """
    try:
        full_path = validate_path(file_path)

        # Create directories (if needed)
        if create_dirs:
            full_path.parent.mkdir(parents=True, exist_ok=True)

        # Backup existing file (only when explicitly requested)
        backup_created = False
        if full_path.exists() and create_backup:
            backup_path = full_path.with_suffix(full_path.suffix + ".backup")
            shutil.copy2(full_path, backup_path)
            backup_created = True

        # Write file
        with open(full_path, "w", encoding="utf-8") as f:
            f.write(content)

        # Update current file record
        CURRENT_FILES[file_path] = {
            "last_modified": datetime.now().isoformat(),
            "size_bytes": len(content.encode("utf-8")),
            "lines": len(content.split("\n")),
        }

        result = {
            "status": "success",
            "message": f"File written successfully: {file_path}",
            "file_path": file_path,
            "size_bytes": len(content.encode("utf-8")),
            "lines_written": len(content.split("\n")),
            "backup_created": backup_created,
        }

        log_operation(
            "write_file",
            {
                "file_path": file_path,
                "size_bytes": len(content.encode("utf-8")),
                "lines": len(content.split("\n")),
                "backup_created": backup_created,
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to write file: {str(e)}",
            "file_path": file_path,
        }
        log_operation("write_file_error", {"file_path": file_path, "error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def write_multiple_files(
    file_implementations: str,
    create_dirs: bool = True,
    create_backup: bool = False,
    max_files: int = 5,
) -> str:
    """
    Write multiple files in a single operation (for batch implementation)

    Args:
        file_implementations: JSON string mapping file paths to content, e.g.,
                            '{"file1.py": "content1", "file2.py": "content2"}'
        create_dirs: Whether to create directories if they don't exist
        create_backup: Whether to create backup files if they already exist
        max_files: Maximum number of files to write in one operation (default: 5)

    Returns:
        JSON string of operation results for all files
    """
    try:
        # Parse the file implementations
        try:
            files_dict = json.loads(file_implementations)
        except json.JSONDecodeError as e:
            return json.dumps(
                {
                    "status": "error",
                    "message": f"Invalid JSON format for file_implementations: {str(e)}",
                    "operation_type": "multi_file",
                    "timestamp": datetime.now().isoformat(),
                },
                ensure_ascii=False,
                indent=2,
            )

        # Validate input
        if not isinstance(files_dict, dict):
            return json.dumps(
                {
                    "status": "error",
                    "message": "file_implementations must be a JSON object mapping file paths to content",
                    "operation_type": "multi_file",
                    "timestamp": datetime.now().isoformat(),
                },
                ensure_ascii=False,
                indent=2,
            )

        if len(files_dict) == 0:
            return json.dumps(
                {
                    "status": "error",
                    "message": "No files provided for writing",
                    "operation_type": "multi_file",
                    "timestamp": datetime.now().isoformat(),
                },
                ensure_ascii=False,
                indent=2,
            )

        if len(files_dict) > max_files:
            return json.dumps(
                {
                    "status": "error",
                    "message": f"Too many files provided ({len(files_dict)}), maximum is {max_files}",
                    "operation_type": "multi_file",
                    "timestamp": datetime.now().isoformat(),
                },
                ensure_ascii=False,
                indent=2,
            )

        # Process each file
        results = {
            "status": "success",
            "message": f"Successfully processed {len(files_dict)} files",
            "operation_type": "multi_file",
            "timestamp": datetime.now().isoformat(),
            "files_processed": len(files_dict),
            "files": {},
            "summary": {
                "successful": 0,
                "failed": 0,
                "total_size_bytes": 0,
                "total_lines": 0,
                "backups_created": 0,
            },
        }

        # Process each file individually
        for file_path, content in files_dict.items():
            try:
                full_path = validate_path(file_path)

                # Create directories (if needed)
                if create_dirs:
                    full_path.parent.mkdir(parents=True, exist_ok=True)

                # Backup existing file (only when explicitly requested)
                backup_created = False
                if full_path.exists() and create_backup:
                    backup_path = full_path.with_suffix(full_path.suffix + ".backup")
                    shutil.copy2(full_path, backup_path)
                    backup_created = True
                    results["summary"]["backups_created"] += 1

                # Write file
                with open(full_path, "w", encoding="utf-8") as f:
                    f.write(content)

                # Calculate file metrics
                size_bytes = len(content.encode("utf-8"))
                lines_count = len(content.split("\n"))

                # Update current file record
                CURRENT_FILES[file_path] = {
                    "last_modified": datetime.now().isoformat(),
                    "size_bytes": size_bytes,
                    "lines": lines_count,
                }

                # Record individual file result
                results["files"][file_path] = {
                    "status": "success",
                    "message": f"File written successfully: {file_path}",
                    "size_bytes": size_bytes,
                    "lines_written": lines_count,
                    "backup_created": backup_created,
                }

                # Update summary
                results["summary"]["successful"] += 1
                results["summary"]["total_size_bytes"] += size_bytes
                results["summary"]["total_lines"] += lines_count

                # Log individual file operation
                log_operation(
                    "write_file_multi",
                    {
                        "file_path": file_path,
                        "size_bytes": size_bytes,
                        "lines": lines_count,
                        "backup_created": backup_created,
                        "batch_operation": True,
                    },
                )

            except Exception as file_error:
                # Record individual file error
                results["files"][file_path] = {
                    "status": "error",
                    "message": f"Failed to write file: {str(file_error)}",
                    "size_bytes": 0,
                    "lines_written": 0,
                    "backup_created": False,
                }

                results["summary"]["failed"] += 1

                # Log individual file error
                log_operation(
                    "write_file_multi_error",
                    {
                        "file_path": file_path,
                        "error": str(file_error),
                        "batch_operation": True,
                    },
                )

        # Determine overall status
        if results["summary"]["failed"] > 0:
            if results["summary"]["successful"] > 0:
                results["status"] = "partial_success"
                results["message"] = (
                    f"Processed {results['summary']['successful']} files successfully, {results['summary']['failed']} failed"
                )
            else:
                results["status"] = "failed"
                results["message"] = (
                    f"All {results['summary']['failed']} files failed to write"
                )

        # Log overall operation
        log_operation(
            "write_multiple_files",
            {
                "files_count": len(files_dict),
                "successful": results["summary"]["successful"],
                "failed": results["summary"]["failed"],
                "total_size_bytes": results["summary"]["total_size_bytes"],
                "status": results["status"],
            },
        )

        return json.dumps(results, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to write multiple files: {str(e)}",
            "operation_type": "multi_file",
            "timestamp": datetime.now().isoformat(),
            "files_processed": 0,
        }
        log_operation("write_multiple_files_error", {"error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


# ==================== Code Execution Tools ====================


@mcp.tool()
async def execute_python(code: str, timeout: int = 30) -> str:
    """
    Execute Python code and return output

    Args:
        code: Python code to execute
        timeout: Timeout in seconds

    Returns:
        JSON string of execution result
    """
    try:
        # Create temporary file
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".py", delete=False, encoding="utf-8"
        ) as f:
            f.write(code)
            temp_file = f.name

        try:
            # Ensure workspace directory exists
            ensure_workspace_exists()

            # Execute Python code
            result = subprocess.run(
                [sys.executable, temp_file],
                cwd=WORKSPACE_DIR,
                capture_output=True,
                text=True,
                timeout=timeout,
                encoding="utf-8",
            )

            execution_result = {
                "status": "success" if result.returncode == 0 else "error",
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "timeout": timeout,
            }

            if result.returncode != 0:
                execution_result["message"] = "Python code execution failed"
            else:
                execution_result["message"] = "Python code execution successful"

            log_operation(
                "execute_python",
                {
                    "return_code": result.returncode,
                    "stdout_length": len(result.stdout),
                    "stderr_length": len(result.stderr),
                },
            )

            return json.dumps(execution_result, ensure_ascii=False, indent=2)

        finally:
            # Clean up temporary file
            os.unlink(temp_file)

    except subprocess.TimeoutExpired:
        result = {
            "status": "error",
            "message": f"Python code execution timeout ({timeout}Áßí)",
            "timeout": timeout,
        }
        log_operation("execute_python_timeout", {"timeout": timeout})
        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Python code execution failed: {str(e)}",
        }
        log_operation("execute_python_error", {"error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def execute_bash(command: str, timeout: int = 30) -> str:
    """
    Execute bash command

    Args:
        command: Bash command to execute
        timeout: Timeout in seconds

    Returns:
        JSON string of execution result
    """
    try:
        # ÂÆâÂÖ®Ê£ÄÊü•ÔºöÁ¶ÅÊ≠¢Âç±Èô©ÂëΩ‰ª§
        dangerous_commands = ["rm -rf", "sudo", "chmod 777", "mkfs", "dd if="]
        if any(dangerous in command.lower() for dangerous in dangerous_commands):
            result = {
                "status": "error",
                "message": f"Dangerous command execution prohibited: {command}",
            }
            log_operation(
                "execute_bash_blocked",
                {"command": command, "reason": "dangerous_command"},
            )
            return json.dumps(result, ensure_ascii=False, indent=2)

        # Ensure workspace directory exists
        ensure_workspace_exists()

        # Execute command
        result = subprocess.run(
            command,
            shell=True,
            cwd=WORKSPACE_DIR,
            capture_output=True,
            text=True,
            timeout=timeout,
            encoding="utf-8",
        )

        execution_result = {
            "status": "success" if result.returncode == 0 else "error",
            "return_code": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "command": command,
            "timeout": timeout,
        }

        if result.returncode != 0:
            execution_result["message"] = "Bash command execution failed"
        else:
            execution_result["message"] = "Bash command execution successful"

        log_operation(
            "execute_bash",
            {
                "command": command,
                "return_code": result.returncode,
                "stdout_length": len(result.stdout),
                "stderr_length": len(result.stderr),
            },
        )

        return json.dumps(execution_result, ensure_ascii=False, indent=2)

    except subprocess.TimeoutExpired:
        result = {
            "status": "error",
            "message": f"Bash command execution timeout ({timeout} seconds)",
            "command": command,
            "timeout": timeout,
        }
        log_operation("execute_bash_timeout", {"command": command, "timeout": timeout})
        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to execute bash command: {str(e)}",
            "command": command,
        }
        log_operation("execute_bash_error", {"command": command, "error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def read_code_mem(file_paths: List[str]) -> str:
    """
    Check if file summaries exist in implement_code_summary.md for multiple files

    Args:
        file_paths: List of file paths to check for summary information in implement_code_summary.md

    Returns:
        Summary information for all requested files if available
    """
    try:
        if not file_paths or not isinstance(file_paths, list):
            result = {
                "status": "error",
                "message": "file_paths parameter is required and must be a list",
            }
            log_operation(
                "read_code_mem_error", {"error": "missing_or_invalid_file_paths"}
            )
            return json.dumps(result, ensure_ascii=False, indent=2)

        # Remove duplicates while preserving order
        unique_file_paths = list(dict.fromkeys(file_paths))

        # Ensure workspace exists
        ensure_workspace_exists()

        # Look for implement_code_summary.md in the workspace
        current_path = Path(WORKSPACE_DIR)
        summary_file_path = current_path.parent / "implement_code_summary.md"

        if not summary_file_path.exists():
            result = {
                "status": "no_summary",
                "file_paths": unique_file_paths,
                "message": "No summary file found.",
                "results": [],
            }
            log_operation(
                "read_code_mem",
                {"file_paths": unique_file_paths, "status": "no_summary_file"},
            )
            return json.dumps(result, ensure_ascii=False, indent=2)

        # Read the summary file
        with open(summary_file_path, "r", encoding="utf-8") as f:
            summary_content = f.read()

        if not summary_content.strip():
            result = {
                "status": "no_summary",
                "file_paths": unique_file_paths,
                "message": "Summary file is empty.",
                "results": [],
            }
            log_operation(
                "read_code_mem",
                {"file_paths": unique_file_paths, "status": "empty_summary"},
            )
            return json.dumps(result, ensure_ascii=False, indent=2)

        # Process each file path and collect results
        results = []
        summaries_found = 0

        for file_path in unique_file_paths:
            # Extract file-specific section from summary
            file_section = _extract_file_section_from_summary(
                summary_content, file_path
            )

            if file_section:
                file_result = {
                    "file_path": file_path,
                    "status": "summary_found",
                    "summary_content": file_section,
                    "message": f"Summary information found for {file_path}",
                }
                summaries_found += 1
            else:
                file_result = {
                    "file_path": file_path,
                    "status": "no_summary",
                    "summary_content": None,
                    "message": f"No summary found for {file_path}",
                }

            results.append(file_result)

        # Determine overall status
        if summaries_found == len(unique_file_paths):
            overall_status = "all_summaries_found"
        elif summaries_found > 0:
            overall_status = "partial_summaries_found"
        else:
            overall_status = "no_summaries_found"

        result = {
            "status": overall_status,
            "file_paths": unique_file_paths,
            "total_requested": len(unique_file_paths),
            "summaries_found": summaries_found,
            "message": f"Found summaries for {summaries_found}/{len(unique_file_paths)} files",
            "results": results,
        }

        log_operation(
            "read_code_mem",
            {
                "file_paths": unique_file_paths,
                "status": overall_status,
                "total_requested": len(unique_file_paths),
                "summaries_found": summaries_found,
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to check code memory: {str(e)}",
            "file_paths": file_paths
            if isinstance(file_paths, list)
            else [str(file_paths)],
            "results": [],
        }
        log_operation(
            "read_code_mem_error", {"file_paths": file_paths, "error": str(e)}
        )
        return json.dumps(result, ensure_ascii=False, indent=2)


def _extract_file_section_from_summary(
    summary_content: str, target_file_path: str
) -> str:
    """
    Extract the specific section for a file from the summary content

    Args:
        summary_content: Full summary content
        target_file_path: Path of the target file

    Returns:
        File-specific section or None if not found
    """
    import re

    # Normalize the target path for comparison
    normalized_target = _normalize_file_path(target_file_path)

    # Pattern to match implementation sections with separator lines
    section_pattern = r"={80}\s*\n## IMPLEMENTATION File ([^;]+); ROUND \d+\s*\n={80}(.*?)(?=\n={80}|\Z)"

    matches = re.findall(section_pattern, summary_content, re.DOTALL)

    for file_path_in_summary, section_content in matches:
        file_path_in_summary = file_path_in_summary.strip()
        section_content = section_content.strip()

        # Normalize the path from summary for comparison
        normalized_summary_path = _normalize_file_path(file_path_in_summary)

        # Check if paths match using multiple strategies
        if _paths_match(
            normalized_target,
            normalized_summary_path,
            target_file_path,
            file_path_in_summary,
        ):
            # Return the complete section with proper formatting
            file_section = f"""================================================================================
## IMPLEMENTATION File {file_path_in_summary}; ROUND [X]
================================================================================

{section_content}

---
*Extracted from implement_code_summary.md*"""
            return file_section

    # If no section-based match, try alternative parsing method
    return _extract_file_section_alternative(summary_content, target_file_path)


def _normalize_file_path(file_path: str) -> str:
    """Normalize file path for comparison"""
    # Remove leading/trailing slashes and convert to lowercase
    normalized = file_path.strip("/").lower()
    # Replace backslashes with forward slashes
    normalized = normalized.replace("\\", "/")

    # Remove common prefixes to make matching more flexible
    common_prefixes = ["src/", "./src/", "./", "core/", "lib/", "main/"]
    for prefix in common_prefixes:
        if normalized.startswith(prefix):
            normalized = normalized[len(prefix) :]
            break

    return normalized


def _paths_match(
    normalized_target: str,
    normalized_summary: str,
    original_target: str,
    original_summary: str,
) -> bool:
    """Check if two file paths match using multiple strategies"""

    # Strategy 1: Exact normalized match
    if normalized_target == normalized_summary:
        return True

    # Strategy 2: Basename match (filename only)
    target_basename = os.path.basename(original_target)
    summary_basename = os.path.basename(original_summary)
    if target_basename == summary_basename and len(target_basename) > 4:
        return True

    # Strategy 3: Suffix match (remove common prefixes and compare)
    target_suffix = _remove_common_prefixes(normalized_target)
    summary_suffix = _remove_common_prefixes(normalized_summary)
    if target_suffix == summary_suffix:
        return True

    # Strategy 4: Ends with match
    if normalized_target.endswith(normalized_summary) or normalized_summary.endswith(
        normalized_target
    ):
        return True

    # Strategy 5: Contains match for longer paths
    if len(normalized_target) > 10 and normalized_target in normalized_summary:
        return True
    if len(normalized_summary) > 10 and normalized_summary in normalized_target:
        return True

    return False


def _remove_common_prefixes(file_path: str) -> str:
    """Remove common prefixes from file path"""
    prefixes_to_remove = ["src/", "core/", "./", "lib/", "main/"]
    path = file_path

    for prefix in prefixes_to_remove:
        if path.startswith(prefix):
            path = path[len(prefix) :]

    return path


def _extract_file_section_alternative(
    summary_content: str, target_file_path: str
) -> str:
    """Alternative method to extract file section using simpler pattern matching"""

    # Get the basename for fallback matching
    target_basename = os.path.basename(target_file_path)

    # Split by separator lines to get individual sections
    sections = summary_content.split("=" * 80)

    for i, section in enumerate(sections):
        if "## IMPLEMENTATION File" in section:
            # Extract the file path from the header
            lines = section.strip().split("\n")
            for line in lines:
                if "## IMPLEMENTATION File" in line:
                    # Extract file path between "File " and "; ROUND"
                    try:
                        file_part = line.split("File ")[1].split("; ROUND")[0].strip()

                        # Check if this matches our target
                        if (
                            _normalize_file_path(target_file_path)
                            == _normalize_file_path(file_part)
                            or target_basename == os.path.basename(file_part)
                            or target_file_path in file_part
                            or file_part.endswith(target_file_path)
                        ):
                            # Get the next section which contains the content
                            if i + 1 < len(sections):
                                content_section = sections[i + 1].strip()
                                return f"""================================================================================
## IMPLEMENTATION File {file_part}
================================================================================

{content_section}

---
*Extracted from implement_code_summary.md using alternative method*"""
                    except (IndexError, AttributeError):
                        continue

    return None


# ==================== Code Search Tools ====================


@mcp.tool()
async def search_code(
    pattern: str,
    file_pattern: str = "*.json",
    use_regex: bool = False,
    search_directory: str = None,
) -> str:
    """
    Search patterns in code files

    Args:
        pattern: Search pattern
        file_pattern: File pattern (e.g., '*.py')
        use_regex: Whether to use regular expressions
        search_directory: Specify search directory (optional, uses WORKSPACE_DIR if not specified)

    Returns:
        JSON string of search results
    """
    try:
        # Determine search directory
        if search_directory:
            # If search directory is specified, use the specified directory
            if os.path.isabs(search_directory):
                search_path = Path(search_directory)
            else:
                # Relative path, relative to current working directory
                search_path = Path.cwd() / search_directory
        else:
            # Â¶ÇÊûúÊ≤°ÊúâÊåáÂÆöSearch directoryÔºå‰ΩøÁî®ÈªòËÆ§ÁöÑWORKSPACE_DIR
            ensure_workspace_exists()
            search_path = WORKSPACE_DIR

        # Ê£ÄÊü•Search directoryÊòØÂê¶Â≠òÂú®
        if not search_path.exists():
            result = {
                "status": "error",
                "message": f"Search directory‰∏çÂ≠òÂú®: {search_path}",
                "pattern": pattern,
            }
            return json.dumps(result, ensure_ascii=False, indent=2)

        import glob

        # Get matching files
        file_paths = glob.glob(str(search_path / "**" / file_pattern), recursive=True)

        matches = []
        total_files_searched = 0

        for file_path in file_paths:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    lines = f.readlines()

                total_files_searched += 1
                relative_path = os.path.relpath(file_path, search_path)

                for line_num, line in enumerate(lines, 1):
                    if use_regex:
                        if re.search(pattern, line):
                            matches.append(
                                {
                                    "file": relative_path,
                                    "line_number": line_num,
                                    "line_content": line.strip(),
                                    "match_type": "regex",
                                }
                            )
                    else:
                        if pattern.lower() in line.lower():
                            matches.append(
                                {
                                    "file": relative_path,
                                    "line_number": line_num,
                                    "line_content": line.strip(),
                                    "match_type": "substring",
                                }
                            )

            except Exception as e:
                logger.warning(f"Error searching file {file_path}: {e}")
                continue

        result = {
            "status": "success",
            "pattern": pattern,
            "file_pattern": file_pattern,
            "use_regex": use_regex,
            "search_directory": str(search_path),
            "total_matches": len(matches),
            "total_files_searched": total_files_searched,
            "matches": matches[:50],  # ÈôêÂà∂ËøîÂõûÂâç50‰∏™ÂåπÈÖç
        }

        if len(matches) > 50:
            result["note"] = f"ÊòæÁ§∫Ââç50‰∏™ÂåπÈÖçÔºåÊÄªÂÖ±ÊâæÂà∞{len(matches)}‰∏™ÂåπÈÖç"

        log_operation(
            "search_code",
            {
                "pattern": pattern,
                "file_pattern": file_pattern,
                "use_regex": use_regex,
                "search_directory": str(search_path),
                "total_matches": len(matches),
                "files_searched": total_files_searched,
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Code search failed: {str(e)}",
            "pattern": pattern,
        }
        log_operation("search_code_error", {"pattern": pattern, "error": str(e)})
        return json.dumps(result, ensure_ascii=False, indent=2)


# ==================== File Structure Tools ====================


@mcp.tool()
async def get_file_structure(directory: str = ".", max_depth: int = 5) -> str:
    """
    Get directory file structure

    Args:
        directory: Directory path, relative to workspace
        max_depth: ÊúÄÂ§ßÈÅçÂéÜÊ∑±Â∫¶

    Returns:
        JSON string of file structure
    """
    try:
        ensure_workspace_exists()

        if directory == ".":
            target_dir = WORKSPACE_DIR
        else:
            target_dir = validate_path(directory)

        if not target_dir.exists():
            result = {
                "status": "error",
                "message": f"Directory does not exist: {directory}",
            }
            return json.dumps(result, ensure_ascii=False, indent=2)

        def scan_directory(path: Path, current_depth: int = 0) -> Dict[str, Any]:
            """Recursively scan directory"""
            if current_depth >= max_depth:
                return {"type": "directory", "name": path.name, "truncated": True}

            items = []
            try:
                for item in sorted(path.iterdir()):
                    relative_path = os.path.relpath(item, WORKSPACE_DIR)

                    if item.is_file():
                        file_info = {
                            "type": "file",
                            "name": item.name,
                            "path": relative_path,
                            "size_bytes": item.stat().st_size,
                            "extension": item.suffix,
                        }
                        items.append(file_info)
                    elif item.is_dir() and not item.name.startswith("."):
                        dir_info = scan_directory(item, current_depth + 1)
                        dir_info["path"] = relative_path
                        items.append(dir_info)
            except PermissionError:
                pass

            return {
                "type": "directory",
                "name": path.name,
                "items": items,
                "item_count": len(items),
            }

        structure = scan_directory(target_dir)

        # ÁªüËÆ°‰ø°ÊÅØ
        def count_items(node):
            if node["type"] == "file":
                return {"files": 1, "directories": 0}
            else:
                counts = {"files": 0, "directories": 1}
                for item in node.get("items", []):
                    item_counts = count_items(item)
                    counts["files"] += item_counts["files"]
                    counts["directories"] += item_counts["directories"]
                return counts

        counts = count_items(structure)

        result = {
            "status": "success",
            "directory": directory,
            "max_depth": max_depth,
            "structure": structure,
            "summary": {
                "total_files": counts["files"],
                "total_directories": counts["directories"]
                - 1,  # Exclude root directory
            },
        }

        log_operation(
            "get_file_structure",
            {
                "directory": directory,
                "max_depth": max_depth,
                "total_files": counts["files"],
                "total_directories": counts["directories"] - 1,
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to get file structure: {str(e)}",
            "directory": directory,
        }
        log_operation(
            "get_file_structure_error", {"directory": directory, "error": str(e)}
        )
        return json.dumps(result, ensure_ascii=False, indent=2)


# ==================== Workspace Management Tools ====================


@mcp.tool()
async def set_workspace(workspace_path: str) -> str:
    """
    Set workspace directory

    Called by workflow to set workspace to: {plan_file_parent}/generate_code
    This ensures all file operations are executed relative to the correct project directory

    Args:
        workspace_path: Workspace path (Usually {plan_file_parent}/generate_code)

    Returns:
        JSON string of operation result
    """
    try:
        global WORKSPACE_DIR
        new_workspace = Path(workspace_path).resolve()

        # Create directory (if it does not exist)
        new_workspace.mkdir(parents=True, exist_ok=True)

        old_workspace = WORKSPACE_DIR
        WORKSPACE_DIR = new_workspace

        logger.info(f"New Workspace: {WORKSPACE_DIR}")

        result = {
            "status": "success",
            "message": f"Workspace setup successful: {workspace_path}",
            "new_workspace": str(WORKSPACE_DIR),
        }

        log_operation(
            "set_workspace",
            {
                "old_workspace": str(old_workspace) if old_workspace else None,
                "new_workspace": str(WORKSPACE_DIR),
                "workspace_alignment": "plan_file_parent/generate_code",
            },
        )

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to set workspace: {str(e)}",
            "workspace_path": workspace_path,
        }
        log_operation(
            "set_workspace_error", {"workspace_path": workspace_path, "error": str(e)}
        )
        return json.dumps(result, ensure_ascii=False, indent=2)


@mcp.tool()
async def get_operation_history(last_n: int = 10) -> str:
    """
    Get operation history

    Args:
        last_n: Return the last N operations

    Returns:
        JSON string of operation history
    """
    try:
        recent_history = (
            OPERATION_HISTORY[-last_n:] if last_n > 0 else OPERATION_HISTORY
        )

        result = {
            "status": "success",
            "total_operations": len(OPERATION_HISTORY),
            "returned_operations": len(recent_history),
            "workspace": str(WORKSPACE_DIR) if WORKSPACE_DIR else None,
            "history": recent_history,
        }

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        result = {
            "status": "error",
            "message": f"Failed to get operation history: {str(e)}",
        }
        return json.dumps(result, ensure_ascii=False, indent=2)


# ==================== Server Initialization ====================


def main():
    """Start MCP server"""
    print("üöÄ Code Implementation MCP Server")
    print(
        "üìù Paper Code Implementation Tool Server / Paper Code Implementation Tool Server"
    )
    print("")
    print("Available tools / Available tools:")
    # print("  ‚Ä¢ read_file           - Read file contents / Read file contents")
    print(
        "  ‚Ä¢ read_code_mem       - Read code summary from implement_code_summary.md / Read code summary from implement_code_summary.md"
    )
    print("  ‚Ä¢ write_file          - Write file contents / Write file contents")
    print("  ‚Ä¢ execute_python      - Execute Python code / Execute Python code")
    print("  ‚Ä¢ execute_bash        - Execute bash command / Execute bash commands")
    print("  ‚Ä¢ search_code         - Search code patterns / Search code patterns")
    print("  ‚Ä¢ get_file_structure  - Get file structure / Get file structure")
    print("  ‚Ä¢ set_workspace       - Set workspace / Set workspace")
    print("  ‚Ä¢ get_operation_history - Get operation history / Get operation history")
    print("")
    print("üîß Server starting...")

    # Initialize default workspace
    initialize_workspace()

    # Start server
    mcp.run()


if __name__ == "__main__":
    main()


--- tools/code_indexer.py ---
"""
Code Indexer for Repository Analysis

Analyzes code repositories to build comprehensive indexes for each subdirectory,
identifying file relationships and reusable components for implementation.

Features:
- Recursive file traversal
- LLM-powered code similarity analysis using augmented LLM classes
- JSON-based relationship storage
- Configurable matching strategies
- Progress tracking and error handling
- Automatic LLM provider selection based on API key availability
"""

import asyncio
import json
import logging
import os
import re
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Any

# MCP Agent imports for LLM
from utils.llm_utils import get_preferred_llm_class, get_default_models


@dataclass
class FileRelationship:
    """Represents a relationship between a repo file and target structure file"""

    repo_file_path: str
    target_file_path: str
    relationship_type: str  # 'direct_match', 'partial_match', 'reference', 'utility'
    confidence_score: float  # 0.0 to 1.0
    helpful_aspects: List[str]
    potential_contributions: List[str]
    usage_suggestions: str


@dataclass
class FileSummary:
    """Summary information for a repository file"""

    file_path: str
    file_type: str
    main_functions: List[str]
    key_concepts: List[str]
    dependencies: List[str]
    summary: str
    lines_of_code: int
    last_modified: str


@dataclass
class RepoIndex:
    """Complete index for a repository"""

    repo_name: str
    total_files: int
    file_summaries: List[FileSummary]
    relationships: List[FileRelationship]
    analysis_metadata: Dict[str, Any]


class CodeIndexer:
    """Main class for building code repository indexes"""

    def __init__(
        self,
        code_base_path: str = None,
        target_structure: str = None,
        output_dir: str = None,
        config_path: str = "mcp_agent.secrets.yaml",
        indexer_config_path: str = None,
        enable_pre_filtering: bool = True,
    ):
        # Load configurations first
        self.config_path = config_path
        self.indexer_config_path = indexer_config_path
        # Derive main config path from secrets path (same directory)
        secrets_dir = os.path.dirname(os.path.abspath(config_path))
        self.main_config_path = os.path.join(secrets_dir, "mcp_agent.config.yaml")
        self.api_config = self._load_api_config()
        self.indexer_config = self._load_indexer_config()
        self.default_models = get_default_models(self.main_config_path)

        # Use config paths if not provided as parameters
        paths_config = self.indexer_config.get("paths", {})
        self.code_base_path = Path(
            code_base_path or paths_config.get("code_base_path", "code_base")
        )
        self.output_dir = Path(output_dir or paths_config.get("output_dir", "indexes"))
        self.target_structure = (
            target_structure  # This must be provided as it's project-specific
        )
        self.enable_pre_filtering = enable_pre_filtering

        # LLM clients
        self.llm_client = None
        self.llm_client_type = None

        # Initialize logger early
        self.logger = self._setup_logger()

        # Create output directory if it doesn't exist
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Load file analysis configuration
        file_analysis_config = self.indexer_config.get("file_analysis", {})
        self.supported_extensions = set(
            file_analysis_config.get(
                "supported_extensions",
                [
                    ".py",
                    ".js",
                    ".ts",
                    ".java",
                    ".cpp",
                    ".c",
                    ".h",
                    ".hpp",
                    ".cs",
                    ".php",
                    ".rb",
                    ".go",
                    ".rs",
                    ".scala",
                    ".kt",
                    ".swift",
                    ".m",
                    ".mm",
                    ".r",
                    ".matlab",
                    ".sql",
                    ".sh",
                    ".bat",
                    ".ps1",
                    ".yaml",
                    ".yml",
                    ".json",
                    ".xml",
                    ".toml",
                ],
            )
        )

        self.skip_directories = set(
            file_analysis_config.get(
                "skip_directories",
                [
                    "__pycache__",
                    "node_modules",
                    "target",
                    "build",
                    "dist",
                    "venv",
                    "env",
                ],
            )
        )

        self.max_file_size = file_analysis_config.get("max_file_size", 1048576)  # 1MB
        self.max_content_length = file_analysis_config.get("max_content_length", 3000)

        # Load LLM configuration
        llm_config = self.indexer_config.get("llm", {})
        self.model_provider = llm_config.get("model_provider", "anthropic")
        self.llm_max_tokens = llm_config.get("max_tokens", 4000)
        self.llm_temperature = llm_config.get("temperature", 0.3)
        self.llm_system_prompt = llm_config.get(
            "system_prompt",
            "You are a code analysis expert. Provide precise, structured analysis of code relationships and similarities.",
        )
        self.request_delay = llm_config.get("request_delay", 0.1)
        self.max_retries = llm_config.get("max_retries", 3)
        self.retry_delay = llm_config.get("retry_delay", 1.0)

        # Load relationship configuration
        relationship_config = self.indexer_config.get("relationships", {})
        self.min_confidence_score = relationship_config.get("min_confidence_score", 0.3)
        self.high_confidence_threshold = relationship_config.get(
            "high_confidence_threshold", 0.7
        )
        self.relationship_types = relationship_config.get(
            "relationship_types",
            {
                "direct_match": 1.0,
                "partial_match": 0.8,
                "reference": 0.6,
                "utility": 0.4,
            },
        )

        # Load performance configuration
        performance_config = self.indexer_config.get("performance", {})
        self.enable_concurrent_analysis = performance_config.get(
            "enable_concurrent_analysis", False
        )
        self.max_concurrent_files = performance_config.get("max_concurrent_files", 5)
        self.enable_content_caching = performance_config.get(
            "enable_content_caching", False
        )
        self.max_cache_size = performance_config.get("max_cache_size", 100)

        # Load debug configuration
        debug_config = self.indexer_config.get("debug", {})
        self.save_raw_responses = debug_config.get("save_raw_responses", False)
        self.raw_responses_dir = debug_config.get(
            "raw_responses_dir", "debug_responses"
        )
        self.verbose_output = debug_config.get("verbose_output", False)
        self.mock_llm_responses = debug_config.get("mock_llm_responses", False)

        # Load output configuration
        output_config = self.indexer_config.get("output", {})
        self.generate_summary = output_config.get("generate_summary", True)
        self.generate_statistics = output_config.get("generate_statistics", True)
        self.include_metadata = output_config.get("include_metadata", True)
        self.index_filename_pattern = output_config.get(
            "index_filename_pattern", "{repo_name}_index.json"
        )
        self.summary_filename = output_config.get(
            "summary_filename", "indexing_summary.json"
        )
        self.stats_filename = output_config.get(
            "stats_filename", "indexing_statistics.json"
        )

        # Initialize caching if enabled
        self.content_cache = {} if self.enable_content_caching else None

        # Create debug directory if needed
        if self.save_raw_responses:
            Path(self.raw_responses_dir).mkdir(parents=True, exist_ok=True)

        # Debug logging
        if self.verbose_output:
            self.logger.info(
                f"Initialized CodeIndexer with config: {self.indexer_config_path}"
            )
            self.logger.info(f"Code base path: {self.code_base_path}")
            self.logger.info(f"Output directory: {self.output_dir}")
            self.logger.info(f"Model provider: {self.model_provider}")
            self.logger.info(f"Concurrent analysis: {self.enable_concurrent_analysis}")
            self.logger.info(f"Content caching: {self.enable_content_caching}")
            self.logger.info(f"Mock LLM responses: {self.mock_llm_responses}")

    def _setup_logger(self) -> logging.Logger:
        """Setup logging configuration from config file"""
        logger = logging.getLogger("CodeIndexer")

        # Get logging config
        logging_config = self.indexer_config.get("logging", {})
        log_level = logging_config.get("level", "INFO")
        log_format = logging_config.get(
            "log_format", "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )

        logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))

        # Clear existing handlers
        logger.handlers.clear()

        # Console handler
        handler = logging.StreamHandler()
        formatter = logging.Formatter(log_format)
        handler.setFormatter(formatter)
        logger.addHandler(handler)

        # File handler if enabled
        if logging_config.get("log_to_file", False):
            log_file = logging_config.get("log_file", "indexer.log")
            file_handler = logging.FileHandler(log_file, encoding="utf-8")
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)

        return logger

    def _load_api_config(self) -> Dict[str, Any]:
        """Load API configuration from YAML file"""
        try:
            import yaml

            with open(self.config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            # Create a basic logger for this error since self.logger doesn't exist yet
            print(f"Warning: Failed to load API config from {self.config_path}: {e}")
            return {}

    def _load_indexer_config(self) -> Dict[str, Any]:
        """Load indexer configuration from YAML file"""
        try:
            import yaml

            with open(self.indexer_config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
                if config is None:
                    config = {}
                return config
        except Exception as e:
            print(
                f"Warning: Failed to load indexer config from {self.indexer_config_path}: {e}"
            )
            print("Using default configuration values")
            return {}

    async def _initialize_llm_client(self):
        """Initialize LLM client (Anthropic or OpenAI) based on API key availability"""
        if self.llm_client is not None:
            return self.llm_client, self.llm_client_type

        # Check if mock responses are enabled
        if self.mock_llm_responses:
            self.logger.info("Using mock LLM responses for testing")
            self.llm_client = "mock"
            self.llm_client_type = "mock"
            return "mock", "mock"

        # Check which API has available key and try that first
        anthropic_key = self.api_config.get("anthropic", {}).get("api_key", "")
        openai_key = self.api_config.get("openai", {}).get("api_key", "")

        # Try Anthropic API first if key is available
        if anthropic_key and anthropic_key.strip():
            try:
                from anthropic import AsyncAnthropic

                client = AsyncAnthropic(api_key=anthropic_key)
                # Test connection with default model from config
                await client.messages.create(
                    model=self.default_models["anthropic"],
                    max_tokens=10,
                    messages=[{"role": "user", "content": "test"}],
                )
                self.logger.info(
                    f"Using Anthropic API with model: {self.default_models['anthropic']}"
                )
                self.llm_client = client
                self.llm_client_type = "anthropic"
                return client, "anthropic"
            except Exception as e:
                self.logger.warning(f"Anthropic API unavailable: {e}")

        # Try OpenAI API if Anthropic failed or key not available
        if openai_key and openai_key.strip():
            try:
                from openai import AsyncOpenAI

                # Handle custom base_url if specified
                openai_config = self.api_config.get("openai", {})
                base_url = openai_config.get("base_url")

                if base_url:
                    client = AsyncOpenAI(api_key=openai_key, base_url=base_url)
                else:
                    client = AsyncOpenAI(api_key=openai_key)

                # Test connection with default model from config
                await client.chat.completions.create(
                    model=self.default_models["openai"],
                    max_tokens=10,
                    messages=[{"role": "user", "content": "test"}],
                )
                self.logger.info(
                    f"Using OpenAI API with model: {self.default_models['openai']}"
                )
                if base_url:
                    self.logger.info(f"Using custom base URL: {base_url}")
                self.llm_client = client
                self.llm_client_type = "openai"
                return client, "openai"
            except Exception as e:
                self.logger.warning(f"OpenAI API unavailable: {e}")

        raise ValueError(
            "No available LLM API - please check your API keys in configuration"
        )

    async def _call_llm(
        self, prompt: str, system_prompt: str = None, max_tokens: int = None
    ) -> str:
        """Call LLM for code analysis with retry mechanism and debugging support"""
        if system_prompt is None:
            system_prompt = self.llm_system_prompt
        if max_tokens is None:
            max_tokens = self.llm_max_tokens

        # Mock response for testing
        if self.mock_llm_responses:
            mock_response = self._generate_mock_response(prompt)
            if self.save_raw_responses:
                self._save_debug_response("mock", prompt, mock_response)
            return mock_response

        last_error = None

        # Retry mechanism
        for attempt in range(self.max_retries):
            try:
                if self.verbose_output and attempt > 0:
                    self.logger.info(
                        f"LLM call attempt {attempt + 1}/{self.max_retries}"
                    )

                client, client_type = await self._initialize_llm_client()

                if client_type == "anthropic":
                    response = await client.messages.create(
                        model=self.default_models["anthropic"],
                        system=system_prompt,
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=max_tokens,
                        temperature=self.llm_temperature,
                    )

                    content = ""
                    for block in response.content:
                        if block.type == "text":
                            content += block.text

                    # Save debug response if enabled
                    if self.save_raw_responses:
                        self._save_debug_response("anthropic", prompt, content)

                    return content

                elif client_type == "openai":
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt},
                    ]

                    response = await client.chat.completions.create(
                        model=self.default_models["openai"],
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=self.llm_temperature,
                    )

                    content = response.choices[0].message.content or ""

                    # Save debug response if enabled
                    if self.save_raw_responses:
                        self._save_debug_response("openai", prompt, content)

                    return content
                else:
                    raise ValueError(f"Unsupported client type: {client_type}")

            except Exception as e:
                last_error = e
                self.logger.warning(f"LLM call attempt {attempt + 1} failed: {e}")

                if attempt < self.max_retries - 1:
                    await asyncio.sleep(
                        self.retry_delay * (attempt + 1)
                    )  # Exponential backoff

        # All retries failed
        error_msg = f"LLM call failed after {self.max_retries} attempts. Last error: {str(last_error)}"
        self.logger.error(error_msg)
        return f"Error in LLM analysis: {error_msg}"

    def _generate_mock_response(self, prompt: str) -> str:
        """Generate mock LLM response for testing"""
        if "JSON format" in prompt and "file_type" in prompt:
            # File analysis mock
            return """
            {
                "file_type": "Python module",
                "main_functions": ["main_function", "helper_function"],
                "key_concepts": ["data_processing", "algorithm"],
                "dependencies": ["numpy", "pandas"],
                "summary": "Mock analysis of code file functionality."
            }
            """
        elif "relationships" in prompt:
            # Relationship analysis mock
            return """
            {
                "relationships": [
                    {
                        "target_file_path": "src/core/mock.py",
                        "relationship_type": "partial_match",
                        "confidence_score": 0.8,
                        "helpful_aspects": ["algorithm implementation", "data structures"],
                        "potential_contributions": ["core functionality", "utility methods"],
                        "usage_suggestions": "Mock relationship suggestion for testing."
                    }
                ]
            }
            """
        elif "relevant_files" in prompt:
            # File filtering mock
            return """
            {
                "relevant_files": [
                    {
                        "file_path": "mock_file.py",
                        "relevance_reason": "Mock relevance reason",
                        "confidence": 0.9,
                        "expected_contribution": "Mock contribution"
                    }
                ],
                "summary": {
                    "total_files_analyzed": "10",
                    "relevant_files_count": "1",
                    "filtering_strategy": "Mock filtering strategy"
                }
            }
            """
        else:
            return "Mock LLM response for testing purposes."

    def _save_debug_response(self, provider: str, prompt: str, response: str):
        """Save LLM response for debugging"""
        try:
            import hashlib
            from datetime import datetime

            # Create a hash of the prompt for filename
            prompt_hash = hashlib.md5(prompt.encode()).hexdigest()[:8]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{provider}_{timestamp}_{prompt_hash}.json"

            debug_data = {
                "timestamp": datetime.now().isoformat(),
                "provider": provider,
                "prompt": prompt[:500] + "..." if len(prompt) > 500 else prompt,
                "response": response,
                "full_prompt_length": len(prompt),
            }

            debug_file = Path(self.raw_responses_dir) / filename
            with open(debug_file, "w", encoding="utf-8") as f:
                json.dump(debug_data, f, indent=2, ensure_ascii=False)

        except Exception as e:
            self.logger.warning(f"Failed to save debug response: {e}")

    def get_all_repo_files(self, repo_path: Path) -> List[Path]:
        """Recursively get all supported files in a repository"""
        files = []

        try:
            for root, dirs, filenames in os.walk(repo_path):
                # Skip common non-code directories
                dirs[:] = [
                    d
                    for d in dirs
                    if not d.startswith(".") and d not in self.skip_directories
                ]

                for filename in filenames:
                    file_path = Path(root) / filename
                    if file_path.suffix.lower() in self.supported_extensions:
                        files.append(file_path)

        except Exception as e:
            self.logger.error(f"Error traversing {repo_path}: {e}")

        return files

    def generate_file_tree(self, repo_path: Path, max_depth: int = 5) -> str:
        """Generate file tree structure string for the repository"""
        tree_lines = []

        def add_to_tree(current_path: Path, prefix: str = "", depth: int = 0):
            if depth > max_depth:
                return

            try:
                items = sorted(
                    current_path.iterdir(), key=lambda x: (x.is_file(), x.name.lower())
                )
                # Filter out irrelevant directories and files
                items = [
                    item
                    for item in items
                    if not item.name.startswith(".")
                    and item.name not in self.skip_directories
                ]

                for i, item in enumerate(items):
                    is_last = i == len(items) - 1
                    current_prefix = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
                    tree_lines.append(f"{prefix}{current_prefix}{item.name}")

                    if item.is_dir():
                        extension_prefix = "    " if is_last else "‚îÇ   "
                        add_to_tree(item, prefix + extension_prefix, depth + 1)
                    elif item.suffix.lower() in self.supported_extensions:
                        # Add file size information
                        try:
                            size = item.stat().st_size
                            if size > 1024:
                                size_str = f" ({size // 1024}KB)"
                            else:
                                size_str = f" ({size}B)"
                            tree_lines[-1] += size_str
                        except (OSError, PermissionError):
                            pass

            except PermissionError:
                tree_lines.append(f"{prefix}‚îú‚îÄ‚îÄ [Permission Denied]")
            except Exception as e:
                tree_lines.append(f"{prefix}‚îú‚îÄ‚îÄ [Error: {str(e)}]")

        tree_lines.append(f"{repo_path.name}/")
        add_to_tree(repo_path)
        return "\n".join(tree_lines)

    async def pre_filter_files(self, repo_path: Path, file_tree: str) -> List[str]:
        """Use LLM to pre-filter relevant files based on target structure"""
        filter_prompt = f"""
        You are a code analysis expert. Please analyze the following code repository file tree based on the target project structure and filter out files that may be relevant to the target project.

        Target Project Structure:
        {self.target_structure}

        Code Repository File Tree:
        {file_tree}

        Please analyze which files might be helpful for implementing the target project structure, including:
        - Core algorithm implementation files (such as GCN, recommendation systems, graph neural networks, etc.)
        - Data processing and preprocessing files
        - Loss functions and evaluation metric files
        - Configuration and utility files
        - Test files
        - Documentation files

        Please return the filtering results in JSON format:
        {{
            "relevant_files": [
                {{
                    "file_path": "file path relative to repository root",
                    "relevance_reason": "why this file is relevant",
                    "confidence": 0.0-1.0,
                    "expected_contribution": "expected contribution to the target project"
                }}
            ],
            "summary": {{
                "total_files_analyzed": "total number of files analyzed",
                "relevant_files_count": "number of relevant files",
                "filtering_strategy": "explanation of filtering strategy"
            }}
        }}

        Only return files with confidence > {self.min_confidence_score}. Focus on files related to recommendation systems, graph neural networks, and diffusion models.
        """

        try:
            self.logger.info("Starting LLM pre-filtering of files...")
            llm_response = await self._call_llm(
                filter_prompt,
                system_prompt="You are a professional code analysis and project architecture expert, skilled at identifying code file functionality and relevance.",
                max_tokens=2000,
            )

            # Parse JSON response
            match = re.search(r"\{.*\}", llm_response, re.DOTALL)
            if not match:
                self.logger.warning(
                    "Unable to parse LLM filtering response, will use all files"
                )
                return []

            filter_data = json.loads(match.group(0))
            relevant_files = filter_data.get("relevant_files", [])

            # Extract file paths
            selected_files = []
            for file_info in relevant_files:
                file_path = file_info.get("file_path", "")
                confidence = file_info.get("confidence", 0.0)
                # Use configured minimum confidence threshold
                if file_path and confidence > self.min_confidence_score:
                    selected_files.append(file_path)

            summary = filter_data.get("summary", {})
            self.logger.info(
                f"LLM filtering completed: {summary.get('relevant_files_count', len(selected_files))} relevant files selected"
            )
            self.logger.info(
                f"Filtering strategy: {summary.get('filtering_strategy', 'Not provided')}"
            )

            return selected_files

        except Exception as e:
            self.logger.error(f"LLM pre-filtering failed: {e}")
            self.logger.info("Will fallback to analyzing all files")
            return []

    def filter_files_by_paths(
        self, all_files: List[Path], selected_paths: List[str], repo_path: Path
    ) -> List[Path]:
        """Filter file list based on LLM-selected paths"""
        if not selected_paths:
            return all_files

        filtered_files = []

        for file_path in all_files:
            # Get path relative to repository root
            relative_path = str(file_path.relative_to(repo_path))

            # Check if it's in the selected list
            for selected_path in selected_paths:
                # Normalize path comparison
                if (
                    relative_path == selected_path
                    or relative_path.replace("\\", "/")
                    == selected_path.replace("\\", "/")
                    or selected_path in relative_path
                    or relative_path in selected_path
                ):
                    filtered_files.append(file_path)
                    break

        return filtered_files

    def _get_cache_key(self, file_path: Path) -> str:
        """Generate cache key for file content"""
        try:
            stats = file_path.stat()
            return f"{file_path}:{stats.st_mtime}:{stats.st_size}"
        except (OSError, PermissionError):
            return str(file_path)

    def _manage_cache_size(self):
        """Manage cache size to stay within limits"""
        if not self.enable_content_caching or not self.content_cache:
            return

        if len(self.content_cache) > self.max_cache_size:
            # Remove oldest entries (simple FIFO strategy)
            excess_count = len(self.content_cache) - self.max_cache_size + 10
            keys_to_remove = list(self.content_cache.keys())[:excess_count]

            for key in keys_to_remove:
                del self.content_cache[key]

            if self.verbose_output:
                self.logger.info(
                    f"Cache cleaned: removed {excess_count} entries, {len(self.content_cache)} entries remaining"
                )

    async def analyze_file_content(self, file_path: Path) -> FileSummary:
        """Analyze a single file and create summary with caching support"""
        try:
            # Check file size before reading
            file_size = file_path.stat().st_size
            if file_size > self.max_file_size:
                self.logger.warning(
                    f"Skipping file {file_path} - size {file_size} bytes exceeds limit {self.max_file_size}"
                )
                return FileSummary(
                    file_path=str(file_path.relative_to(self.code_base_path)),
                    file_type="skipped - too large",
                    main_functions=[],
                    key_concepts=[],
                    dependencies=[],
                    summary=f"File skipped - size {file_size} bytes exceeds {self.max_file_size} byte limit",
                    lines_of_code=0,
                    last_modified=datetime.fromtimestamp(
                        file_path.stat().st_mtime
                    ).isoformat(),
                )

            # Check cache if enabled
            cache_key = None
            if self.enable_content_caching:
                cache_key = self._get_cache_key(file_path)
                if cache_key in self.content_cache:
                    if self.verbose_output:
                        self.logger.info(f"Using cached analysis for {file_path.name}")
                    return self.content_cache[cache_key]

            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()

            # Get file stats
            stats = file_path.stat()
            lines_of_code = len([line for line in content.split("\n") if line.strip()])

            # Truncate content based on config
            content_for_analysis = content[: self.max_content_length]
            content_suffix = "..." if len(content) > self.max_content_length else ""

            # Create analysis prompt
            analysis_prompt = f"""
            Analyze this code file and provide a structured summary:

            File: {file_path.name}
            Content:
            ```
            {content_for_analysis}{content_suffix}
            ```

            Please provide analysis in this JSON format:
            {{
                "file_type": "description of what type of file this is",
                "main_functions": ["list", "of", "main", "functions", "or", "classes"],
                "key_concepts": ["important", "concepts", "algorithms", "patterns"],
                "dependencies": ["external", "libraries", "or", "imports"],
                "summary": "2-3 sentence summary of what this file does"
            }}

            Focus on the core functionality and potential reusability.
            """

            # Get LLM analysis with configured parameters
            llm_response = await self._call_llm(analysis_prompt, max_tokens=1000)

            try:
                # Try to parse JSON response
                match = re.search(r"\{.*\}", llm_response, re.DOTALL)
                analysis_data = json.loads(match.group(0))
            except json.JSONDecodeError:
                # Fallback to basic analysis if JSON parsing fails
                analysis_data = {
                    "file_type": f"{file_path.suffix} file",
                    "main_functions": [],
                    "key_concepts": [],
                    "dependencies": [],
                    "summary": "File analysis failed - JSON parsing error",
                }

            file_summary = FileSummary(
                file_path=str(file_path.relative_to(self.code_base_path)),
                file_type=analysis_data.get("file_type", "unknown"),
                main_functions=analysis_data.get("main_functions", []),
                key_concepts=analysis_data.get("key_concepts", []),
                dependencies=analysis_data.get("dependencies", []),
                summary=analysis_data.get("summary", "No summary available"),
                lines_of_code=lines_of_code,
                last_modified=datetime.fromtimestamp(stats.st_mtime).isoformat(),
            )

            # Cache the result if caching is enabled
            if self.enable_content_caching and cache_key:
                self.content_cache[cache_key] = file_summary
                self._manage_cache_size()

            return file_summary

        except Exception as e:
            self.logger.error(f"Error analyzing file {file_path}: {e}")
            return FileSummary(
                file_path=str(file_path.relative_to(self.code_base_path)),
                file_type="error",
                main_functions=[],
                key_concepts=[],
                dependencies=[],
                summary=f"Analysis failed: {str(e)}",
                lines_of_code=0,
                last_modified="",
            )

    async def find_relationships(
        self, file_summary: FileSummary
    ) -> List[FileRelationship]:
        """Find relationships between a repo file and target structure"""

        # Build relationship type description from config
        relationship_type_desc = []
        for rel_type, weight in self.relationship_types.items():
            relationship_type_desc.append(f"- {rel_type} (priority: {weight})")

        relationship_prompt = f"""
        Analyze the relationship between this existing code file and the target project structure.

        Existing File Analysis:
        - Path: {file_summary.file_path}
        - Type: {file_summary.file_type}
        - Functions: {', '.join(file_summary.main_functions)}
        - Concepts: {', '.join(file_summary.key_concepts)}
        - Summary: {file_summary.summary}

        Target Project Structure:
        {self.target_structure}

        Available relationship types (with priority weights):
        {chr(10).join(relationship_type_desc)}

        Identify potential relationships and provide analysis in this JSON format:
        {{
            "relationships": [
                {{
                    "target_file_path": "path/in/target/structure",
                    "relationship_type": "direct_match|partial_match|reference|utility",
                    "confidence_score": 0.0-1.0,
                    "helpful_aspects": ["specific", "aspects", "that", "could", "help"],
                    "potential_contributions": ["how", "this", "could", "contribute"],
                    "usage_suggestions": "detailed suggestion on how to use this file"
                }}
            ]
        }}

        Consider the priority weights when determining relationship types. Higher weight types should be preferred when multiple types apply.
        Only include relationships with confidence > {self.min_confidence_score}. Focus on concrete, actionable connections.
        """

        try:
            llm_response = await self._call_llm(relationship_prompt, max_tokens=1500)

            match = re.search(r"\{.*\}", llm_response, re.DOTALL)
            relationship_data = json.loads(match.group(0))

            relationships = []
            for rel_data in relationship_data.get("relationships", []):
                confidence_score = float(rel_data.get("confidence_score", 0.0))
                relationship_type = rel_data.get("relationship_type", "reference")

                # Validate relationship type is in config
                if relationship_type not in self.relationship_types:
                    if self.verbose_output:
                        self.logger.warning(
                            f"Unknown relationship type '{relationship_type}', using 'reference'"
                        )
                    relationship_type = "reference"

                # Apply configured minimum confidence filter
                if confidence_score > self.min_confidence_score:
                    relationship = FileRelationship(
                        repo_file_path=file_summary.file_path,
                        target_file_path=rel_data.get("target_file_path", ""),
                        relationship_type=relationship_type,
                        confidence_score=confidence_score,
                        helpful_aspects=rel_data.get("helpful_aspects", []),
                        potential_contributions=rel_data.get(
                            "potential_contributions", []
                        ),
                        usage_suggestions=rel_data.get("usage_suggestions", ""),
                    )
                    relationships.append(relationship)

            return relationships

        except Exception as e:
            self.logger.error(
                f"Error finding relationships for {file_summary.file_path}: {e}"
            )
            return []

    async def _analyze_single_file_with_relationships(
        self, file_path: Path, index: int, total: int
    ) -> tuple:
        """Analyze a single file and its relationships (for concurrent processing)"""
        if self.verbose_output:
            self.logger.info(f"Analyzing file {index}/{total}: {file_path.name}")

        # Get file summary
        file_summary = await self.analyze_file_content(file_path)

        # Find relationships
        relationships = await self.find_relationships(file_summary)

        return file_summary, relationships

    async def process_repository(self, repo_path: Path) -> RepoIndex:
        """Process a single repository and create complete index with optional concurrent processing"""
        repo_name = repo_path.name
        self.logger.info(f"Processing repository: {repo_name}")

        # Step 1: Generate file tree
        self.logger.info("Generating file tree structure...")
        file_tree = self.generate_file_tree(repo_path)

        # Step 2: Get all files
        all_files = self.get_all_repo_files(repo_path)
        self.logger.info(f"Found {len(all_files)} files in {repo_name}")

        # Step 3: LLM pre-filtering of relevant files
        if self.enable_pre_filtering:
            self.logger.info("Using LLM for file pre-filtering...")
            selected_file_paths = await self.pre_filter_files(repo_path, file_tree)
        else:
            self.logger.info("Pre-filtering is disabled, will analyze all files")
            selected_file_paths = []

        # Step 4: Filter file list based on filtering results
        if selected_file_paths:
            files_to_analyze = self.filter_files_by_paths(
                all_files, selected_file_paths, repo_path
            )
            self.logger.info(
                f"After LLM filtering, will analyze {len(files_to_analyze)} relevant files (from {len(all_files)} total)"
            )
        else:
            files_to_analyze = all_files
            self.logger.info("LLM filtering failed, will analyze all files")

        # Step 5: Analyze filtered files (concurrent or sequential)
        if self.enable_concurrent_analysis and len(files_to_analyze) > 1:
            self.logger.info(
                f"Using concurrent analysis with max {self.max_concurrent_files} parallel files"
            )
            file_summaries, all_relationships = await self._process_files_concurrently(
                files_to_analyze
            )
        else:
            self.logger.info("Using sequential file analysis")
            file_summaries, all_relationships = await self._process_files_sequentially(
                files_to_analyze
            )

        # Step 6: Create repository index
        repo_index = RepoIndex(
            repo_name=repo_name,
            total_files=len(all_files),  # Record original file count
            file_summaries=file_summaries,
            relationships=all_relationships,
            analysis_metadata={
                "analysis_date": datetime.now().isoformat(),
                "target_structure_analyzed": self.target_structure[:200] + "...",
                "total_relationships_found": len(all_relationships),
                "high_confidence_relationships": len(
                    [
                        r
                        for r in all_relationships
                        if r.confidence_score > self.high_confidence_threshold
                    ]
                ),
                "analyzer_version": "1.4.0",  # Updated version to reflect augmented LLM support
                "pre_filtering_enabled": self.enable_pre_filtering,
                "files_before_filtering": len(all_files),
                "files_after_filtering": len(files_to_analyze),
                "filtering_efficiency": round(
                    (1 - len(files_to_analyze) / len(all_files)) * 100, 2
                )
                if all_files
                else 0,
                "config_file_used": self.indexer_config_path,
                "min_confidence_score": self.min_confidence_score,
                "high_confidence_threshold": self.high_confidence_threshold,
                "concurrent_analysis_used": self.enable_concurrent_analysis,
                "content_caching_enabled": self.enable_content_caching,
                "cache_hits": len(self.content_cache) if self.content_cache else 0,
            },
        )

        return repo_index

    async def _process_files_sequentially(self, files_to_analyze: list) -> tuple:
        """Process files sequentially (original method)"""
        file_summaries = []
        all_relationships = []

        for i, file_path in enumerate(files_to_analyze, 1):
            (
                file_summary,
                relationships,
            ) = await self._analyze_single_file_with_relationships(
                file_path, i, len(files_to_analyze)
            )
            file_summaries.append(file_summary)
            all_relationships.extend(relationships)

            # Add configured delay to avoid overwhelming the LLM API
            await asyncio.sleep(self.request_delay)

        return file_summaries, all_relationships

    async def _process_files_concurrently(self, files_to_analyze: list) -> tuple:
        """Process files concurrently with semaphore limiting"""
        file_summaries = []
        all_relationships = []

        # Create semaphore to limit concurrent tasks
        semaphore = asyncio.Semaphore(self.max_concurrent_files)
        tasks = []

        async def _process_with_semaphore(file_path: Path, index: int, total: int):
            async with semaphore:
                # Add a small delay to space out concurrent requests
                if index > 1:
                    await asyncio.sleep(
                        self.request_delay * 0.5
                    )  # Reduced delay for concurrent processing
                return await self._analyze_single_file_with_relationships(
                    file_path, index, total
                )

        try:
            # Create tasks for all files
            tasks = [
                _process_with_semaphore(file_path, i, len(files_to_analyze))
                for i, file_path in enumerate(files_to_analyze, 1)
            ]

            # Process tasks and collect results
            if self.verbose_output:
                self.logger.info(
                    f"Starting concurrent analysis of {len(tasks)} files..."
                )

            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)

                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        self.logger.error(
                            f"Failed to analyze file {files_to_analyze[i]}: {result}"
                        )
                        # Create error summary
                        error_summary = FileSummary(
                            file_path=str(
                                files_to_analyze[i].relative_to(self.code_base_path)
                            ),
                            file_type="error",
                            main_functions=[],
                            key_concepts=[],
                            dependencies=[],
                            summary=f"Concurrent analysis failed: {str(result)}",
                            lines_of_code=0,
                            last_modified="",
                        )
                        file_summaries.append(error_summary)
                    else:
                        file_summary, relationships = result
                        file_summaries.append(file_summary)
                        all_relationships.extend(relationships)

            except Exception as e:
                self.logger.error(f"Concurrent processing failed: {e}")
                # Cancel any remaining tasks
                for task in tasks:
                    if not task.done() and not task.cancelled():
                        task.cancel()

                # Wait for cancelled tasks to complete
                try:
                    await asyncio.sleep(0.1)  # Brief wait for cancellation
                except Exception:
                    pass

                # Fallback to sequential processing
                self.logger.info("Falling back to sequential processing...")
                return await self._process_files_sequentially(files_to_analyze)

            if self.verbose_output:
                self.logger.info(
                    f"Concurrent analysis completed: {len(file_summaries)} files processed"
                )

            return file_summaries, all_relationships

        except Exception as e:
            # Ensure all tasks are cancelled in case of unexpected errors
            if tasks:
                for task in tasks:
                    if not task.done() and not task.cancelled():
                        task.cancel()

            # Wait briefly for cancellation to complete
            try:
                await asyncio.sleep(0.1)
            except Exception:
                pass

            self.logger.error(f"Critical error in concurrent processing: {e}")
            # Fallback to sequential processing
            self.logger.info(
                "Falling back to sequential processing due to critical error..."
            )
            return await self._process_files_sequentially(files_to_analyze)

        finally:
            # Final cleanup: ensure all tasks are properly finished
            if tasks:
                for task in tasks:
                    if not task.done() and not task.cancelled():
                        task.cancel()

            # Clear task references to help with garbage collection
            tasks.clear()

            # Force garbage collection to help clean up semaphore and related resources
            import gc

            gc.collect()

    async def build_all_indexes(self) -> Dict[str, str]:
        """Build indexes for all repositories in code_base"""
        if not self.code_base_path.exists():
            raise FileNotFoundError(
                f"Code base path does not exist: {self.code_base_path}"
            )

        # Get all repository directories
        repo_dirs = [
            d
            for d in self.code_base_path.iterdir()
            if d.is_dir() and not d.name.startswith(".")
        ]

        if not repo_dirs:
            raise ValueError(f"No repositories found in {self.code_base_path}")

        self.logger.info(f"Found {len(repo_dirs)} repositories to process")

        # Process each repository
        output_files = {}
        statistics_data = []

        for repo_dir in repo_dirs:
            try:
                # Process repository
                repo_index = await self.process_repository(repo_dir)

                # Generate output filename using configured pattern
                output_filename = self.index_filename_pattern.format(
                    repo_name=repo_index.repo_name
                )
                output_file = self.output_dir / output_filename

                # Get output configuration
                output_config = self.indexer_config.get("output", {})
                json_indent = output_config.get("json_indent", 2)
                ensure_ascii = not output_config.get("ensure_ascii", False)

                # Save to JSON file
                with open(output_file, "w", encoding="utf-8") as f:
                    if self.include_metadata:
                        json.dump(
                            asdict(repo_index),
                            f,
                            indent=json_indent,
                            ensure_ascii=ensure_ascii,
                        )
                    else:
                        # Save without metadata if disabled
                        index_data = asdict(repo_index)
                        index_data.pop("analysis_metadata", None)
                        json.dump(
                            index_data, f, indent=json_indent, ensure_ascii=ensure_ascii
                        )

                output_files[repo_index.repo_name] = str(output_file)
                self.logger.info(
                    f"Saved index for {repo_index.repo_name} to {output_file}"
                )

                # Collect statistics for report
                if self.generate_statistics:
                    stats = self._extract_repository_statistics(repo_index)
                    statistics_data.append(stats)

            except Exception as e:
                self.logger.error(f"Failed to process repository {repo_dir.name}: {e}")
                continue

        # Generate additional reports if configured
        if self.generate_summary:
            summary_path = self.generate_summary_report(output_files)
            self.logger.info(f"Generated summary report: {summary_path}")

        if self.generate_statistics:
            stats_path = self.generate_statistics_report(statistics_data)
            self.logger.info(f"Generated statistics report: {stats_path}")

        return output_files

    def _extract_repository_statistics(self, repo_index: RepoIndex) -> Dict[str, Any]:
        """Extract statistical information from a repository index"""
        metadata = repo_index.analysis_metadata

        # Count relationship types
        relationship_type_counts = {}
        for rel in repo_index.relationships:
            rel_type = rel.relationship_type
            relationship_type_counts[rel_type] = (
                relationship_type_counts.get(rel_type, 0) + 1
            )

        # Count file types
        file_type_counts = {}
        for file_summary in repo_index.file_summaries:
            file_type = file_summary.file_type
            file_type_counts[file_type] = file_type_counts.get(file_type, 0) + 1

        # Calculate statistics
        total_lines = sum(fs.lines_of_code for fs in repo_index.file_summaries)
        avg_lines = (
            total_lines / len(repo_index.file_summaries)
            if repo_index.file_summaries
            else 0
        )

        avg_confidence = (
            sum(r.confidence_score for r in repo_index.relationships)
            / len(repo_index.relationships)
            if repo_index.relationships
            else 0
        )

        return {
            "repo_name": repo_index.repo_name,
            "total_files": repo_index.total_files,
            "analyzed_files": len(repo_index.file_summaries),
            "total_relationships": len(repo_index.relationships),
            "high_confidence_relationships": metadata.get(
                "high_confidence_relationships", 0
            ),
            "relationship_type_counts": relationship_type_counts,
            "file_type_counts": file_type_counts,
            "total_lines_of_code": total_lines,
            "average_lines_per_file": round(avg_lines, 2),
            "average_confidence_score": round(avg_confidence, 3),
            "filtering_efficiency": metadata.get("filtering_efficiency", 0),
            "concurrent_analysis_used": metadata.get("concurrent_analysis_used", False),
            "cache_hits": metadata.get("cache_hits", 0),
            "analysis_date": metadata.get("analysis_date", "unknown"),
        }

    def generate_statistics_report(self, statistics_data: List[Dict[str, Any]]) -> str:
        """Generate a detailed statistics report"""
        stats_path = self.output_dir / self.stats_filename

        # Calculate aggregate statistics
        total_repos = len(statistics_data)
        total_files_analyzed = sum(stat["analyzed_files"] for stat in statistics_data)
        total_relationships = sum(
            stat["total_relationships"] for stat in statistics_data
        )
        total_lines = sum(stat["total_lines_of_code"] for stat in statistics_data)

        # Aggregate relationship types
        aggregated_rel_types = {}
        for stat in statistics_data:
            for rel_type, count in stat["relationship_type_counts"].items():
                aggregated_rel_types[rel_type] = (
                    aggregated_rel_types.get(rel_type, 0) + count
                )

        # Aggregate file types
        aggregated_file_types = {}
        for stat in statistics_data:
            for file_type, count in stat["file_type_counts"].items():
                aggregated_file_types[file_type] = (
                    aggregated_file_types.get(file_type, 0) + count
                )

        # Calculate averages
        avg_files_per_repo = total_files_analyzed / total_repos if total_repos else 0
        avg_relationships_per_repo = (
            total_relationships / total_repos if total_repos else 0
        )
        avg_lines_per_repo = total_lines / total_repos if total_repos else 0

        # Build statistics report
        statistics_report = {
            "report_generation_time": datetime.now().isoformat(),
            "analyzer_version": "1.4.0",
            "configuration_used": {
                "config_file": self.indexer_config_path,
                "concurrent_analysis_enabled": self.enable_concurrent_analysis,
                "content_caching_enabled": self.enable_content_caching,
                "pre_filtering_enabled": self.enable_pre_filtering,
                "min_confidence_score": self.min_confidence_score,
                "high_confidence_threshold": self.high_confidence_threshold,
            },
            "aggregate_statistics": {
                "total_repositories_processed": total_repos,
                "total_files_analyzed": total_files_analyzed,
                "total_relationships_found": total_relationships,
                "total_lines_of_code": total_lines,
                "average_files_per_repository": round(avg_files_per_repo, 2),
                "average_relationships_per_repository": round(
                    avg_relationships_per_repo, 2
                ),
                "average_lines_per_repository": round(avg_lines_per_repo, 2),
            },
            "relationship_type_distribution": aggregated_rel_types,
            "file_type_distribution": aggregated_file_types,
            "repository_details": statistics_data,
            "performance_metrics": {
                "concurrent_processing_repos": sum(
                    1
                    for s in statistics_data
                    if s.get("concurrent_analysis_used", False)
                ),
                "cache_efficiency": {
                    "total_cache_hits": sum(
                        s.get("cache_hits", 0) for s in statistics_data
                    ),
                    "repositories_with_caching": sum(
                        1 for s in statistics_data if s.get("cache_hits", 0) > 0
                    ),
                },
                "filtering_efficiency": {
                    "average_filtering_efficiency": round(
                        sum(s.get("filtering_efficiency", 0) for s in statistics_data)
                        / total_repos,
                        2,
                    )
                    if total_repos
                    else 0,
                    "max_filtering_efficiency": max(
                        (s.get("filtering_efficiency", 0) for s in statistics_data),
                        default=0,
                    ),
                    "min_filtering_efficiency": min(
                        (s.get("filtering_efficiency", 0) for s in statistics_data),
                        default=0,
                    ),
                },
            },
        }

        # Get output configuration
        output_config = self.indexer_config.get("output", {})
        json_indent = output_config.get("json_indent", 2)
        ensure_ascii = not output_config.get("ensure_ascii", False)

        with open(stats_path, "w", encoding="utf-8") as f:
            json.dump(
                statistics_report, f, indent=json_indent, ensure_ascii=ensure_ascii
            )

        return str(stats_path)

    def generate_summary_report(self, output_files: Dict[str, str]) -> str:
        """Generate a summary report of all indexes created"""
        report_path = self.output_dir / "indexing_summary.json"

        # Get output configuration from config file
        output_config = self.indexer_config.get("output", {})
        json_indent = output_config.get("json_indent", 2)
        ensure_ascii = not output_config.get("ensure_ascii", False)

        summary_data = {
            "indexing_completion_time": datetime.now().isoformat(),
            "total_repositories_processed": len(output_files),
            "output_files": output_files,
            "target_structure": self.target_structure,
            "code_base_path": str(self.code_base_path),
            "configuration": {
                "config_file_used": self.indexer_config_path,
                "api_config_file": self.config_path,
                "pre_filtering_enabled": self.enable_pre_filtering,
                "min_confidence_score": self.min_confidence_score,
                "high_confidence_threshold": self.high_confidence_threshold,
                "max_file_size": self.max_file_size,
                "max_content_length": self.max_content_length,
                "request_delay": self.request_delay,
                "supported_extensions_count": len(self.supported_extensions),
                "skip_directories_count": len(self.skip_directories),
            },
        }

        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, indent=json_indent, ensure_ascii=ensure_ascii)

        return str(report_path)


async def main():
    """Main function to run the code indexer with full configuration support"""

    # Configuration - can be overridden by config file
    config_file = "DeepCode/tools/indexer_config.yaml"
    api_config_file = "DeepCode/mcp_agent.secrets.yaml"

    # You can override these parameters or let them be read from config
    code_base_path = "DeepCode/deepcode_lab/papers/1/code_base/"  # Will use config file value if None
    output_dir = (
        "DeepCode/deepcode_lab/papers/1/indexes/"  # Will use config file value if None
    )

    # Target structure - this should be customized for your specific project
    target_structure = """
    project/
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ core/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gcn.py        # GCN encoder
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diffusion.py  # forward/reverse processes
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ denoiser.py   # denoising MLP
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fusion.py     # fusion combiner
    ‚îÇ   ‚îú‚îÄ‚îÄ models/           # model wrapper classes
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recdiff.py
    ‚îÇ   ‚îú‚îÄ‚îÄ utils/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.py       # loading & preprocessing
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predictor.py  # scoring functions
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loss.py       # loss functions
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py    # NDCG, Recall etc.
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sched.py      # beta/alpha schedule utils
    ‚îÇ   ‚îî‚îÄ‚îÄ configs/
    ‚îÇ       ‚îî‚îÄ‚îÄ default.yaml  # hyperparameters, paths
    ‚îú‚îÄ‚îÄ tests/
    ‚îÇ   ‚îú‚îÄ‚îÄ test_gcn.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_diffusion.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_denoiser.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_loss.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_pipeline.py
    ‚îú‚îÄ‚îÄ docs/
    ‚îÇ   ‚îú‚îÄ‚îÄ architecture.md
    ‚îÇ   ‚îú‚îÄ‚îÄ api_reference.md
    ‚îÇ   ‚îî‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ experiments/
    ‚îÇ   ‚îú‚îÄ‚îÄ run_experiment.py
    ‚îÇ   ‚îî‚îÄ‚îÄ notebooks/
    ‚îÇ       ‚îî‚îÄ‚îÄ analysis.ipynb
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îî‚îÄ‚îÄ setup.py
    """

    print("üöÄ Starting Code Indexer with Enhanced Configuration Support")
    print(f"üìã Configuration file: {config_file}")
    print(f"üîë API configuration file: {api_config_file}")

    # Create indexer with full configuration support
    try:
        indexer = CodeIndexer(
            code_base_path=code_base_path,  # None = read from config
            target_structure=target_structure,  # Required - project specific
            output_dir=output_dir,  # None = read from config
            config_path=api_config_file,  # API configuration file
            indexer_config_path=config_file,  # Configuration file
            enable_pre_filtering=True,  # Can be overridden in config
        )

        # Display configuration information
        print(f"üìÅ Code base path: {indexer.code_base_path}")
        print(f"üìÇ Output directory: {indexer.output_dir}")
        print(
            f"ü§ñ Default models: Anthropic={indexer.default_models['anthropic']}, OpenAI={indexer.default_models['openai']}"
        )
        print(f"üîß Preferred LLM: {get_preferred_llm_class(api_config_file).__name__}")
        print(
            f"‚ö° Concurrent analysis: {'enabled' if indexer.enable_concurrent_analysis else 'disabled'}"
        )
        print(
            f"üóÑÔ∏è  Content caching: {'enabled' if indexer.enable_content_caching else 'disabled'}"
        )
        print(
            f"üîç Pre-filtering: {'enabled' if indexer.enable_pre_filtering else 'disabled'}"
        )
        print(f"üêõ Debug mode: {'enabled' if indexer.verbose_output else 'disabled'}")
        print(
            f"üé≠ Mock responses: {'enabled' if indexer.mock_llm_responses else 'disabled'}"
        )

        # Validate configuration
        if not indexer.code_base_path.exists():
            raise FileNotFoundError(
                f"Code base path does not exist: {indexer.code_base_path}"
            )

        if not target_structure:
            raise ValueError("Target structure is required for analysis")

        print("\nüîß Starting indexing process...")

        # Build all indexes
        output_files = await indexer.build_all_indexes()

        # Display results
        print("\n‚úÖ Indexing completed successfully!")
        print(f"üìä Processed {len(output_files)} repositories")
        print("üìÅ Output files:")
        for repo_name, file_path in output_files.items():
            print(f"   - {repo_name}: {file_path}")

        # Display additional reports generated
        if indexer.generate_summary:
            summary_file = indexer.output_dir / indexer.summary_filename
            if summary_file.exists():
                print(f"üìã Summary report: {summary_file}")

        if indexer.generate_statistics:
            stats_file = indexer.output_dir / indexer.stats_filename
            if stats_file.exists():
                print(f"üìà Statistics report: {stats_file}")

        # Performance information
        if indexer.enable_content_caching and indexer.content_cache:
            print(f"üóÑÔ∏è  Cache performance: {len(indexer.content_cache)} items cached")

        print("\nüéâ Code indexing process completed successfully!")

    except FileNotFoundError as e:
        print(f"‚ùå File not found error: {e}")
        print("üí° Please check your configuration file paths")
    except ValueError as e:
        print(f"‚ùå Configuration error: {e}")
        print("üí° Please check your configuration file settings")
    except Exception as e:
        print(f"‚ùå Indexing failed: {e}")
        print("üí° Check the logs for more details")

        # Print debug information if available
        try:
            indexer
            if indexer.verbose_output:
                import traceback

                print("\nüêõ Debug information:")
                traceback.print_exc()
        except NameError:
            pass


def print_usage_example():
    """Print usage examples for different scenarios"""
    print("""
    üìñ Code Indexer Usage Examples:

    1. Basic usage with config file:
       - Update paths in indexer_config.yaml
       - Run: python code_indexer.py

    2. Enable debugging:
       - Set debug.verbose_output: true in config
       - Set debug.save_raw_responses: true to save LLM responses

    3. Enable concurrent processing:
       - Set performance.enable_concurrent_analysis: true
       - Adjust performance.max_concurrent_files as needed

    4. Enable caching:
       - Set performance.enable_content_caching: true
       - Adjust performance.max_cache_size as needed

    5. Mock mode for testing:
       - Set debug.mock_llm_responses: true
       - No API calls will be made

    6. Custom output:
       - Modify output.index_filename_pattern
       - Set output.generate_statistics: true for detailed reports

    üìã Configuration file location: tools/indexer_config.yaml
    """)


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] in ["--help", "-h", "help"]:
        print_usage_example()
    else:
        asyncio.run(main())


--- tools/command_executor.py ---
#!/usr/bin/env python3
"""
Command Executor MCP Tool / ÂëΩ‰ª§ÊâßË°åÂô® MCP Â∑•ÂÖ∑

‰∏ìÈó®Ë¥üË¥£ÊâßË°åLLMÁîüÊàêÁöÑshellÂëΩ‰ª§Êù•ÂàõÂª∫Êñá‰ª∂Ê†ëÁªìÊûÑ
Specialized in executing LLM-generated shell commands to create file tree structures
"""

import subprocess
from pathlib import Path
from typing import List, Dict
from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio

# ÂàõÂª∫MCPÊúçÂä°Âô®ÂÆû‰æã / Create MCP server instance
app = Server("command-executor")


@app.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """
    ÂàóÂá∫ÂèØÁî®Â∑•ÂÖ∑ / List available tools
    """
    return [
        types.Tool(
            name="execute_commands",
            description="""
            ÊâßË°åshellÂëΩ‰ª§ÂàóË°®Êù•ÂàõÂª∫Êñá‰ª∂Ê†ëÁªìÊûÑ
            Execute shell command list to create file tree structure

            Args:
                commands: Ë¶ÅÊâßË°åÁöÑshellÂëΩ‰ª§ÂàóË°®ÔºàÊØèË°å‰∏Ä‰∏™ÂëΩ‰ª§Ôºâ
                working_directory: ÊâßË°åÂëΩ‰ª§ÁöÑÂ∑•‰ΩúÁõÆÂΩï

            Returns:
                ÂëΩ‰ª§ÊâßË°åÁªìÊûúÂíåËØ¶ÁªÜÊä•Âëä
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "commands": {
                        "type": "string",
                        "title": "Commands",
                        "description": "Ë¶ÅÊâßË°åÁöÑshellÂëΩ‰ª§ÂàóË°®ÔºåÊØèË°å‰∏Ä‰∏™ÂëΩ‰ª§",
                    },
                    "working_directory": {
                        "type": "string",
                        "title": "Working Directory",
                        "description": "ÊâßË°åÂëΩ‰ª§ÁöÑÂ∑•‰ΩúÁõÆÂΩï",
                    },
                },
                "required": ["commands", "working_directory"],
            },
        ),
        types.Tool(
            name="execute_single_command",
            description="""
            ÊâßË°åÂçï‰∏™shellÂëΩ‰ª§
            Execute single shell command

            Args:
                command: Ë¶ÅÊâßË°åÁöÑÂçï‰∏™ÂëΩ‰ª§
                working_directory: ÊâßË°åÂëΩ‰ª§ÁöÑÂ∑•‰ΩúÁõÆÂΩï

            Returns:
                ÂëΩ‰ª§ÊâßË°åÁªìÊûú
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "title": "Command",
                        "description": "Ë¶ÅÊâßË°åÁöÑÂçï‰∏™shellÂëΩ‰ª§",
                    },
                    "working_directory": {
                        "type": "string",
                        "title": "Working Directory",
                        "description": "ÊâßË°åÂëΩ‰ª§ÁöÑÂ∑•‰ΩúÁõÆÂΩï",
                    },
                },
                "required": ["command", "working_directory"],
            },
        ),
    ]


@app.call_tool()
async def handle_call_tool(name: str, arguments: dict) -> list[types.TextContent]:
    """
    Â§ÑÁêÜÂ∑•ÂÖ∑Ë∞ÉÁî® / Handle tool calls
    """
    try:
        if name == "execute_commands":
            return await execute_command_batch(
                arguments.get("commands", ""), arguments.get("working_directory", ".")
            )
        elif name == "execute_single_command":
            return await execute_single_command(
                arguments.get("command", ""), arguments.get("working_directory", ".")
            )
        else:
            raise ValueError(f"Êú™Áü•Â∑•ÂÖ∑ / Unknown tool: {name}")

    except Exception as e:
        return [
            types.TextContent(
                type="text",
                text=f"Â∑•ÂÖ∑ÊâßË°åÈîôËØØ / Error executing tool {name}: {str(e)}",
            )
        ]


async def execute_command_batch(
    commands: str, working_directory: str
) -> list[types.TextContent]:
    """
    ÊâßË°åÂ§ö‰∏™shellÂëΩ‰ª§ / Execute multiple shell commands

    Args:
        commands: ÂëΩ‰ª§ÂàóË°®ÔºåÊØèË°å‰∏Ä‰∏™ÂëΩ‰ª§ / Command list, one command per line
        working_directory: Â∑•‰ΩúÁõÆÂΩï / Working directory

    Returns:
        ÊâßË°åÁªìÊûú / Execution results
    """
    try:
        # Á°Æ‰øùÂ∑•‰ΩúÁõÆÂΩïÂ≠òÂú® / Ensure working directory exists
        Path(working_directory).mkdir(parents=True, exist_ok=True)

        # ÂàÜÂâ≤ÂëΩ‰ª§Ë°å / Split command lines
        command_lines = [
            cmd.strip() for cmd in commands.strip().split("\n") if cmd.strip()
        ]

        if not command_lines:
            return [
                types.TextContent(
                    type="text", text="Ê≤°ÊúâÊèê‰æõÊúâÊïàÂëΩ‰ª§ / No valid commands provided"
                )
            ]

        results = []
        stats = {"successful": 0, "failed": 0, "timeout": 0}

        for i, command in enumerate(command_lines, 1):
            try:
                # ÊâßË°åÂëΩ‰ª§ / Execute command
                result = subprocess.run(
                    command,
                    shell=True,
                    cwd=working_directory,
                    capture_output=True,
                    text=True,
                    timeout=30,  # 30ÁßíË∂ÖÊó∂
                )

                if result.returncode == 0:
                    results.append(f"‚úÖ Command {i}: {command}")
                    if result.stdout.strip():
                        results.append(f"   ËæìÂá∫ / Output: {result.stdout.strip()}")
                    stats["successful"] += 1
                else:
                    results.append(f"‚ùå Command {i}: {command}")
                    if result.stderr.strip():
                        results.append(f"   ÈîôËØØ / Error: {result.stderr.strip()}")
                    stats["failed"] += 1

            except subprocess.TimeoutExpired:
                results.append(f"‚è±Ô∏è Command {i} Ë∂ÖÊó∂ / timeout: {command}")
                stats["timeout"] += 1
            except Exception as e:
                results.append(f"üí• Command {i} ÂºÇÂ∏∏ / exception: {command} - {str(e)}")
                stats["failed"] += 1

        # ÁîüÊàêÊâßË°åÊä•Âëä / Generate execution report
        summary = generate_execution_summary(working_directory, command_lines, stats)
        final_result = summary + "\n" + "\n".join(results)

        return [types.TextContent(type="text", text=final_result)]

    except Exception as e:
        return [
            types.TextContent(
                type="text",
                text=f"ÊâπÈáèÂëΩ‰ª§ÊâßË°åÂ§±Ë¥• / Failed to execute command batch: {str(e)}",
            )
        ]


async def execute_single_command(
    command: str, working_directory: str
) -> list[types.TextContent]:
    """
    ÊâßË°åÂçï‰∏™shellÂëΩ‰ª§ / Execute single shell command

    Args:
        command: Ë¶ÅÊâßË°åÁöÑÂëΩ‰ª§ / Command to execute
        working_directory: Â∑•‰ΩúÁõÆÂΩï / Working directory

    Returns:
        ÊâßË°åÁªìÊûú / Execution result
    """
    try:
        # Á°Æ‰øùÂ∑•‰ΩúÁõÆÂΩïÂ≠òÂú® / Ensure working directory exists
        Path(working_directory).mkdir(parents=True, exist_ok=True)

        # ÊâßË°åÂëΩ‰ª§ / Execute command
        result = subprocess.run(
            command,
            shell=True,
            cwd=working_directory,
            capture_output=True,
            text=True,
            timeout=30,
        )

        # Ê†ºÂºèÂåñËæìÂá∫ / Format output
        output = format_single_command_result(command, working_directory, result)

        return [types.TextContent(type="text", text=output)]

    except subprocess.TimeoutExpired:
        return [
            types.TextContent(
                type="text", text=f"‚è±Ô∏è ÂëΩ‰ª§Ë∂ÖÊó∂ / Command timeout: {command}"
            )
        ]
    except Exception as e:
        return [
            types.TextContent(
                type="text", text=f"üí• ÂëΩ‰ª§ÊâßË°åÈîôËØØ / Command execution error: {str(e)}"
            )
        ]


def generate_execution_summary(
    working_directory: str, command_lines: List[str], stats: Dict[str, int]
) -> str:
    """
    ÁîüÊàêÊâßË°åÊÄªÁªì / Generate execution summary

    Args:
        working_directory: Â∑•‰ΩúÁõÆÂΩï / Working directory
        command_lines: ÂëΩ‰ª§ÂàóË°® / Command list
        stats: ÁªüËÆ°‰ø°ÊÅØ / Statistics

    Returns:
        Ê†ºÂºèÂåñÁöÑÊÄªÁªì / Formatted summary
    """
    return f"""
ÂëΩ‰ª§ÊâßË°åÊÄªÁªì / Command Execution Summary:
{'='*50}
Â∑•‰ΩúÁõÆÂΩï / Working Directory: {working_directory}
ÊÄªÂëΩ‰ª§Êï∞ / Total Commands: {len(command_lines)}
ÊàêÂäü / Successful: {stats['successful']}
Â§±Ë¥• / Failed: {stats['failed']}
Ë∂ÖÊó∂ / Timeout: {stats['timeout']}

ËØ¶ÁªÜÁªìÊûú / Detailed Results:
{'-'*50}"""


def format_single_command_result(
    command: str, working_directory: str, result: subprocess.CompletedProcess
) -> str:
    """
    Ê†ºÂºèÂåñÂçïÂëΩ‰ª§ÊâßË°åÁªìÊûú / Format single command execution result

    Args:
        command: ÊâßË°åÁöÑÂëΩ‰ª§ / Executed command
        working_directory: Â∑•‰ΩúÁõÆÂΩï / Working directory
        result: ÊâßË°åÁªìÊûú / Execution result

    Returns:
        Ê†ºÂºèÂåñÁöÑÁªìÊûú / Formatted result
    """
    output = f"""
ÂçïÂëΩ‰ª§ÊâßË°å / Single Command Execution:
{'='*40}
Â∑•‰ΩúÁõÆÂΩï / Working Directory: {working_directory}
ÂëΩ‰ª§ / Command: {command}
ËøîÂõûÁ†Å / Return Code: {result.returncode}

"""

    if result.returncode == 0:
        output += "‚úÖ Áä∂ÊÄÅ / Status: SUCCESS / ÊàêÂäü\n"
        if result.stdout.strip():
            output += f"ËæìÂá∫ / Output:\n{result.stdout.strip()}\n"
    else:
        output += "‚ùå Áä∂ÊÄÅ / Status: FAILED / Â§±Ë¥•\n"
        if result.stderr.strip():
            output += f"ÈîôËØØ / Error:\n{result.stderr.strip()}\n"

    return output


async def main():
    """
    ËøêË°åMCPÊúçÂä°Âô® / Run MCP server
    """
    # ÈÄöËøástdioËøêË°åÊúçÂä°Âô® / Run server via stdio
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await app.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="command-executor",
                server_version="1.0.0",
                capabilities=app.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())


--- tools/document_segmentation_server.py ---
#!/usr/bin/env python3
"""
Document Segmentation MCP Server

This MCP server provides intelligent document segmentation and retrieval functions for handling
large research papers and technical documents that exceed LLM token limits.

==== CORE FUNCTIONALITY ====
1. Analyze document structure and type using semantic content analysis
2. Create intelligent segments based on content semantics, not just structure
3. Provide query-aware segment retrieval with relevance scoring
4. Support both structured (papers with headers) and unstructured documents
5. Configurable segmentation strategies based on document complexity

==== MCP TOOLS PROVIDED ====

üìÑ analyze_and_segment_document(paper_dir: str, force_refresh: bool = False)
   Purpose: Analyzes document structure and creates intelligent segments
   - Detects document type (research paper, technical doc, algorithm-focused, etc.)
   - Selects optimal segmentation strategy based on content analysis
   - Creates semantic segments preserving algorithm and concept integrity
   - Stores segmentation index for efficient retrieval
   - Returns: JSON with segmentation status, strategy used, and segment count

üìñ read_document_segments(paper_dir: str, query_type: str, keywords: List[str] = None,
                         max_segments: int = 3, max_total_chars: int = None)
   Purpose: Intelligently retrieves relevant document segments based on query context
   - query_type: "concept_analysis", "algorithm_extraction", or "code_planning"
   - Uses semantic relevance scoring to rank segments
   - Applies query-specific filtering and keyword matching
   - Dynamically calculates optimal character limits based on content complexity
   - Returns: JSON with selected segments optimized for the specific query type

üìã get_document_overview(paper_dir: str)
   Purpose: Provides high-level overview of document structure and available segments
   - Shows document type and segmentation strategy used
   - Lists all segments with titles, content types, and relevance scores
   - Displays segment statistics (character counts, keyword summaries)
   - Returns: JSON with complete document analysis metadata

==== SEGMENTATION STRATEGIES ====
- semantic_research_focused: For academic papers with complex algorithmic content
- algorithm_preserve_integrity: Maintains algorithm blocks and formula chains intact
- concept_implementation_hybrid: Merges related concepts with implementation details
- semantic_chunking_enhanced: Advanced boundary detection for long documents
- content_aware_segmentation: Adaptive chunking based on content density

==== INTELLIGENT FEATURES ====
- Semantic boundary detection (not just structural)
- Algorithm block identification and preservation
- Formula chain recognition and grouping
- Concept-implementation relationship mapping
- Multi-level relevance scoring (content type, importance, keyword matching)
- Backward compatibility with existing document indexes
- Configurable via mcp_agent.config.yaml (enabled/disabled, size thresholds)

Usage:
python tools/document_segmentation_server.py
"""

import os
import re
import json
import sys
import io
from typing import Dict, List, Tuple
import hashlib
import logging
from datetime import datetime
from dataclasses import dataclass, asdict

# Set standard output encoding to UTF-8
if sys.stdout.encoding != "utf-8":
    try:
        if hasattr(sys.stdout, "reconfigure"):
            sys.stdout.reconfigure(encoding="utf-8")
            sys.stderr.reconfigure(encoding="utf-8")
        else:
            sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding="utf-8")
            sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding="utf-8")
    except Exception as e:
        print(f"Warning: Could not set UTF-8 encoding: {e}")

# Import MCP related modules
from mcp.server.fastmcp import FastMCP

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create FastMCP server instance
mcp = FastMCP("document-segmentation-server")


@dataclass
class DocumentSegment:
    """Represents a document segment with metadata"""

    id: str
    title: str
    content: str
    content_type: str  # "introduction", "methodology", "algorithm", "results", etc.
    keywords: List[str]
    char_start: int
    char_end: int
    char_count: int
    relevance_scores: Dict[str, float]  # Scores for different query types
    section_path: str  # e.g., "3.2.1" for nested sections


@dataclass
class DocumentIndex:
    """Document index containing all segments and metadata"""

    document_path: str
    document_type: str  # "academic_paper", "technical_doc", "code_doc", "general"
    segmentation_strategy: str
    total_segments: int
    total_chars: int
    segments: List[DocumentSegment]
    created_at: str


class DocumentAnalyzer:
    """Enhanced document analyzer using semantic content analysis instead of mechanical structure detection"""

    # More precise semantic indicators, weighted by importance
    ALGORITHM_INDICATORS = {
        "high": [
            "algorithm",
            "procedure",
            "method",
            "approach",
            "technique",
            "framework",
        ],
        "medium": ["step", "process", "implementation", "computation", "calculation"],
        "low": ["example", "illustration", "demonstration"],
    }

    TECHNICAL_CONCEPT_INDICATORS = {
        "high": ["formula", "equation", "theorem", "lemma", "proof", "definition"],
        "medium": ["parameter", "variable", "function", "model", "architecture"],
        "low": ["notation", "symbol", "term"],
    }

    IMPLEMENTATION_INDICATORS = {
        "high": ["code", "implementation", "programming", "software", "system"],
        "medium": ["design", "structure", "module", "component", "interface"],
        "low": ["tool", "library", "package"],
    }

    # Semantic features of document types (not just based on titles)
    RESEARCH_PAPER_PATTERNS = [
        r"(?i)\babstract\b.*?\n.*?(introduction|motivation|background)",
        r"(?i)(methodology|method).*?(experiment|evaluation|result)",
        r"(?i)(conclusion|future work|limitation).*?(reference|bibliography)",
        r"(?i)(related work|literature review|prior art)",
    ]

    TECHNICAL_DOC_PATTERNS = [
        r"(?i)(getting started|installation|setup).*?(usage|example)",
        r"(?i)(api|interface|specification).*?(parameter|endpoint)",
        r"(?i)(tutorial|guide|walkthrough).*?(step|instruction)",
        r"(?i)(troubleshooting|faq|common issues)",
    ]

    def analyze_document_type(self, content: str) -> Tuple[str, float]:
        """
        Enhanced document type analysis based on semantic content patterns

        Returns:
            Tuple[str, float]: (document_type, confidence_score)
        """
        content_lower = content.lower()

        # Calculate weighted semantic indicator scores
        algorithm_score = self._calculate_weighted_score(
            content_lower, self.ALGORITHM_INDICATORS
        )
        concept_score = self._calculate_weighted_score(
            content_lower, self.TECHNICAL_CONCEPT_INDICATORS
        )
        implementation_score = self._calculate_weighted_score(
            content_lower, self.IMPLEMENTATION_INDICATORS
        )

        # Detect semantic patterns of document types
        research_pattern_score = self._detect_pattern_score(
            content, self.RESEARCH_PAPER_PATTERNS
        )
        technical_pattern_score = self._detect_pattern_score(
            content, self.TECHNICAL_DOC_PATTERNS
        )

        # Comprehensive evaluation of document type
        total_research_score = (
            algorithm_score + concept_score + research_pattern_score * 2
        )
        total_technical_score = implementation_score + technical_pattern_score * 2

        # Determine document type based on content density and pattern matching
        if research_pattern_score > 0.5 and total_research_score > 3.0:
            return "research_paper", min(0.95, 0.6 + research_pattern_score * 0.35)
        elif algorithm_score > 2.0 and concept_score > 1.5:
            return "algorithm_focused", 0.85
        elif total_technical_score > 2.5:
            return "technical_doc", 0.8
        elif implementation_score > 1.5:
            return "implementation_guide", 0.75
        else:
            return "general_document", 0.5

    def _calculate_weighted_score(
        self, content: str, indicators: Dict[str, List[str]]
    ) -> float:
        """Calculate weighted semantic indicator scores"""
        score = 0.0
        for weight_level, terms in indicators.items():
            weight = {"high": 3.0, "medium": 2.0, "low": 1.0}[weight_level]
            for term in terms:
                if term in content:
                    score += weight * (
                        content.count(term) * 0.5 + 1
                    )  # Consider term frequency
        return score

    def _detect_pattern_score(self, content: str, patterns: List[str]) -> float:
        """Detect semantic pattern matching scores"""
        matches = 0
        for pattern in patterns:
            if re.search(pattern, content, re.DOTALL):
                matches += 1
        return matches / len(patterns)

    def detect_segmentation_strategy(self, content: str, doc_type: str) -> str:
        """
        Intelligently determine the best segmentation strategy based on content semantics rather than mechanical structure
        """
        # Analyze content characteristics
        algorithm_density = self._calculate_algorithm_density(content)
        concept_complexity = self._calculate_concept_complexity(content)
        implementation_detail_level = self._calculate_implementation_detail_level(
            content
        )

        # Select strategy based on document type and content characteristics
        if doc_type == "research_paper" and algorithm_density > 0.3:
            return "semantic_research_focused"
        elif doc_type == "algorithm_focused" or algorithm_density > 0.5:
            return "algorithm_preserve_integrity"
        elif concept_complexity > 0.4 and implementation_detail_level > 0.3:
            return "concept_implementation_hybrid"
        elif len(content) > 15000:  # Long documents
            return "semantic_chunking_enhanced"
        else:
            return "content_aware_segmentation"

    def _calculate_algorithm_density(self, content: str) -> float:
        """Calculate algorithm content density"""
        total_chars = len(content)
        algorithm_chars = 0

        # Identify algorithm blocks
        algorithm_patterns = [
            r"(?i)(algorithm\s+\d+|procedure\s+\d+)",
            r"(?i)(step\s+\d+|phase\s+\d+)",
            r"(?i)(input:|output:|return:|initialize:)",
            r"(?i)(for\s+each|while|if.*then|else)",
            r"(?i)(function|method|procedure).*\(",
        ]

        for pattern in algorithm_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                # Estimate algorithm block size (expand forward and backward from match point)
                start = max(0, match.start() - 200)
                end = min(len(content), match.end() + 800)
                algorithm_chars += end - start

        return min(1.0, algorithm_chars / total_chars)

    def _calculate_concept_complexity(self, content: str) -> float:
        """Calculate concept complexity"""
        concept_indicators = self.TECHNICAL_CONCEPT_INDICATORS
        complexity_score = 0.0

        for level, terms in concept_indicators.items():
            weight = {"high": 3.0, "medium": 2.0, "low": 1.0}[level]
            for term in terms:
                complexity_score += content.lower().count(term) * weight

        # Normalize to 0-1 range
        return min(1.0, complexity_score / 100)

    def _calculate_implementation_detail_level(self, content: str) -> float:
        """Calculate implementation detail level"""
        implementation_patterns = [
            r"(?i)(code|implementation|programming)",
            r"(?i)(class|function|method|variable)",
            r"(?i)(import|include|library)",
            r"(?i)(parameter|argument|return)",
            r"(?i)(example|demo|tutorial)",
        ]

        detail_score = 0
        for pattern in implementation_patterns:
            detail_score += len(re.findall(pattern, content))

        return min(1.0, detail_score / 50)


class DocumentSegmenter:
    """Creates intelligent segments from documents"""

    def __init__(self):
        self.analyzer = DocumentAnalyzer()

    def segment_document(self, content: str, strategy: str) -> List[DocumentSegment]:
        """
        Perform intelligent segmentation using the specified strategy
        """
        if strategy == "semantic_research_focused":
            return self._segment_research_paper_semantically(content)
        elif strategy == "algorithm_preserve_integrity":
            return self._segment_preserve_algorithm_integrity(content)
        elif strategy == "concept_implementation_hybrid":
            return self._segment_concept_implementation_hybrid(content)
        elif strategy == "semantic_chunking_enhanced":
            return self._segment_by_enhanced_semantic_chunks(content)
        elif strategy == "content_aware_segmentation":
            return self._segment_content_aware(content)
        else:
            # Compatibility with legacy strategies
            return self._segment_by_enhanced_semantic_chunks(content)

    def _segment_by_headers(self, content: str) -> List[DocumentSegment]:
        """Segment document based on markdown headers"""
        segments = []
        lines = content.split("\n")
        current_segment = []
        current_header = None
        char_pos = 0

        for line in lines:
            line_with_newline = line + "\n"

            # Check if line is a header
            header_match = re.match(r"^(#{1,6})\s+(.+)$", line)

            if header_match:
                # Save previous segment if exists
                if current_segment and current_header:
                    segment_content = "\n".join(current_segment).strip()
                    if segment_content:
                        # Analyze content type and importance
                        content_type = self._classify_content_type(
                            current_header, segment_content
                        )
                        importance_score = (
                            0.8 if content_type in ["algorithm", "formula"] else 0.7
                        )

                        segment = self._create_enhanced_segment(
                            segment_content,
                            current_header,
                            char_pos - len(segment_content.encode("utf-8")),
                            char_pos,
                            importance_score,
                            content_type,
                        )
                        segments.append(segment)

                # Start new segment
                current_header = header_match.group(2).strip()
                current_segment = [line]
            else:
                if current_segment is not None:
                    current_segment.append(line)

            char_pos += len(line_with_newline.encode("utf-8"))

        # Add final segment
        if current_segment and current_header:
            segment_content = "\n".join(current_segment).strip()
            if segment_content:
                # Analyze content type and importance
                content_type = self._classify_content_type(
                    current_header, segment_content
                )
                importance_score = (
                    0.8 if content_type in ["algorithm", "formula"] else 0.7
                )

                segment = self._create_enhanced_segment(
                    segment_content,
                    current_header,
                    char_pos - len(segment_content.encode("utf-8")),
                    char_pos,
                    importance_score,
                    content_type,
                )
                segments.append(segment)

        return segments

    def _segment_preserve_algorithm_integrity(
        self, content: str
    ) -> List[DocumentSegment]:
        """Smart segmentation strategy that preserves algorithm integrity"""
        segments = []

        # 1. Identify algorithm blocks and related descriptions
        algorithm_blocks = self._identify_algorithm_blocks(content)

        # 2. Identify concept definition groups
        concept_groups = self._identify_concept_groups(content)

        # 3. Identify formula derivation chains
        formula_chains = self._identify_formula_chains(content)

        # 4. Merge related content blocks to ensure integrity
        content_blocks = self._merge_related_content_blocks(
            algorithm_blocks, concept_groups, formula_chains, content
        )

        # 5. Convert to DocumentSegment
        for i, block in enumerate(content_blocks):
            segment = self._create_enhanced_segment(
                block["content"],
                block["title"],
                block["start_pos"],
                block["end_pos"],
                block["importance_score"],
                block["content_type"],
            )
            segments.append(segment)

        return segments

    def _segment_research_paper_semantically(
        self, content: str
    ) -> List[DocumentSegment]:
        """Semantic segmentation specifically for research papers"""
        segments = []

        # Identify semantic structure of research papers
        paper_sections = self._identify_research_paper_sections(content)

        for section in paper_sections:
            # Ensure each section contains sufficient context
            enhanced_content = self._enhance_section_with_context(section, content)

            segment = self._create_enhanced_segment(
                enhanced_content["content"],
                enhanced_content["title"],
                enhanced_content["start_pos"],
                enhanced_content["end_pos"],
                enhanced_content["importance_score"],
                enhanced_content["content_type"],
            )
            segments.append(segment)

        return segments

    def _segment_concept_implementation_hybrid(
        self, content: str
    ) -> List[DocumentSegment]:
        """Intelligent segmentation combining concepts and implementation"""
        segments = []

        # Identify concept-implementation correspondence
        concept_impl_pairs = self._identify_concept_implementation_pairs(content)

        for pair in concept_impl_pairs:
            # Merge related concepts and implementations into one segment
            merged_content = self._merge_concept_with_implementation(pair, content)

            segment = self._create_enhanced_segment(
                merged_content["content"],
                merged_content["title"],
                merged_content["start_pos"],
                merged_content["end_pos"],
                merged_content["importance_score"],
                merged_content["content_type"],
            )
            segments.append(segment)

        return segments

    def _segment_by_enhanced_semantic_chunks(
        self, content: str
    ) -> List[DocumentSegment]:
        """Enhanced semantic chunk segmentation"""
        segments = []

        # Use improved semantic boundary detection
        semantic_boundaries = self._detect_semantic_boundaries(content)

        current_start = 0
        for i, boundary in enumerate(semantic_boundaries):
            chunk_content = content[current_start : boundary["position"]]

            if len(chunk_content.strip()) > 200:  # Minimum content threshold
                segment = self._create_enhanced_segment(
                    chunk_content,
                    boundary["suggested_title"],
                    current_start,
                    boundary["position"],
                    boundary["importance_score"],
                    boundary["content_type"],
                )
                segments.append(segment)

            current_start = boundary["position"]

        # Handle the final segment
        if current_start < len(content):
            final_content = content[current_start:]
            if len(final_content.strip()) > 200:
                segment = self._create_enhanced_segment(
                    final_content,
                    "Final Section",
                    current_start,
                    len(content),
                    0.7,
                    "general",
                )
                segments.append(segment)

        return segments

    def _segment_content_aware(self, content: str) -> List[DocumentSegment]:
        """Content-aware intelligent segmentation"""
        segments = []

        # Adaptive segmentation size
        optimal_chunk_size = self._calculate_optimal_chunk_size(content)

        # Segment based on content density
        content_chunks = self._create_content_aware_chunks(content, optimal_chunk_size)

        for chunk in content_chunks:
            segment = self._create_enhanced_segment(
                chunk["content"],
                chunk["title"],
                chunk["start_pos"],
                chunk["end_pos"],
                chunk["importance_score"],
                chunk["content_type"],
            )
            segments.append(segment)

        return segments

    def _segment_academic_paper(self, content: str) -> List[DocumentSegment]:
        """Segment academic paper using semantic understanding"""
        # First try header-based segmentation
        headers = re.findall(r"^(#{1,6})\s+(.+)$", content, re.MULTILINE)
        if len(headers) >= 2:
            return self._segment_by_headers(content)

        # Fallback to semantic detection of academic sections
        sections = self._detect_academic_sections(content)
        segments = []

        for section in sections:
            # Determine importance based on section type
            section_type = section.get("type", "general")
            content_type = (
                section_type
                if section_type
                in ["algorithm", "formula", "introduction", "conclusion"]
                else "general"
            )
            importance_score = {
                "algorithm": 0.95,
                "formula": 0.9,
                "introduction": 0.85,
                "conclusion": 0.8,
            }.get(content_type, 0.7)

            segment = self._create_enhanced_segment(
                section["content"],
                section["title"],
                section["start_pos"],
                section["end_pos"],
                importance_score,
                content_type,
            )
            segments.append(segment)

        return segments

    def _detect_academic_sections(self, content: str) -> List[Dict]:
        """Detect academic paper sections even without clear headers"""
        sections = []

        # Common academic section patterns
        section_patterns = [
            (r"(?i)(abstract|ÊëòË¶Å)", "introduction"),
            (r"(?i)(introduction|ÂºïË®Ä|ÁÆÄ‰ªã)", "introduction"),
            (r"(?i)(related work|Áõ∏ÂÖ≥Â∑•‰Ωú|ËÉåÊôØ)", "background"),
            (r"(?i)(method|methodology|approach|ÊñπÊ≥ï)", "methodology"),
            (r"(?i)(algorithm|ÁÆóÊ≥ï)", "algorithm"),
            (r"(?i)(experiment|ÂÆûÈ™å|evaluation|ËØÑ‰º∞)", "experiment"),
            (r"(?i)(result|ÁªìÊûú|finding)", "results"),
            (r"(?i)(conclusion|ÁªìËÆ∫|ÊÄªÁªì)", "conclusion"),
            (r"(?i)(reference|ÂèÇËÄÉÊñáÁåÆ|bibliography)", "references"),
        ]

        current_pos = 0
        for i, (pattern, section_type) in enumerate(section_patterns):
            match = re.search(pattern, content[current_pos:], re.IGNORECASE)
            if match:
                start_pos = current_pos + match.start()

                # Find end position (next section or end of document)
                next_pos = len(content)
                for next_pattern, _ in section_patterns[i + 1 :]:
                    next_match = re.search(
                        next_pattern, content[start_pos + 100 :], re.IGNORECASE
                    )
                    if next_match:
                        next_pos = start_pos + 100 + next_match.start()
                        break

                section_content = content[start_pos:next_pos].strip()
                if len(section_content) > 50:  # Minimum content length
                    # Calculate importance score and content type
                    importance_score = self._calculate_paragraph_importance(
                        section_content, section_type
                    )
                    content_type = self._classify_content_type(
                        match.group(1), section_content
                    )

                    sections.append(
                        {
                            "title": match.group(1),
                            "content": section_content,
                            "start_pos": start_pos,
                            "end_pos": next_pos,
                            "type": section_type,
                            "importance_score": importance_score,
                            "content_type": content_type,
                        }
                    )

                current_pos = next_pos

        return sections

    def _segment_by_semantic_chunks(self, content: str) -> List[DocumentSegment]:
        """Segment long documents into semantic chunks"""
        # Split into paragraphs first
        paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]

        segments = []
        current_chunk = []
        current_chunk_size = 0
        chunk_size_limit = 3000  # characters
        overlap_size = 200

        char_pos = 0

        for para in paragraphs:
            para_size = len(para)

            # If adding this paragraph exceeds limit, create a segment
            if current_chunk_size + para_size > chunk_size_limit and current_chunk:
                chunk_content = "\n\n".join(current_chunk)
                # Analyze semantic chunk content type
                content_type = self._classify_paragraph_type(chunk_content)
                importance_score = self._calculate_paragraph_importance(
                    chunk_content, content_type
                )

                segment = self._create_enhanced_segment(
                    chunk_content,
                    f"Section {len(segments) + 1}",
                    char_pos - len(chunk_content.encode("utf-8")),
                    char_pos,
                    importance_score,
                    content_type,
                )
                segments.append(segment)

                # Keep last part for overlap
                overlap_content = (
                    chunk_content[-overlap_size:]
                    if len(chunk_content) > overlap_size
                    else ""
                )
                current_chunk = [overlap_content, para] if overlap_content else [para]
                current_chunk_size = len(overlap_content) + para_size
            else:
                current_chunk.append(para)
                current_chunk_size += para_size

            char_pos += para_size + 2  # +2 for \n\n

        # Add final chunk
        if current_chunk:
            chunk_content = "\n\n".join(current_chunk)
            # Analyze final chunk content type
            content_type = self._classify_paragraph_type(chunk_content)
            importance_score = self._calculate_paragraph_importance(
                chunk_content, content_type
            )

            segment = self._create_enhanced_segment(
                chunk_content,
                f"Section {len(segments) + 1}",
                char_pos - len(chunk_content.encode("utf-8")),
                char_pos,
                importance_score,
                content_type,
            )
            segments.append(segment)

        return segments

    def _segment_by_paragraphs(self, content: str) -> List[DocumentSegment]:
        """Simple paragraph-based segmentation for short documents"""
        paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]
        segments = []
        char_pos = 0

        for i, para in enumerate(paragraphs):
            if len(para) > 100:  # Only include substantial paragraphs
                # Analyze paragraph type and importance
                content_type = self._classify_paragraph_type(para)
                importance_score = self._calculate_paragraph_importance(
                    para, content_type
                )

                segment = self._create_enhanced_segment(
                    para,
                    f"Paragraph {i + 1}",
                    char_pos,
                    char_pos + len(para.encode("utf-8")),
                    importance_score,
                    content_type,
                )
                segments.append(segment)
            char_pos += len(para.encode("utf-8")) + 2

        return segments

    # =============== Enhanced intelligent segmentation helper methods ===============

    def _identify_algorithm_blocks(self, content: str) -> List[Dict]:
        """Identify algorithm blocks and related descriptions"""
        algorithm_blocks = []

        # Algorithm block identification patterns
        algorithm_patterns = [
            r"(?i)(algorithm\s+\d+|procedure\s+\d+|method\s+\d+).*?(?=algorithm\s+\d+|procedure\s+\d+|method\s+\d+|$)",
            r"(?i)(input:|output:|returns?:|require:|ensure:).*?(?=\n\s*\n|\n\s*(?:input:|output:|returns?:|require:|ensure:)|$)",
            r"(?i)(for\s+each|while|if.*then|repeat.*until).*?(?=\n\s*\n|$)",
            r"(?i)(step\s+\d+|phase\s+\d+).*?(?=step\s+\d+|phase\s+\d+|\n\s*\n|$)",
        ]

        for pattern in algorithm_patterns:
            matches = re.finditer(pattern, content, re.DOTALL)
            for match in matches:
                # Expand context to include complete descriptions
                start = max(0, match.start() - 300)
                end = min(len(content), match.end() + 500)

                # Find natural boundaries
                while start > 0 and content[start] not in "\n.!?":
                    start -= 1
                while end < len(content) and content[end] not in "\n.!?":
                    end += 1

                algorithm_blocks.append(
                    {
                        "start_pos": start,
                        "end_pos": end,
                        "content": content[start:end].strip(),
                        "title": self._extract_algorithm_title(
                            content[match.start() : match.end()]
                        ),
                        "importance_score": 0.95,  # High importance for algorithm blocks
                        "content_type": "algorithm",
                    }
                )

        return algorithm_blocks

    def _identify_concept_groups(self, content: str) -> List[Dict]:
        """Identify concept definition groups"""
        concept_groups = []

        # Concept definition patterns
        concept_patterns = [
            r"(?i)(definition|define|let|denote|given).*?(?=\n\s*\n|definition|define|let|denote|$)",
            r"(?i)(theorem|lemma|proposition|corollary).*?(?=\n\s*\n|theorem|lemma|proposition|corollary|$)",
            r"(?i)(notation|symbol|parameter).*?(?=\n\s*\n|notation|symbol|parameter|$)",
        ]

        for pattern in concept_patterns:
            matches = re.finditer(pattern, content, re.DOTALL)
            for match in matches:
                # Expand context
                start = max(0, match.start() - 200)
                end = min(len(content), match.end() + 300)

                concept_groups.append(
                    {
                        "start_pos": start,
                        "end_pos": end,
                        "content": content[start:end].strip(),
                        "title": self._extract_concept_title(
                            content[match.start() : match.end()]
                        ),
                        "importance_score": 0.85,
                        "content_type": "concept",
                    }
                )

        return concept_groups

    def _identify_formula_chains(self, content: str) -> List[Dict]:
        """Identify formula derivation chains"""
        formula_chains = []

        # Formula patterns
        formula_patterns = [
            r"\$\$.*?\$\$",  # Block-level mathematical formulas
            r"\$[^$]+\$",  # Inline mathematical formulas
            r"(?i)(equation|formula).*?(?=\n\s*\n|equation|formula|$)",
            r"(?i)(where|such that|given that).*?(?=\n\s*\n|where|such that|given that|$)",
        ]

        # Find dense formula regions
        formula_positions = []
        for pattern in formula_patterns:
            matches = re.finditer(pattern, content, re.DOTALL)
            for match in matches:
                formula_positions.append((match.start(), match.end()))

        # Merge nearby formulas into formula chains
        formula_positions.sort()
        if formula_positions:
            current_chain_start = formula_positions[0][0]
            current_chain_end = formula_positions[0][1]

            for start, end in formula_positions[1:]:
                if (
                    start - current_chain_end < 500
                ):  # Merge formulas within 500 characters
                    current_chain_end = end
                else:
                    # Save current chain
                    formula_chains.append(
                        {
                            "start_pos": max(0, current_chain_start - 200),
                            "end_pos": min(len(content), current_chain_end + 200),
                            "content": content[
                                max(0, current_chain_start - 200) : min(
                                    len(content), current_chain_end + 200
                                )
                            ].strip(),
                            "title": "Mathematical Formulation",
                            "importance_score": 0.9,
                            "content_type": "formula",
                        }
                    )
                    current_chain_start = start
                    current_chain_end = end

            # Add the last chain
            formula_chains.append(
                {
                    "start_pos": max(0, current_chain_start - 200),
                    "end_pos": min(len(content), current_chain_end + 200),
                    "content": content[
                        max(0, current_chain_start - 200) : min(
                            len(content), current_chain_end + 200
                        )
                    ].strip(),
                    "title": "Mathematical Formulation",
                    "importance_score": 0.9,
                    "content_type": "formula",
                }
            )

        return formula_chains

    def _merge_related_content_blocks(
        self,
        algorithm_blocks: List[Dict],
        concept_groups: List[Dict],
        formula_chains: List[Dict],
        content: str,
    ) -> List[Dict]:
        """Merge related content blocks to ensure integrity"""
        all_blocks = algorithm_blocks + concept_groups + formula_chains
        all_blocks.sort(key=lambda x: x["start_pos"])

        merged_blocks = []
        i = 0

        while i < len(all_blocks):
            current_block = all_blocks[i]

            # Check if can merge with the next block
            while i + 1 < len(all_blocks):
                next_block = all_blocks[i + 1]

                # If blocks are close or content related, merge them
                if next_block["start_pos"] - current_block[
                    "end_pos"
                ] < 300 or self._are_blocks_related(current_block, next_block):
                    # Merge blocks
                    merged_content = content[
                        current_block["start_pos"] : next_block["end_pos"]
                    ]
                    current_block = {
                        "start_pos": current_block["start_pos"],
                        "end_pos": next_block["end_pos"],
                        "content": merged_content.strip(),
                        "title": f"{current_block['title']} & {next_block['title']}",
                        "importance_score": max(
                            current_block["importance_score"],
                            next_block["importance_score"],
                        ),
                        "content_type": "merged",
                    }
                    i += 1
                else:
                    break

            merged_blocks.append(current_block)
            i += 1

        return merged_blocks

    def _are_blocks_related(self, block1: Dict, block2: Dict) -> bool:
        """Determine if two content blocks are related"""
        # Check content type associations
        related_types = [
            ("algorithm", "formula"),
            ("concept", "algorithm"),
            ("formula", "concept"),
        ]

        for type1, type2 in related_types:
            if (
                block1["content_type"] == type1 and block2["content_type"] == type2
            ) or (block1["content_type"] == type2 and block2["content_type"] == type1):
                return True

        return False

    def _extract_algorithm_title(self, text: str) -> str:
        """Extract title from algorithm text"""
        lines = text.split("\n")[:3]  # First 3 lines
        for line in lines:
            line = line.strip()
            if line and len(line) < 100:  # Reasonable title length
                # Clean title
                title = re.sub(r"[^\w\s-]", "", line)
                if title:
                    return title[:50]  # Limit title length
        return "Algorithm Block"

    def _extract_concept_title(self, text: str) -> str:
        """Extract title from concept text"""
        lines = text.split("\n")[:2]
        for line in lines:
            line = line.strip()
            if line and len(line) < 80:
                title = re.sub(r"[^\w\s-]", "", line)
                if title:
                    return title[:50]
        return "Concept Definition"

    def _create_enhanced_segment(
        self,
        content: str,
        title: str,
        start_pos: int,
        end_pos: int,
        importance_score: float,
        content_type: str,
    ) -> DocumentSegment:
        """Create enhanced document segment"""
        # Generate unique ID
        segment_id = hashlib.md5(
            f"{title}_{start_pos}_{end_pos}_{importance_score}".encode()
        ).hexdigest()[:8]

        # Extract keywords
        keywords = self._extract_enhanced_keywords(content, content_type)

        # Calculate enhanced relevance scores
        relevance_scores = self._calculate_enhanced_relevance_scores(
            content, content_type, importance_score
        )

        return DocumentSegment(
            id=segment_id,
            title=title,
            content=content,
            content_type=content_type,
            keywords=keywords,
            char_start=start_pos,
            char_end=end_pos,
            char_count=len(content),
            relevance_scores=relevance_scores,
            section_path=title,
        )

    def _extract_enhanced_keywords(self, content: str, content_type: str) -> List[str]:
        """Extract enhanced keywords based on content type"""
        words = re.findall(r"\b[a-zA-Z]{3,}\b", content.lower())

        # Adjust stopwords based on content type
        if content_type == "algorithm":
            algorithm_stopwords = {
                "step",
                "then",
                "else",
                "end",
                "begin",
                "start",
                "stop",
            }
            words = [w for w in words if w not in algorithm_stopwords]
        elif content_type == "formula":
            formula_keywords = ["equation", "formula", "where", "given", "such", "that"]
            words.extend(formula_keywords)

        # General stopwords
        general_stopwords = {
            "the",
            "and",
            "for",
            "are",
            "but",
            "not",
            "you",
            "all",
            "can",
            "her",
            "was",
            "one",
            "our",
            "had",
            "but",
            "have",
            "this",
            "that",
            "with",
            "from",
            "they",
            "she",
            "been",
            "were",
            "said",
            "each",
            "which",
            "their",
        }

        keywords = [w for w in set(words) if w not in general_stopwords and len(w) > 3]
        return keywords[:25]  # Increase keyword count

    def _calculate_enhanced_relevance_scores(
        self, content: str, content_type: str, importance_score: float
    ) -> Dict[str, float]:
        """Calculate enhanced relevance scores"""
        content_lower = content.lower()

        base_scores = {
            "concept_analysis": 0.5,
            "algorithm_extraction": 0.5,
            "code_planning": 0.5,
        }

        # Adjust base scores based on content type and importance
        if content_type == "algorithm":
            base_scores["algorithm_extraction"] = importance_score
            base_scores["code_planning"] = importance_score * 0.9
            base_scores["concept_analysis"] = importance_score * 0.7
        elif content_type == "concept":
            base_scores["concept_analysis"] = importance_score
            base_scores["algorithm_extraction"] = importance_score * 0.8
            base_scores["code_planning"] = importance_score * 0.6
        elif content_type == "formula":
            base_scores["algorithm_extraction"] = importance_score
            base_scores["concept_analysis"] = importance_score * 0.8
            base_scores["code_planning"] = importance_score * 0.9
        elif content_type == "merged":
            # Merged content is usually important
            base_scores = {k: importance_score * 0.95 for k in base_scores}

        # Additional bonus based on content density
        algorithm_indicators = ["algorithm", "method", "procedure", "step", "process"]
        concept_indicators = ["definition", "concept", "framework", "approach"]
        implementation_indicators = ["implementation", "code", "function", "design"]

        for query_type, indicators in [
            ("algorithm_extraction", algorithm_indicators),
            ("concept_analysis", concept_indicators),
            ("code_planning", implementation_indicators),
        ]:
            density_bonus = (
                sum(1 for indicator in indicators if indicator in content_lower) * 0.1
            )
            base_scores[query_type] = min(1.0, base_scores[query_type] + density_bonus)

        return base_scores

    # Placeholder methods - can be further implemented later
    def _identify_research_paper_sections(self, content: str) -> List[Dict]:
        """Identify research paper sections - simplified implementation"""
        # Temporarily use improved semantic detection
        return self._detect_academic_sections(content)

    def _enhance_section_with_context(self, section: Dict, content: str) -> Dict:
        """Add context to sections - simplified implementation"""
        return section

    def _identify_concept_implementation_pairs(self, content: str) -> List[Dict]:
        """Identify concept-implementation pairs - simplified implementation"""
        return []

    def _merge_concept_with_implementation(self, pair: Dict, content: str) -> Dict:
        """Merge concepts with implementation - simplified implementation"""
        return pair

    def _detect_semantic_boundaries(self, content: str) -> List[Dict]:
        """Detect semantic boundaries - based on paragraphs and logical separators"""
        boundaries = []

        # Split paragraphs by double line breaks
        paragraphs = content.split("\n\n")
        current_pos = 0

        for i, para in enumerate(paragraphs):
            if len(para.strip()) > 100:  # Valid paragraph
                # Analyze paragraph type
                content_type = self._classify_paragraph_type(para)
                importance_score = self._calculate_paragraph_importance(
                    para, content_type
                )

                boundaries.append(
                    {
                        "position": current_pos + len(para),
                        "suggested_title": self._extract_paragraph_title(para, i + 1),
                        "importance_score": importance_score,
                        "content_type": content_type,
                    }
                )

            current_pos += len(para) + 2  # +2 for \n\n

        return boundaries

    def _classify_paragraph_type(self, paragraph: str) -> str:
        """Classify paragraph type"""
        para_lower = paragraph.lower()

        if "algorithm" in para_lower or "procedure" in para_lower:
            return "algorithm"
        elif "formula" in para_lower or "$$" in paragraph:
            return "formula"
        elif any(
            word in para_lower for word in ["introduction", "overview", "abstract"]
        ):
            return "introduction"
        elif any(word in para_lower for word in ["conclusion", "summary", "result"]):
            return "conclusion"
        else:
            return "general"

    def _calculate_paragraph_importance(
        self, paragraph: str, content_type: str
    ) -> float:
        """Calculate paragraph importance"""
        if content_type == "algorithm":
            return 0.95
        elif content_type == "formula":
            return 0.9
        elif content_type == "introduction":
            return 0.85
        elif content_type == "conclusion":
            return 0.8
        else:
            return 0.7

    def _extract_paragraph_title(self, paragraph: str, index: int) -> str:
        """Extract paragraph title"""
        lines = paragraph.split("\n")
        for line in lines[:2]:
            if line.startswith("#"):
                return line.strip("# ")
            elif len(line) < 80 and line.strip():
                return line.strip()
        return f"Section {index}"

    def _calculate_optimal_chunk_size(self, content: str) -> int:
        """Calculate optimal chunk size"""
        # Dynamically adjust based on content complexity
        complexity = self.analyzer._calculate_concept_complexity(content)
        if complexity > 0.7:
            return 4000  # Complex content needs larger chunks
        elif complexity > 0.4:
            return 3000
        else:
            return 2000

    def _create_content_aware_chunks(self, content: str, chunk_size: int) -> List[Dict]:
        """Create content-aware chunks - simplified implementation"""
        chunks = []
        paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]

        current_chunk = []
        current_size = 0
        start_pos = 0

        for para in paragraphs:
            para_size = len(para)

            if current_size + para_size > chunk_size and current_chunk:
                chunk_content = "\n\n".join(current_chunk)
                chunks.append(
                    {
                        "content": chunk_content,
                        "title": f"Section {len(chunks) + 1}",
                        "start_pos": start_pos,
                        "end_pos": start_pos + len(chunk_content),
                        "importance_score": 0.7,
                        "content_type": "general",
                    }
                )

                current_chunk = [para]
                current_size = para_size
                start_pos += len(chunk_content) + 2
            else:
                current_chunk.append(para)
                current_size += para_size

        # Add the last chunk
        if current_chunk:
            chunk_content = "\n\n".join(current_chunk)
            chunks.append(
                {
                    "content": chunk_content,
                    "title": f"Section {len(chunks) + 1}",
                    "start_pos": start_pos,
                    "end_pos": start_pos + len(chunk_content),
                    "importance_score": 0.7,
                    "content_type": "general",
                }
            )

        return chunks

    def _create_segment(
        self, content: str, title: str, start_pos: int, end_pos: int
    ) -> DocumentSegment:
        """Create a DocumentSegment with metadata"""
        # Generate unique ID
        segment_id = hashlib.md5(f"{title}_{start_pos}_{end_pos}".encode()).hexdigest()[
            :8
        ]

        # Extract keywords from content
        keywords = self._extract_keywords(content)

        # Determine content type
        content_type = self._classify_content_type(title, content)

        # Calculate relevance scores for different query types
        relevance_scores = self._calculate_relevance_scores(content, content_type)

        return DocumentSegment(
            id=segment_id,
            title=title,
            content=content,
            content_type=content_type,
            keywords=keywords,
            char_start=start_pos,
            char_end=end_pos,
            char_count=len(content),
            relevance_scores=relevance_scores,
            section_path=title,  # Simplified for now
        )

    def _extract_keywords(self, content: str) -> List[str]:
        """Extract relevant keywords from content"""
        # Simple keyword extraction - could be enhanced with NLP
        words = re.findall(r"\b[a-zA-Z]{3,}\b", content.lower())

        # Remove common words
        stopwords = {
            "the",
            "and",
            "for",
            "are",
            "but",
            "not",
            "you",
            "all",
            "can",
            "her",
            "was",
            "one",
            "our",
            "had",
            "but",
            "have",
            "this",
            "that",
            "with",
            "from",
            "they",
            "she",
            "been",
            "were",
            "said",
            "each",
            "which",
            "their",
        }

        keywords = [w for w in set(words) if w not in stopwords and len(w) > 3]
        return keywords[:20]  # Top 20 keywords

    def _classify_content_type(self, title: str, content: str) -> str:
        """Classify the type of content based on title and content"""
        title_lower = title.lower()
        content_lower = content.lower()

        if any(
            word in title_lower for word in ["introduction", "abstract", "overview"]
        ):
            return "introduction"
        elif any(word in title_lower for word in ["method", "approach", "algorithm"]):
            return "methodology"
        elif any(
            word in title_lower for word in ["experiment", "evaluation", "result"]
        ):
            return "experiment"
        elif any(
            word in title_lower for word in ["conclusion", "discussion", "summary"]
        ):
            return "conclusion"
        elif any(word in title_lower for word in ["reference", "bibliography"]):
            return "references"
        elif "algorithm" in content_lower or "procedure" in content_lower:
            return "algorithm"
        else:
            return "general"

    def _calculate_relevance_scores(
        self, content: str, content_type: str
    ) -> Dict[str, float]:
        """Calculate relevance scores for different query types"""
        content_lower = content.lower()

        scores = {
            "concept_analysis": 0.5,
            "algorithm_extraction": 0.5,
            "code_planning": 0.5,
        }

        # Concept analysis relevance
        concept_indicators = [
            "introduction",
            "overview",
            "architecture",
            "system",
            "framework",
            "concept",
            "approach",
        ]
        concept_score = sum(
            1 for indicator in concept_indicators if indicator in content_lower
        ) / len(concept_indicators)
        scores["concept_analysis"] = min(
            1.0, concept_score + (0.8 if content_type == "introduction" else 0)
        )

        # Algorithm extraction relevance
        algorithm_indicators = [
            "algorithm",
            "method",
            "procedure",
            "formula",
            "equation",
            "step",
            "process",
        ]
        algorithm_score = sum(
            1 for indicator in algorithm_indicators if indicator in content_lower
        ) / len(algorithm_indicators)
        scores["algorithm_extraction"] = min(
            1.0, algorithm_score + (0.9 if content_type == "methodology" else 0)
        )

        # Code planning relevance
        code_indicators = [
            "implementation",
            "code",
            "function",
            "class",
            "module",
            "structure",
            "design",
        ]
        code_score = sum(
            1 for indicator in code_indicators if indicator in content_lower
        ) / len(code_indicators)
        scores["code_planning"] = min(
            1.0,
            code_score + (0.7 if content_type in ["methodology", "algorithm"] else 0),
        )

        return scores


# Global variables
DOCUMENT_INDEXES: Dict[str, DocumentIndex] = {}
segmenter = DocumentSegmenter()


def get_segments_dir(paper_dir: str) -> str:
    """Get the segments directory path"""
    return os.path.join(paper_dir, "document_segments")


def ensure_segments_dir_exists(segments_dir: str):
    """Ensure segments directory exists"""
    os.makedirs(segments_dir, exist_ok=True)


@mcp.tool()
async def analyze_and_segment_document(
    paper_dir: str, force_refresh: bool = False
) -> str:
    """
    Analyze document structure and create intelligent segments

    Args:
        paper_dir: Path to the paper directory
        force_refresh: Whether to force re-analysis even if segments exist

    Returns:
        JSON string with segmentation results
    """
    try:
        # Find markdown file in paper directory
        md_files = [f for f in os.listdir(paper_dir) if f.endswith(".md")]
        if not md_files:
            return json.dumps(
                {
                    "status": "error",
                    "message": f"No markdown file found in {paper_dir}",
                },
                ensure_ascii=False,
                indent=2,
            )

        md_file_path = os.path.join(paper_dir, md_files[0])
        segments_dir = get_segments_dir(paper_dir)
        index_file_path = os.path.join(segments_dir, "document_index.json")

        # Check if analysis already exists and is recent
        if not force_refresh and os.path.exists(index_file_path):
            try:
                with open(index_file_path, "r", encoding="utf-8") as f:
                    existing_index = json.load(f)

                    # Compatibility handling: ensure segments data structure is correct
                    if "segments" in existing_index:
                        segments_data = []
                        for seg_data in existing_index["segments"]:
                            # Ensure all required fields exist
                            segment_dict = dict(seg_data)

                            if "content_type" not in segment_dict:
                                segment_dict["content_type"] = "general"
                            if "keywords" not in segment_dict:
                                segment_dict["keywords"] = []
                            if "relevance_scores" not in segment_dict:
                                segment_dict["relevance_scores"] = {
                                    "concept_analysis": 0.5,
                                    "algorithm_extraction": 0.5,
                                    "code_planning": 0.5,
                                }
                            if "section_path" not in segment_dict:
                                segment_dict["section_path"] = segment_dict.get(
                                    "title", "Unknown"
                                )

                            segments_data.append(DocumentSegment(**segment_dict))

                        existing_index["segments"] = segments_data

                    DOCUMENT_INDEXES[paper_dir] = DocumentIndex(**existing_index)
                return json.dumps(
                    {
                        "status": "success",
                        "message": "Using existing document analysis",
                        "segments_dir": segments_dir,
                        "total_segments": existing_index["total_segments"],
                    },
                    ensure_ascii=False,
                    indent=2,
                )

            except Exception as e:
                logger.error(f"Failed to load existing index: {e}")
                logger.info("Will perform fresh analysis instead")
                # Remove corrupted index file and continue with new analysis
                try:
                    os.remove(index_file_path)
                except Exception as e:
                    pass

        # Read document content
        with open(md_file_path, "r", encoding="utf-8") as f:
            content = f.read()

        # Analyze document
        analyzer = DocumentAnalyzer()
        doc_type, confidence = analyzer.analyze_document_type(content)
        strategy = analyzer.detect_segmentation_strategy(content, doc_type)

        # Create segments
        segments = segmenter.segment_document(content, strategy)

        # Create document index
        document_index = DocumentIndex(
            document_path=md_file_path,
            document_type=doc_type,
            segmentation_strategy=strategy,
            total_segments=len(segments),
            total_chars=len(content),
            segments=segments,
            created_at=datetime.now().isoformat(),
        )

        # Save segments
        ensure_segments_dir_exists(segments_dir)

        # Save document index
        with open(index_file_path, "w", encoding="utf-8") as f:
            json.dump(
                asdict(document_index), f, ensure_ascii=False, indent=2, default=str
            )

        # Save individual segment files for fallback
        for segment in segments:
            segment_file_path = os.path.join(segments_dir, f"segment_{segment.id}.md")
            with open(segment_file_path, "w", encoding="utf-8") as f:
                f.write(f"# {segment.title}\n\n")
                f.write(f"**Content Type:** {segment.content_type}\n")
                f.write(f"**Keywords:** {', '.join(segment.keywords[:10])}\n\n")
                f.write(segment.content)

        # Store in memory
        DOCUMENT_INDEXES[paper_dir] = document_index

        logger.info(
            f"Document segmentation completed: {len(segments)} segments created"
        )

        return json.dumps(
            {
                "status": "success",
                "message": f"Document analysis completed with {strategy} strategy",
                "document_type": doc_type,
                "segmentation_strategy": strategy,
                "segments_dir": segments_dir,
                "total_segments": len(segments),
                "total_chars": len(content),
            },
            ensure_ascii=False,
            indent=2,
        )

    except Exception as e:
        logger.error(f"Error in analyze_and_segment_document: {e}")
        return json.dumps(
            {"status": "error", "message": f"Failed to analyze document: {str(e)}"},
            ensure_ascii=False,
            indent=2,
        )


@mcp.tool()
async def read_document_segments(
    paper_dir: str,
    query_type: str,
    keywords: List[str] = None,
    max_segments: int = 3,
    max_total_chars: int = None,
) -> str:
    """
    Intelligently retrieve relevant document segments based on query type

    Args:
        paper_dir: Path to the paper directory
        query_type: Type of query - "concept_analysis", "algorithm_extraction", or "code_planning"
        keywords: Optional list of keywords to search for
        max_segments: Maximum number of segments to return
        max_total_chars: Maximum total characters to return

    Returns:
        JSON string with selected segments
    """
    try:
        # Ensure document is analyzed
        if paper_dir not in DOCUMENT_INDEXES:
            segments_dir = get_segments_dir(paper_dir)
            index_file_path = os.path.join(segments_dir, "document_index.json")

            if os.path.exists(index_file_path):
                with open(index_file_path, "r", encoding="utf-8") as f:
                    index_data = json.load(f)
                    # Convert dict back to DocumentIndex with backward compatibility
                    segments_data = []
                    for seg_data in index_data.get("segments", []):
                        # Ensure all required fields exist, provide default values
                        segment_dict = dict(seg_data)

                        # Compatibility handling: add missing fields
                        if "content_type" not in segment_dict:
                            segment_dict["content_type"] = "general"
                        if "keywords" not in segment_dict:
                            segment_dict["keywords"] = []
                        if "relevance_scores" not in segment_dict:
                            segment_dict["relevance_scores"] = {
                                "concept_analysis": 0.5,
                                "algorithm_extraction": 0.5,
                                "code_planning": 0.5,
                            }
                        if "section_path" not in segment_dict:
                            segment_dict["section_path"] = segment_dict.get(
                                "title", "Unknown"
                            )

                        segment = DocumentSegment(**segment_dict)
                        segments_data.append(segment)

                    index_data["segments"] = segments_data
                    DOCUMENT_INDEXES[paper_dir] = DocumentIndex(**index_data)
            else:
                # Auto-analyze if not found
                await analyze_and_segment_document(paper_dir)

        document_index = DOCUMENT_INDEXES[paper_dir]

        # Dynamically calculate character limit
        if max_total_chars is None:
            max_total_chars = _calculate_adaptive_char_limit(document_index, query_type)

        # Score and rank segments with enhanced algorithm
        scored_segments = []
        for segment in document_index.segments:
            # Base relevance score (already enhanced in new system)
            relevance_score = segment.relevance_scores.get(query_type, 0.5)

            # Enhanced keyword matching with position weighting
            if keywords:
                keyword_score = _calculate_enhanced_keyword_score(segment, keywords)
                relevance_score += keyword_score

            # Content completeness bonus
            completeness_bonus = _calculate_completeness_bonus(segment, document_index)
            relevance_score += completeness_bonus

            scored_segments.append((segment, relevance_score))

        # Sort by enhanced relevance score
        scored_segments.sort(key=lambda x: x[1], reverse=True)

        # Intelligent segment selection with integrity preservation
        selected_segments = _select_segments_with_integrity(
            scored_segments, max_segments, max_total_chars, query_type
        )

        total_chars = sum(seg["char_count"] for seg in selected_segments)

        logger.info(
            f"Selected {len(selected_segments)} segments for {query_type} query"
        )

        return json.dumps(
            {
                "status": "success",
                "query_type": query_type,
                "keywords": keywords or [],
                "total_segments_available": len(document_index.segments),
                "segments_selected": len(selected_segments),
                "total_chars": total_chars,
                "max_chars_used": max_total_chars,
                "segments": selected_segments,
            },
            ensure_ascii=False,
            indent=2,
        )

    except Exception as e:
        logger.error(f"Error in read_document_segments: {e}")
        return json.dumps(
            {
                "status": "error",
                "message": f"Failed to read document segments: {str(e)}",
            },
            ensure_ascii=False,
            indent=2,
        )


@mcp.tool()
async def get_document_overview(paper_dir: str) -> str:
    """
    Get overview of document structure and available segments

    Args:
        paper_dir: Path to the paper directory

    Returns:
        JSON string with document overview
    """
    try:
        # Ensure document is analyzed
        if paper_dir not in DOCUMENT_INDEXES:
            await analyze_and_segment_document(paper_dir)

        document_index = DOCUMENT_INDEXES[paper_dir]

        # Create overview
        segment_summaries = []
        for segment in document_index.segments:
            segment_summaries.append(
                {
                    "id": segment.id,
                    "title": segment.title,
                    "content_type": segment.content_type,
                    "char_count": segment.char_count,
                    "keywords": segment.keywords[:5],  # Top 5 keywords
                    "relevance_scores": segment.relevance_scores,
                }
            )

        return json.dumps(
            {
                "status": "success",
                "document_path": document_index.document_path,
                "document_type": document_index.document_type,
                "segmentation_strategy": document_index.segmentation_strategy,
                "total_segments": document_index.total_segments,
                "total_chars": document_index.total_chars,
                "created_at": document_index.created_at,
                "segments_overview": segment_summaries,
            },
            ensure_ascii=False,
            indent=2,
        )

    except Exception as e:
        logger.error(f"Error in get_document_overview: {e}")
        return json.dumps(
            {
                "status": "error",
                "message": f"Failed to get document overview: {str(e)}",
            },
            ensure_ascii=False,
            indent=2,
        )


# =============== Enhanced retrieval system helper methods ===============


def _calculate_adaptive_char_limit(
    document_index: DocumentIndex, query_type: str
) -> int:
    """Dynamically calculate character limit based on document complexity and query type"""
    base_limit = 6000

    # Adjust based on document type
    if document_index.document_type == "research_paper":
        base_limit = 10000
    elif document_index.document_type == "algorithm_focused":
        base_limit = 12000
    elif document_index.segmentation_strategy == "algorithm_preserve_integrity":
        base_limit = 15000

    # Adjust based on query type
    query_multipliers = {
        "algorithm_extraction": 1.5,  # Algorithms need more context
        "concept_analysis": 1.2,
        "code_planning": 1.3,
    }

    multiplier = query_multipliers.get(query_type, 1.0)
    return int(base_limit * multiplier)


def _calculate_enhanced_keyword_score(
    segment: DocumentSegment, keywords: List[str]
) -> float:
    """Calculate enhanced keyword matching score"""
    score = 0.0
    content_lower = segment.content.lower()
    title_lower = segment.title.lower()

    for keyword in keywords:
        keyword_lower = keyword.lower()

        # Title matching has higher weight
        if keyword_lower in title_lower:
            score += 0.3

        # Content matching
        content_matches = content_lower.count(keyword_lower)
        if content_matches > 0:
            # Consider term frequency and position
            frequency_score = min(0.2, content_matches * 0.05)

            # Check if in important position (first 25% of content)
            early_content = content_lower[: len(content_lower) // 4]
            if keyword_lower in early_content:
                frequency_score += 0.1

            score += frequency_score

    return min(0.6, score)  # Limit maximum bonus


def _calculate_completeness_bonus(
    segment: DocumentSegment, document_index: DocumentIndex
) -> float:
    """Calculate content completeness bonus"""
    bonus = 0.0

    # Completeness bonus for algorithm and formula content
    if segment.content_type in ["algorithm", "formula", "merged"]:
        bonus += 0.2

    # Long paragraphs usually contain more complete information
    if segment.char_count > 2000:
        bonus += 0.1
    elif segment.char_count > 4000:
        bonus += 0.15

    # High importance paragraph bonus
    if segment.relevance_scores.get("algorithm_extraction", 0) > 0.8:
        bonus += 0.1

    return min(0.3, bonus)


def _select_segments_with_integrity(
    scored_segments: List[Tuple],
    max_segments: int,
    max_total_chars: int,
    query_type: str,
) -> List[Dict]:
    """Intelligently select segments while maintaining content integrity"""
    selected_segments = []
    total_chars = 0

    # First select the highest scoring segments
    for segment, score in scored_segments:
        if len(selected_segments) >= max_segments:
            break

        if total_chars + segment.char_count <= max_total_chars:
            selected_segments.append(
                {
                    "id": segment.id,
                    "title": segment.title,
                    "content": segment.content,
                    "content_type": segment.content_type,
                    "relevance_score": score,
                    "char_count": segment.char_count,
                }
            )
            total_chars += segment.char_count
        elif len(selected_segments) == 0:
            # If the first segment exceeds the limit, truncate but preserve it
            truncated_content = (
                segment.content[: max_total_chars - 200]
                + "\n\n[Content truncated for length...]"
            )
            selected_segments.append(
                {
                    "id": segment.id,
                    "title": segment.title,
                    "content": truncated_content,
                    "content_type": segment.content_type,
                    "relevance_score": score,
                    "char_count": len(truncated_content),
                }
            )
            break

    # If there's remaining space, try to add relevant small segments
    remaining_chars = max_total_chars - total_chars
    if remaining_chars > 500 and len(selected_segments) < max_segments:
        for segment, score in scored_segments[len(selected_segments) :]:
            if (
                segment.char_count <= remaining_chars
                and len(selected_segments) < max_segments
            ):
                selected_segments.append(
                    {
                        "id": segment.id,
                        "title": segment.title,
                        "content": segment.content,
                        "content_type": segment.content_type,
                        "relevance_score": score,
                        "char_count": segment.char_count,
                    }
                )
                remaining_chars -= segment.char_count

    return selected_segments


if __name__ == "__main__":
    # Run the MCP server
    mcp.run()


--- tools/git_command.py ---
#!/usr/bin/env python3
"""
GitHub Repository Downloader MCP Tool using FastMCP
"""

import asyncio
import os
import re
from typing import Dict, List, Optional
from pathlib import Path

from mcp.server import FastMCP

# ÂàõÂª∫ FastMCP ÂÆû‰æã
mcp = FastMCP("github-downloader")


class GitHubURLExtractor:
    """ÊèêÂèñGitHub URLÁöÑÂ∑•ÂÖ∑Á±ª"""

    @staticmethod
    def extract_github_urls(text: str) -> List[str]:
        """‰ªéÊñáÊú¨‰∏≠ÊèêÂèñGitHub URLs"""
        patterns = [
            # Ê†áÂáÜHTTPS URL
            r"https?://github\.com/[\w\-\.]+/[\w\-\.]+(?:\.git)?",
            # SSH URL
            r"git@github\.com:[\w\-\.]+/[\w\-\.]+(?:\.git)?",
            # Áü≠Ê†ºÂºè owner/repo - Êõ¥‰∏•Ê†ºÁöÑÂåπÈÖç
            r"(?<!\S)(?<!/)(?<!\.)([\w\-\.]+/[\w\-\.]+)(?!/)(?!\S)",
        ]

        urls = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # Â§ÑÁêÜÁü≠Ê†ºÂºè
                if isinstance(match, tuple):
                    match = match[0]

                # Ê∏ÖÁêÜURL
                if match.startswith("git@"):
                    url = match.replace("git@github.com:", "https://github.com/")
                elif match.startswith("http"):
                    url = match
                else:
                    # Â§ÑÁêÜÁü≠Ê†ºÂºè (owner/repo) - Ê∑ªÂä†Êõ¥Â§öÈ™åËØÅ
                    if "/" in match and not any(
                        x in match for x in ["./", "../", "deepcode_lab", "tools"]
                    ):
                        parts = match.split("/")
                        if (
                            len(parts) == 2
                            and all(
                                part.replace("-", "").replace("_", "").isalnum()
                                for part in parts
                            )
                            and not any(part.startswith(".") for part in parts)
                        ):
                            url = f"https://github.com/{match}"
                        else:
                            continue
                    else:
                        continue

                # ËßÑËåÉÂåñ URL
                url = url.rstrip(".git")
                url = url.rstrip("/")

                # ‰øÆÂ§çÈáçÂ§çÁöÑ github.com
                if "github.com/github.com/" in url:
                    url = url.replace("github.com/github.com/", "github.com/")

                urls.append(url)

        return list(set(urls))  # ÂéªÈáç

    @staticmethod
    def extract_target_path(text: str) -> Optional[str]:
        """‰ªéÊñáÊú¨‰∏≠ÊèêÂèñÁõÆÊ†áË∑ØÂæÑ"""
        # Ë∑ØÂæÑÊåáÁ§∫ËØçÊ®°Âºè
        patterns = [
            r'(?:to|into|in|at)\s+(?:folder|directory|path)?\s*["\']?([^\s"\']+)["\']?',
            r'(?:save|download|clone)\s+(?:to|into|at)\s+["\']?([^\s"\']+)["\']?',
            # ‰∏≠ÊñáÊîØÊåÅ
            r'(?:Âà∞|Âú®|‰øùÂ≠òÂà∞|‰∏ãËΩΩÂà∞|ÂÖãÈöÜÂà∞)\s*["\']?([^\s"\']+)["\']?',
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                path = match.group(1).strip("„ÄÇÔºå,.")
                # ËøáÊª§ÊéâÈÄöÁî®ËØç
                if path and path.lower() not in [
                    "here",
                    "there",
                    "current",
                    "local",
                    "ËøôÈáå",
                    "ÂΩìÂâç",
                    "Êú¨Âú∞",
                ]:
                    return path

        return None

    @staticmethod
    def infer_repo_name(url: str) -> str:
        """‰ªéURLÊé®Êñ≠‰ªìÂ∫ìÂêçÁß∞"""
        url = url.rstrip(".git")
        if "github.com" in url:
            parts = url.split("/")
            if len(parts) >= 2:
                return parts[-1]
        return "repository"


async def check_git_installed() -> bool:
    """Ê£ÄÊü•GitÊòØÂê¶ÂÆâË£Ö"""
    try:
        proc = await asyncio.create_subprocess_exec(
            "git",
            "--version",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        await proc.wait()
        return proc.returncode == 0
    except Exception:
        return False


async def clone_repository(repo_url: str, target_path: str) -> Dict[str, any]:
    """ÊâßË°ågit cloneÂëΩ‰ª§"""
    try:
        proc = await asyncio.create_subprocess_exec(
            "git",
            "clone",
            repo_url,
            target_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        stdout, stderr = await proc.communicate()

        return {
            "success": proc.returncode == 0,
            "stdout": stdout.decode("utf-8", errors="replace"),
            "stderr": stderr.decode("utf-8", errors="replace"),
            "returncode": proc.returncode,
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


@mcp.tool()
async def download_github_repo(instruction: str) -> str:
    """
    Download GitHub repositories from natural language instructions.

    Args:
        instruction: Natural language text containing GitHub URLs and optional target paths

    Returns:
        Status message about the download operation

    Examples:
        - "Download https://github.com/openai/gpt-3"
        - "Clone microsoft/vscode to my-projects folder"
        - "Get https://github.com/facebook/react"
    """
    # Ê£ÄÊü•GitÊòØÂê¶ÂÆâË£Ö
    if not await check_git_installed():
        return "‚ùå Error: Git is not installed or not in system PATH"

    extractor = GitHubURLExtractor()

    # ÊèêÂèñGitHub URLs
    urls = extractor.extract_github_urls(instruction)
    if not urls:
        return "‚ùå No GitHub URLs found in the instruction"

    # ÊèêÂèñÁõÆÊ†áË∑ØÂæÑ
    target_path = extractor.extract_target_path(instruction)

    # ‰∏ãËΩΩ‰ªìÂ∫ì
    results = []
    for url in urls:
        try:
            # ÂáÜÂ§áÁõÆÊ†áË∑ØÂæÑ
            if target_path:
                # Âà§Êñ≠ÊòØÂê¶‰∏∫ÁªùÂØπË∑ØÂæÑ
                if os.path.isabs(target_path):
                    # Â¶ÇÊûúÊòØÁªùÂØπË∑ØÂæÑÔºåÁõ¥Êé•‰ΩøÁî®
                    final_path = target_path
                    # Â¶ÇÊûúÁõÆÊ†áË∑ØÂæÑÊòØÁõÆÂΩïÔºåÊ∑ªÂä†‰ªìÂ∫ìÂêç
                    if os.path.basename(target_path) == "" or target_path.endswith("/"):
                        final_path = os.path.join(
                            target_path, extractor.infer_repo_name(url)
                        )
                else:
                    # Â¶ÇÊûúÊòØÁõ∏ÂØπË∑ØÂæÑÔºå‰øùÊåÅÁõ∏ÂØπË∑ØÂæÑ
                    final_path = target_path
                    # Â¶ÇÊûúÁõÆÊ†áË∑ØÂæÑÊòØÁõÆÂΩïÔºåÊ∑ªÂä†‰ªìÂ∫ìÂêç
                    if os.path.basename(target_path) == "" or target_path.endswith("/"):
                        final_path = os.path.join(
                            target_path, extractor.infer_repo_name(url)
                        )
            else:
                final_path = extractor.infer_repo_name(url)

            # Â¶ÇÊûúÊòØÁõ∏ÂØπË∑ØÂæÑÔºåÁ°Æ‰øù‰ΩøÁî®Áõ∏ÂØπË∑ØÂæÑÊ†ºÂºè
            if not os.path.isabs(final_path):
                final_path = os.path.normpath(final_path)
                if final_path.startswith("/"):
                    final_path = final_path.lstrip("/")

            # Á°Æ‰øùÁà∂ÁõÆÂΩïÂ≠òÂú®
            parent_dir = os.path.dirname(final_path)
            if parent_dir:
                os.makedirs(parent_dir, exist_ok=True)

            # Ê£ÄÊü•ÁõÆÊ†áË∑ØÂæÑÊòØÂê¶Â∑≤Â≠òÂú®
            if os.path.exists(final_path):
                results.append(
                    f"‚ùå Failed to download {url}: Target path already exists: {final_path}"
                )
                continue

            # ÊâßË°åÂÖãÈöÜ
            result = await clone_repository(url, final_path)

            if result["success"]:
                msg = f"‚úÖ Successfully downloaded: {url}\n"
                msg += f"   Location: {final_path}"
                if result.get("stdout"):
                    msg += f"\n   {result['stdout'].strip()}"
            else:
                msg = f"‚ùå Failed to download: {url}\n"
                msg += f"   Error: {result.get('error', result.get('stderr', 'Unknown error'))}"

        except Exception as e:
            msg = f"‚ùå Failed to download: {url}\n"
            msg += f"   Error: {str(e)}"

        results.append(msg)

    return "\n\n".join(results)


@mcp.tool()
async def parse_github_urls(text: str) -> str:
    """
    Extract GitHub URLs and target paths from text.

    Args:
        text: Text containing GitHub URLs

    Returns:
        Parsed GitHub URLs and target path information
    """
    extractor = GitHubURLExtractor()

    urls = extractor.extract_github_urls(text)
    target_path = extractor.extract_target_path(text)

    content = "üìù Parsed information:\n\n"

    if urls:
        content += "GitHub URLs found:\n"
        for url in urls:
            content += f"  ‚Ä¢ {url}\n"
    else:
        content += "No GitHub URLs found\n"

    if target_path:
        content += f"\nTarget path: {target_path}"
    else:
        content += "\nTarget path: Not specified (will use repository name)"

    return content


@mcp.tool()
async def git_clone(
    repo_url: str, target_path: Optional[str] = None, branch: Optional[str] = None
) -> str:
    """
    Clone a specific GitHub repository.

    Args:
        repo_url: GitHub repository URL
        target_path: Optional target directory path
        branch: Optional branch name to clone

    Returns:
        Status message about the clone operation
    """
    # Ê£ÄÊü•GitÊòØÂê¶ÂÆâË£Ö
    if not await check_git_installed():
        return "‚ùå Error: Git is not installed or not in system PATH"

    # ÂáÜÂ§áÁõÆÊ†áË∑ØÂæÑ
    if not target_path:
        extractor = GitHubURLExtractor()
        target_path = extractor.infer_repo_name(repo_url)

    # ËΩ¨Êç¢‰∏∫ÁªùÂØπË∑ØÂæÑ
    if not os.path.isabs(target_path):
        target_path = str(Path.cwd() / target_path)

    # Ê£ÄÊü•ÁõÆÊ†áË∑ØÂæÑ
    if os.path.exists(target_path):
        return f"‚ùå Error: Target path already exists: {target_path}"

    # ÊûÑÂª∫ÂëΩ‰ª§
    cmd = ["git", "clone"]
    if branch:
        cmd.extend(["-b", branch])
    cmd.extend([repo_url, target_path])

    # ÊâßË°åÂÖãÈöÜ
    try:
        proc = await asyncio.create_subprocess_exec(
            *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )

        stdout, stderr = await proc.communicate()

        if proc.returncode == 0:
            result = "‚úÖ Successfully cloned repository\n"
            result += f"Repository: {repo_url}\n"
            result += f"Location: {target_path}"
            if branch:
                result += f"\nBranch: {branch}"
            return result
        else:
            return f"‚ùå Clone failed\nError: {stderr.decode('utf-8', errors='replace')}"

    except Exception as e:
        return f"‚ùå Clone failed\nError: {str(e)}"


# ‰∏ªÁ®ãÂ∫èÂÖ•Âè£
if __name__ == "__main__":
    print("üöÄ GitHub Repository Downloader MCP Tool")
    print("üìù Starting server with FastMCP...")
    print("\nAvailable tools:")
    print("  ‚Ä¢ download_github_repo - Download repos from natural language")
    print("  ‚Ä¢ parse_github_urls - Extract GitHub URLs from text")
    print("  ‚Ä¢ git_clone - Clone a specific repository")
    print("")

    # ËøêË°åÊúçÂä°Âô®
    mcp.run()


--- tools/__init__.py ---


--- tools/pdf_converter.py ---
#!/usr/bin/env python3
"""
PDF Converter Utility

This module provides functionality for converting various document formats to PDF,
including Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) and text files (.txt, .md).

Requirements:
- LibreOffice for Office document conversion
- ReportLab for text-to-PDF conversion
"""

from __future__ import annotations

import argparse
import logging
import subprocess
import tempfile
import shutil
import platform
from pathlib import Path
from typing import Union, Optional, Dict, Any


class PDFConverter:
    """
    PDF conversion utility class.

    Provides methods to convert Office documents and text files to PDF format.
    """

    # Define supported file formats
    OFFICE_FORMATS = {".doc", ".docx", ".ppt", ".pptx", ".xls", ".xlsx"}
    TEXT_FORMATS = {".txt", ".md"}

    # Class-level logger
    logger = logging.getLogger(__name__)

    def __init__(self) -> None:
        """Initialize the PDF converter."""
        pass

    @staticmethod
    def convert_office_to_pdf(
        doc_path: Union[str, Path], output_dir: Optional[str] = None
    ) -> Path:
        """
        Convert Office document (.doc, .docx, .ppt, .pptx, .xls, .xlsx) to PDF.
        Requires LibreOffice to be installed.

        Args:
            doc_path: Path to the Office document file
            output_dir: Output directory for the PDF file

        Returns:
            Path to the generated PDF file
        """
        try:
            # Convert to Path object for easier handling
            doc_path = Path(doc_path)
            if not doc_path.exists():
                raise FileNotFoundError(f"Office document does not exist: {doc_path}")

            name_without_suff = doc_path.stem

            # Prepare output directory
            if output_dir:
                base_output_dir = Path(output_dir)
            else:
                base_output_dir = doc_path.parent / "pdf_output"

            base_output_dir.mkdir(parents=True, exist_ok=True)

            # Check if LibreOffice is available
            libreoffice_available = False
            working_libreoffice_cmd: Optional[str] = None

            # Prepare subprocess parameters to hide console window on Windows
            subprocess_kwargs: Dict[str, Any] = {
                "capture_output": True,
                "check": True,
                "timeout": 10,
                "encoding": "utf-8",
                "errors": "ignore",
            }

            # Hide console window on Windows
            if platform.system() == "Windows":
                subprocess_kwargs["creationflags"] = (
                    0x08000000  # subprocess.CREATE_NO_WINDOW
                )

            try:
                result = subprocess.run(
                    ["libreoffice", "--version"], **subprocess_kwargs
                )
                libreoffice_available = True
                working_libreoffice_cmd = "libreoffice"
                logging.info(f"LibreOffice detected: {result.stdout.strip()}")  # type: ignore
            except (
                subprocess.CalledProcessError,
                FileNotFoundError,
                subprocess.TimeoutExpired,
            ):
                pass

            # Try alternative commands for LibreOffice
            if not libreoffice_available:
                for cmd in ["soffice", "libreoffice"]:
                    try:
                        result = subprocess.run([cmd, "--version"], **subprocess_kwargs)
                        libreoffice_available = True
                        working_libreoffice_cmd = cmd
                        logging.info(
                            f"LibreOffice detected with command '{cmd}': {result.stdout.strip()}"  # type: ignore
                        )
                        break
                    except (
                        subprocess.CalledProcessError,
                        FileNotFoundError,
                        subprocess.TimeoutExpired,
                    ):
                        continue

            if not libreoffice_available:
                raise RuntimeError(
                    "LibreOffice is required for Office document conversion but was not found.\n"
                    "Please install LibreOffice:\n"
                    "- Windows: Download from https://www.libreoffice.org/download/download/\n"
                    "- macOS: brew install --cask libreoffice\n"
                    "- Ubuntu/Debian: sudo apt-get install libreoffice\n"
                    "- CentOS/RHEL: sudo yum install libreoffice\n"
                    "Alternatively, convert the document to PDF manually."
                )

            # Create temporary directory for PDF conversion
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)

                # Convert to PDF using LibreOffice
                logging.info(f"Converting {doc_path.name} to PDF using LibreOffice...")

                # Use the working LibreOffice command first, then try alternatives if it fails
                commands_to_try = [working_libreoffice_cmd]
                if working_libreoffice_cmd == "libreoffice":
                    commands_to_try.append("soffice")
                else:
                    commands_to_try.append("libreoffice")

                conversion_successful = False
                for cmd in commands_to_try:
                    if cmd is None:
                        continue
                    try:
                        convert_cmd = [
                            cmd,
                            "--headless",
                            "--convert-to",
                            "pdf",
                            "--outdir",
                            str(temp_path),
                            str(doc_path),
                        ]

                        # Prepare conversion subprocess parameters
                        convert_subprocess_kwargs: Dict[str, Any] = {
                            "capture_output": True,
                            "text": True,
                            "timeout": 60,  # 60 second timeout
                            "encoding": "utf-8",
                            "errors": "ignore",
                        }

                        # Hide console window on Windows
                        if platform.system() == "Windows":
                            convert_subprocess_kwargs["creationflags"] = (
                                0x08000000  # subprocess.CREATE_NO_WINDOW
                            )

                        result = subprocess.run(
                            convert_cmd, **convert_subprocess_kwargs
                        )

                        if result.returncode == 0:  # type: ignore
                            conversion_successful = True
                            logging.info(
                                f"Successfully converted {doc_path.name} to PDF"
                            )
                            break
                        else:
                            logging.warning(
                                f"LibreOffice command '{cmd}' failed: {result.stderr}"  # type: ignore
                            )
                    except subprocess.TimeoutExpired:
                        logging.warning(f"LibreOffice command '{cmd}' timed out")
                    except Exception as e:
                        logging.error(
                            f"LibreOffice command '{cmd}' failed with exception: {e}"
                        )

                if not conversion_successful:
                    raise RuntimeError(
                        f"LibreOffice conversion failed for {doc_path.name}. "
                        f"Please check if the file is corrupted or try converting manually."
                    )

                # Find the generated PDF
                pdf_files = list(temp_path.glob("*.pdf"))
                if not pdf_files:
                    raise RuntimeError(
                        f"PDF conversion failed for {doc_path.name} - no PDF file generated. "
                        f"Please check LibreOffice installation or try manual conversion."
                    )

                pdf_path = pdf_files[0]
                logging.info(
                    f"Generated PDF: {pdf_path.name} ({pdf_path.stat().st_size} bytes)"
                )

                # Validate the generated PDF
                if pdf_path.stat().st_size < 100:  # Very small file, likely empty
                    raise RuntimeError(
                        "Generated PDF appears to be empty or corrupted. "
                        "Original file may have issues or LibreOffice conversion failed."
                    )

                # Copy PDF to final output directory
                final_pdf_path = base_output_dir / f"{name_without_suff}.pdf"
                shutil.copy2(pdf_path, final_pdf_path)

                return final_pdf_path

        except Exception as e:
            logging.error(f"Error in convert_office_to_pdf: {str(e)}")
            raise

    @staticmethod
    def convert_text_to_pdf(
        text_path: Union[str, Path], output_dir: Optional[str] = None
    ) -> Path:
        """
        Convert text file (.txt, .md) to PDF using ReportLab with full markdown support.

        Args:
            text_path: Path to the text file
            output_dir: Output directory for the PDF file

        Returns:
            Path to the generated PDF file
        """
        try:
            text_path = Path(text_path)
            if not text_path.exists():
                raise FileNotFoundError(f"Text file does not exist: {text_path}")

            # Supported text formats
            supported_text_formats = {".txt", ".md"}
            if text_path.suffix.lower() not in supported_text_formats:
                raise ValueError(f"Unsupported text format: {text_path.suffix}")

            # Read the text content
            try:
                with open(text_path, "r", encoding="utf-8") as f:
                    text_content = f.read()
            except UnicodeDecodeError:
                # Try with different encodings
                for encoding in ["gbk", "latin-1", "cp1252"]:
                    try:
                        with open(text_path, "r", encoding=encoding) as f:
                            text_content = f.read()
                        logging.info(f"Successfully read file with {encoding} encoding")
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    raise RuntimeError(
                        f"Could not decode text file {text_path.name} with any supported encoding"
                    )

            # Prepare output directory
            if output_dir:
                base_output_dir = Path(output_dir)
            else:
                base_output_dir = text_path.parent / "pdf_output"

            base_output_dir.mkdir(parents=True, exist_ok=True)
            pdf_path = base_output_dir / f"{text_path.stem}.pdf"

            # Convert text to PDF
            logging.info(f"Converting {text_path.name} to PDF...")

            try:
                from reportlab.lib.pagesizes import A4
                from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
                from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
                from reportlab.lib.units import inch
                from reportlab.pdfbase import pdfmetrics

                # Create PDF document
                doc = SimpleDocTemplate(
                    str(pdf_path),
                    pagesize=A4,
                    leftMargin=inch,
                    rightMargin=inch,
                    topMargin=inch,
                    bottomMargin=inch,
                )

                # Get styles
                styles = getSampleStyleSheet()
                normal_style = styles["Normal"]
                heading_style = styles["Heading1"]

                # Try to register a font that supports Chinese characters
                try:
                    # Try to use system fonts that support Chinese
                    system = platform.system()
                    if system == "Windows":
                        # Try common Windows fonts
                        for font_name in ["SimSun", "SimHei", "Microsoft YaHei"]:
                            try:
                                from reportlab.pdfbase.cidfonts import (
                                    UnicodeCIDFont,
                                )

                                pdfmetrics.registerFont(UnicodeCIDFont(font_name))  # type: ignore
                                normal_style.fontName = font_name
                                heading_style.fontName = font_name
                                break
                            except Exception:
                                continue
                    elif system == "Darwin":  # macOS
                        for font_name in ["STSong-Light", "STHeiti"]:
                            try:
                                from reportlab.pdfbase.cidfonts import (
                                    UnicodeCIDFont,
                                )

                                pdfmetrics.registerFont(UnicodeCIDFont(font_name))  # type: ignore
                                normal_style.fontName = font_name
                                heading_style.fontName = font_name
                                break
                            except Exception:
                                continue
                except Exception:
                    pass  # Use default fonts if Chinese font setup fails

                # Build content
                story = []

                # Handle markdown or plain text
                if text_path.suffix.lower() == ".md":
                    # Handle markdown content - simplified implementation
                    lines = text_content.split("\n")
                    for line in lines:
                        line = line.strip()
                        if not line:
                            story.append(Spacer(1, 12))
                            continue

                        # Headers
                        if line.startswith("#"):
                            level = len(line) - len(line.lstrip("#"))
                            header_text = line.lstrip("#").strip()
                            if header_text:
                                header_style = ParagraphStyle(
                                    name=f"Heading{level}",
                                    parent=heading_style,
                                    fontSize=max(16 - level, 10),
                                    spaceAfter=8,
                                    spaceBefore=16 if level <= 2 else 12,
                                )
                                story.append(Paragraph(header_text, header_style))
                        else:
                            # Regular text
                            processed_line = PDFConverter._process_inline_markdown(line)
                            story.append(Paragraph(processed_line, normal_style))
                            story.append(Spacer(1, 6))
                else:
                    # Handle plain text files (.txt)
                    logging.info(
                        f"Processing plain text file with {len(text_content)} characters..."
                    )

                    # Split text into lines and process each line
                    lines = text_content.split("\n")
                    line_count = 0

                    for line in lines:
                        line = line.rstrip()
                        line_count += 1

                        # Empty lines
                        if not line.strip():
                            story.append(Spacer(1, 6))
                            continue

                        # Regular text lines
                        # Escape special characters for ReportLab
                        safe_line = (
                            line.replace("&", "&amp;")
                            .replace("<", "&lt;")
                            .replace(">", "&gt;")
                        )

                        # Create paragraph
                        story.append(Paragraph(safe_line, normal_style))
                        story.append(Spacer(1, 3))

                    logging.info(f"Added {line_count} lines to PDF")

                    # If no content was added, add a placeholder
                    if not story:
                        story.append(Paragraph("(Empty text file)", normal_style))

                # Build PDF
                doc.build(story)
                logging.info(
                    f"Successfully converted {text_path.name} to PDF ({pdf_path.stat().st_size / 1024:.1f} KB)"
                )

            except ImportError:
                raise RuntimeError(
                    "reportlab is required for text-to-PDF conversion. "
                    "Please install it using: pip install reportlab"
                )
            except Exception as e:
                raise RuntimeError(
                    f"Failed to convert text file {text_path.name} to PDF: {str(e)}"
                )

            # Validate the generated PDF
            if not pdf_path.exists() or pdf_path.stat().st_size < 100:
                raise RuntimeError(
                    f"PDF conversion failed for {text_path.name} - generated PDF is empty or corrupted."
                )

            return pdf_path

        except Exception as e:
            logging.error(f"Error in convert_text_to_pdf: {str(e)}")
            raise

    @staticmethod
    def _process_inline_markdown(text: str) -> str:
        """
        Process inline markdown formatting (bold, italic, code, links)

        Args:
            text: Raw text with markdown formatting

        Returns:
            Text with ReportLab markup
        """
        import re

        # Escape special characters for ReportLab
        text = text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

        # Bold text: **text** or __text__
        text = re.sub(r"\*\*(.*?)\*\*", r"<b>\1</b>", text)
        text = re.sub(r"__(.*?)__", r"<b>\1</b>", text)

        # Italic text: *text* or _text_ (but not in the middle of words)
        text = re.sub(r"(?<!\w)\*([^*\n]+?)\*(?!\w)", r"<i>\1</i>", text)
        text = re.sub(r"(?<!\w)_([^_\n]+?)_(?!\w)", r"<i>\1</i>", text)

        # Inline code: `code`
        text = re.sub(
            r"`([^`]+?)`",
            r'<font name="Courier" size="9" color="darkred">\1</font>',
            text,
        )

        # Links: [text](url) - convert to text with URL annotation
        def link_replacer(match):
            link_text = match.group(1)
            url = match.group(2)
            return f'<link href="{url}" color="blue"><u>{link_text}</u></link>'

        text = re.sub(r"\[([^\]]+?)\]\(([^)]+?)\)", link_replacer, text)

        # Strikethrough: ~~text~~
        text = re.sub(r"~~(.*?)~~", r"<strike>\1</strike>", text)

        return text

    def convert_to_pdf(
        self,
        file_path: Union[str, Path],
        output_dir: Optional[str] = None,
    ) -> Path:
        """
        Convert document to PDF based on file extension

        Args:
            file_path: Path to the file to be converted
            output_dir: Output directory path

        Returns:
            Path to the generated PDF file
        """
        # Convert to Path object
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"File does not exist: {file_path}")

        # Get file extension
        ext = file_path.suffix.lower()

        # Choose appropriate conversion method based on file type
        if ext in self.OFFICE_FORMATS:
            return self.convert_office_to_pdf(file_path, output_dir)
        elif ext in self.TEXT_FORMATS:
            return self.convert_text_to_pdf(file_path, output_dir)
        else:
            raise ValueError(
                f"Unsupported file format: {ext}. "
                f"Supported formats: {', '.join(self.OFFICE_FORMATS | self.TEXT_FORMATS)}"
            )

    def check_dependencies(self) -> dict:
        """
        Check if required dependencies are available

        Returns:
            dict: Dictionary with dependency check results
        """
        results = {
            "libreoffice": False,
            "reportlab": False,
        }

        # Check LibreOffice
        try:
            subprocess_kwargs: Dict[str, Any] = {
                "capture_output": True,
                "text": True,
                "check": True,
                "encoding": "utf-8",
                "errors": "ignore",
            }

            if platform.system() == "Windows":
                subprocess_kwargs["creationflags"] = (
                    0x08000000  # subprocess.CREATE_NO_WINDOW
                )

            subprocess.run(["libreoffice", "--version"], **subprocess_kwargs)
            results["libreoffice"] = True
        except (subprocess.CalledProcessError, FileNotFoundError):
            try:
                subprocess.run(["soffice", "--version"], **subprocess_kwargs)
                results["libreoffice"] = True
            except (subprocess.CalledProcessError, FileNotFoundError):
                pass

        # Check ReportLab
        import importlib.util

        if importlib.util.find_spec("reportlab") is not None:
            results["reportlab"] = True

        return results


def main():
    """
    Main function to run the PDF converter from command line
    """
    parser = argparse.ArgumentParser(description="Convert documents to PDF format")
    parser.add_argument("file_path", nargs="?", help="Path to the document to convert")
    parser.add_argument("--output", "-o", help="Output directory path")
    parser.add_argument(
        "--check",
        action="store_true",
        help="Check dependencies installation",
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable verbose logging"
    )

    args = parser.parse_args()

    # Configure logging
    log_level = logging.INFO if args.verbose else logging.WARNING
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # Initialize converter
    converter = PDFConverter()

    # Check dependencies if requested
    if args.check:
        print("üîç Checking dependencies...")
        deps = converter.check_dependencies()

        print(
            f"LibreOffice: {'‚úÖ Available' if deps['libreoffice'] else '‚ùå Not found'}"
        )
        print(f"ReportLab: {'‚úÖ Available' if deps['reportlab'] else '‚ùå Not found'}")

        if not deps["libreoffice"]:
            print("\nüìã To install LibreOffice:")
            print("  - Windows: Download from https://www.libreoffice.org/")
            print("  - macOS: brew install --cask libreoffice")
            print("  - Ubuntu/Debian: sudo apt-get install libreoffice")

        if not deps["reportlab"]:
            print("\nüìã To install ReportLab:")
            print("  pip install reportlab")

        return 0

    # If not checking dependencies, file_path is required
    if not args.file_path:
        parser.error("file_path is required when not using --check")

    try:
        # Convert the file
        output_pdf = converter.convert_to_pdf(
            file_path=args.file_path,
            output_dir=args.output,
        )

        print(f"‚úÖ Successfully converted to PDF: {output_pdf}")
        print(f"üìÑ File size: {output_pdf.stat().st_size / 1024:.1f} KB")

    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())


## Links discovered
- [text](https://github.com/HKUDS/DeepCode/blob/main/tools/url.md)

--- tools/pdf_downloader.py ---
#!/usr/bin/env python3
"""
Smart PDF Downloader MCP Tool

A standardized MCP tool using FastMCP for intelligent file downloading and document conversion.
Supports natural language instructions for downloading files from URLs, moving local files,
and automatic conversion to Markdown format with image extraction.

Features:
- Natural language instruction parsing
- URL and local path extraction
- Automatic document conversion (PDF, DOCX, PPTX, HTML, etc.)
- Image extraction and preservation
- Multi-format support with fallback options
"""

import os
import re
import aiohttp
import aiofiles
import shutil
import sys
import io
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse, unquote
from datetime import datetime

from mcp.server import FastMCP

# Docling imports for document conversion
try:
    from docling.document_converter import DocumentConverter
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions
    from docling.document_converter import PdfFormatOption

    DOCLING_AVAILABLE = True
except ImportError:
    DOCLING_AVAILABLE = False
    print(
        "Warning: docling package not available. Document conversion will be disabled."
    )

# Fallback PDF text extraction
try:
    import PyPDF2

    PYPDF2_AVAILABLE = True
except ImportError:
    PYPDF2_AVAILABLE = False
    print(
        "Warning: PyPDF2 package not available. Fallback PDF extraction will be disabled."
    )

# ËÆæÁΩÆÊ†áÂáÜËæìÂá∫ÁºñÁ†Å‰∏∫UTF-8
if sys.stdout.encoding != "utf-8":
    try:
        if hasattr(sys.stdout, "reconfigure"):
            sys.stdout.reconfigure(encoding="utf-8")
            sys.stderr.reconfigure(encoding="utf-8")
        else:
            sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding="utf-8")
            sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding="utf-8")
    except Exception as e:
        print(f"Warning: Could not set UTF-8 encoding: {e}")

# ÂàõÂª∫ FastMCP ÂÆû‰æã
mcp = FastMCP("smart-pdf-downloader")


# ËæÖÂä©ÂáΩÊï∞
def format_success_message(action: str, details: Dict[str, Any]) -> str:
    """Ê†ºÂºèÂåñÊàêÂäüÊ∂àÊÅØ"""
    return f"‚úÖ {action}\n" + "\n".join(f"   {k}: {v}" for k, v in details.items())


def format_error_message(action: str, error: str) -> str:
    """Ê†ºÂºèÂåñÈîôËØØÊ∂àÊÅØ"""
    return f"‚ùå {action}\n   Error: {error}"


def format_warning_message(action: str, warning: str) -> str:
    """Ê†ºÂºèÂåñË≠¶ÂëäÊ∂àÊÅØ"""
    return f"‚ö†Ô∏è {action}\n   Warning: {warning}"


async def perform_document_conversion(
    file_path: str, extract_images: bool = True
) -> Optional[str]:
    """
    ÊâßË°åÊñáÊ°£ËΩ¨Êç¢ÁöÑÂÖ±Áî®ÈÄªËæë

    Args:
        file_path: Êñá‰ª∂Ë∑ØÂæÑ
        extract_images: ÊòØÂê¶ÊèêÂèñÂõæÁâá

    Returns:
        ËΩ¨Êç¢‰ø°ÊÅØÂ≠óÁ¨¶‰∏≤ÔºåÂ¶ÇÊûúÊ≤°ÊúâËΩ¨Êç¢ÂàôËøîÂõûNone
    """
    if not file_path:
        return None

    conversion_msg = ""

    # È¶ñÂÖàÂ∞ùËØï‰ΩøÁî®ÁÆÄÂçïÁöÑPDFËΩ¨Êç¢Âô®ÔºàÂØπ‰∫éPDFÊñá‰ª∂Ôºâ
    # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶ÂÆûÈôÖ‰∏∫PDFÔºàÊó†ËÆ∫Êâ©Â±ïÂêçÂ¶Ç‰ΩïÔºâ
    is_pdf_file = False
    if PYPDF2_AVAILABLE:
        try:
            with open(file_path, "rb") as f:
                header = f.read(8)
                is_pdf_file = header.startswith(b"%PDF")
        except Exception:
            is_pdf_file = file_path.lower().endswith(".pdf")

    if is_pdf_file and PYPDF2_AVAILABLE:
        try:
            simple_converter = SimplePdfConverter()
            conversion_result = simple_converter.convert_pdf_to_markdown(file_path)
            if conversion_result["success"]:
                conversion_msg = "\n   [INFO] PDF converted to Markdown (PyPDF2)"
                conversion_msg += (
                    f"\n   Markdown file: {conversion_result['output_file']}"
                )
                conversion_msg += (
                    f"\n   Conversion time: {conversion_result['duration']:.2f} seconds"
                )
                conversion_msg += (
                    f"\n   Pages extracted: {conversion_result['pages_extracted']}"
                )

            else:
                conversion_msg = f"\n   [WARNING] PDF conversion failed: {conversion_result['error']}"
        except Exception as conv_error:
            conversion_msg = f"\n   [WARNING] PDF conversion error: {str(conv_error)}"

    # Â¶ÇÊûúÁÆÄÂçïËΩ¨Êç¢Â§±Ë¥•ÔºåÂ∞ùËØï‰ΩøÁî®doclingÔºàÊîØÊåÅÂõæÁâáÊèêÂèñÔºâ
    # if not conversion_success and DOCLING_AVAILABLE:
    #     try:
    #         converter = DoclingConverter()
    #         if converter.is_supported_format(file_path):
    #             conversion_result = converter.convert_to_markdown(
    #                 file_path, extract_images=extract_images
    #             )
    #             if conversion_result["success"]:
    #                 conversion_msg = (
    #                     "\n   [INFO] Document converted to Markdown (docling)"
    #                 )
    #                 conversion_msg += (
    #                     f"\n   Markdown file: {conversion_result['output_file']}"
    #                 )
    #                 conversion_msg += f"\n   Conversion time: {conversion_result['duration']:.2f} seconds"
    #                 if conversion_result.get("images_extracted", 0) > 0:
    #                     conversion_msg += f"\n   Images extracted: {conversion_result['images_extracted']}"
    #                     images_dir = os.path.join(
    #                         os.path.dirname(conversion_result["output_file"]), "images"
    #                     )
    #                     conversion_msg += f"\n   Images saved to: {images_dir}"
    #             else:
    #                 conversion_msg = f"\n   [WARNING] Docling conversion failed: {conversion_result['error']}"
    #     except Exception as conv_error:
    #         conversion_msg = (
    #             f"\n   [WARNING] Docling conversion error: {str(conv_error)}"
    #         )

    return conversion_msg if conversion_msg else None


def format_file_operation_result(
    operation: str,
    source: str,
    destination: str,
    result: Dict[str, Any],
    conversion_msg: Optional[str] = None,
) -> str:
    """
    Ê†ºÂºèÂåñÊñá‰ª∂Êìç‰ΩúÁªìÊûúÁöÑÂÖ±Áî®ÈÄªËæë

    Args:
        operation: Êìç‰ΩúÁ±ªÂûã ("download", "copy", Êàñ "move")
        source: Ê∫êÊñá‰ª∂/URL
        destination: ÁõÆÊ†áË∑ØÂæÑ
        result: Êìç‰ΩúÁªìÊûúÂ≠óÂÖ∏
        conversion_msg: ËΩ¨Êç¢Ê∂àÊÅØ

    Returns:
        Ê†ºÂºèÂåñÁöÑÁªìÊûúÊ∂àÊÅØ
    """
    if result["success"]:
        size_mb = result["size"] / (1024 * 1024)

        # Â§ÑÁêÜ‰∏çÂêåÊìç‰ΩúÁ±ªÂûãÁöÑÂä®ËØçÂΩ¢Âºè
        if operation == "copy":
            operation_verb = "copied"
        elif operation == "download":
            operation_verb = "downloaded"
        else:  # move
            operation_verb = "moved"

        msg = f"[SUCCESS] Successfully {operation_verb}: {source}\n"

        if operation == "download":
            msg += f"   File: {destination}\n"
            msg += f"   Size: {size_mb:.2f} MB\n"
            msg += f"   Time: {result['duration']:.2f} seconds\n"
            speed_mb = result.get("speed", 0) / (1024 * 1024)
            msg += f"   Speed: {speed_mb:.2f} MB/s"
        else:  # copy or move
            msg += f"   To: {destination}\n"
            msg += f"   Size: {size_mb:.2f} MB\n"
            msg += f"   Time: {result['duration']:.2f} seconds"
            if operation == "copy":
                msg += "\n   Note: Original file preserved"

        if conversion_msg:
            msg += conversion_msg

        return msg
    else:
        return f"[ERROR] Failed to {operation}: {source}\n   Error: {result.get('error', 'Unknown error')}"


class LocalPathExtractor:
    """Êú¨Âú∞Ë∑ØÂæÑÊèêÂèñÂô®"""

    @staticmethod
    def is_local_path(path: str) -> bool:
        """Âà§Êñ≠ÊòØÂê¶‰∏∫Êú¨Âú∞Ë∑ØÂæÑ"""
        path = path.strip("\"'")

        # Ê£ÄÊü•ÊòØÂê¶‰∏∫URL
        if re.match(r"^https?://", path, re.IGNORECASE) or re.match(
            r"^ftp://", path, re.IGNORECASE
        ):
            return False

        # Ë∑ØÂæÑÊåáÁ§∫Á¨¶
        path_indicators = [os.path.sep, "/", "\\", "~", ".", ".."]
        has_extension = bool(os.path.splitext(path)[1])

        if any(indicator in path for indicator in path_indicators) or has_extension:
            expanded_path = os.path.expanduser(path)
            return os.path.exists(expanded_path) or any(
                indicator in path for indicator in path_indicators
            )

        return False

    @staticmethod
    def extract_local_paths(text: str) -> List[str]:
        """‰ªéÊñáÊú¨‰∏≠ÊèêÂèñÊú¨Âú∞Êñá‰ª∂Ë∑ØÂæÑ"""
        patterns = [
            r'"([^"]+)"',
            r"'([^']+)'",
            r"(?:^|\s)((?:[~./\\]|[A-Za-z]:)?(?:[^/\\\s]+[/\\])*[^/\\\s]+\.[A-Za-z0-9]+)(?:\s|$)",
            r"(?:^|\s)((?:~|\.{1,2})?/[^\s]+)(?:\s|$)",
            r"(?:^|\s)([A-Za-z]:[/\\][^\s]+)(?:\s|$)",
            r"(?:^|\s)(\.{1,2}[/\\][^\s]+)(?:\s|$)",
        ]

        local_paths = []
        potential_paths = []

        for pattern in patterns:
            matches = re.findall(pattern, text, re.MULTILINE)
            potential_paths.extend(matches)

        for path in potential_paths:
            path = path.strip()
            if path and LocalPathExtractor.is_local_path(path):
                expanded_path = os.path.expanduser(path)
                if expanded_path not in local_paths:
                    local_paths.append(expanded_path)

        return local_paths


class URLExtractor:
    """URLÊèêÂèñÂô®"""

    URL_PATTERNS = [
        r"https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:/(?:[-\w._~!$&\'()*+,;=:@]|%[\da-fA-F]{2})*)*(?:\?(?:[-\w._~!$&\'()*+,;=:@/?]|%[\da-fA-F]{2})*)?(?:#(?:[-\w._~!$&\'()*+,;=:@/?]|%[\da-fA-F]{2})*)?",
        r"ftp://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:/(?:[-\w._~!$&\'()*+,;=:@]|%[\da-fA-F]{2})*)*",
        r"(?<!\S)(?:www\.)?[-\w]+(?:\.[-\w]+)+/(?:[-\w._~!$&\'()*+,;=:@/]|%[\da-fA-F]{2})+",
    ]

    @staticmethod
    def convert_arxiv_url(url: str) -> str:
        """Â∞ÜarXivÁΩëÈ°µÈìæÊé•ËΩ¨Êç¢‰∏∫PDF‰∏ãËΩΩÈìæÊé•"""
        # ÂåπÈÖçarXivËÆ∫ÊñáIDÁöÑÊ≠£ÂàôË°®ËææÂºè
        arxiv_pattern = r"arxiv\.org/abs/(\d+\.\d+)(?:v\d+)?"
        match = re.search(arxiv_pattern, url, re.IGNORECASE)
        if match:
            paper_id = match.group(1)
            return f"https://arxiv.org/pdf/{paper_id}.pdf"
        return url

    @classmethod
    def extract_urls(cls, text: str) -> List[str]:
        """‰ªéÊñáÊú¨‰∏≠ÊèêÂèñURL"""
        urls = []

        # È¶ñÂÖàÂ§ÑÁêÜÁâπÊÆäÊÉÖÂÜµÔºö@ÂºÄÂ§¥ÁöÑURL
        at_url_pattern = r"@(https?://[^\s]+)"
        at_matches = re.findall(at_url_pattern, text, re.IGNORECASE)
        for match in at_matches:
            # Â§ÑÁêÜarXivÈìæÊé•
            url = cls.convert_arxiv_url(match.rstrip("/"))
            urls.append(url)

        # ÁÑ∂Âêé‰ΩøÁî®ÂéüÊúâÁöÑÊ≠£ÂàôÊ®°Âºè
        for pattern in cls.URL_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # Â§ÑÁêÜÂèØËÉΩÁº∫Â∞ëÂçèËÆÆÁöÑURL
                if not match.startswith(("http://", "https://", "ftp://")):
                    # Ê£ÄÊü•ÊòØÂê¶ÊòØ www ÂºÄÂ§¥
                    if match.startswith("www."):
                        match = "https://" + match
                    else:
                        # ÂÖ∂‰ªñÊÉÖÂÜµ‰πüÊ∑ªÂä† https
                        match = "https://" + match

                # Â§ÑÁêÜarXivÈìæÊé•
                url = cls.convert_arxiv_url(match.rstrip("/"))
                urls.append(url)

        # ÂéªÈáçÂπ∂‰øùÊåÅÈ°∫Â∫è
        seen = set()
        unique_urls = []
        for url in urls:
            if url not in seen:
                seen.add(url)
                unique_urls.append(url)

        return unique_urls

    @staticmethod
    def infer_filename_from_url(url: str) -> str:
        """‰ªéURLÊé®Êñ≠Êñá‰ª∂Âêç"""
        parsed = urlparse(url)
        path = unquote(parsed.path)

        # ‰ªéË∑ØÂæÑ‰∏≠ÊèêÂèñÊñá‰ª∂Âêç
        filename = os.path.basename(path)

        # ÁâπÊÆäÂ§ÑÁêÜÔºöarxiv PDFÈìæÊé•
        if "arxiv.org" in parsed.netloc and "/pdf/" in path:
            if filename:
                # Ê£ÄÊü•ÊòØÂê¶Â∑≤ÁªèÊúâÂêàÈÄÇÁöÑÊñá‰ª∂Êâ©Â±ïÂêç
                if not filename.lower().endswith((".pdf", ".doc", ".docx", ".txt")):
                    filename = f"{filename}.pdf"
            else:
                path_parts = [p for p in path.split("/") if p]
                if path_parts and path_parts[-1]:
                    filename = f"{path_parts[-1]}.pdf"
                else:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"arxiv_paper_{timestamp}.pdf"

        # Â¶ÇÊûúÊ≤°ÊúâÊñá‰ª∂ÂêçÊàñÊ≤°ÊúâÊâ©Â±ïÂêçÔºåÁîüÊàê‰∏Ä‰∏™
        elif not filename or "." not in filename:
            # Â∞ùËØï‰ªéURLÁîüÊàêÊúâÊÑè‰πâÁöÑÊñá‰ª∂Âêç
            domain = parsed.netloc.replace("www.", "").replace(".", "_")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # Â∞ùËØïÊ†πÊçÆË∑ØÂæÑÊé®Êñ≠Êñá‰ª∂Á±ªÂûã
            if not path or path == "/":
                filename = f"{domain}_{timestamp}.html"
            else:
                # ‰ΩøÁî®Ë∑ØÂæÑÁöÑÊúÄÂêé‰∏ÄÈÉ®ÂàÜ
                path_parts = [p for p in path.split("/") if p]
                if path_parts:
                    filename = f"{path_parts[-1]}_{timestamp}"
                else:
                    filename = f"{domain}_{timestamp}"

                # Â¶ÇÊûúËøòÊòØÊ≤°ÊúâÊâ©Â±ïÂêçÔºåÊ†πÊçÆË∑ØÂæÑÊé®Êñ≠
                if "." not in filename:
                    # Ê†πÊçÆË∑ØÂæÑ‰∏≠ÁöÑÂÖ≥ÈîÆËØçÊé®Êñ≠Êñá‰ª∂Á±ªÂûã
                    if "/pdf/" in path.lower() or path.lower().endswith("pdf"):
                        filename += ".pdf"
                    elif any(
                        ext in path.lower() for ext in ["/doc/", "/word/", ".docx"]
                    ):
                        filename += ".docx"
                    elif any(
                        ext in path.lower()
                        for ext in ["/ppt/", "/powerpoint/", ".pptx"]
                    ):
                        filename += ".pptx"
                    elif any(ext in path.lower() for ext in ["/csv/", ".csv"]):
                        filename += ".csv"
                    elif any(ext in path.lower() for ext in ["/zip/", ".zip"]):
                        filename += ".zip"
                    else:
                        filename += ".html"

        return filename


class PathExtractor:
    """Ë∑ØÂæÑÊèêÂèñÂô®"""

    @staticmethod
    def extract_target_path(text: str) -> Optional[str]:
        """‰ªéÊñáÊú¨‰∏≠ÊèêÂèñÁõÆÊ†áË∑ØÂæÑ"""
        patterns = [
            r'(?:save|download|store|put|place|write|copy|move)\s+(?:to|into|in|at)\s+["\']?([^\s"\']+)["\']?',
            r'(?:to|into|in|at)\s+(?:folder|directory|dir|path|location)\s*["\']?([^\s"\']+)["\']?',
            r'(?:destination|target|output)\s*(?:is|:)?\s*["\']?([^\s"\']+)["\']?',
            r'(?:‰øùÂ≠ò|‰∏ãËΩΩ|Â≠òÂÇ®|ÊîæÂà∞|ÂÜôÂÖ•|Â§çÂà∂|ÁßªÂä®)(?:Âà∞|Ëá≥|Âéª)\s*["\']?([^\s"\']+)["\']?',
            r'(?:Âà∞|Âú®|Ëá≥)\s*["\']?([^\s"\']+)["\']?\s*(?:Êñá‰ª∂Â§π|ÁõÆÂΩï|Ë∑ØÂæÑ|‰ΩçÁΩÆ)',
        ]

        filter_words = {
            "here",
            "there",
            "current",
            "local",
            "this",
            "that",
            "ËøôÈáå",
            "ÈÇ£Èáå",
            "ÂΩìÂâç",
            "Êú¨Âú∞",
            "Ëøô‰∏™",
            "ÈÇ£‰∏™",
        }

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                path = match.group(1).strip("„ÄÇÔºå,.„ÄÅ")
                if path and path.lower() not in filter_words:
                    return path

        return None


class SimplePdfConverter:
    """ÁÆÄÂçïÁöÑPDFËΩ¨Êç¢Âô®Ôºå‰ΩøÁî®PyPDF2ÊèêÂèñÊñáÊú¨"""

    def convert_pdf_to_markdown(
        self, input_file: str, output_file: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ‰ΩøÁî®PyPDF2Â∞ÜPDFËΩ¨Êç¢‰∏∫MarkdownÊ†ºÂºè

        Args:
            input_file: ËæìÂÖ•PDFÊñá‰ª∂Ë∑ØÂæÑ
            output_file: ËæìÂá∫MarkdownÊñá‰ª∂Ë∑ØÂæÑÔºàÂèØÈÄâÔºâ

        Returns:
            ËΩ¨Êç¢ÁªìÊûúÂ≠óÂÖ∏
        """
        if not PYPDF2_AVAILABLE:
            return {"success": False, "error": "PyPDF2 package is not available"}

        try:
            # Ê£ÄÊü•ËæìÂÖ•Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
            if not os.path.exists(input_file):
                return {
                    "success": False,
                    "error": f"Input file not found: {input_file}",
                }

            # Â¶ÇÊûúÊ≤°ÊúâÊåáÂÆöËæìÂá∫Êñá‰ª∂ÔºåËá™Âä®ÁîüÊàê
            if not output_file:
                base_name = os.path.splitext(input_file)[0]
                output_file = f"{base_name}.md"

            # Á°Æ‰øùËæìÂá∫ÁõÆÂΩïÂ≠òÂú®
            output_dir = os.path.dirname(output_file)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)

            # ÊâßË°åËΩ¨Êç¢
            start_time = datetime.now()

            # ËØªÂèñPDFÊñá‰ª∂
            with open(input_file, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_content = []

                # ÊèêÂèñÊØèÈ°µÊñáÊú¨
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    text = page.extract_text()
                    if text.strip():
                        text_content.append(f"## Page {page_num}\n\n{text.strip()}\n\n")

            # ÁîüÊàêMarkdownÂÜÖÂÆπ
            markdown_content = f"# Extracted from {os.path.basename(input_file)}\n\n"
            markdown_content += f"*Total pages: {len(pdf_reader.pages)}*\n\n"
            markdown_content += "---\n\n"
            markdown_content += "".join(text_content)

            # ‰øùÂ≠òÂà∞Êñá‰ª∂
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            # ËÆ°ÁÆóËΩ¨Êç¢Êó∂Èó¥
            duration = (datetime.now() - start_time).total_seconds()

            # Ëé∑ÂèñÊñá‰ª∂Â§ßÂ∞è
            input_size = os.path.getsize(input_file)
            output_size = os.path.getsize(output_file)

            return {
                "success": True,
                "input_file": input_file,
                "output_file": output_file,
                "input_size": input_size,
                "output_size": output_size,
                "duration": duration,
                "markdown_content": markdown_content,
                "pages_extracted": len(pdf_reader.pages),
            }

        except Exception as e:
            return {
                "success": False,
                "input_file": input_file,
                "error": f"Conversion failed: {str(e)}",
            }


class DoclingConverter:
    """ÊñáÊ°£ËΩ¨Êç¢Âô®Ôºå‰ΩøÁî®doclingÂ∞ÜÊñáÊ°£ËΩ¨Êç¢‰∏∫MarkdownÊ†ºÂºèÔºåÊîØÊåÅÂõæÁâáÊèêÂèñ"""

    def __init__(self):
        if not DOCLING_AVAILABLE:
            raise ImportError(
                "docling package is not available. Please install it first."
            )

        # ÈÖçÁΩÆPDFÂ§ÑÁêÜÈÄâÈ°π
        pdf_pipeline_options = PdfPipelineOptions()
        pdf_pipeline_options.do_ocr = False  # ÊöÇÊó∂Á¶ÅÁî®OCR‰ª•ÈÅøÂÖçËÆ§ËØÅÈóÆÈ¢ò
        pdf_pipeline_options.do_table_structure = False  # ÊöÇÊó∂Á¶ÅÁî®Ë°®Ê†ºÁªìÊûÑËØÜÂà´

        # ÂàõÂª∫ÊñáÊ°£ËΩ¨Êç¢Âô®Ôºà‰ΩøÁî®Âü∫Á°ÄÊ®°ÂºèÔºâ
        try:
            self.converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_options=pdf_pipeline_options
                    )
                }
            )
        except Exception:
            # Â¶ÇÊûúÂ§±Ë¥•ÔºåÂ∞ùËØïÊõ¥ÁÆÄÂçïÁöÑÈÖçÁΩÆ
            self.converter = DocumentConverter()

    def is_supported_format(self, file_path: str) -> bool:
        """Ê£ÄÊü•Êñá‰ª∂Ê†ºÂºèÊòØÂê¶ÊîØÊåÅËΩ¨Êç¢"""
        if not DOCLING_AVAILABLE:
            return False

        supported_extensions = {".pdf", ".docx", ".pptx", ".html", ".md", ".txt"}
        file_extension = os.path.splitext(file_path)[1].lower()
        return file_extension in supported_extensions

    def is_url(self, path: str) -> bool:
        """Ê£ÄÊü•Ë∑ØÂæÑÊòØÂê¶‰∏∫URL"""
        try:
            result = urlparse(path)
            return result.scheme in ("http", "https")
        except Exception:
            return False

    def extract_images(self, doc, output_dir: str) -> Dict[str, str]:
        """
        ÊèêÂèñÊñáÊ°£‰∏≠ÁöÑÂõæÁâáÂπ∂‰øùÂ≠òÂà∞Êú¨Âú∞

        Args:
            doc: doclingÊñáÊ°£ÂØπË±°
            output_dir: ËæìÂá∫ÁõÆÂΩï

        Returns:
            ÂõæÁâáIDÂà∞Êú¨Âú∞Êñá‰ª∂Ë∑ØÂæÑÁöÑÊò†Â∞Ñ
        """
        images_dir = os.path.join(output_dir, "images")
        os.makedirs(images_dir, exist_ok=True)
        image_map = {}  # doclingÂõæÁâáid -> Êú¨Âú∞Êñá‰ª∂Âêç

        try:
            # Ëé∑ÂèñÊñáÊ°£‰∏≠ÁöÑÂõæÁâá
            images = getattr(doc, "images", [])

            for idx, img in enumerate(images):
                try:
                    # Ëé∑ÂèñÂõæÁâáÊ†ºÂºèÔºåÈªòËÆ§‰∏∫png
                    ext = getattr(img, "format", None) or "png"
                    if ext.lower() not in ["png", "jpg", "jpeg", "gif", "bmp", "webp"]:
                        ext = "png"

                    # ÁîüÊàêÊñá‰ª∂Âêç
                    filename = f"image_{idx+1}.{ext}"
                    filepath = os.path.join(images_dir, filename)

                    # ‰øùÂ≠òÂõæÁâáÊï∞ÊçÆ
                    img_data = getattr(img, "data", None)
                    if img_data:
                        with open(filepath, "wb") as f:
                            f.write(img_data)

                        # ËÆ°ÁÆóÁõ∏ÂØπË∑ØÂæÑ
                        rel_path = os.path.relpath(filepath, output_dir)
                        img_id = getattr(img, "id", str(idx + 1))
                        image_map[img_id] = rel_path

                except Exception as img_error:
                    print(f"Warning: Failed to extract image {idx+1}: {img_error}")
                    continue

        except Exception as e:
            print(f"Warning: Failed to extract images: {e}")

        return image_map

    def process_markdown_with_images(
        self, markdown_content: str, image_map: Dict[str, str]
    ) -> str:
        """
        Â§ÑÁêÜMarkdownÂÜÖÂÆπÔºåÊõøÊç¢ÂõæÁâáÂç†‰ΩçÁ¨¶‰∏∫ÂÆûÈôÖÁöÑÂõæÁâáË∑ØÂæÑ

        Args:
            markdown_content: ÂéüÂßãMarkdownÂÜÖÂÆπ
            image_map: ÂõæÁâáIDÂà∞Êú¨Âú∞Ë∑ØÂæÑÁöÑÊò†Â∞Ñ

        Returns:
            Â§ÑÁêÜÂêéÁöÑMarkdownÂÜÖÂÆπ
        """

        def replace_img(match):
            img_id = match.group(1)
            if img_id in image_map:
                return f"![Image]({image_map[img_id]})"
            else:
                return match.group(0)

        # ÊõøÊç¢doclingÁöÑÂõæÁâáÂç†‰ΩçÁ¨¶
        processed_content = re.sub(
            r"!\[Image\]\(docling://image/([^)]+)\)", replace_img, markdown_content
        )

        return processed_content

    def convert_to_markdown(
        self,
        input_file: str,
        output_file: Optional[str] = None,
        extract_images: bool = True,
    ) -> Dict[str, Any]:
        """
        Â∞ÜÊñáÊ°£ËΩ¨Êç¢‰∏∫MarkdownÊ†ºÂºèÔºåÊîØÊåÅÂõæÁâáÊèêÂèñ

        Args:
            input_file: ËæìÂÖ•Êñá‰ª∂Ë∑ØÂæÑÊàñURL
            output_file: ËæìÂá∫MarkdownÊñá‰ª∂Ë∑ØÂæÑÔºàÂèØÈÄâÔºâ
            extract_images: ÊòØÂê¶ÊèêÂèñÂõæÁâáÔºàÈªòËÆ§TrueÔºâ

        Returns:
            ËΩ¨Êç¢ÁªìÊûúÂ≠óÂÖ∏
        """
        if not DOCLING_AVAILABLE:
            return {"success": False, "error": "docling package is not available"}

        try:
            # Ê£ÄÊü•ËæìÂÖ•Êñá‰ª∂ÔºàÂ¶ÇÊûú‰∏çÊòØURLÔºâ
            if not self.is_url(input_file):
                if not os.path.exists(input_file):
                    return {
                        "success": False,
                        "error": f"Input file not found: {input_file}",
                    }

                # Ê£ÄÊü•Êñá‰ª∂Ê†ºÂºèÊòØÂê¶ÊîØÊåÅ
                if not self.is_supported_format(input_file):
                    return {
                        "success": False,
                        "error": f"Unsupported file format: {os.path.splitext(input_file)[1]}",
                    }
            else:
                # ÂØπ‰∫éURLÔºåÊ£ÄÊü•ÊòØÂê¶‰∏∫ÊîØÊåÅÁöÑÊ†ºÂºè
                if not input_file.lower().endswith(
                    (".pdf", ".docx", ".pptx", ".html", ".md", ".txt")
                ):
                    return {
                        "success": False,
                        "error": f"Unsupported URL format: {input_file}",
                    }

            # Â¶ÇÊûúÊ≤°ÊúâÊåáÂÆöËæìÂá∫Êñá‰ª∂ÔºåËá™Âä®ÁîüÊàê
            if not output_file:
                if self.is_url(input_file):
                    # ‰ªéURLÁîüÊàêÊñá‰ª∂Âêç
                    filename = URLExtractor.infer_filename_from_url(input_file)
                    base_name = os.path.splitext(filename)[0]
                else:
                    base_name = os.path.splitext(input_file)[0]
                output_file = f"{base_name}.md"

            # Á°Æ‰øùËæìÂá∫ÁõÆÂΩïÂ≠òÂú®
            output_dir = os.path.dirname(output_file) or "."
            os.makedirs(output_dir, exist_ok=True)

            # ÊâßË°åËΩ¨Êç¢
            start_time = datetime.now()
            result = self.converter.convert(input_file)
            doc = result.document

            # ÊèêÂèñÂõæÁâáÔºàÂ¶ÇÊûúÂêØÁî®Ôºâ
            image_map = {}
            images_extracted = 0
            if extract_images:
                image_map = self.extract_images(doc, output_dir)
                images_extracted = len(image_map)

            # Ëé∑ÂèñMarkdownÂÜÖÂÆπ
            markdown_content = doc.export_to_markdown()

            # Â§ÑÁêÜÂõæÁâáÂç†‰ΩçÁ¨¶
            if extract_images and image_map:
                markdown_content = self.process_markdown_with_images(
                    markdown_content, image_map
                )

            # ‰øùÂ≠òÂà∞Êñá‰ª∂
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(markdown_content)

            # ËÆ°ÁÆóËΩ¨Êç¢Êó∂Èó¥
            duration = (datetime.now() - start_time).total_seconds()

            # Ëé∑ÂèñÊñá‰ª∂Â§ßÂ∞è
            if self.is_url(input_file):
                input_size = 0  # URLÊó†Ê≥ïÁõ¥Êé•Ëé∑ÂèñÂ§ßÂ∞è
            else:
                input_size = os.path.getsize(input_file)
            output_size = os.path.getsize(output_file)

            return {
                "success": True,
                "input_file": input_file,
                "output_file": output_file,
                "input_size": input_size,
                "output_size": output_size,
                "duration": duration,
                "markdown_content": markdown_content,
                "images_extracted": images_extracted,
                "image_map": image_map,
            }

        except Exception as e:
            return {
                "success": False,
                "input_file": input_file,
                "error": f"Conversion failed: {str(e)}",
            }


async def check_url_accessible(url: str) -> Dict[str, Any]:
    """Ê£ÄÊü•URLÊòØÂê¶ÂèØËÆøÈóÆ"""
    try:
        timeout = aiohttp.ClientTimeout(total=10)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.head(url, allow_redirects=True) as response:
                return {
                    "accessible": response.status < 400,
                    "status": response.status,
                    "content_type": response.headers.get("Content-Type", ""),
                    "content_length": response.headers.get("Content-Length", 0),
                }
    except Exception:
        return {
            "accessible": False,
            "status": 0,
            "content_type": "",
            "content_length": 0,
        }


async def download_file(url: str, destination: str) -> Dict[str, Any]:
    """‰∏ãËΩΩÂçï‰∏™Êñá‰ª∂"""
    start_time = datetime.now()
    chunk_size = 8192

    try:
        timeout = aiohttp.ClientTimeout(total=300)  # 5ÂàÜÈíüË∂ÖÊó∂
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url) as response:
                # Ê£ÄÊü•ÂìçÂ∫îÁä∂ÊÄÅ
                response.raise_for_status()

                # Ëé∑ÂèñÊñá‰ª∂‰ø°ÊÅØ
                content_type = response.headers.get(
                    "Content-Type", "application/octet-stream"
                )

                # Á°Æ‰øùÁõÆÊ†áÁõÆÂΩïÂ≠òÂú®
                parent_dir = os.path.dirname(destination)
                if parent_dir:
                    os.makedirs(parent_dir, exist_ok=True)

                # ‰∏ãËΩΩÊñá‰ª∂
                downloaded = 0
                async with aiofiles.open(destination, "wb") as file:
                    async for chunk in response.content.iter_chunked(chunk_size):
                        await file.write(chunk)
                        downloaded += len(chunk)

                # ËÆ°ÁÆó‰∏ãËΩΩÊó∂Èó¥
                duration = (datetime.now() - start_time).total_seconds()

                return {
                    "success": True,
                    "url": url,
                    "destination": destination,
                    "size": downloaded,
                    "content_type": content_type,
                    "duration": duration,
                    "speed": downloaded / duration if duration > 0 else 0,
                }

    except aiohttp.ClientError as e:
        return {
            "success": False,
            "url": url,
            "destination": destination,
            "error": f"Network error: {str(e)}",
        }
    except Exception as e:
        return {
            "success": False,
            "url": url,
            "destination": destination,
            "error": f"Download error: {str(e)}",
        }


async def move_local_file(source_path: str, destination: str) -> Dict[str, Any]:
    """Â§çÂà∂Êú¨Âú∞Êñá‰ª∂Âà∞ÁõÆÊ†á‰ΩçÁΩÆÔºà‰øùÁïôÂéüÊñá‰ª∂Ôºâ"""
    start_time = datetime.now()

    try:
        # Ê£ÄÊü•Ê∫êÊñá‰ª∂ÊòØÂê¶Â≠òÂú®
        if not os.path.exists(source_path):
            return {
                "success": False,
                "source": source_path,
                "destination": destination,
                "error": f"Source file not found: {source_path}",
            }

        # Ëé∑ÂèñÊ∫êÊñá‰ª∂‰ø°ÊÅØ
        source_size = os.path.getsize(source_path)

        # Á°Æ‰øùÁõÆÊ†áÁõÆÂΩïÂ≠òÂú®
        parent_dir = os.path.dirname(destination)
        if parent_dir:
            os.makedirs(parent_dir, exist_ok=True)

        # ÊâßË°åÂ§çÂà∂Êìç‰ΩúÔºà‰øùÁïôÂéüÊñá‰ª∂ÔºåÈò≤Ê≠¢Êï∞ÊçÆ‰∏¢Â§±Ôºâ
        shutil.copy2(source_path, destination)

        # ËÆ°ÁÆóÊìç‰ΩúÊó∂Èó¥
        duration = (datetime.now() - start_time).total_seconds()

        return {
            "success": True,
            "source": source_path,
            "destination": destination,
            "size": source_size,
            "duration": duration,
            "operation": "copy",  # Êîπ‰∏∫copy
        }

    except Exception as e:
        return {
            "success": False,
            "source": source_path,
            "destination": destination,
            "error": f"Copy error: {str(e)}",
        }


@mcp.tool()
async def download_files(instruction: str) -> str:
    """
    Download files from URLs or move local files mentioned in natural language instructions.

    Args:
        instruction: Natural language instruction containing URLs/local paths and optional destination paths

    Returns:
        Status message about the download/move operations

    Examples:
        - "Download https://example.com/file.pdf to documents folder"
        - "Move /home/user/file.pdf to documents folder"
        - "Please get https://raw.githubusercontent.com/user/repo/main/data.csv and save it to ~/downloads"
        - "ÁßªÂä® ~/Desktop/report.docx Âà∞ /tmp/documents/"
        - "Download www.example.com/report.xlsx"
    """
    urls = URLExtractor.extract_urls(instruction)
    local_paths = LocalPathExtractor.extract_local_paths(instruction)

    if not urls and not local_paths:
        return format_error_message(
            "Failed to parse instruction",
            "No downloadable URLs or movable local files found",
        )

    target_path = PathExtractor.extract_target_path(instruction)

    # Â§ÑÁêÜÊñá‰ª∂
    results = []

    # Â§ÑÁêÜURL‰∏ãËΩΩ
    for url in urls:
        try:
            # Êé®Êñ≠Êñá‰ª∂Âêç
            filename = URLExtractor.infer_filename_from_url(url)

            # ÊûÑÂª∫ÂÆåÊï¥ÁöÑÁõÆÊ†áË∑ØÂæÑ
            if target_path:
                # Â§ÑÁêÜË∑ØÂæÑ
                if target_path.startswith("~"):
                    target_path = os.path.expanduser(target_path)

                # Á°Æ‰øù‰ΩøÁî®Áõ∏ÂØπË∑ØÂæÑÔºàÂ¶ÇÊûú‰∏çÊòØÁªùÂØπË∑ØÂæÑÔºâ
                if not os.path.isabs(target_path):
                    target_path = os.path.normpath(target_path)

                # Âà§Êñ≠ÊòØÊñá‰ª∂Ë∑ØÂæÑËøòÊòØÁõÆÂΩïË∑ØÂæÑ
                if os.path.splitext(target_path)[1]:  # ÊúâÊâ©Â±ïÂêçÔºåÊòØÊñá‰ª∂
                    destination = target_path
                else:  # ÊòØÁõÆÂΩï
                    destination = os.path.join(target_path, filename)
            else:
                # ÈªòËÆ§‰∏ãËΩΩÂà∞ÂΩìÂâçÁõÆÂΩï
                destination = filename

            # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Â∑≤Â≠òÂú®
            if os.path.exists(destination):
                results.append(
                    f"[WARNING] Skipped {url}: File already exists at {destination}"
                )
                continue

            # ÂÖàÊ£ÄÊü•URLÊòØÂê¶ÂèØËÆøÈóÆ
            check_result = await check_url_accessible(url)
            if not check_result["accessible"]:
                results.append(
                    f"[ERROR] Failed to access {url}: HTTP {check_result['status'] or 'Connection failed'}"
                )
                continue

            # ÊâßË°å‰∏ãËΩΩ
            result = await download_file(url, destination)

            # ÊâßË°åËΩ¨Êç¢ÔºàÂ¶ÇÊûúÊàêÂäü‰∏ãËΩΩÔºâ
            conversion_msg = None
            if result["success"]:
                conversion_msg = await perform_document_conversion(
                    destination, extract_images=True
                )

            # Ê†ºÂºèÂåñÁªìÊûú
            msg = format_file_operation_result(
                "download", url, destination, result, conversion_msg
            )

        except Exception as e:
            msg = f"[ERROR] Failed to download: {url}\n"
            msg += f"   Error: {str(e)}"

        results.append(msg)

    # Â§ÑÁêÜÊú¨Âú∞Êñá‰ª∂ÁßªÂä®
    for local_path in local_paths:
        try:
            # Ëé∑ÂèñÊñá‰ª∂Âêç
            filename = os.path.basename(local_path)

            # ÊûÑÂª∫ÂÆåÊï¥ÁöÑÁõÆÊ†áË∑ØÂæÑ
            if target_path:
                # Â§ÑÁêÜË∑ØÂæÑ
                if target_path.startswith("~"):
                    target_path = os.path.expanduser(target_path)

                # Á°Æ‰øù‰ΩøÁî®Áõ∏ÂØπË∑ØÂæÑÔºàÂ¶ÇÊûú‰∏çÊòØÁªùÂØπË∑ØÂæÑÔºâ
                if not os.path.isabs(target_path):
                    target_path = os.path.normpath(target_path)

                # Âà§Êñ≠ÊòØÊñá‰ª∂Ë∑ØÂæÑËøòÊòØÁõÆÂΩïË∑ØÂæÑ
                if os.path.splitext(target_path)[1]:  # ÊúâÊâ©Â±ïÂêçÔºåÊòØÊñá‰ª∂
                    destination = target_path
                else:  # ÊòØÁõÆÂΩï
                    destination = os.path.join(target_path, filename)
            else:
                # ÈªòËÆ§ÁßªÂä®Âà∞ÂΩìÂâçÁõÆÂΩï
                destination = filename

            # Ê£ÄÊü•ÁõÆÊ†áÊñá‰ª∂ÊòØÂê¶Â∑≤Â≠òÂú®
            if os.path.exists(destination):
                results.append(
                    f"[WARNING] Skipped {local_path}: File already exists at {destination}"
                )
                continue

            # ÊâßË°åÂ§çÂà∂Ôºà‰øùÁïôÂéüÊñá‰ª∂Ôºâ
            result = await move_local_file(local_path, destination)

            # ÊâßË°åËΩ¨Êç¢ÔºàÂ¶ÇÊûúÊàêÂäüÂ§çÂà∂Ôºâ
            conversion_msg = None
            if result["success"]:
                conversion_msg = await perform_document_conversion(
                    destination, extract_images=True
                )

            # Ê†ºÂºèÂåñÁªìÊûú
            msg = format_file_operation_result(
                "copy", local_path, destination, result, conversion_msg
            )

        except Exception as e:
            msg = f"[ERROR] Failed to copy: {local_path}\n"
            msg += f"   Error: {str(e)}"

        results.append(msg)

    return "\n\n".join(results)


@mcp.tool()
async def parse_download_urls(text: str) -> str:
    """
    Extract URLs, local paths and target paths from text without downloading or moving.

    Args:
        text: Text containing URLs, local paths and optional destination paths

    Returns:
        Parsed URLs, local paths and target path information
    """
    urls = URLExtractor.extract_urls(text)
    local_paths = LocalPathExtractor.extract_local_paths(text)
    target_path = PathExtractor.extract_target_path(text)

    content = "üìã Parsed file operation information:\n\n"

    if urls:
        content += f"üîó URLs found ({len(urls)}):\n"
        for i, url in enumerate(urls, 1):
            filename = URLExtractor.infer_filename_from_url(url)
            content += f"  {i}. {url}\n     üìÑ Filename: {filename}\n"
    else:
        content += "üîó No URLs found\n"

    if local_paths:
        content += f"\nüìÅ Local files found ({len(local_paths)}):\n"
        for i, path in enumerate(local_paths, 1):
            exists = os.path.exists(path)
            content += f"  {i}. {path}\n"
            content += f"     ‚úÖ Exists: {'Yes' if exists else 'No'}\n"
            if exists:
                size_mb = os.path.getsize(path) / (1024 * 1024)
                content += f"     üìä Size: {size_mb:.2f} MB\n"
    else:
        content += "\nüìÅ No local files found\n"

    if target_path:
        content += f"\nüéØ Target path: {target_path}"
        if target_path.startswith("~"):
            content += f"\n   (Expanded: {os.path.expanduser(target_path)})"
    else:
        content += "\nüéØ Target path: Not specified (will use current directory)"

    return content


@mcp.tool()
async def download_file_to(
    url: str, destination: Optional[str] = None, filename: Optional[str] = None
) -> str:
    """
    Download a specific file with detailed options.

    Args:
        url: URL to download from
        destination: Target directory or full file path (optional)
        filename: Specific filename to use (optional, ignored if destination is a full file path)

    Returns:
        Status message about the download operation
    """
    # Á°ÆÂÆöÊñá‰ª∂Âêç

    url = URLExtractor.extract_urls(url)[0]

    if not filename:
        filename = URLExtractor.infer_filename_from_url(url)

    if not filename:
        filename = URLExtractor.infer_filename_from_url(url)
    else:
        name_source, extension_source = os.path.splitext(
            os.path.basename(URLExtractor.infer_filename_from_url(url))
        )
        name_destination, extension_destination = os.path.splitext(
            os.path.basename(filename)
        )
        if extension_source:
            filename = name_destination + extension_source
        else:
            filename = name_destination + extension_destination

    # Á°ÆÂÆöÂÆåÊï¥Ë∑ØÂæÑ
    if destination:
        # Â±ïÂºÄÁî®Êà∑ÁõÆÂΩï
        if destination.startswith("~"):
            destination = os.path.expanduser(destination)

        # Ê£ÄÊü•ÊòØÂê¶ÊòØÂÆåÊï¥Êñá‰ª∂Ë∑ØÂæÑ
        if os.path.splitext(destination)[1]:  # ÊúâÊâ©Â±ïÂêç
            target_path = destination
        else:  # ÊòØÁõÆÂΩï
            target_path = os.path.join(destination, filename)
    else:
        target_path = filename

    # Á°Æ‰øù‰ΩøÁî®Áõ∏ÂØπË∑ØÂæÑÔºàÂ¶ÇÊûú‰∏çÊòØÁªùÂØπË∑ØÂæÑÔºâ
    if not os.path.isabs(target_path):
        target_path = os.path.normpath(target_path)

    # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Â∑≤Â≠òÂú®
    if os.path.exists(target_path):
        return format_error_message(
            "Download aborted", f"File already exists at {target_path}"
        )

    # ÂÖàÊ£ÄÊü•URL
    check_result = await check_url_accessible(url)
    if not check_result["accessible"]:
        return format_error_message(
            "Cannot access URL",
            f"{url} (HTTP {check_result['status'] or 'Connection failed'})",
        )

    # ÊòæÁ§∫‰∏ãËΩΩ‰ø°ÊÅØ
    size_mb = (
        int(check_result["content_length"]) / (1024 * 1024)
        if check_result["content_length"]
        else 0
    )
    msg = "[INFO] Downloading file:\n"
    msg += f"   URL: {url}\n"
    msg += f"   Target: {target_path}\n"
    if size_mb > 0:
        msg += f"   Expected size: {size_mb:.2f} MB\n"
    msg += "\n"

    # ÊâßË°å‰∏ãËΩΩ
    result = await download_file(url, target_path)

    # ÊâßË°åËΩ¨Êç¢ÔºàÂ¶ÇÊûúÊàêÂäü‰∏ãËΩΩÔºâ
    conversion_msg = None
    if result["success"]:
        conversion_msg = await perform_document_conversion(
            target_path, extract_images=True
        )

        # Ê∑ªÂä†‰∏ãËΩΩ‰ø°ÊÅØÂâçÁºÄ
        actual_size_mb = result["size"] / (1024 * 1024)
        speed_mb = result["speed"] / (1024 * 1024)
        info_msg = "[SUCCESS] Download completed!\n"
        info_msg += f"   Saved to: {target_path}\n"
        info_msg += f"   Size: {actual_size_mb:.2f} MB\n"
        info_msg += f"   Duration: {result['duration']:.2f} seconds\n"
        info_msg += f"   Speed: {speed_mb:.2f} MB/s\n"
        info_msg += f"   Type: {result['content_type']}"

        if conversion_msg:
            info_msg += conversion_msg

        return msg + info_msg
    else:
        return msg + f"[ERROR] Download failed!\n   Error: {result['error']}"


@mcp.tool()
async def move_file_to(
    source: str, destination: Optional[str] = None, filename: Optional[str] = None
) -> str:
    """
    Copy a local file to a new location (preserves original file).

    Note: Despite the name "move_file_to", this tool COPIES the file to preserve the original.
    This prevents data loss during file processing workflows.

    Args:
        source: Source file path to copy
        destination: Target directory or full file path (optional)
        filename: Specific filename to use (optional, ignored if destination is a full file path)

    Returns:
        Status message about the copy operation
    """
    # Â±ïÂºÄÊ∫êË∑ØÂæÑ
    if source.startswith("~"):
        source = os.path.expanduser(source)

    # Ê£ÄÊü•Ê∫êÊñá‰ª∂ÊòØÂê¶Â≠òÂú®
    if not os.path.exists(source):
        return format_error_message("Copy aborted", f"Source file not found: {source}")

    # Á°ÆÂÆöÊñá‰ª∂Âêç
    if not filename:
        filename = os.path.basename(source)
    else:
        name_source, extension_source = os.path.splitext(os.path.basename(source))
        name_destination, extension_destination = os.path.splitext(
            os.path.basename(filename)
        )
        if extension_source:
            filename = name_destination + extension_source
        else:
            filename = name_destination + extension_destination

    # Á°ÆÂÆöÂÆåÊï¥Ë∑ØÂæÑ
    if destination:
        # Â±ïÂºÄÁî®Êà∑ÁõÆÂΩï
        if destination.startswith("~"):
            destination = os.path.expanduser(destination)

        # Ê£ÄÊü•ÊòØÂê¶ÊòØÂÆåÊï¥Êñá‰ª∂Ë∑ØÂæÑ
        if os.path.splitext(destination)[1]:  # ÊúâÊâ©Â±ïÂêç
            target_path = destination
        else:  # ÊòØÁõÆÂΩï
            target_path = os.path.join(destination, filename)

    else:
        target_path = filename

    # Á°Æ‰øù‰ΩøÁî®Áõ∏ÂØπË∑ØÂæÑÔºàÂ¶ÇÊûú‰∏çÊòØÁªùÂØπË∑ØÂæÑÔºâ
    if not os.path.isabs(target_path):
        target_path = os.path.normpath(target_path)

    # Ê£ÄÊü•ÁõÆÊ†áÊñá‰ª∂ÊòØÂê¶Â∑≤Â≠òÂú®
    if os.path.exists(target_path):
        return f"[ERROR] Target file already exists: {target_path}"

    # ÊòæÁ§∫Â§çÂà∂‰ø°ÊÅØ
    source_size_mb = os.path.getsize(source) / (1024 * 1024)
    msg = "[INFO] Copying file (original preserved):\n"
    msg += f"   Source: {source}\n"
    msg += f"   Target: {target_path}\n"
    msg += f"   Size: {source_size_mb:.2f} MB\n"
    msg += "\n"

    # ÊâßË°åÂ§çÂà∂Ôºà‰øùÁïôÂéüÊñá‰ª∂Ôºâ
    result = await move_local_file(source, target_path)

    # ÊâßË°åËΩ¨Êç¢ÔºàÂ¶ÇÊûúÊàêÂäüÂ§çÂà∂Ôºâ
    conversion_msg = None
    if result["success"]:
        conversion_msg = await perform_document_conversion(
            target_path, extract_images=True
        )

        # Ê∑ªÂä†Â§çÂà∂‰ø°ÊÅØÂâçÁºÄ
        info_msg = "[SUCCESS] File copied successfully (original preserved)!\n"
        info_msg += f"   From: {source}\n"
        info_msg += f"   To: {target_path}\n"
        info_msg += f"   Duration: {result['duration']:.2f} seconds"

        if conversion_msg:
            info_msg += conversion_msg

        return msg + info_msg
    else:
        return msg + f"[ERROR] Copy failed!\n   Error: {result['error']}"


# @mcp.tool()
# async def convert_document_to_markdown(
#     file_path: str, output_path: Optional[str] = None, extract_images: bool = True
# ) -> str:
#     """
#     Convert a document to Markdown format with image extraction support.

#     Supports both local files and URLs. Uses docling for advanced conversion with image extraction,
#     or falls back to PyPDF2 for simple PDF text extraction.

#     Args:
#         file_path: Path to the input document file or URL (supports PDF, DOCX, PPTX, HTML, TXT, MD)
#         output_path: Path for the output Markdown file (optional, auto-generated if not provided)
#         extract_images: Whether to extract images from the document (default: True)

#     Returns:
#         Status message about the conversion operation with preview of converted content

#     Examples:
#         - "convert_document_to_markdown('paper.pdf')"
#         - "convert_document_to_markdown('https://example.com/doc.pdf', 'output.md')"
#         - "convert_document_to_markdown('presentation.pptx', extract_images=False)"
#     """
#     # Ê£ÄÊü•ÊòØÂê¶‰∏∫URL
#     is_url_input = False
#     try:
#         parsed = urlparse(file_path)
#         is_url_input = parsed.scheme in ("http", "https")
#     except Exception:
#         is_url_input = False

#     # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Â≠òÂú®ÔºàÂ¶ÇÊûú‰∏çÊòØURLÔºâ
#     if not is_url_input and not os.path.exists(file_path):
#         return f"[ERROR] Input file not found: {file_path}"

#     # Ê£ÄÊü•ÊòØÂê¶ÊòØPDFÊñá‰ª∂Ôºå‰ºòÂÖà‰ΩøÁî®ÁÆÄÂçïËΩ¨Êç¢Âô®Ôºà‰ªÖÂØπÊú¨Âú∞Êñá‰ª∂Ôºâ
#     if (
#         not is_url_input
#         and file_path.lower().endswith(".pdf")
#         and PYPDF2_AVAILABLE
#         and not extract_images
#     ):
#         try:
#             simple_converter = SimplePdfConverter()
#             result = simple_converter.convert_pdf_to_markdown(file_path, output_path)
#         except Exception as e:
#             return f"[ERROR] PDF conversion error: {str(e)}"
#     elif DOCLING_AVAILABLE:
#         try:
#             converter = DoclingConverter()

#             # Ê£ÄÊü•Êñá‰ª∂Ê†ºÂºèÊòØÂê¶ÊîØÊåÅ
#             if not is_url_input and not converter.is_supported_format(file_path):
#                 supported_formats = [".pdf", ".docx", ".pptx", ".html", ".md", ".txt"]
#                 return f"[ERROR] Unsupported file format. Supported formats: {', '.join(supported_formats)}"
#             elif is_url_input and not file_path.lower().endswith(
#                 (".pdf", ".docx", ".pptx", ".html", ".md", ".txt")
#             ):
#                 return f"[ERROR] Unsupported URL format: {file_path}"

#             # ÊâßË°åËΩ¨Êç¢ÔºàÊîØÊåÅÂõæÁâáÊèêÂèñÔºâ
#             result = converter.convert_to_markdown(
#                 file_path, output_path, extract_images
#             )
#         except Exception as e:
#             return f"[ERROR] Docling conversion error: {str(e)}"
#     else:
#         return (
#             "[ERROR] No conversion tools available. Please install docling or PyPDF2."
#         )

#     if result["success"]:
#         msg = "[SUCCESS] Document converted successfully!\n"
#         msg += f"   Input: {result['input_file']}\n"
#         msg += f"   Output file: {result['output_file']}\n"
#         msg += f"   Conversion time: {result['duration']:.2f} seconds\n"

#         if result["input_size"] > 0:
#             msg += f"   Original size: {result['input_size'] / 1024:.1f} KB\n"
#         msg += f"   Markdown size: {result['output_size'] / 1024:.1f} KB\n"

#         # ÊòæÁ§∫ÂõæÁâáÊèêÂèñ‰ø°ÊÅØ
#         if extract_images and "images_extracted" in result:
#             images_count = result["images_extracted"]
#             if images_count > 0:
#                 msg += f"   Images extracted: {images_count}\n"
#                 msg += f"   Images saved to: {os.path.join(os.path.dirname(result['output_file']), 'images')}\n"
#             else:
#                 msg += "   No images found in document\n"

#         # ÊòæÁ§∫MarkdownÂÜÖÂÆπÁöÑÂâçÂá†Ë°å‰Ωú‰∏∫È¢ÑËßà
#         content_lines = result["markdown_content"].split("\n")
#         preview_lines = content_lines[:5]
#         if len(content_lines) > 5:
#             preview_lines.append("...")

#         msg += "\n[PREVIEW] First few lines of converted Markdown:\n"
#         for line in preview_lines:
#             msg += f"   {line}\n"
#     else:
#         msg = "[ERROR] Conversion failed!\n"
#         msg += f"   Error: {result['error']}"

#     return msg


if __name__ == "__main__":
    print("üìÑ Smart PDF Downloader MCP Tool")
    print("üìù Starting server with FastMCP...")

    if DOCLING_AVAILABLE:
        print("‚úÖ Document conversion to Markdown is ENABLED (docling available)")
    else:
        print("‚ùå Document conversion to Markdown is DISABLED (docling not available)")
        print("   Install docling to enable: pip install docling")

    print("\nAvailable tools:")
    print(
        "  ‚Ä¢ download_files - Download files or move local files from natural language"
    )
    print("  ‚Ä¢ parse_download_urls - Extract URLs, local paths and destination paths")
    print("  ‚Ä¢ download_file_to - Download a specific file with options")
    print("  ‚Ä¢ move_file_to - Move a specific local file with options")
    print("  ‚Ä¢ convert_document_to_markdown - Convert documents to Markdown format")

    if DOCLING_AVAILABLE:
        print("\nSupported formats: PDF, DOCX, PPTX, HTML, TXT, MD")
        print("Features: Image extraction, Layout preservation, Automatic conversion")

    print("")

    # ËøêË°åÊúçÂä°Âô®
    mcp.run()


## Links discovered
- [Image](https://github.com/HKUDS/DeepCode/blob/main/tools/{image_map[img_id]}.md)

--- tools/pdf_utils.py ---
"""
PDF utility functions for the DeepCode agent system.
"""

from pathlib import Path
import PyPDF2


def read_pdf_metadata(file_path: Path) -> dict:
    """Read PDF metadata with proper encoding handling."""
    try:
        print(f"\nAttempting to read PDF metadata from: {file_path}")
        with open(file_path, "rb") as file:
            pdf_reader = PyPDF2.PdfReader(file)
            info = pdf_reader.metadata
            first_page = pdf_reader.pages[0]
            text = first_page.extract_text()
            lines = text.split("\n")[:10]

            title = None
            authors = []

            if info:
                title = info.get("/Title", "").strip().replace("\x00", "")
                author = info.get("/Author", "").strip().replace("\x00", "")
                if author:
                    authors = [author]

            if not title and lines:
                title = lines[0].strip()

            if not authors and len(lines) > 1:
                for line in lines[1:3]:
                    if "author" in line.lower() or "by" in line.lower():
                        authors = [line.strip()]
                        break

            return {
                "title": title if title else "Unknown Title",
                "authors": authors if authors else ["Unknown Author"],
                "year": info.get("/CreationDate", "")[:4] if info else "Unknown Year",
                "first_lines": lines,
            }

    except Exception as e:
        print(f"\nError reading PDF: {str(e)}")
        return {
            "title": "Error reading PDF",
            "authors": ["Unknown"],
            "year": "Unknown",
            "first_lines": [],
        }


--- ui/app.py ---
"""
DeepCode UI Application Entry Point

This file serves as the unified entry point for the UI module
"""

from .streamlit_app import main

# Directly export main function for external calls
__all__ = ["main"]

if __name__ == "__main__":
    main()


--- ui/components.py ---
# -*- coding: utf-8 -*-
"""
Streamlit UI Components - Cyber Edition
Contains all reusable UI components with new styling plus
the operational widgets required by the handlers.
"""

from __future__ import annotations

import html
import base64
import sys
from datetime import datetime
from functools import lru_cache
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

import streamlit as st

from utils.cross_platform_file_handler import get_file_handler

BASE_DIR = Path(__file__).resolve().parents[1]
ICON_DIR = BASE_DIR / "assets" / "icons"


@lru_cache(maxsize=64)
def _icon_data_uri(name: str) -> str:
    path = ICON_DIR / f"{name}.png"
    if not path.exists():
        return ""

    try:
        data = path.read_bytes()
    except OSError:
        return ""

    encoded = base64.b64encode(data).decode("utf-8")
    return f"data:image/png;base64,{encoded}"


def icon_img(name: str, size: int = 32, extra_style: str = "") -> str:
    """
    Render an inline <img> tag for icons stored in assets/icons via data URI.
    """
    data_uri = _icon_data_uri(name)
    if not data_uri:
        return ""
    return f'<img src="{data_uri}" alt="{name}" style="width:{size}px;height:{size}px;{extra_style}"/>'


def clear_guided_answer_inputs():
    """Remove temporary answer widgets from session state."""
    keys_to_delete = [
        key for key in st.session_state.keys() if key.startswith("guided_answer_")
    ]
    for key in keys_to_delete:
        del st.session_state[key]


def display_header():
    """Display the Cyber-styled header"""
    st.markdown(
        """
        <div class="cyber-header">
            <div class="brand-container">
                <div class="brand-title">DEEPCODE</div>
                <div class="brand-subtitle">Autonomous Research & Engineering Matrix</div>
                    </div>
            <div class="status-indicator">
                <div class="status-dot"></div>
                <span>SYSTEM ONLINE</span>
        </div>
    </div>
    """,
        unsafe_allow_html=True,
    )


def display_features():
    """Display feature cards grid"""
    feature_cards = [
        {
            "icon": "feature_synthesis",
            "fallback": "üß¨",
            "title": "Neural Synthesis",
            "desc": "Transform research papers directly into executable repositories via multi-agent LLM pipelines.",
        },
        {
            "icon": "feature_hyper",
            "fallback": "‚ö°",
            "title": "Hyper-Speed Mode",
            "desc": "Acceleration layer that parallelizes retrieval, planning, and implementation for fastest delivery.",
        },
        {
            "icon": "feature_cognition",
            "fallback": "üß†",
            "title": "Cognitive Context",
            "desc": "Semantic memory graphs retain methodology, datasets, and evaluation strategy during reasoning.",
        },
        {
            "icon": "feature_secure",
            "fallback": "üõ°Ô∏è",
            "title": "Secure Sandbox(Coming Soon)",
            "desc": "Isolated execution & validation environment keeps experiments safe and reproducible.",
        },
    ]

    cards_html = ""
    for card in feature_cards:
        icon_markup = icon_img(
            card["icon"],
            48,
            "filter:drop-shadow(0 0 10px rgba(0,242,255,0.4));",
        )
        if not icon_markup:
            icon_markup = f'<span style="font-size:2rem;">{card["fallback"]}</span>'

        cards_html += f"""
        <div class="cyber-card">
            <div class="card-icon">
                {icon_markup}
                </div>
            <div class="card-title">{card['title']}</div>
            <div class="card-desc">{card['desc']}</div>
                </div>
        """

    st.markdown(
        f"""
        <div class="feature-grid">
            {cards_html}
        </div>
    """,
        unsafe_allow_html=True,
    )


def display_status(message: str, status_type: str = "info"):
    """Display status message with cyber styling"""
    colors = {
        "success": "var(--success)",
        "error": "var(--error)",
        "warning": "var(--warning)",
        "info": "var(--primary)",
    }
    color = colors.get(status_type, "var(--primary)")

    st.markdown(
        f"""
        <div style="padding: 1rem; border-left: 3px solid {color}; background: rgba(255,255,255,0.03); margin: 1rem 0; border-radius: 0 4px 4px 0;">
            <span style="color: {color}; font-weight: bold; margin-right: 0.5rem;">[{status_type.upper()}]</span>
            <span style="font-family: var(--font-code);">{message}</span>
    </div>
    """,
        unsafe_allow_html=True,
    )


def _render_step_card(title: str, subtitle: str, state: str) -> str:
    """Return HTML for a workflow step badge."""
    colors = {
        "completed": "var(--success)",
        "active": "var(--primary)",
        "pending": "rgba(255,255,255,0.3)",
        "error": "var(--error)",
    }
    icon = {
        "completed": "‚úî",
        "active": "‚û§",
        "pending": "‚Ä¢",
        "error": "!",
    }.get(state, "‚Ä¢")
    color = colors.get(state, "rgba(255,255,255,0.3)")
    return f"""
        <div style="
            border:1px solid rgba(255,255,255,0.08);
            padding:0.75rem;
            border-radius:4px;
            min-height:110px;
            background:rgba(0,0,0,0.15);
        ">
            <div style="font-size:1.2rem;color:{color};">{icon}</div>
            <div style="font-family:var(--font-display);color:white;">{title}</div>
            <div style="font-size:0.8rem;color:rgba(255,255,255,0.5);">{subtitle}</div>
        </div>
    """


def enhanced_progress_display_component(
    enable_indexing: bool, chat_mode: bool
) -> Tuple[Any, Any, List[Any], List[Dict[str, str]]]:
    """
    Render the progress panel required by handlers.handle_processing_workflow.
    """

    if chat_mode:
        workflow_steps = [
            {"title": "INIT", "subtitle": "Boot agents"},
            {"title": "PLAN", "subtitle": "Analyze intent"},
            {"title": "SETUP", "subtitle": "Workspace"},
            {"title": "DRAFT", "subtitle": "Generate plan"},
            {"title": "CODE", "subtitle": "Implement"},
        ]
    elif not enable_indexing:
        workflow_steps = [
            {"title": "INIT", "subtitle": "Load systems"},
            {"title": "ANALYZE", "subtitle": "Parse paper"},
            {"title": "DOWNLOAD", "subtitle": "Collect refs"},
            {"title": "PLAN", "subtitle": "Blueprint"},
            {"title": "CODE", "subtitle": "Implement"},
        ]
    else:
        workflow_steps = [
            {"title": "INIT", "subtitle": "Load systems"},
            {"title": "ANALYZE", "subtitle": "Paper scan"},
            {"title": "DOWNLOAD", "subtitle": "Docs & data"},
            {"title": "PLAN", "subtitle": "Architect"},
            {"title": "REF", "subtitle": "Key refs"},
            {"title": "REPO", "subtitle": "GitHub sync"},
            {"title": "INDEX", "subtitle": "Vectorize"},
            {"title": "CODE", "subtitle": "Implementation"},
        ]

    st.markdown("### üõ∞Ô∏è Workflow Monitor")
    progress_bar = st.progress(0)
    status_text = st.empty()

    cols = st.columns(len(workflow_steps))
    step_indicators: List[Any] = []
    for col, step in zip(cols, workflow_steps):
        with col:
            placeholder = st.empty()
            placeholder.markdown(
                _render_step_card(step["title"], step["subtitle"], "pending"),
                unsafe_allow_html=True,
            )
            step_indicators.append(placeholder)

    return progress_bar, status_text, step_indicators, workflow_steps


def update_step_indicator(
    step_indicators: List[Any],
    workflow_steps: List[Dict[str, str]],
    current_step: int,
    status: str,
):
    """
    Update the workflow step indicators in-place.
    """
    total_steps = len(workflow_steps)

    for idx, placeholder in enumerate(step_indicators):
        if status == "error" and idx == current_step:
            state = "error"
        elif current_step >= total_steps:
            state = "completed"
        elif idx < current_step:
            state = "completed"
        elif idx == current_step:
            state = "active"
        else:
            state = "pending"

        step = workflow_steps[idx]
        placeholder.markdown(
            _render_step_card(step["title"], step["subtitle"], state),
            unsafe_allow_html=True,
        )


def chat_input_component(task_counter: int = 0) -> Optional[str]:
    """Render modern chat input for guided mode"""
    st.markdown("### üí¨ Neural Link Interface")

    user_input = st.chat_input(
        placeholder="Input research directive or query...",
        key=f"chat_input_{task_counter}",
    )
    return user_input


def _save_uploaded_pdf(uploaded_file) -> Optional[str]:
    """Persist uploaded PDF to a temp file and return its path."""
    try:
        file_bytes = uploaded_file.read()
        suffix = Path(uploaded_file.name).suffix or ".pdf"
        handler = get_file_handler()
        temp_path = handler.create_safe_temp_file(
            suffix=suffix, prefix="deepcode_upload_", content=file_bytes
        )
        return str(temp_path)
    except Exception as exc:
        st.error(f"Failed to save uploaded file: {exc}")
        return None


def input_method_selector(task_counter: int) -> Tuple[Optional[str], Optional[str]]:
    """Render the input method selection tabs with modern styling"""

    tab1, tab2, tab3 = st.tabs(["üìÑ PDF UPLOAD", "üîó URL LINK", "‚ö° QUICK COMMAND"])

    input_source: Optional[str] = None
    input_type: Optional[str] = None

    with tab1:
        st.markdown('<div style="padding:1rem;"></div>', unsafe_allow_html=True)
        uploaded_file = st.file_uploader(
            "Upload Research Paper (PDF)",
            type="pdf",
            key=f"file_uploader_{task_counter}",
        )
        if uploaded_file:
            saved_path = _save_uploaded_pdf(uploaded_file)
            if saved_path:
                st.session_state["uploaded_filename"] = uploaded_file.name
                input_source = saved_path
                input_type = "file"

    with tab2:
        st.markdown('<div style="padding:1rem;"></div>', unsafe_allow_html=True)
        url = st.text_input(
            "ArXiv / GitHub Resource URL",
            placeholder="https://arxiv.org/abs/...",
            key=f"url_input_{task_counter}",
        )
        if url:
            input_source = url.strip()
            input_type = "url"

    with tab3:
        st.markdown('<div style="padding:1rem;"></div>', unsafe_allow_html=True)
        query = st.text_area(
            "Code Specifications / Abstract",
            placeholder="Describe the algorithm or system requirements...",
            height=150,
            key=f"text_input_{task_counter}",
        )
        if query:
            input_source = query.strip()
            input_type = "chat"

    return input_source, input_type


def results_display_component(result: Any, task_counter: int):
    """Display results in a tech-styled container"""

    status = result.get("status", "unknown")
    is_success = status == "success"
    status_label = "Mission Complete" if is_success else "Execution Failed"
    status_color = "var(--success)" if is_success else "var(--error)"
    status_icon = icon_img("status_success" if is_success else "status_error", 56)
    if not status_icon:
        status_icon = "‚úÖ" if is_success else "‚ö†Ô∏è"
    status_message = (
        "Computation sequence completed successfully."
        if is_success
        else result.get("error", "Unknown error occurred during processing.")
    )

    st.markdown('<div style="height: 2rem;"></div>', unsafe_allow_html=True)
    st.markdown("### üöÄ Operation Result")

    with st.container():
        if is_success:
            st.success("Workflow completed across all stages ‚úÖ")
        else:
            st.error("Workflow interrupted. Check the logs below ‚ö†Ô∏è")

        col1, col2 = st.columns([2, 1])
        with col1:
            with st.expander("üìú Execution Logs & Metadata", expanded=True):
                st.json(result)

        with col2:
            st.markdown(
                f"""
                <div style="padding: 1.5rem; border: 1px solid rgba(255,255,255,0.1); border-radius: 6px; background: rgba(255,255,255,0.02); text-align: center; margin-bottom: 1rem;">
                    <div style="margin-bottom:0.5rem;">{status_icon}</div>
                    <div style="font-family: var(--font-display); font-size: 1.3rem; color: {status_color};">{status_label}</div>
                    <div style="font-size: 0.85rem; color: rgba(255,255,255,0.6); margin-top: 0.3rem;">{status_message}</div>
                </div>
                """,
                unsafe_allow_html=True,
            )
            st.download_button(
                label="üì• DOWNLOAD ARTIFACTS" if is_success else "üì• DOWNLOAD LOGS",
                data=str(result),
                file_name=f"deepcode_result_{task_counter}.json",
                mime="application/json",
                use_container_width=True,
            )


def system_status_component():
    """System status check component"""
    st.markdown("### üîß System Diagnostics")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("#### üìä Core Metrics")
        st.info(f"**Python:** {sys.version.split()[0]}")
        st.info(f"**Platform:** {sys.platform}")

    with col2:
        st.markdown("#### ‚öôÔ∏è Runtime Status")
        try:
            import asyncio

            loop = asyncio.get_event_loop()
            if loop.is_running():
                st.success("Event Loop: ACTIVE")
            else:
                st.warning("Event Loop: STANDBY")
        except Exception:
            st.info("Event Loop: MANAGED")


def error_troubleshooting_component():
    """Error troubleshooting component"""
    with st.expander("üõ†Ô∏è Diagnostics & Troubleshooting", expanded=False):
        st.warning(
            "If you encounter issues, please check your API keys in the sidebar."
        )


def footer_component():
    """Minimal futuristic footer"""
    st.markdown(
        """
        <div style="text-align: center; margin-top: 6rem; padding: 2rem; color: rgba(255,255,255,0.2); font-family: var(--font-code); font-size: 0.7rem; border-top: 1px solid rgba(255,255,255,0.05);">
            DEEPCODE_SYSTEMS // <span style="color: var(--primary);">OPERATIONAL</span> // VERSION 3.0.1
    </div>
    """,
        unsafe_allow_html=True,
    )


def render_sidebar_feed(max_items: int = 12):
    """Render live mission feed inside sidebar."""
    st.markdown("#### üì° Mission Feed")
    events = list(st.session_state.get("sidebar_events", []))

    col1, col2 = st.columns([1, 1])
    with col1:
        st.caption("Real-time agent telemetry")
    with col2:
        if st.button("Clear Feed", key="sidebar_clear_feed"):
            st.session_state.sidebar_events = []
            events = []
            st.session_state.sidebar_feed_last_cleared = datetime.utcnow().strftime(
                "%H:%M:%S"
            )

    if not events:
        st.caption("Awaiting activity...")
        return

    recent_events = list(reversed(events[-max_items:]))
    for event in recent_events:
        stage = event.get("stage", "STAGE")
        message = html.escape(str(event.get("message", "")))
        timestamp = event.get("timestamp", "--:--:--")
        level = event.get("level", "info")
        extra = event.get("extra")

        st.markdown(
            f"""
            <div class="sidebar-feed-card level-{level}">
                <div class="stage-line">
                    <span class="stage">{stage}</span>
                    <span class="time">{timestamp}</span>
                </div>
                <div class="message">{message}</div>
            </div>
            """,
            unsafe_allow_html=True,
        )

        if isinstance(extra, dict) and extra:
            with st.expander("Details", expanded=False):
                st.json(extra)


def render_system_monitor():
    """Display current backend + command telemetry."""
    st.markdown("#### üß¨ System Monitor")
    processing = st.session_state.get("processing", False)
    mode = st.session_state.get("requirement_analysis_mode", "direct").upper()
    indexing_enabled = st.session_state.get("enable_indexing", True)
    task_counter = st.session_state.get("task_counter", 0)
    last_error = st.session_state.get("last_error")
    events = st.session_state.get("sidebar_events", [])
    latest_event = events[-1] if events else None
    last_stage = latest_event.get("stage") if latest_event else "--"
    last_message = (
        html.escape(str(latest_event.get("message", ""))) if latest_event else ""
    )
    last_progress = (
        latest_event.get("extra", {}).get("progress") if latest_event else None
    )
    state_label = "ACTIVE" if processing else "IDLE"

    st.markdown(
        f"""
        <div class="system-monitor-card">
            <div class="status-grid">
                <div class="status-chip"><span>STATE</span><span>{state_label}</span></div>
                <div class="status-chip"><span>MODE</span><span>{mode}</span></div>
                <div class="status-chip"><span>INDEXING</span><span>{"ON" if indexing_enabled else "OFF"}</span></div>
                <div class="status-chip"><span>TASKS</span><span>{task_counter}</span></div>
            </div>
            <div class="latest-stage">
                <strong>{last_stage if last_stage else "--"}</strong>
                {"¬∑ " + str(last_progress) + "%" if last_progress is not None else ""}
                <br/>{last_message or "Awaiting telemetry..."}
            </div>
        </div>
        """,
        unsafe_allow_html=True,
    )

    if last_error:
        st.warning(f"Last error: {last_error}")


def render_log_viewer(max_lines: int = 50):
    """Display live log stream for current mission in a scrollable container."""
    st.markdown("#### üìÅ Live Log Stream")
    logs_dir = BASE_DIR / "logs"
    if not logs_dir.exists():
        st.info("Logs directory not found.")
        return

    log_files = sorted(
        [p for p in logs_dir.glob("*.jsonl") if p.is_file()],
        key=lambda p: p.stat().st_mtime,
        reverse=True,
    )
    if not log_files:
        st.info("No log files available yet.")
        return

    start_ts = st.session_state.get("workflow_start_time")
    selected_path = None

    waiting_for_new_log = False

    if start_ts:
        # Use a tolerance window: accept logs created within 10 seconds before workflow_start_time
        tolerance = 10.0
        for candidate in log_files:
            file_mtime = candidate.stat().st_mtime
            if file_mtime >= (start_ts - tolerance):
                selected_path = candidate
                break
        if selected_path is None:
            waiting_for_new_log = True
    else:
        prev = st.session_state.get("active_log_file")
        if prev:
            prev_path = Path(prev)
            if prev_path.exists():
                selected_path = prev_path
        if selected_path is None:
            selected_path = log_files[0]

    if waiting_for_new_log:
        st.caption("Waiting for current task log to be created...")
        return

    st.session_state.active_log_file = str(selected_path)

    try:
        content = selected_path.read_text(encoding="utf-8", errors="ignore")
    except Exception as exc:
        st.error(f"Failed to read {selected_path.name}: {exc}")
        return

    lines = content.splitlines()
    tail_lines = lines[-max_lines:]

    # Show file info
    processing = st.session_state.get("processing", False)
    status_icon = "üîÑ" if processing else "‚úÖ"
    st.caption(f"{status_icon} {selected_path.name} | Last {len(tail_lines)} lines")

    if not tail_lines:
        st.info("Log file is empty.")
        return

    # Build log HTML with scrollable container
    import json

    log_html_parts = []

    for line in tail_lines:
        line = line.strip()
        if not line:
            continue

        try:
            event = json.loads(line)
            timestamp = event.get("timestamp", "")
            level = event.get("level", "INFO")
            message = event.get("message", "")
            namespace = event.get("namespace", "")

            # Color code by level
            if level == "ERROR":
                level_color = "#ff4444"
            elif level == "WARNING":
                level_color = "#ffaa00"
            elif "SUCCESS" in level.upper():
                level_color = "#00ff88"
            else:
                level_color = "#00d4ff"

            # Format display
            time_str = (
                timestamp.split("T")[-1][:12] if "T" in timestamp else timestamp[-12:]
            )
            namespace_short = namespace.split(".")[-1] if namespace else ""

            log_html_parts.append(
                f'<div style="font-family: var(--font-code); font-size: 0.8rem; padding: 0.25rem 0.4rem; '
                f"border-left: 2px solid {level_color}; margin-bottom: 0.2rem; background: rgba(255,255,255,0.02); "
                f'border-radius: 2px;">'
                f'<span style="color: rgba(255,255,255,0.4); font-size: 0.75rem;">{time_str}</span> '
                f'<span style="color: {level_color}; font-weight: 600; font-size: 0.75rem;">[{level}]</span> '
                f'<span style="color: var(--primary); font-size: 0.75rem;">{namespace_short}</span><br/>'
                f'<span style="color: rgba(255,255,255,0.85); margin-left: 0.5rem;">{message[:200]}</span>'
                f"</div>"
            )
        except json.JSONDecodeError:
            # Raw text fallback
            log_html_parts.append(
                f'<div style="font-family: var(--font-code); font-size: 0.75rem; padding: 0.2rem; '
                f'color: rgba(255,255,255,0.6);">{line[:200]}</div>'
            )

    # Render in scrollable container
    full_log_html = f"""
    <div style="max-height: 600px; overflow-y: auto; overflow-x: hidden;
                padding: 0.5rem; background: rgba(0,0,0,0.2); border-radius: 4px;
                border: 1px solid rgba(255,255,255,0.1);">
        {''.join(log_html_parts)}
    </div>
    """

    st.markdown(full_log_html, unsafe_allow_html=True)


def reset_guided_workflow_state(preserve_initial: bool = False):
    """
    Reset guided requirement workflow state machine.
    """
    if preserve_initial:
        initial_text = st.session_state.get(
            "guided_initial_requirement",
            st.session_state.get("initial_requirement", ""),
        )
    else:
        initial_text = ""
        st.session_state.initial_requirement = ""

    st.session_state.guided_initial_requirement = initial_text
    st.session_state.guided_edit_feedback = ""
    st.session_state.requirement_analysis_step = "input"
    st.session_state.generated_questions = []
    st.session_state.user_answers = {}
    st.session_state.detailed_requirements = ""
    st.session_state.questions_generating = False
    st.session_state.requirements_generating = False
    st.session_state.requirements_confirmed = False
    st.session_state.requirements_editing = False
    st.session_state.edit_feedback = ""
    st.session_state.confirmed_requirement_text = None
    clear_guided_answer_inputs()


def requirement_mode_selector() -> str:
    """
    Render the requirement workflow mode selector.
    """
    mode_labels = {"direct": "üöÄ Direct Mode", "guided": "üß≠ Guided Mode"}
    current_mode = st.session_state.get("requirement_analysis_mode", "direct")

    selection = st.radio(
        "Requirement Intake Mode",
        options=list(mode_labels.keys()),
        index=0 if current_mode != "guided" else 1,
        horizontal=True,
        format_func=lambda key: mode_labels[key],
        key="requirement_mode_selector_radio",
    )

    if selection != current_mode:
        st.session_state.requirement_analysis_mode = selection
        if selection == "direct":
            reset_guided_workflow_state(preserve_initial=False)
        else:
            st.session_state.requirement_analysis_step = "input"

    return selection


def guided_requirement_workflow() -> Tuple[Optional[str], bool]:
    """
    Render the guided requirement analysis workflow.
    """

    st.markdown("### üß≠ Guided Requirement Workflow")

    step = st.session_state.get("requirement_analysis_step", "input")
    st.session_state.setdefault(
        "guided_initial_requirement", st.session_state.get("initial_requirement", "")
    )
    st.session_state.setdefault(
        "guided_edit_feedback", st.session_state.get("edit_feedback", "")
    )

    step_titles = {
        "input": "Step 1 ¬∑ Describe Requirements",
        "questions": "Step 2 ¬∑ Answer Guiding Questions",
        "summary": "Step 3 ¬∑ Review Requirement Document",
        "editing": "Step 4 ¬∑ Request Changes",
    }
    st.caption(
        f"Current Stage: {step_titles.get(step, 'Step 1 ¬∑ Describe Requirements')}"
    )

    confirmed_doc = st.session_state.get("confirmed_requirement_text")

    if step == "input":
        st.markdown("#### 1 ¬∑ Describe your project")
        st.text_area(
            "Describe the product scope, tech stack, performance targets, and constraints:",
            key="guided_initial_requirement",
            height=180,
        )
        initial_text = st.session_state.get("guided_initial_requirement", "")

        col1, col2 = st.columns(2)
        with col1:
            if st.button("Generate guiding questions", type="primary"):
                if not initial_text.strip():
                    st.warning("Please enter your project requirements first.")
                else:
                    st.session_state.initial_requirement = initial_text.strip()
                    st.session_state.questions_generating = True
                    st.session_state.requirement_analysis_step = "questions"
                    st.session_state.generated_questions = []
                    st.session_state.user_answers = {}
                    st.session_state.detailed_requirements = ""
                    st.session_state.confirmed_requirement_text = None
                    st.session_state.requirements_generating = False
                    st.session_state.requirements_confirmed = False
                    st.session_state.requirements_editing = False
                    st.session_state.edit_feedback = ""
                    clear_guided_answer_inputs()
                    st.rerun()

        with col2:
            if st.button("Skip Q&A and use current spec", type="secondary"):
                if not initial_text.strip():
                    st.warning("Please enter your project requirements first.")
                else:
                    final_doc = initial_text.strip()
                    st.session_state.initial_requirement = final_doc
                    st.session_state.confirmed_requirement_text = final_doc
                    st.session_state.requirements_confirmed = True
                    st.success(
                        "Current description locked as the requirement document. Implementation will proceed next."
                    )

    elif step == "questions":
        st.markdown("#### 2 ¬∑ Answer guiding questions")
        if st.session_state.get("questions_generating"):
            st.info("LLM is crafting guiding questions. Please wait...")

        questions = st.session_state.get("generated_questions", [])
        question_ids: List[str] = []

        if not questions:
            st.caption("Guiding questions will appear once generation is complete.")
        else:
            for idx, question in enumerate(questions):
                if isinstance(question, dict):
                    q_id = str(
                        question.get("id")
                        or question.get("question_id")
                        or question.get("qid")
                        or idx
                    )
                    q_text = question.get("question") or question.get("content") or ""
                    category = question.get("category")
                    importance = question.get("importance")
                    hint = question.get("hint")
                else:
                    q_id = str(idx)
                    q_text = str(question)
                    category = importance = hint = None

                question_ids.append(q_id)

                st.markdown(
                    f"**Q{idx + 1}. {q_text or 'Please answer this question'}**"
                )
                meta_parts = [part for part in [category, importance] if part]
                if meta_parts:
                    st.caption(" / ".join(meta_parts))
                if hint:
                    st.caption(f"Hint: {hint}")

                answer_key = f"guided_answer_{idx}"
                if answer_key not in st.session_state:
                    default_answer = st.session_state.user_answers.get(q_id, "")
                    st.session_state[answer_key] = default_answer

                st.text_area("Your answer", key=answer_key, height=100)

        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button(
                "Generate requirement document", type="primary", disabled=not questions
            ):
                answers_payload = {}
                for idx, q_id in enumerate(question_ids):
                    answer_value = st.session_state.get(
                        f"guided_answer_{idx}", ""
                    ).strip()
                    if answer_value:
                        answers_payload[q_id] = answer_value

                st.session_state.user_answers = answers_payload
                st.session_state.requirements_generating = True
                st.session_state.requirement_analysis_step = "summary"
                st.session_state.detailed_requirements = ""
                st.session_state.confirmed_requirement_text = None
                st.session_state.requirements_confirmed = False
                st.rerun()

        with col2:
            if st.button(
                "Generate without answers", type="secondary", disabled=not questions
            ):
                st.session_state.user_answers = {}
                st.session_state.requirements_generating = True
                st.session_state.requirement_analysis_step = "summary"
                st.session_state.detailed_requirements = ""
                st.session_state.confirmed_requirement_text = None
                st.session_state.requirements_confirmed = False
                st.rerun()

        with col3:
            if st.button("Back to Step 1"):
                reset_guided_workflow_state(preserve_initial=True)
                st.rerun()

    elif step == "summary":
        st.markdown("#### 3 ¬∑ AI-generated requirement document")
        if st.session_state.get("requirements_generating"):
            st.info("Generating requirement document. Please wait...")

        summary = (st.session_state.get("detailed_requirements") or "").strip()

        if summary:
            st.markdown(summary)
            st.download_button(
                "Download requirement document",
                summary,
                file_name="deepcode_requirements.md",
                mime="text/markdown",
                use_container_width=True,
            )
        else:
            st.caption("Waiting for requirement document to be generated...")

        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button(
                "Confirm and start implementation ‚úÖ",
                type="primary",
                disabled=not summary,
            ):
                final_doc = summary or st.session_state.get("initial_requirement", "")
                if final_doc.strip():
                    st.session_state.confirmed_requirement_text = final_doc.strip()
                    st.session_state.requirements_confirmed = True
                    st.success(
                        "Requirement document confirmed. Implementation pipeline will start next."
                    )
                else:
                    st.warning("No requirement document available yet.")

        with col2:
            if st.button("Request edits", type="secondary", disabled=not summary):
                st.session_state.requirement_analysis_step = "editing"
                st.session_state.guided_edit_feedback = ""

        with col3:
            if st.button("Restart Q&A", type="secondary"):
                reset_guided_workflow_state(preserve_initial=True)
                st.rerun()

    elif step == "editing":
        st.markdown("#### 4 ¬∑ Modify requirement document")
        st.text_area(
            "Describe the changes or clarifications you need:",
            key="guided_edit_feedback",
            height=160,
        )
        feedback_value = st.session_state.get("guided_edit_feedback", "")

        col1, col2 = st.columns(2)
        with col1:
            if st.button("Submit change request", type="primary"):
                if not feedback_value.strip():
                    st.warning("Please describe the requested changes.")
                else:
                    st.session_state.edit_feedback = feedback_value.strip()
                    st.session_state.requirements_editing = True
                    st.info("Updating requirement document based on your feedback...")

        with col2:
            if st.button("Back to requirement document"):
                st.session_state.requirement_analysis_step = "summary"
                st.session_state.guided_edit_feedback = ""

        if st.session_state.get("requirements_editing"):
            st.info("Requirement document is updating...")

    if confirmed_doc:
        st.success("Requirement document locked. You can start implementation anytime.")

    return (confirmed_doc if confirmed_doc else None, bool(confirmed_doc))


def sidebar_control_panel():
    """Sidebar configuration"""
    with st.sidebar:
        st.markdown(
            """
            <div style="margin-bottom: 2rem; padding-bottom: 1rem; border-bottom: 1px solid rgba(255,255,255,0.1);">
                <h2 style="margin:0; color:white;">CONTROL DECK</h2>
                <div style="font-family:var(--font-code); color:var(--primary); font-size:0.8rem;">// MISSION CONTROL</div>
        </div>
        """,
            unsafe_allow_html=True,
        )

        workflow_start = st.session_state.get("workflow_start_time")

        if workflow_start:
            render_log_viewer()
        else:
            st.info("Awaiting next mission run to stream logs.")
    st.markdown(
        """
            <div style="font-size: 0.7rem; color: rgba(255,255,255,0.3); text-align: center; margin-top: 1rem;">
                ¬© 2024 DeepCode Research
    </div>
    """,
        unsafe_allow_html=True,
    )

    return {}


--- ui/handlers.py ---
"""
Streamlit Event Handlers Module

Contains all event handling and business logic
"""

import asyncio
import time
import os
import traceback
import atexit
import signal
from datetime import datetime
from typing import Dict, Any

import streamlit as st
import nest_asyncio
import concurrent.futures

# Import necessary modules
from mcp_agent.app import MCPApp
from workflows.agent_orchestration_engine import (
    execute_multi_agent_research_pipeline,
    execute_chat_based_planning_pipeline,
)
from .sidebar_feed import log_sidebar_event, ensure_sidebar_logging


def _emergency_cleanup():
    """
    Emergency resource cleanup function
    Called when program exits abnormally
    """
    try:
        cleanup_resources()
    except Exception:
        pass  # Silent handling to avoid new exceptions during exit


def _signal_handler(signum, frame):
    """
    Signal handler for program termination signals
    """
    try:
        cleanup_resources()
    except Exception:
        pass
    finally:
        # Restore default signal handling and resend signal
        signal.signal(signum, signal.SIG_DFL)
        os.kill(os.getpid(), signum)


# Register exit cleanup function
atexit.register(_emergency_cleanup)


def _safe_register_signal_handlers():
    """Safely register signal handlers"""
    try:
        # Check if running in main thread
        import threading

        if threading.current_thread() is not threading.main_thread():
            return  # Signal handlers can only be registered in main thread

        # Try to register signal handlers
        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)
        if hasattr(signal, "SIGBREAK"):  # Windows
            signal.signal(signal.SIGBREAK, _signal_handler)
    except (AttributeError, OSError, ValueError):
        # Some signals are not available on certain platforms or disabled in some environments
        # This is common in web frameworks like Streamlit
        pass


# Delayed signal handler registration to avoid import-time errors
try:
    _safe_register_signal_handlers()
except Exception:
    # If registration fails, silently ignore and don't affect app startup
    pass


async def process_input_async(
    input_source: str,
    input_type: str,
    enable_indexing: bool = True,
    progress_callback=None,
) -> Dict[str, Any]:
    """
    Process input asynchronously

    Args:
        input_source: Input source
        input_type: Input type
        enable_indexing: Whether to enable indexing functionality
        progress_callback: Progress callback function

    Returns:
        Processing result
    """
    try:
        # Create and use MCP app in the same async context
        app = MCPApp(name="paper_to_code")

        async with app.run() as agent_app:
            logger = agent_app.logger
            context = agent_app.context
            context.config.mcp.servers["filesystem"].args.extend([os.getcwd()])

            # Initialize progress
            if progress_callback:
                if input_type == "chat":
                    progress_callback(
                        5, "üöÄ Initializing chat-based planning pipeline..."
                    )
                else:
                    progress_callback(5, "üöÄ Initializing AI research engine...")

            # Choose pipeline based on input type
            if input_type == "chat":
                # Use chat-based planning pipeline for user requirements
                repo_result = await execute_chat_based_planning_pipeline(
                    input_source,  # User's coding requirements
                    logger,
                    progress_callback,
                    enable_indexing=enable_indexing,  # Pass indexing control parameter
                )
            else:
                # Use traditional multi-agent research pipeline for files/URLs
                repo_result = await execute_multi_agent_research_pipeline(
                    input_source,
                    logger,
                    progress_callback,
                    enable_indexing=enable_indexing,  # Pass indexing control parameter
                )

            return {
                "analysis_result": "Integrated into complete workflow",
                "download_result": "Integrated into complete workflow",
                "repo_result": repo_result,
                "status": "success",
            }

    except Exception as e:
        error_msg = str(e)
        traceback_msg = traceback.format_exc()

        return {"error": error_msg, "traceback": traceback_msg, "status": "error"}


def run_async_task(coro):
    """
    Helper function to run async tasks

    Args:
        coro: Coroutine object

    Returns:
        Task result
    """
    # Apply nest_asyncio to support nested event loops
    nest_asyncio.apply()

    # Save current Streamlit context
    try:
        from streamlit.runtime.scriptrunner import get_script_run_ctx
        from streamlit.runtime.scriptrunner.script_run_context import (
            SCRIPT_RUN_CONTEXT_ATTR_NAME,
        )

        current_ctx = get_script_run_ctx()
        context_available = True
    except ImportError:
        # If Streamlit context modules can't be imported, use fallback method
        current_ctx = None
        context_available = False

    def run_in_new_loop():
        """Run coroutine in new event loop"""
        # Set Streamlit context in new thread (if available)
        if context_available and current_ctx:
            try:
                import threading

                setattr(
                    threading.current_thread(),
                    SCRIPT_RUN_CONTEXT_ATTR_NAME,
                    current_ctx,
                )
            except Exception:
                pass  # Ignore context setting errors

        loop = None
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(coro)
            return result
        except Exception as e:
            raise e
        finally:
            # Clean up resources
            if loop:
                try:
                    loop.close()
                except Exception:
                    pass
            asyncio.set_event_loop(None)

            # Clean up thread context (if available)
            if context_available:
                try:
                    import threading

                    if hasattr(
                        threading.current_thread(), SCRIPT_RUN_CONTEXT_ATTR_NAME
                    ):
                        delattr(
                            threading.current_thread(), SCRIPT_RUN_CONTEXT_ATTR_NAME
                        )
                except Exception:
                    pass  # Ignore cleanup errors

            # Force garbage collection
            import gc

            gc.collect()

    # Use thread pool to run async task, avoiding event loop conflicts
    executor = None
    try:
        executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=1, thread_name_prefix="deepcode_ctx_async"
        )
        future = executor.submit(run_in_new_loop)
        result = future.result(timeout=300)  # 5 minute timeout
        return result
    except concurrent.futures.TimeoutError:
        st.error("Processing timeout after 5 minutes. Please try again.")
        raise TimeoutError("Processing timeout")
    except Exception as e:
        # If thread pool execution fails, try direct execution
        st.warning(f"Threaded async execution failed: {e}, trying direct execution...")
        try:
            # Fallback method: run directly in current thread
            loop = None
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                result = loop.run_until_complete(coro)
                return result
            finally:
                if loop:
                    try:
                        loop.close()
                    except Exception:
                        pass
                asyncio.set_event_loop(None)
                import gc

                gc.collect()
        except Exception as backup_error:
            st.error(f"All execution methods failed: {backup_error}")
            raise backup_error
    finally:
        # Ensure thread pool is properly closed
        if executor:
            try:
                executor.shutdown(wait=True, cancel_futures=True)
            except Exception:
                pass
        # Force garbage collection
        import gc

        gc.collect()


def run_async_task_simple(coro):
    """
    Simple async task runner, avoiding threading issues

    Args:
        coro: Coroutine object

    Returns:
        Task result
    """
    # Apply nest_asyncio to support nested event loops
    nest_asyncio.apply()

    try:
        # Try to run in current event loop
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If current loop is running, use improved thread pool method
            import concurrent.futures
            import gc

            def run_in_thread():
                # Create new event loop and set as current thread's loop
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    result = new_loop.run_until_complete(coro)
                    return result
                except Exception as e:
                    # Ensure exception information is properly passed
                    raise e
                finally:
                    # Ensure loop is properly closed
                    try:
                        new_loop.close()
                    except Exception:
                        pass
                    # Clear current thread's event loop reference
                    asyncio.set_event_loop(None)
                    # Force garbage collection
                    gc.collect()

            # Use context manager to ensure thread pool is properly closed
            executor = None
            try:
                executor = concurrent.futures.ThreadPoolExecutor(
                    max_workers=1, thread_name_prefix="deepcode_async"
                )
                future = executor.submit(run_in_thread)
                result = future.result(timeout=300)  # 5 minute timeout
                return result
            except concurrent.futures.TimeoutError:
                st.error(
                    "Processing timeout after 5 minutes. Please try again with a smaller file."
                )
                raise TimeoutError("Processing timeout")
            except Exception as e:
                st.error(f"Async processing error: {e}")
                raise e
            finally:
                # Ensure thread pool is properly closed
                if executor:
                    try:
                        executor.shutdown(wait=True, cancel_futures=True)
                    except Exception:
                        pass
                # Force garbage collection
                gc.collect()
        else:
            # Run directly in current loop
            return loop.run_until_complete(coro)
    except Exception:
        # Final fallback method: create new event loop
        loop = None
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(coro)
            return result
        except Exception as backup_error:
            st.error(f"All async methods failed: {backup_error}")
            raise backup_error
        finally:
            if loop:
                try:
                    loop.close()
                except Exception:
                    pass
            asyncio.set_event_loop(None)
            # Force garbage collection
            import gc

            gc.collect()


def handle_processing_workflow(
    input_source: str, input_type: str, enable_indexing: bool = True
) -> Dict[str, Any]:
    """
    Main processing function for workflow

    Args:
        input_source: Input source
        input_type: Input type
        enable_indexing: Whether to enable indexing functionality

    Returns:
        Processing result
    """
    from .components import (
        enhanced_progress_display_component,
        update_step_indicator,
        display_status,
    )

    # Display enhanced progress components
    chat_mode = input_type == "chat"
    progress_bar, status_text, step_indicators, workflow_steps = (
        enhanced_progress_display_component(enable_indexing, chat_mode)
    )
    log_sidebar_event(
        "SYSTEM",
        f"Workflow started ({'guided/chat' if chat_mode else 'research'} mode)",
        extra={"input_type": input_type, "indexing": enable_indexing},
    )

    # Step mapping: map progress percentages to step indices - adjust based on mode and indexing toggle
    if chat_mode:
        # Chat mode step mapping: Initialize -> Planning -> Setup -> Save Plan -> Implement
        step_mapping = {
            5: 0,  # Initialize
            30: 1,  # Planning (analyzing requirements)
            50: 2,  # Setup (creating workspace)
            70: 3,  # Save Plan (saving implementation plan)
            85: 4,  # Implement (generating code)
            100: 4,  # Complete
        }
    elif not enable_indexing:
        # Skip indexing-related steps progress mapping - fast mode order: Initialize -> Analyze -> Download -> Plan -> Implement
        step_mapping = {
            5: 0,  # Initialize
            10: 1,  # Analyze
            25: 2,  # Download
            40: 3,  # Plan (now prioritized over References, 40%)
            85: 4,  # Implement (skip References, Repos and Index)
            100: 4,  # Complete
        }
    else:
        # Full workflow step mapping - new order: Initialize -> Analyze -> Download -> Plan -> References -> Repos -> Index -> Implement
        step_mapping = {
            5: 0,  # Initialize
            10: 1,  # Analyze
            25: 2,  # Download
            40: 3,  # Plan (now 4th position, 40%)
            50: 4,  # References (now 5th position, conditional, 50%)
            60: 5,  # Repos (GitHub download)
            70: 6,  # Index (code indexing)
            85: 7,  # Implement (code implementation)
            100: 7,  # Complete
        }

    current_step = 0

    # Define enhanced progress callback function
    def update_progress(progress: int, message: str):
        nonlocal current_step

        # Update progress bar
        progress_bar.progress(progress)
        status_text.markdown(f"**{message}**")

        # Determine current step
        new_step = step_mapping.get(progress, current_step)
        if new_step != current_step:
            current_step = new_step
            update_step_indicator(
                step_indicators, workflow_steps, current_step, "active"
            )

        stage_index = (
            min(current_step, len(workflow_steps) - 1) if workflow_steps else 0
        )
        stage_label = (
            workflow_steps[stage_index]["title"] if workflow_steps else "STAGE"
        )
        log_sidebar_event(
            stage_label,
            message,
            extra={"progress": progress},
        )
        time.sleep(0.3)  # Brief pause for users to see progress changes

    # Step 1: Initialization
    if chat_mode:
        update_progress(5, "üöÄ Initializing chat-based planning engine...")
    elif enable_indexing:
        update_progress(5, "üöÄ Initializing AI research engine and loading models...")
    else:
        update_progress(
            5, "üöÄ Initializing AI research engine (Fast mode - indexing disabled)..."
        )
    update_step_indicator(step_indicators, workflow_steps, 0, "active")

    # Start async processing with progress callback
    with st.spinner("üîÑ Processing workflow stages..."):
        try:
            # First try using simple async processing method
            result = run_async_task_simple(
                process_input_async(
                    input_source, input_type, enable_indexing, update_progress
                )
            )
        except Exception as e:
            st.warning(f"Primary async method failed: {e}")
            # Fallback method: use original thread pool method
            try:
                result = run_async_task(
                    process_input_async(
                        input_source, input_type, enable_indexing, update_progress
                    )
                )
            except Exception as backup_error:
                st.error(f"Both async methods failed. Error: {backup_error}")
                return {
                    "status": "error",
                    "error": str(backup_error),
                    "traceback": traceback.format_exc(),
                }

    # Update final status based on results
    if result["status"] == "success":
        # Complete all steps
        update_progress(100, "‚úÖ All processing stages completed successfully!")
        update_step_indicator(
            step_indicators, workflow_steps, len(workflow_steps), "completed"
        )

        # Display success information
        st.balloons()  # Add celebration animation
        if chat_mode:
            display_status(
                "üéâ Chat workflow completed! Your requirements have been analyzed and code has been generated.",
                "success",
            )
        elif enable_indexing:
            display_status(
                "üéâ Workflow completed! Your research paper has been successfully processed and code has been generated.",
                "success",
            )
        else:
            display_status(
                "üéâ Fast workflow completed! Your research paper has been processed (indexing skipped for faster processing).",
                "success",
            )
        log_sidebar_event(
            "COMPLETE",
            "All stages completed successfully.",
            level="success",
            extra={
                "input_type": input_type,
                "indexing": enable_indexing,
                "timestamp": datetime.utcnow().isoformat(),
            },
        )

    else:
        # Processing failed
        update_progress(0, "‚ùå Processing failed - see error details below")
        update_step_indicator(step_indicators, workflow_steps, current_step, "error")
        display_status(
            f"‚ùå Processing encountered an error: {result.get('error', 'Unknown error')}",
            "error",
        )
        failure_stage = (
            workflow_steps[current_step]["title"]
            if workflow_steps and current_step < len(workflow_steps)
            else "ERROR"
        )
        log_sidebar_event(
            failure_stage,
            f"Processing failed: {result.get('error', 'Unknown error')}",
            level="error",
        )

    # Wait a moment for users to see completion status
    time.sleep(2.5)

    return result


def update_session_state_with_result(result: Dict[str, Any], input_type: str):
    """
    Update session state with result

    Args:
        result: Processing result
        input_type: Input type
    """
    if result["status"] == "success":
        # Save result to session state
        st.session_state.last_result = result
        st.session_state.show_results = True

        # Save to history
        st.session_state.results.append(
            {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "input_type": input_type,
                "status": "success",
                "result": result,
            }
        )
    else:
        # Save error information to session state for display
        st.session_state.last_error = result.get("error", "Unknown error")

        # Save error to history
        st.session_state.results.append(
            {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "input_type": input_type,
                "status": "error",
                "error": result.get("error", "Unknown error"),
            }
        )

    # Limit history to maximum 50 records
    if len(st.session_state.results) > 50:
        st.session_state.results = st.session_state.results[-50:]


def cleanup_temp_file(input_source: str, input_type: str):
    """
    Cleanup temporary file using cross-platform safe method.

    Args:
        input_source: Input source
        input_type: Input type
    """
    if input_type == "file" and input_source:
        try:
            from utils.cross_platform_file_handler import get_file_handler

            file_handler = get_file_handler()
            file_handler.safe_remove_file(input_source)
        except Exception as e:
            # Log but don't fail - cleanup is best effort
            import logging

            logging.getLogger(__name__).warning(
                f"Failed to cleanup temp file {input_source}: {e}"
            )


async def handle_requirement_analysis_workflow(
    user_input: str, analysis_mode: str, user_answers: Dict[str, str] = None
) -> Dict[str, Any]:
    """
    Handle requirement analysis workflow

    Args:
        user_input: User initial requirements
        analysis_mode: Analysis mode ("generate_questions" or "summarize_requirements")
        user_answers: User answer dictionary

    Returns:
        Processing result dictionary
    """
    try:
        # Import required modules
        from workflows.agent_orchestration_engine import (
            execute_requirement_analysis_workflow,
        )

        # Create progress callback function
        def update_progress(progress: int, message: str):
            # Display progress in Streamlit
            st.session_state.current_progress = progress
            st.session_state.current_message = message

        # Execute requirement analysis workflow
        result = await execute_requirement_analysis_workflow(
            user_input=user_input,
            analysis_mode=analysis_mode,
            user_answers=user_answers,
            logger=None,  # Can pass in logger
            progress_callback=update_progress,
        )

        return result

    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "message": f"Requirement analysis workflow execution failed: {str(e)}",
        }


async def handle_requirement_modification_workflow(
    current_requirements: str, modification_feedback: str
) -> Dict[str, Any]:
    """
    Handle requirement modification workflow

    Args:
        current_requirements: Current requirement document content
        modification_feedback: User's modification requests and feedback

    Returns:
        Processing result dictionary
    """
    try:
        # Import required modules
        from workflows.agents.requirement_analysis_agent import RequirementAnalysisAgent

        # Create progress callback function
        def update_progress(progress: int, message: str):
            # Display progress in Streamlit
            st.session_state.current_progress = progress
            st.session_state.current_message = message

        update_progress(10, "üîß Initializing requirement modification agent...")

        # Initialize RequirementAnalysisAgent
        agent = RequirementAnalysisAgent()

        # Initialize agent (LLM is initialized internally)
        await agent.initialize()

        update_progress(50, "‚úèÔ∏è Modifying requirements based on your feedback...")

        # Modify requirements
        result = await agent.modify_requirements(
            current_requirements=current_requirements,
            modification_feedback=modification_feedback,
        )

        # Cleanup
        await agent.cleanup()

        update_progress(100, "‚úÖ Requirements modification completed!")

        return {
            "status": "success",
            "result": result,
            "message": "Requirements modification completed successfully",
        }

    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "message": f"Requirements modification workflow execution failed: {str(e)}",
        }


def handle_guided_mode_processing():
    """Handle asynchronous processing for guided mode"""
    # Check if questions need to be generated
    if st.session_state.get("questions_generating", False):
        st.session_state.questions_generating = False

        # Asynchronously generate questions
        initial_req = st.session_state.get("initial_requirement", "")
        if initial_req:
            try:
                # Use asynchronous processing to generate questions
                result = run_async_task_simple(
                    handle_requirement_analysis_workflow(
                        user_input=initial_req, analysis_mode="generate_questions"
                    )
                )

                if result["status"] == "success":
                    # Parse JSON result
                    import json

                    questions = json.loads(result["result"])
                    st.session_state.generated_questions = questions
                else:
                    st.error(
                        f"Question generation failed: {result.get('error', 'Unknown error')}"
                    )

            except Exception as e:
                st.error(f"Question generation exception: {str(e)}")

    # Check if detailed requirements need to be generated
    if st.session_state.get("requirements_generating", False):
        st.session_state.requirements_generating = False

        # Asynchronously generate detailed requirements
        initial_req = st.session_state.get("initial_requirement", "")
        user_answers = st.session_state.get("user_answers", {})

        if initial_req:
            try:
                # Use asynchronous processing to generate requirement summary
                result = run_async_task_simple(
                    handle_requirement_analysis_workflow(
                        user_input=initial_req,
                        analysis_mode="summarize_requirements",
                        user_answers=user_answers,
                    )
                )

                if result["status"] == "success":
                    st.session_state.detailed_requirements = result["result"]
                else:
                    st.error(
                        f"Requirement summary generation failed: {result.get('error', 'Unknown error')}"
                    )

            except Exception as e:
                st.error(f"Requirement summary generation exception: {str(e)}")

    # Check if requirements need to be edited
    if st.session_state.get("requirements_editing", False):
        st.session_state.requirements_editing = False
        st.info("üîß Starting requirement modification process...")

        # Asynchronously modify requirements based on user feedback
        current_requirements = st.session_state.get("detailed_requirements", "")
        edit_feedback = st.session_state.get("edit_feedback", "")

        if current_requirements and edit_feedback:
            try:
                # Use asynchronous processing to modify requirements
                result = run_async_task_simple(
                    handle_requirement_modification_workflow(
                        current_requirements=current_requirements,
                        modification_feedback=edit_feedback,
                    )
                )

                if result["status"] == "success":
                    st.session_state.detailed_requirements = result["result"]
                    st.session_state.requirement_analysis_step = "summary"
                    st.session_state.edit_feedback = ""
                    st.success("‚úÖ Requirements updated successfully!")
                    st.rerun()
                else:
                    st.error(
                        f"Requirements modification failed: {result.get('error', 'Unknown error')}"
                    )

            except Exception as e:
                st.error(f"Requirements modification exception: {str(e)}")


def _background_workflow_runner(
    input_source: str, input_type: str, enable_indexing: bool, session_id: str
):
    """
    Background thread function to run the workflow WITHOUT any Streamlit UI calls
    This runs in a separate thread to avoid blocking Streamlit's main thread
    """
    import logging

    # Store results in a thread-safe way using a simple dict
    if not hasattr(_background_workflow_runner, "results"):
        _background_workflow_runner.results = {}

    # Create a simple progress callback that only logs (no Streamlit UI calls)
    def background_progress_callback(progress: int, message: str):
        # Just log to Python logger, which will be captured by our logging handler
        logging.info(f"Progress: {progress}% - {message}")

    try:
        # Call the core async workflow directly without UI components
        import asyncio
        import nest_asyncio

        nest_asyncio.apply()

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(
                process_input_async(
                    input_source,
                    input_type,
                    enable_indexing,
                    background_progress_callback,
                )
            )
            _background_workflow_runner.results[session_id] = {
                "status": "completed",
                "result": result,
            }
        finally:
            loop.close()
            asyncio.set_event_loop(None)

    except Exception as e:
        logging.error(f"Background workflow error: {e}", exc_info=True)
        _background_workflow_runner.results[session_id] = {
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc(),
        }


def handle_start_processing_button(input_source: str, input_type: str):
    """
    Handle start processing button click - synchronous execution

    Args:
        input_source: Input source
        input_type: Input type
    """
    from .components import display_status

    st.session_state.processing = True
    st.session_state.workflow_start_time = time.time()
    st.session_state.active_log_file = None

    # Get indexing toggle status
    enable_indexing = st.session_state.get("enable_indexing", True)
    log_sidebar_event(
        "SYSTEM",
        "Engaging DeepCode pipeline...",
        extra={
            "input_type": input_type,
            "indexing": enable_indexing,
        },
    )

    try:
        # Process workflow synchronously
        result = handle_processing_workflow(input_source, input_type, enable_indexing)

        # Display result status
        if result["status"] == "success":
            display_status("All operations completed successfully! üéâ", "success")
        else:
            display_status("Error during processing", "error")

        # Update session state
        update_session_state_with_result(result, input_type)

    except Exception as e:
        # Handle exceptional cases
        st.error(f"Unexpected error during processing: {e}")
        result = {"status": "error", "error": str(e)}
        update_session_state_with_result(result, input_type)

    finally:
        # Reset state and clean up resources after processing
        st.session_state.processing = False

        # Clean up temporary files
        cleanup_temp_file(input_source, input_type)

        # Clean up system resources
        cleanup_resources()

        # Rerun to display results or errors
        st.rerun()


def check_background_workflow_status():
    """
    Check if background workflow has completed and handle results
    This should be called on every Streamlit rerun
    """
    from .components import display_status

    if not st.session_state.get("processing"):
        return

    session_id = st.session_state.get("workflow_session_id")
    if not session_id:
        return

    # Check if background thread has finished
    if (
        hasattr(_background_workflow_runner, "results")
        and session_id in _background_workflow_runner.results
    ):
        workflow_result = _background_workflow_runner.results[session_id]

        # Clean up the result from the cache
        del _background_workflow_runner.results[session_id]

        # Process the result
        if workflow_result["status"] == "completed":
            result = workflow_result["result"]

            # Display result status
            if result["status"] == "success":
                display_status("All operations completed successfully! üéâ", "success")
            else:
                display_status("Error during processing", "error")

            # Update session state
            update_session_state_with_result(
                result, st.session_state.get("workflow_input_type", "")
            )

        elif workflow_result["status"] == "error":
            st.error(f"Unexpected error during processing: {workflow_result['error']}")
            result = {"status": "error", "error": workflow_result["error"]}
            update_session_state_with_result(
                result, st.session_state.get("workflow_input_type", "")
            )

        # Clean up
        st.session_state.processing = False
        cleanup_temp_file(
            st.session_state.get("workflow_input_source"),
            st.session_state.get("workflow_input_type"),
        )
        cleanup_resources()

        # Clear workflow tracking variables
        st.session_state.workflow_session_id = None
        st.session_state.workflow_thread = None
        st.session_state.workflow_input_source = None
        st.session_state.workflow_input_type = None

        # Rerun to show results
        st.rerun()


def handle_error_display():
    """Handle error display"""
    if hasattr(st.session_state, "last_error") and st.session_state.last_error:
        st.error(f"‚ùå Error: {st.session_state.last_error}")
        if st.button("üîÑ Try Again", type="secondary", use_container_width=True):
            st.session_state.last_error = None
            st.session_state.task_counter += 1
            st.rerun()


def initialize_session_state():
    """Initialize session state"""
    if "processing" not in st.session_state:
        st.session_state.processing = False
    if "results" not in st.session_state:
        st.session_state.results = []
    if "current_step" not in st.session_state:
        st.session_state.current_step = 0
    if "task_counter" not in st.session_state:
        st.session_state.task_counter = 0
    if "show_results" not in st.session_state:
        st.session_state.show_results = False
    if "last_result" not in st.session_state:
        st.session_state.last_result = None
    if "last_error" not in st.session_state:
        st.session_state.last_error = None
    if "enable_indexing" not in st.session_state:
        st.session_state.enable_indexing = (
            False  # Default enable indexing functionality
        )

    # Requirement analysis related states
    if "requirement_analysis_mode" not in st.session_state:
        st.session_state.requirement_analysis_mode = "direct"  # direct/guided
    if "requirement_analysis_step" not in st.session_state:
        st.session_state.requirement_analysis_step = "input"  # input/questions/summary
    if "generated_questions" not in st.session_state:
        st.session_state.generated_questions = []
    if "user_answers" not in st.session_state:
        st.session_state.user_answers = {}
    if "detailed_requirements" not in st.session_state:
        st.session_state.detailed_requirements = ""
    if "initial_requirement" not in st.session_state:
        st.session_state.initial_requirement = ""
    if "questions_generating" not in st.session_state:
        st.session_state.questions_generating = False
    if "requirements_generating" not in st.session_state:
        st.session_state.requirements_generating = False
    if "requirements_confirmed" not in st.session_state:
        st.session_state.requirements_confirmed = False
    if "edit_feedback" not in st.session_state:
        st.session_state.edit_feedback = ""
    if "requirements_editing" not in st.session_state:
        st.session_state.requirements_editing = False
    if "guided_initial_requirement" not in st.session_state:
        st.session_state.guided_initial_requirement = ""
    if "guided_edit_feedback" not in st.session_state:
        st.session_state.guided_edit_feedback = ""
    if "confirmed_requirement_text" not in st.session_state:
        st.session_state.confirmed_requirement_text = None
    if "sidebar_events" not in st.session_state:
        st.session_state.sidebar_events = []
    ensure_sidebar_logging()
    if "workflow_start_time" not in st.session_state:
        st.session_state.workflow_start_time = None
    if "active_log_file" not in st.session_state:
        st.session_state.active_log_file = None
    if "workflow_session_id" not in st.session_state:
        st.session_state.workflow_session_id = None
    if "workflow_thread" not in st.session_state:
        st.session_state.workflow_thread = None
    if "workflow_input_source" not in st.session_state:
        st.session_state.workflow_input_source = None
    if "workflow_input_type" not in st.session_state:
        st.session_state.workflow_input_type = None
    if "guided_payload" not in st.session_state:
        st.session_state.guided_payload = None


def cleanup_resources():
    """
    Clean up system resources to prevent memory leaks
    """
    try:
        import gc
        import threading
        import multiprocessing
        import asyncio
        import sys

        # 1. Clean up asyncio-related resources
        try:
            # Get current event loop (if exists)
            try:
                loop = asyncio.get_running_loop()
                # Cancel all pending tasks
                if loop and not loop.is_closed():
                    pending_tasks = [
                        task for task in asyncio.all_tasks(loop) if not task.done()
                    ]
                    if pending_tasks:
                        for task in pending_tasks:
                            if not task.cancelled():
                                task.cancel()
                        # Wait for task cancellation to complete
                        try:
                            if pending_tasks:
                                # Use timeout to avoid blocking too long
                                import time

                                time.sleep(0.1)
                        except Exception:
                            pass
            except RuntimeError:
                # No running event loop, continue with other cleanup
                pass
        except Exception:
            pass

        # 2. Force garbage collection
        gc.collect()

        # 3. Clean up active threads (except main thread)
        active_threads = threading.active_count()
        if active_threads > 1:
            # Wait some time for threads to naturally finish
            import time

            time.sleep(0.5)

        # 4. Clean up multiprocessing resources
        try:
            # Clean up possible multiprocessing resources
            if hasattr(multiprocessing, "active_children"):
                for child in multiprocessing.active_children():
                    if child.is_alive():
                        child.terminate()
                        child.join(timeout=1.0)
                        # If join times out, force kill
                        if child.is_alive():
                            try:
                                child.kill()
                                child.join(timeout=0.5)
                            except Exception:
                                pass

            # Clean up multiprocessing-related resource tracker
            try:
                import multiprocessing.resource_tracker

                if hasattr(multiprocessing.resource_tracker, "_resource_tracker"):
                    tracker = multiprocessing.resource_tracker._resource_tracker
                    if tracker and hasattr(tracker, "_stop"):
                        tracker._stop()
            except Exception:
                pass

        except Exception:
            pass

        # 5. Force clean up Python internal caches
        try:
            # Clean up some temporary objects in module cache
            import sys

            # Don't delete key modules, only clean up possible temporary resources
            if hasattr(sys, "_clear_type_cache"):
                sys._clear_type_cache()
        except Exception:
            pass

        # 6. Final garbage collection
        gc.collect()

    except Exception as e:
        # Silently handle cleanup errors to avoid affecting main flow
        # But can log errors in debug mode
        try:
            import logging

            logging.getLogger(__name__).debug(f"Resource cleanup warning: {e}")
        except Exception:
            pass


--- ui/__init__.py ---
"""
UI Module

Streamlit application user interface components module

Contains the following submodules:
- styles: CSS styles
- components: UI components
- layout: Page layout
- handlers: Event handlers
- streamlit_app: Main application
- app: Application entry
"""

__version__ = "1.0.0"
__author__ = "DeepCode Team"

# Import main components
from .layout import main_layout
from .components import display_header, display_features, display_status
from .handlers import initialize_session_state
from .styles import get_main_styles

# Import application main function
try:
    from .streamlit_app import main as streamlit_main
except ImportError:
    # Fallback to absolute import if relative import fails
    import sys
    import os

    sys.path.insert(0, os.path.dirname(__file__))
    from streamlit_app import main as streamlit_main

__all__ = [
    "main_layout",
    "display_header",
    "display_features",
    "display_status",
    "initialize_session_state",
    "get_main_styles",
    "streamlit_main",
]


--- ui/layout.py ---
"""
DeepCode Layout Manager
Organizes the visual structure using the Cyber components.
"""

from typing import Optional

import streamlit as st
from .components import (
    display_features,
    display_header,
    footer_component,
    guided_requirement_workflow,
    input_method_selector,
    requirement_mode_selector,
    results_display_component,
    sidebar_control_panel,
    system_status_component,
)
from .styles import get_main_styles
from .handlers import (
    initialize_session_state,
    handle_start_processing_button,
    handle_error_display,
    handle_guided_mode_processing,
)


def setup_page_config():
    st.set_page_config(
        page_title="DeepCode",
        page_icon="assets/logo.png",
        layout="wide",
        initial_sidebar_state="expanded",
        menu_items={
            "Get Help": "https://github.com/deepcode",
            "About": "DeepCode AI Research Engine v3.0",
        },
    )


def main_layout():
    """Main layout execution"""
    # Initialize Core
    initialize_session_state()
    setup_page_config()

    # Inject Cyber Styles
    st.markdown(get_main_styles(), unsafe_allow_html=True)

    # Render Sidebar
    sidebar_control_panel()

    # Main Content Area
    display_header()

    # Determine Content State
    show_results = st.session_state.get("show_results", False)
    last_result = st.session_state.get("last_result", None)

    if show_results and last_result:
        results_display_component(last_result, st.session_state.task_counter)
    else:
        # Landing State
        display_features()
        system_status_component()

        st.markdown('<div style="height: 2rem;"></div>', unsafe_allow_html=True)

        # Input Interface
        render_input_area()

    # Global Error Handler (Always active)
    handle_error_display()

    # Footer
    footer_component()

    return {}


def render_input_area():
    """Handles the logic for which input to show"""

    # Handle guided mode async processing (background)
    handle_guided_mode_processing()

    mode = requirement_mode_selector()
    is_guided = mode == "guided"
    processing = st.session_state.get("processing", False)
    requirements_confirmed = st.session_state.get("requirements_confirmed", False)

    input_source: Optional[str] = None
    input_type: Optional[str] = None

    with st.container():
        if is_guided:
            input_source, _ = guided_requirement_workflow()
            input_type = "chat" if input_source else None
        else:
            input_source, input_type = input_method_selector(
                st.session_state.task_counter
            )

        st.markdown('<div style="height: 1.5rem;"></div>', unsafe_allow_html=True)

        if is_guided and requirements_confirmed and input_source and not processing:
            payload = input_source
            st.session_state.requirements_confirmed = False
            st.session_state.confirmed_requirement_text = None
            handle_start_processing_button(payload, input_type or "chat")

        elif input_source and not processing:
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                if st.button(
                    "START CODING üöÄ", type="primary", use_container_width=True
                ):
                    if is_guided:
                        st.session_state.confirmed_requirement_text = None
                    handle_start_processing_button(input_source, input_type or "chat")

        elif processing:
            st.markdown(
                """
                <div style="padding:1.5rem; border:1px solid var(--primary); border-radius:4px; background:rgba(0, 242, 255, 0.05); text-align:center;">
                    <div class="status-dot" style="display:inline-block; margin-right:10px;"></div>
                    <span style="font-family: var(--font-code); color: var(--primary); animation: pulse-glow 2s infinite;">NEURAL PROCESSING ACTIVE...</span>
                </div>
                """,
                unsafe_allow_html=True,
            )

        elif not input_source and not is_guided:
            st.markdown(
                """
                <div style="text-align:center; color:rgba(255,255,255,0.3); font-family:var(--font-code); font-size:0.8rem;">
                    AWAITING INPUT SIGNAL...
                </div>
                """,
                unsafe_allow_html=True,
            )


--- ui/sidebar_feed.py ---
"""
Sidebar mission feed utilities.
"""

from __future__ import annotations

import logging
from datetime import datetime
from typing import Optional, Dict, Any

import streamlit as st


def _init_event_store():
    if "sidebar_events" not in st.session_state:
        st.session_state.sidebar_events = []


def log_sidebar_event(
    stage: str,
    message: str,
    level: str = "info",
    extra: Optional[Dict[str, Any]] = None,
):
    """
    Record a sidebar feed event for live mission status display.
    Thread-safe: if called from background thread, just log to Python logger instead.
    """
    try:
        # Check if we're in a Streamlit context
        from streamlit.runtime.scriptrunner import get_script_run_ctx

        if get_script_run_ctx() is None:
            # Running in background thread, just use Python logging
            import logging

            logging.info(f"[{stage}] {message}")
            return

        _init_event_store()
        events = list(st.session_state.sidebar_events)
        events.append(
            {
                "timestamp": datetime.utcnow().strftime("%H:%M:%S"),
                "stage": stage.upper(),
                "message": message,
                "level": level,
                "extra": extra or {},
            }
        )
        st.session_state.sidebar_events = events[-80:]
    except Exception:
        # Fallback to Python logging
        import logging

        logging.info(f"[{stage}] {message}")


class SidebarLogHandler(logging.Handler):
    """Forward Python logging records to the sidebar mission feed."""

    def emit(self, record: logging.LogRecord):
        try:
            msg = self.format(record)
            stage = getattr(record, "stage", record.name.split(".")[-1]).upper()
            level = record.levelname.lower()
            payload = {
                "logger": record.name,
                "level": record.levelname,
            }
            if record.exc_info:
                payload["exception"] = self.formatException(record.exc_info)
            log_sidebar_event(stage, msg, level=level, extra=payload)
        except Exception:
            pass


def ensure_sidebar_logging():
    """
    Attach sidebar logging handler once per session to bridge backend logs.
    """
    if st.session_state.get("_sidebar_logging_attached"):
        return

    handler = SidebarLogHandler()
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter("%(message)s")
    handler.setFormatter(formatter)

    logging.getLogger().addHandler(handler)
    st.session_state._sidebar_logging_attached = True


--- ui/streamlit_app.py ---
"""
DeepCode - AI Research Engine

Streamlit Web Interface Main Application File
"""

import os
import sys

# Disable .pyc file generation
os.environ["PYTHONDONTWRITEBYTECODE"] = "1"

# Add parent directory to path for module imports
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Import UI modules
from ui.layout import main_layout


def main():
    """
    Main function - Streamlit application entry

    All UI logic has been modularized into ui/ folder
    """
    # Run main layout
    sidebar_info = main_layout()

    # Additional global logic can be added here if needed

    return sidebar_info


if __name__ == "__main__":
    main()


--- ui/styles.py ---
"""
DeepCode UI Styles - Cyber/AI Tech Theme
Modernized with Glassmorphism, Neon Accents, and Fluid Typography.
"""


def get_main_styles() -> str:
    return """
    <style>
        /* ------------------- IMPORT FONTS ------------------- */
        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=JetBrains+Mono:wght@300;400;600&family=Inter:wght@300;400;600;800&display=swap');

        /* ------------------- VARS (CYBER THEME) ------------------- */
        :root {
            /* Colors */
            --bg-dark: #050505;
            --bg-card: rgba(20, 20, 25, 0.6);
            --bg-card-hover: rgba(30, 30, 40, 0.8);

            --primary: #00f2ff;       /* Cyan Neon */
            --secondary: #7000ff;     /* Electric Purple */
            --accent: #ff0055;        /* Cyber Pink */
            --success: #00ff9d;
            --warning: #ffb800;
            --error: #ff2a6d;
            --text-main: #ffffff;
            --text-muted: rgba(255, 255, 255, 0.6);

            /* Glassmorphism */
            --glass-border: 1px solid rgba(255, 255, 255, 0.08);
            --glass-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.37);
            --neon-shadow: 0 0 10px rgba(0, 242, 255, 0.3), 0 0 20px rgba(0, 242, 255, 0.2);

            /* Typography */
            --font-display: 'Orbitron', sans-serif;
            --font-body: 'Inter', sans-serif;
            --font-code: 'JetBrains Mono', monospace;
        }

        /* ------------------- GLOBAL RESET & ANIMATIONS ------------------- */
        .stApp {
            background-color: var(--bg-dark);
            background-image:
                radial-gradient(circle at 15% 50%, rgba(112, 0, 255, 0.08) 0%, transparent 25%),
                radial-gradient(circle at 85% 30%, rgba(0, 242, 255, 0.08) 0%, transparent 25%);
            font-family: var(--font-body);
            color: var(--text-main);
        }

        h1, h2, h3, h4, h5, h6 {
            font-family: var(--font-display) !important;
            letter-spacing: 1px;
        }

        @keyframes pulse-glow {
            0% { box-shadow: 0 0 0 0 rgba(0, 242, 255, 0.4); }
            70% { box-shadow: 0 0 0 10px rgba(0, 242, 255, 0); }
            100% { box-shadow: 0 0 0 0 rgba(0, 242, 255, 0); }
        }

        /* ------------------- CUSTOM COMPONENTS ------------------- */

        /* Header Design */
        .cyber-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 2rem 0;
            border-bottom: 1px solid rgba(255,255,255,0.1);
            margin-bottom: 2rem;
            background: linear-gradient(90deg, rgba(0,0,0,0) 0%, rgba(0, 242, 255, 0.05) 50%, rgba(0,0,0,0) 100%);
        }

        .brand-container {
            display: flex;
            flex-direction: column;
        }

        .brand-title {
            font-family: var(--font-display);
            font-size: 3.5rem;
            font-weight: 900;
            background: linear-gradient(90deg, #fff, var(--primary));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            letter-spacing: -2px;
            text-shadow: 0 0 30px rgba(0, 242, 255, 0.2);
        }

        .brand-subtitle {
            font-family: var(--font-code);
            color: var(--text-muted);
            font-size: 0.9rem;
            letter-spacing: 3px;
            text-transform: uppercase;
            margin-top: 5px;
        }

        .status-indicator {
            display: flex;
            align-items: center;
            gap: 0.8rem;
            padding: 0.6rem 1.2rem;
            background: rgba(0, 255, 157, 0.05);
            border: 1px solid rgba(0, 255, 157, 0.2);
            border-radius: 4px;
            color: var(--success);
            font-family: var(--font-code);
            font-size: 0.8rem;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        .status-dot {
            width: 8px;
            height: 8px;
            background: var(--success);
            border-radius: 50%;
            box-shadow: 0 0 10px var(--success);
            animation: pulse-glow 2s infinite;
        }

        /* Feature Cards */
        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 1.5rem;
            margin-bottom: 3rem;
        }

        .cyber-card {
            background: var(--bg-card);
            backdrop-filter: blur(12px);
            border: var(--glass-border);
            padding: 2rem;
            border-radius: 2px; /* More angular for tech feel */
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
            height: 100%;
        }

        .cyber-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 3px;
            height: 0%;
            background: var(--primary);
            transition: height 0.3s ease;
        }

        .cyber-card:hover::before {
            height: 100%;
        }

        .cyber-card:hover {
            transform: translateY(-5px);
            background: var(--bg-card-hover);
            box-shadow: var(--neon-shadow);
            border-color: rgba(0, 242, 255, 0.4);
        }

        .card-icon {
            font-size: 2rem;
            margin-bottom: 1.5rem;
            color: var(--primary);
            filter: drop-shadow(0 0 10px rgba(0, 242, 255, 0.5));
        }

        .card-title {
            font-family: var(--font-display);
            font-weight: 700;
            font-size: 1.2rem;
            margin-bottom: 0.8rem;
            color: white;
        }

        .card-desc {
            font-family: var(--font-body);
            font-size: 0.95rem;
            color: var(--text-muted);
            line-height: 1.6;
        }

        /* ------------------- STREAMLIT OVERRIDES ------------------- */

        /* Sidebar */
        [data-testid="stSidebar"] {
            background-color: #020203;
            border-right: 1px solid rgba(255,255,255,0.05);
        }
        [data-testid="stSidebar"] h1, [data-testid="stSidebar"] h2, [data-testid="stSidebar"] h3 {
            color: var(--primary) !important;
        }

        /* Inputs (Text, Select, Area) */
        .stTextInput > div > div > input,
        .stSelectbox > div > div > div,
        .stTextArea > div > div > textarea {
            background-color: rgba(255,255,255,0.03);
            border: 1px solid rgba(255,255,255,0.1);
            color: white;
            border-radius: 4px;
            font-family: var(--font-code);
        }

        .stTextInput > div > div > input:focus,
        .stTextArea > div > div > textarea:focus {
            border-color: var(--primary);
            box-shadow: 0 0 15px rgba(0, 242, 255, 0.1);
            background-color: rgba(0,0,0,0.3);
        }

        /* Tabs */
        .stTabs [data-baseweb="tab-list"] {
            gap: 20px;
            border-bottom: 1px solid rgba(255,255,255,0.1);
        }
        .stTabs [data-baseweb="tab"] {
            background-color: transparent;
            border-radius: 4px 4px 0 0;
            color: var(--text-muted);
            font-family: var(--font-display);
            padding: 10px 20px;
        }
        .stTabs [aria-selected="true"] {
            background-color: rgba(0, 242, 255, 0.1);
            color: var(--primary);
            border-bottom: 2px solid var(--primary);
        }

        /* Buttons */
        .stButton > button {
            background: transparent;
            border: 1px solid var(--primary);
            border-radius: 4px;
            color: var(--primary);
            font-family: var(--font-display);
            font-weight: 600;
            letter-spacing: 2px;
            transition: all 0.3s;
            text-transform: uppercase;
            padding: 0.5rem 2rem;
            box-shadow: 0 0 10px rgba(0, 242, 255, 0.1);
        }
        .stButton > button:hover {
            background: var(--primary);
            color: #000;
            box-shadow: 0 0 25px rgba(0, 242, 255, 0.6);
            transform: translateY(-2px);
        }

        /* Primary Action Button Override */
        button[kind="primary"] {
            background: linear-gradient(90deg, var(--secondary) 0%, var(--primary) 100%);
            border: none;
            color: white !important;
        }

        /* Expanders */
        .streamlit-expanderHeader {
            background-color: rgba(255,255,255,0.02);
            border-radius: 4px;
            border: 1px solid rgba(255,255,255,0.05);
        }

        /* Code Blocks */
        code {
            font-family: var(--font-code) !important;
            color: var(--primary) !important;
            background-color: rgba(0,0,0,0.3) !important;
        }

        /* Sidebar feed */
        .sidebar-feed-card {
            border: 1px solid rgba(255,255,255,0.08);
            border-left: 3px solid var(--primary);
            padding: 0.75rem;
            border-radius: 4px;
            margin-bottom: 0.75rem;
            background: rgba(255,255,255,0.02);
            box-shadow: 0 4px 12px rgba(0,0,0,0.25);
        }
        .sidebar-feed-card .stage-line {
            display: flex;
            justify-content: space-between;
            font-family: var(--font-display);
            font-size: 0.8rem;
            letter-spacing: 1px;
            margin-bottom: 0.35rem;
        }
        .sidebar-feed-card .stage {
            color: var(--primary);
        }
        .sidebar-feed-card .time {
            color: rgba(255,255,255,0.4);
            font-family: var(--font-code);
        }
        .sidebar-feed-card .message {
            font-size: 0.9rem;
            color: rgba(255,255,255,0.85);
            line-height: 1.4;
        }
        .sidebar-feed-card.level-success {
            border-left-color: var(--success);
        }
        .sidebar-feed-card.level-error {
            border-left-color: var(--error);
        }
        .sidebar-feed-card.level-warning {
            border-left-color: var(--warning);
        }

        .system-monitor-card {
            border: 1px solid rgba(255,255,255,0.08);
            border-radius: 6px;
            padding: 1rem;
            background: rgba(0,0,0,0.25);
            margin-bottom: 1.5rem;
            box-shadow: 0 6px 18px rgba(0,0,0,0.35);
        }
        .system-monitor-card .status-grid {
            display: grid;
            grid-template-columns: repeat(2, minmax(0, 1fr));
            gap: 0.75rem;
        }
        .system-monitor-card .status-chip {
            border: 1px solid rgba(255,255,255,0.08);
            border-radius: 4px;
            padding: 0.5rem 0.75rem;
            font-size: 0.8rem;
            letter-spacing: 1px;
            text-transform: uppercase;
            display: flex;
            justify-content: space-between;
        }
        .system-monitor-card .status-chip span:last-child {
            color: var(--primary);
            font-family: var(--font-display);
        }
        .system-monitor-card .latest-stage {
            margin-top: 1rem;
            font-size: 0.85rem;
            color: rgba(255,255,255,0.7);
        }
        .system-monitor-card .latest-stage strong {
            color: var(--text-primary);
        }

        /* Footer area override */
        footer {visibility: hidden;}

    </style>
    """


--- utils/cli_interface.py ---
#!/usr/bin/env python3
"""
Professional CLI Interface Module
‰∏ì‰∏öCLIÁïåÈù¢Ê®°Âùó - ÂåÖÂê´logo„ÄÅÈ¢úËâ≤ÂÆö‰πâÂíåÁïåÈù¢ÁªÑ‰ª∂
"""

import os
import time
import platform
from pathlib import Path
from typing import Optional
import tkinter as tk
from tkinter import filedialog


class Colors:
    """ANSI color codes for terminal styling"""

    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"

    # Gradient colors
    PURPLE = "\033[35m"
    MAGENTA = "\033[95m"
    BLUE = "\033[34m"
    CYAN = "\033[36m"
    GREEN = "\033[32m"
    YELLOW = "\033[33m"


class CLIInterface:
    """Professional CLI interface with modern styling"""

    def __init__(self):
        self.uploaded_file = None
        self.is_running = True

        # Check tkinter availability
        self.tkinter_available = True
        try:
            import tkinter as tk

            # Test if tkinter can create a window (some systems have tkinter but no display)
            test_root = tk.Tk()
            test_root.withdraw()
            test_root.destroy()
        except Exception:
            self.tkinter_available = False

    def clear_screen(self):
        """Clear terminal screen"""
        os.system("cls" if os.name == "nt" else "clear")

    def print_logo(self):
        """Print a beautiful ASCII logo with gradient colors and tech elements"""
        # Á°Æ‰øùÊØèË°åÊÄªÂÖ±79‰∏™Â≠óÁ¨¶Ôºà‰∏çÂåÖÊã¨È¢úËâ≤‰ª£Á†ÅÔºâÔºåËæπÊ°ÜÂÆåÁæéÂØπÈΩê
        logo = f"""
{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}{Colors.MAGENTA}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó{Colors.CYAN}                ‚ïë
‚ïë  {Colors.BOLD}{Colors.PURPLE}‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë{Colors.CYAN}                ‚ïë
‚ïë  {Colors.BOLD}{Colors.BLUE}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë{Colors.CYAN}                ‚ïë
‚ïë  {Colors.BOLD}{Colors.OKBLUE}‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë{Colors.CYAN}                ‚ïë
‚ïë  {Colors.BOLD}{Colors.OKCYAN}‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù   ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë{Colors.CYAN}                ‚ïë
‚ïë  {Colors.BOLD}{Colors.GREEN}‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù    ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù{Colors.CYAN}                ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}{Colors.YELLOW}‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê{Colors.CYAN}   ‚ïë
‚ïë  {Colors.BOLD}{Colors.YELLOW}‚îÇ  ü§ñ AI-POWERED RESEARCH PAPER REPRODUCTION ENGINE üöÄ                  ‚îÇ{Colors.CYAN}   ‚ïë
‚ïë  {Colors.BOLD}{Colors.YELLOW}‚îÇ  ‚ö° INTELLIGENT ‚Ä¢ AUTOMATED ‚Ä¢ CUTTING-EDGE ‚ö°                        ‚îÇ{Colors.CYAN}   ‚ïë
‚ïë  {Colors.BOLD}{Colors.YELLOW}‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò{Colors.CYAN}   ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}{Colors.GREEN}üíé CORE CAPABILITIES:{Colors.ENDC}                                                        {Colors.CYAN}‚ïë
‚ïë    {Colors.BOLD}{Colors.OKCYAN}‚ñ∂ Neural PDF Analysis & Code Extraction                                 {Colors.CYAN}‚ïë
‚ïë    {Colors.BOLD}{Colors.OKCYAN}‚ñ∂ Advanced Document Processing Engine                                   {Colors.CYAN}‚ïë
‚ïë    {Colors.BOLD}{Colors.OKCYAN}‚ñ∂ Multi-Format Support (PDF‚Ä¢DOCX‚Ä¢PPTX‚Ä¢HTML)                           {Colors.CYAN}‚ïë
‚ïë    {Colors.BOLD}{Colors.OKCYAN}‚ñ∂ Smart File Upload Interface                                          {Colors.CYAN}‚ïë
‚ïë    {Colors.BOLD}{Colors.OKCYAN}‚ñ∂ Automated Repository Management                                      {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}{Colors.PURPLE}üî¨ TECH STACK: Python‚Ä¢AI‚Ä¢MCP‚Ä¢Docling‚Ä¢LLM                                   {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
        print(logo)

    def print_welcome_banner(self):
        """Print welcome banner with version info"""
        banner = f"""
{Colors.BOLD}{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                              WELCOME TO ReproAI                              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                               ‚ïë
‚ïë  {Colors.YELLOW}Version: 2.0.0 | Build: Professional Edition                                 {Colors.CYAN}‚ïë
‚ïë  {Colors.GREEN}Status: Ready | Engine: Initialized                                          {Colors.CYAN}‚ïë
‚ïë  {Colors.PURPLE}Author: AI Research Team | License: MIT                                      {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
        print(banner)

    def print_separator(self, char="‚ïê", length=79, color=Colors.CYAN):
        """Print a styled separator line"""
        print(f"{color}{char * length}{Colors.ENDC}")

    def print_status(self, message: str, status_type: str = "info"):
        """Print status message with appropriate styling"""
        status_styles = {
            "success": f"{Colors.OKGREEN}‚úÖ",
            "error": f"{Colors.FAIL}‚ùå",
            "warning": f"{Colors.WARNING}‚ö†Ô∏è ",
            "info": f"{Colors.OKBLUE}‚ÑπÔ∏è ",
            "processing": f"{Colors.YELLOW}‚è≥",
            "upload": f"{Colors.PURPLE}üìÅ",
            "download": f"{Colors.CYAN}üì•",
            "analysis": f"{Colors.MAGENTA}üîç",
        }

        icon = status_styles.get(status_type, status_styles["info"])
        print(f"{icon} {Colors.BOLD}{message}{Colors.ENDC}")

    def create_menu(self):
        """Create an interactive menu"""
        menu = f"""
{Colors.BOLD}{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                MAIN MENU                                      ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                               ‚ïë
‚ïë  {Colors.OKGREEN}üåê [U] Process URL       {Colors.CYAN}‚îÇ  {Colors.PURPLE}üìÅ [F] Upload File    {Colors.CYAN}‚îÇ  {Colors.FAIL}‚ùå [Q] Quit{Colors.CYAN}         ‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.YELLOW}üìù Enter a research paper URL (arXiv, IEEE, ACM, etc.)                      {Colors.CYAN}‚ïë
‚ïë  {Colors.YELLOW}   or upload a PDF/DOC file for intelligent analysis                        {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.OKCYAN}üí° Tip: Press 'F' to open file browser or 'U' to enter URL manually        {Colors.CYAN}‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
        print(menu)

    def get_user_input(self):
        """Get user input with styled prompt"""
        print(f"\n{Colors.BOLD}{Colors.OKCYAN}‚û§ Your choice: {Colors.ENDC}", end="")
        return input().strip().lower()

    def upload_file_gui(self) -> Optional[str]:
        """Modern file upload interface using tkinter with cross-platform compatibility"""
        # Check if tkinter is available
        if not self.tkinter_available:
            self.print_status("GUI file dialog not available on this system", "warning")
            self.print_status("Using manual file path input instead", "info")
            return self._get_manual_file_path()

        def select_file():
            try:
                # Create a hidden root window
                root = tk.Tk()
                root.withdraw()  # Hide the main window

                # Platform-specific configurations
                system = platform.system()

                if system == "Darwin":  # macOS
                    # macOS specific settings
                    try:
                        root.call("wm", "attributes", ".", "-topmost", True)
                    except Exception:
                        pass

                    # macOS compatible file types
                    file_types = [
                        ("PDF Files", ".pdf"),
                        ("Word Documents", ".docx .doc"),
                        ("PowerPoint Files", ".pptx .ppt"),
                        ("HTML Files", ".html .htm"),
                        ("Text Files", ".txt .md"),
                        ("All Files", ".*"),
                    ]
                else:
                    # Windows and Linux
                    root.attributes("-topmost", True)

                    # Windows/Linux compatible file types
                    file_types = [
                        ("PDF Files", "*.pdf"),
                        ("Word Documents", "*.docx;*.doc"),
                        ("PowerPoint Files", "*.pptx;*.ppt"),
                        ("HTML Files", "*.html;*.htm"),
                        ("Text Files", "*.txt;*.md"),
                        ("All Files", "*.*"),
                    ]

                # Set window title
                root.title("Repro-AI - File Selector")

                try:
                    # Open file dialog with platform-appropriate settings
                    file_path = filedialog.askopenfilename(
                        title="Select Research Paper File",
                        filetypes=file_types,
                        initialdir=os.getcwd(),
                    )
                except Exception as e:
                    self.print_status(f"File dialog error: {str(e)}", "error")
                    return None
                finally:
                    # Clean up
                    try:
                        root.destroy()
                    except Exception:
                        pass

                return file_path

            except Exception as e:
                # Fallback: destroy root if it exists
                try:
                    if "root" in locals():
                        root.destroy()
                except Exception:
                    pass

                # Print error and suggest alternative
                self.print_status(f"GUI file dialog failed: {str(e)}", "error")
                self.print_status(
                    "Please use manual file path input instead", "warning"
                )
                return self._get_manual_file_path()

        self.print_status("Opening file browser dialog...", "upload")
        file_path = select_file()

        if file_path:
            # Validate file
            if not os.path.exists(file_path):
                self.print_status("File not found!", "error")
                return None

            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB
            file_ext = Path(file_path).suffix.lower()

            # Display file info with beautiful formatting
            file_name = Path(file_path).name
            directory = str(Path(file_path).parent)

            # Truncate long paths for display
            if len(file_name) > 50:
                display_name = file_name[:47] + "..."
            else:
                display_name = file_name

            if len(directory) > 49:
                display_dir = "..." + directory[-46:]
            else:
                display_dir = directory

            print(f"""
{Colors.OKGREEN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                               FILE SELECTED                                   ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}üìÑ File Name:{Colors.ENDC} {Colors.CYAN}{display_name:<50}{Colors.OKGREEN}‚ïë
‚ïë  {Colors.BOLD}üìÅ Directory:{Colors.ENDC} {Colors.YELLOW}{display_dir:<49}{Colors.OKGREEN}‚ïë
‚ïë  {Colors.BOLD}üìä File Size:{Colors.ENDC} {Colors.PURPLE}{file_size:.2f} MB{Colors.OKGREEN}                                      ‚ïë
‚ïë  {Colors.BOLD}üîñ File Type:{Colors.ENDC} {Colors.MAGENTA}{file_ext.upper():<50}{Colors.OKGREEN}‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
""")

            self.print_status(f"File successfully selected: {file_name}", "success")
            return file_path
        else:
            self.print_status("No file selected", "warning")
            return None

    def _get_manual_file_path(self) -> Optional[str]:
        """Fallback method for manual file path input when GUI fails"""
        print(
            f"\n{Colors.BOLD}{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
        )
        print(
            "‚ïë                           MANUAL FILE INPUT                                   ‚ïë"
        )
        print(
            f"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}"
        )

        print(f"\n{Colors.YELLOW}üìù Supported file types:{Colors.ENDC}")
        print(f"   {Colors.CYAN}‚Ä¢ PDF files (.pdf)")
        print(f"   {Colors.CYAN}‚Ä¢ Word documents (.docx, .doc)")
        print(f"   {Colors.CYAN}‚Ä¢ PowerPoint files (.pptx, .ppt)")
        print(f"   {Colors.CYAN}‚Ä¢ HTML files (.html, .htm)")
        print(f"   {Colors.CYAN}‚Ä¢ Text files (.txt, .md){Colors.ENDC}")

        print(
            f"\n{Colors.BOLD}{Colors.OKCYAN}üìÅ Enter file path (or drag & drop): {Colors.ENDC}",
            end="",
        )
        file_path = input().strip()

        # Clean up the path (remove quotes if present)
        file_path = file_path.strip("\"'")

        if file_path:
            # Expand user directory if needed
            file_path = os.path.expanduser(file_path)

            # Check if file exists
            if os.path.exists(file_path):
                self.print_status(
                    f"File found: {os.path.basename(file_path)}", "success"
                )
                return file_path
            else:
                self.print_status("File not found at the specified path", "error")
                return None
        else:
            self.print_status("No file path provided", "warning")
            return None

    def get_url_input(self) -> str:
        """Get URL input with validation and examples"""
        print(
            f"\n{Colors.BOLD}{Colors.CYAN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
        )
        print(
            "‚ïë                              URL INPUT                                        ‚ïë"
        )
        print(
            f"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}"
        )

        print(f"\n{Colors.YELLOW}üìù Supported URL Examples:{Colors.ENDC}")
        print(f"   {Colors.CYAN}‚Ä¢ arXiv: https://arxiv.org/pdf/2403.00813")
        print(f"   {Colors.CYAN}‚Ä¢ arXiv: @https://arxiv.org/pdf/2403.00813")
        print(f"   {Colors.CYAN}‚Ä¢ IEEE:  https://ieeexplore.ieee.org/document/...")
        print(f"   {Colors.CYAN}‚Ä¢ ACM:   https://dl.acm.org/doi/...")
        print(
            f"   {Colors.CYAN}‚Ä¢ Direct PDF: https://example.com/paper.pdf{Colors.ENDC}"
        )

        print(
            f"\n{Colors.BOLD}{Colors.OKCYAN}üåê Enter paper URL: {Colors.ENDC}", end=""
        )
        url = input().strip()

        if url:
            # Basic URL validation
            if any(
                domain in url.lower()
                for domain in ["arxiv.org", "ieee", "acm.org", ".pdf", "researchgate"]
            ):
                self.print_status(f"URL received: {url}", "success")
                return url
            else:
                self.print_status("URL appears valid, proceeding...", "info")
                return url
        else:
            self.print_status("No URL provided", "warning")
            return ""

    def show_progress_bar(self, message: str, duration: float = 2.0):
        """Show a progress animation with enhanced styling"""
        print(f"\n{Colors.YELLOW}{message}{Colors.ENDC}")

        # Progress bar animation with different styles
        bar_length = 50
        for i in range(bar_length + 1):
            percent = (i / bar_length) * 100
            filled = "‚ñà" * i
            empty = "‚ñë" * (bar_length - i)

            # Color gradient effect
            if percent < 33:
                color = Colors.FAIL
            elif percent < 66:
                color = Colors.WARNING
            else:
                color = Colors.OKGREEN

            print(
                f"\r{color}[{filled}{empty}] {percent:6.1f}%{Colors.ENDC}",
                end="",
                flush=True,
            )
            time.sleep(duration / bar_length)

        print(f"\n{Colors.OKGREEN}‚úÖ {message} completed!{Colors.ENDC}\n")

    def show_spinner(self, message: str, duration: float = 1.0):
        """Show a spinner animation"""
        spinner_chars = "‚†ã‚†ô‚†π‚†∏‚†º‚†¥‚†¶‚†ß‚†á‚†è"
        end_time = time.time() + duration

        while time.time() < end_time:
            for char in spinner_chars:
                print(
                    f"\r{Colors.CYAN}{char} {Colors.BOLD}{message}{Colors.ENDC}",
                    end="",
                    flush=True,
                )
                time.sleep(0.1)
                if time.time() >= end_time:
                    break

        print(f"\r{Colors.OKGREEN}‚úÖ {Colors.BOLD}{message} - Done!{Colors.ENDC}")

    def print_results_header(self):
        """Print results section header"""
        header = f"""
{Colors.OKGREEN}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                             PROCESSING RESULTS                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
        print(header)

    def print_error_box(self, title: str, error_msg: str):
        """Print error message in a styled box"""
        print(f"""
{Colors.FAIL}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                  ERROR                                        ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                               ‚ïë
‚ïë  {Colors.BOLD}Title: {title:<66}{Colors.FAIL}‚ïë
‚ïë  {Colors.BOLD}Error: {error_msg:<66}{Colors.FAIL}‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
""")

    def print_goodbye(self):
        """Print goodbye message"""
        goodbye = f"""
{Colors.BOLD}{Colors.YELLOW}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                GOODBYE!                                       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                               ‚ïë
‚ïë  {Colors.CYAN}Thank you for using ReproAI!                                               {Colors.YELLOW}‚ïë
‚ïë  {Colors.GREEN}üåü Star us on GitHub: https://github.com/your-repo                        {Colors.YELLOW}‚ïë
‚ïë  {Colors.PURPLE}üìß Contact: support@reproai.com                                          {Colors.YELLOW}‚ïë
‚ïë  {Colors.MAGENTA}üêõ Report issues: https://github.com/your-repo/issues                    {Colors.YELLOW}‚ïë
‚ïë                                                                               ‚ïë
‚ïë  {Colors.OKGREEN}‚ú® Happy coding! See you next time! ‚ú®                                   {Colors.YELLOW}‚ïë
‚ïë                                                                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{Colors.ENDC}
"""
        print(goodbye)

    def ask_continue(self) -> bool:
        """Ask user if they want to continue"""
        print(
            f"\n{Colors.BOLD}{Colors.CYAN}Press Enter to continue or 'q' to quit: {Colors.ENDC}",
            end="",
        )
        choice = input().strip().lower()
        return choice not in ["q", "quit", "exit"]


--- utils/cross_platform_file_handler.py ---
#!/usr/bin/env python3
"""
Cross-Platform File Handler
Ë∑®Âπ≥Âè∞Êñá‰ª∂Â§ÑÁêÜÊ®°Âùó

This module provides robust file handling utilities that work consistently
across Windows, Linux, and macOS, with proper error handling and cleanup.

Key features:
- Safe temporary file creation with proper cleanup
- Cross-platform path handling
- Atomic file operations
- Comprehensive error handling and logging
"""

import os
import shutil
import tempfile
import logging
import atexit
import platform
from pathlib import Path
from typing import Optional, Union
from contextlib import contextmanager


class CrossPlatformFileHandler:
    """
    Robust cross-platform file handler with proper error handling.

    Handles common pitfalls in file operations across different operating systems:
    - Windows file handle issues
    - Path separator inconsistencies
    - Permission problems
    - Temporary file cleanup
    """

    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        Initialize the file handler.

        Args:
            logger: Optional logger instance for tracking operations
        """
        self.logger = logger or self._create_default_logger()
        self.temp_files = []  # Track temporary files for cleanup
        self.platform = platform.system()

        # Register cleanup handler
        atexit.register(self.cleanup_all_temp_files)

        self.logger.info(f"CrossPlatformFileHandler initialized on {self.platform}")

    def _create_default_logger(self) -> logging.Logger:
        """Create a default logger if none provided."""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger

    @staticmethod
    def normalize_path(path: Union[str, Path]) -> Path:
        """
        Normalize a path to use proper separators for the current OS.

        Args:
            path: Input path (string or Path object)

        Returns:
            Normalized Path object

        Example:
            >>> handler = CrossPlatformFileHandler()
            >>> handler.normalize_path("data/files\\test.txt")
            PosixPath('data/files/test.txt')  # On Linux/Mac
            WindowsPath('data\\files\\test.txt')  # On Windows
        """
        if isinstance(path, str):
            # Replace all path separators with the OS-specific one
            path = path.replace("\\", os.sep).replace("/", os.sep)
            return Path(path).resolve()
        return Path(path).resolve()

    def create_safe_temp_file(
        self,
        suffix: str = "",
        prefix: str = "deepcode_",
        content: Optional[bytes] = None,
    ) -> Path:
        """
        Create a temporary file with proper cross-platform handling.

        This method addresses Windows file handle issues by:
        1. Properly closing the file before returning
        2. Setting delete=False to prevent premature deletion
        3. Tracking the file for later cleanup

        Args:
            suffix: File suffix (e.g., ".pdf", ".txt")
            prefix: File prefix for identification
            content: Optional content to write to the file

        Returns:
            Path to the created temporary file

        Raises:
            IOError: If file creation or writing fails
        """
        try:
            # Create temporary file with proper flags
            fd, temp_path = tempfile.mkstemp(
                suffix=suffix,
                prefix=prefix,
                dir=None,  # Use system default temp directory
                text=False,  # Always use binary mode for consistency
            )

            # Convert to Path object
            temp_path_obj = Path(temp_path)

            # Write content if provided
            if content is not None:
                try:
                    # Write using the file descriptor (more reliable on Windows)
                    os.write(fd, content)
                finally:
                    # Always close the file descriptor
                    os.close(fd)

                self.logger.info(
                    f"Created temp file with content: {temp_path_obj.name} "
                    f"({len(content)} bytes)"
                )
            else:
                # Close immediately if no content
                os.close(fd)
                self.logger.info(f"Created empty temp file: {temp_path_obj.name}")

            # Track for cleanup
            self.temp_files.append(temp_path_obj)

            return temp_path_obj

        except Exception as e:
            self.logger.error(f"Failed to create temporary file: {e}")
            raise IOError(f"Temporary file creation failed: {e}")

    @contextmanager
    def temp_directory(self, prefix: str = "deepcode_"):
        """
        Context manager for temporary directory with automatic cleanup.

        Args:
            prefix: Directory prefix for identification

        Yields:
            Path to temporary directory

        Example:
            >>> with handler.temp_directory() as temp_dir:
            ...     # Use temp_dir
            ...     print(temp_dir)
            # Directory automatically cleaned up after context
        """
        temp_dir = None
        try:
            temp_dir = Path(tempfile.mkdtemp(prefix=prefix))
            self.logger.info(f"Created temporary directory: {temp_dir}")
            yield temp_dir
        finally:
            if temp_dir and temp_dir.exists():
                try:
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    self.logger.info(f"Cleaned up temporary directory: {temp_dir}")
                except Exception as e:
                    self.logger.warning(
                        f"Failed to clean up temporary directory {temp_dir}: {e}"
                    )

    def safe_copy_file(
        self,
        source: Union[str, Path],
        destination: Union[str, Path],
        preserve_metadata: bool = True,
        overwrite: bool = False,
    ) -> Path:
        """
        Safely copy a file with proper error handling.

        This method uses copy instead of move to preserve the original file,
        addressing the issue mentioned by the user.

        Args:
            source: Source file path
            destination: Destination file path
            preserve_metadata: Whether to preserve file metadata (timestamps, etc.)
            overwrite: Whether to overwrite if destination exists

        Returns:
            Path to the destination file

        Raises:
            FileNotFoundError: If source file doesn't exist
            FileExistsError: If destination exists and overwrite=False
            IOError: If copy operation fails
        """
        source_path = self.normalize_path(source)
        dest_path = self.normalize_path(destination)

        # Validate source
        if not source_path.exists():
            raise FileNotFoundError(f"Source file not found: {source_path}")

        # Check destination
        if dest_path.exists() and not overwrite:
            raise FileExistsError(
                f"Destination already exists: {dest_path}. "
                f"Use overwrite=True to replace."
            )

        try:
            # Ensure destination directory exists
            dest_path.parent.mkdir(parents=True, exist_ok=True)

            # Copy file (preserves original!)
            if preserve_metadata:
                shutil.copy2(source_path, dest_path)
            else:
                shutil.copy(source_path, dest_path)

            self.logger.info(
                f"Copied file: {source_path.name} -> {dest_path} "
                f"({source_path.stat().st_size} bytes)"
            )

            return dest_path

        except Exception as e:
            self.logger.error(
                f"Failed to copy file from {source_path} to {dest_path}: {e}"
            )
            raise IOError(f"File copy failed: {e}")

    def safe_move_file(
        self,
        source: Union[str, Path],
        destination: Union[str, Path],
        overwrite: bool = False,
    ) -> Path:
        """
        Safely move a file (only if explicitly needed).

        Note: Prefer safe_copy_file to preserve originals.

        Args:
            source: Source file path
            destination: Destination file path
            overwrite: Whether to overwrite if destination exists

        Returns:
            Path to the destination file

        Raises:
            FileNotFoundError: If source file doesn't exist
            FileExistsError: If destination exists and overwrite=False
            IOError: If move operation fails
        """
        source_path = self.normalize_path(source)
        dest_path = self.normalize_path(destination)

        # Validate source
        if not source_path.exists():
            raise FileNotFoundError(f"Source file not found: {source_path}")

        # Check destination
        if dest_path.exists() and not overwrite:
            raise FileExistsError(
                f"Destination already exists: {dest_path}. "
                f"Use overwrite=True to replace."
            )

        try:
            # Ensure destination directory exists
            dest_path.parent.mkdir(parents=True, exist_ok=True)

            # Move file
            shutil.move(str(source_path), str(dest_path))

            self.logger.info(f"Moved file: {source_path.name} -> {dest_path}")

            return dest_path

        except Exception as e:
            self.logger.error(
                f"Failed to move file from {source_path} to {dest_path}: {e}"
            )
            raise IOError(f"File move failed: {e}")

    def safe_remove_file(self, file_path: Union[str, Path]) -> bool:
        """
        Safely remove a file with proper error handling.

        Args:
            file_path: Path to file to remove

        Returns:
            True if file was removed, False if it didn't exist or removal failed
        """
        path = self.normalize_path(file_path)

        if not path.exists():
            self.logger.debug(f"File already removed or doesn't exist: {path}")
            return False

        try:
            # On Windows, ensure file is not read-only
            if self.platform == "Windows":
                os.chmod(path, 0o777)

            path.unlink()
            self.logger.info(f"Removed file: {path.name}")

            # Remove from tracking list if present
            if path in self.temp_files:
                self.temp_files.remove(path)

            return True

        except PermissionError as e:
            self.logger.warning(f"Permission denied when removing {path}: {e}")
            return False
        except Exception as e:
            self.logger.error(f"Failed to remove file {path}: {e}")
            return False

    def cleanup_all_temp_files(self):
        """
        Clean up all tracked temporary files.

        This is automatically called on program exit via atexit,
        but can also be called manually.
        """
        if not self.temp_files:
            return

        self.logger.info(f"Cleaning up {len(self.temp_files)} temporary files...")

        cleaned = 0
        failed = 0

        for temp_file in self.temp_files[
            :
        ]:  # Copy list to avoid modification during iteration
            if self.safe_remove_file(temp_file):
                cleaned += 1
            else:
                failed += 1

        self.logger.info(f"Cleanup complete: {cleaned} files removed, {failed} failed")

        self.temp_files.clear()

    def get_system_temp_dir(self) -> Path:
        """
        Get the system temporary directory with proper cross-platform handling.

        Returns:
            Path to system temporary directory
        """
        return Path(tempfile.gettempdir())

    def create_workspace_directory(
        self, base_dir: Union[str, Path], workspace_name: str, clean: bool = False
    ) -> Path:
        """
        Create a workspace directory with proper structure.

        Args:
            base_dir: Base directory for workspace
            workspace_name: Name of the workspace
            clean: Whether to clean the directory if it exists

        Returns:
            Path to the created workspace directory
        """
        base_path = self.normalize_path(base_dir)
        workspace_path = base_path / workspace_name

        if clean and workspace_path.exists():
            self.logger.info(f"Cleaning existing workspace: {workspace_path}")
            shutil.rmtree(workspace_path, ignore_errors=True)

        workspace_path.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"Created workspace directory: {workspace_path}")

        return workspace_path


# Singleton instance for convenience
_file_handler_instance: Optional[CrossPlatformFileHandler] = None


def get_file_handler(
    logger: Optional[logging.Logger] = None,
) -> CrossPlatformFileHandler:
    """
    Get or create a singleton file handler instance.

    Args:
        logger: Optional logger instance

    Returns:
        CrossPlatformFileHandler instance
    """
    global _file_handler_instance
    if _file_handler_instance is None:
        _file_handler_instance = CrossPlatformFileHandler(logger)
    return _file_handler_instance


# Example usage
if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(level=logging.INFO)

    # Create handler
    handler = CrossPlatformFileHandler()

    print(f"\n{'='*70}")
    print("Cross-Platform File Handler - Demo")
    print(f"{'='*70}\n")

    print(f"Platform: {handler.platform}")
    print(f"System temp directory: {handler.get_system_temp_dir()}")

    # Demo: Create temporary file
    print("\n1. Creating temporary file...")
    temp_file = handler.create_safe_temp_file(
        suffix=".txt", content=b"Test content for cross-platform file handling"
    )
    print(f"   Created: {temp_file}")

    # Demo: Use temporary directory
    print("\n2. Using temporary directory...")
    with handler.temp_directory() as temp_dir:
        print(f"   Temp directory: {temp_dir}")
        test_file = temp_dir / "test.txt"
        test_file.write_text("Hello from temp directory!")
        print(f"   Created file in temp dir: {test_file}")
    print("   Temp directory automatically cleaned up")

    # Demo: Path normalization
    print("\n3. Path normalization:")
    test_paths = [
        "data/files\\test.txt",
        "data\\files/test.txt",
        "data\\files\\test.txt",
    ]
    for path in test_paths:
        normalized = handler.normalize_path(path)
        print(f"   {path} -> {normalized}")

    # Demo: Cleanup
    print("\n4. Cleaning up tracked files...")
    handler.cleanup_all_temp_files()

    print(f"\n{'='*70}")
    print("Demo completed successfully!")
    print(f"{'='*70}\n")


--- utils/dialogue_logger.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Dialogue Logger for Code Implementation Workflow
Logs complete conversation rounds with detailed formatting and paper-specific organization
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List


class DialogueLogger:
    """
    Comprehensive dialogue logger for code implementation workflow
    Captures complete conversation rounds with proper formatting and organization
    """

    def __init__(self, paper_id: str, base_path: str = None):
        """
        Initialize dialogue logger for a specific paper

        Args:
            paper_id: Paper identifier (e.g., "1", "2", etc.)
            base_path: Base path for logs (defaults to agent_folders structure)
        """
        self.paper_id = paper_id
        self.base_path = (
            base_path
            or "/data2/bjdwhzzh/project-hku/Code-Agent2.0/Code-Agent/deepcode-mcp/agent_folders"
        )
        self.log_directory = os.path.join(
            self.base_path, "papers", str(paper_id), "logs"
        )

        # Create log directory if it doesn't exist
        Path(self.log_directory).mkdir(parents=True, exist_ok=True)

        # Session tracking (initialize before log file creation)
        self.round_counter = 0
        self.session_start_time = datetime.now()
        self.current_round_data = {}

        # Generate log filename with timestamp
        timestamp = self.session_start_time.strftime("%Y%m%d_%H%M%S")
        self.log_filename = f"dialogue_log_{timestamp}.md"
        self.log_filepath = os.path.join(self.log_directory, self.log_filename)

        # Initialize log file with header
        self._initialize_log_file()

        print(f"üìù Dialogue Logger initialized for Paper {paper_id}")
        print(f"üìÅ Log file: {self.log_filepath}")

    def _initialize_log_file(self):
        """Initialize the log file with header information"""
        header = f"""# Code Implementation Dialogue Log

**Paper ID:** {self.paper_id}
**Session Start:** {self.session_start_time.strftime('%Y-%m-%d %H:%M:%S')}
**Log File:** {self.log_filename}

---

## Session Overview

This log contains the complete conversation rounds between the user and assistant during the code implementation workflow. Each round includes:

- System prompts and user messages
- Assistant responses with tool calls
- Tool execution results
- Implementation progress markers

---

"""
        try:
            with open(self.log_filepath, "w", encoding="utf-8") as f:
                f.write(header)
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to initialize log file: {e}")

    def start_new_round(
        self, round_type: str = "implementation", context: Dict[str, Any] = None
    ):
        """
        Start a new dialogue round

        Args:
            round_type: Type of round (implementation, summary, error_handling, etc.)
            context: Additional context information (may include 'iteration' to sync with workflow)
        """
        # Use iteration from context if provided, otherwise increment round_counter
        if context and "iteration" in context:
            self.round_counter = context["iteration"]
        else:
            self.round_counter += 1

        self.current_round_data = {
            "round_number": self.round_counter,
            "round_type": round_type,
            "start_time": datetime.now(),
            "context": context or {},
            "messages": [],
            "tool_calls": [],
            "results": [],
            "metadata": {},
        }

        print(f"üîÑ Starting Round {self.round_counter}: {round_type}")

    def log_system_prompt(self, prompt: str, prompt_type: str = "system"):
        """
        Log system prompt or instructions

        Args:
            prompt: System prompt content
            prompt_type: Type of prompt (system, instruction, etc.)
        """
        if not self.current_round_data:
            self.start_new_round("system_setup")

        self.current_round_data["messages"].append(
            {
                "role": "system",
                "type": prompt_type,
                "content": prompt,
                "timestamp": datetime.now().isoformat(),
            }
        )

    def log_user_message(self, message: str, message_type: str = "user_input"):
        """
        Log user message

        Args:
            message: User message content
            message_type: Type of message (user_input, feedback, guidance, etc.)
        """
        if not self.current_round_data:
            self.start_new_round("user_interaction")

        self.current_round_data["messages"].append(
            {
                "role": "user",
                "type": message_type,
                "content": message,
                "timestamp": datetime.now().isoformat(),
            }
        )

    def log_assistant_response(
        self, response: str, response_type: str = "assistant_response"
    ):
        """
        Log assistant response

        Args:
            response: Assistant response content
            response_type: Type of response (assistant_response, analysis, etc.)
        """
        if not self.current_round_data:
            self.start_new_round("assistant_interaction")

        self.current_round_data["messages"].append(
            {
                "role": "assistant",
                "type": response_type,
                "content": response,
                "timestamp": datetime.now().isoformat(),
            }
        )

    def log_tool_calls(self, tool_calls: List[Dict[str, Any]]):
        """
        Log tool calls made by the assistant

        Args:
            tool_calls: List of tool calls with id, name, and input
        """
        if not self.current_round_data:
            self.start_new_round("tool_execution")

        for tool_call in tool_calls:
            self.current_round_data["tool_calls"].append(
                {
                    "id": tool_call.get("id", ""),
                    "name": tool_call.get("name", ""),
                    "input": tool_call.get("input", {}),
                    "timestamp": datetime.now().isoformat(),
                }
            )

    def log_tool_results(self, tool_results: List[Dict[str, Any]]):
        """
        Log tool execution results

        Args:
            tool_results: List of tool results with tool_name and result
        """
        if not self.current_round_data:
            self.start_new_round("tool_results")

        for result in tool_results:
            self.current_round_data["results"].append(
                {
                    "tool_name": result.get("tool_name", ""),
                    "result": result.get("result", ""),
                    "timestamp": datetime.now().isoformat(),
                }
            )

    def log_metadata(self, key: str, value: Any):
        """
        Log metadata information

        Args:
            key: Metadata key
            value: Metadata value
        """
        if not self.current_round_data:
            self.start_new_round("metadata")

        self.current_round_data["metadata"][key] = value

    def log_memory_optimization(
        self,
        messages_before: List[Dict],
        messages_after: List[Dict],
        optimization_stats: Dict[str, Any],
        approach: str = "memory_optimization",
    ):
        """
        Log memory optimization details including before/after message content

        Args:
            messages_before: Messages before optimization
            messages_after: Messages after optimization
            optimization_stats: Statistics about the optimization
            approach: Optimization approach used
        """
        if not self.current_round_data:
            self.start_new_round("memory_optimization")

        # Calculate what was removed/kept
        removed_count = len(messages_before) - len(messages_after)
        compression_ratio = (
            (removed_count / len(messages_before) * 100) if messages_before else 0
        )

        # Log the optimization details
        optimization_data = {
            "approach": approach,
            "messages_before_count": len(messages_before),
            "messages_after_count": len(messages_after),
            "messages_removed_count": removed_count,
            "compression_ratio": f"{compression_ratio:.1f}%",
            "optimization_stats": optimization_stats,
            "timestamp": datetime.now().isoformat(),
        }

        # Store the optimization data
        if "memory_optimizations" not in self.current_round_data:
            self.current_round_data["memory_optimizations"] = []

        self.current_round_data["memory_optimizations"].append(
            {
                "optimization_data": optimization_data,
                "messages_before": messages_before,
                "messages_after": messages_after,
            }
        )

        # Log metadata
        self.log_metadata("memory_optimization", optimization_data)

        print(
            f"üßπ Memory optimization logged: {len(messages_before)} ‚Üí {len(messages_after)} messages ({compression_ratio:.1f}% compression)"
        )

    def complete_round(self, summary: str = "", status: str = "completed"):
        """
        Complete the current round and write to log file

        Args:
            summary: Round summary
            status: Round completion status
        """
        if not self.current_round_data:
            print("‚ö†Ô∏è No active round to complete")
            return

        self.current_round_data["end_time"] = datetime.now()
        self.current_round_data["duration"] = (
            self.current_round_data["end_time"] - self.current_round_data["start_time"]
        ).total_seconds()
        self.current_round_data["summary"] = summary
        self.current_round_data["status"] = status

        # Write round to log file
        self._write_round_to_log()

        print(f"‚úÖ Round {self.round_counter} completed: {status}")

        # Clear current round data
        self.current_round_data = {}

    def _write_round_to_log(self):
        """Write the current round data to the log file in markdown format"""
        try:
            with open(self.log_filepath, "a", encoding="utf-8") as f:
                round_data = self.current_round_data

                # Round header
                f.write(
                    f"\n## Round {round_data['round_number']}: {round_data['round_type'].title()}\n\n"
                )
                f.write(
                    f"**Start Time:** {round_data['start_time'].strftime('%Y-%m-%d %H:%M:%S')}\n"
                )
                f.write(
                    f"**End Time:** {round_data['end_time'].strftime('%Y-%m-%d %H:%M:%S')}\n"
                )
                f.write(f"**Duration:** {round_data['duration']:.2f} seconds\n")
                f.write(f"**Status:** {round_data['status']}\n\n")

                # Context information
                if round_data.get("context"):
                    f.write("### Context\n\n")
                    for key, value in round_data["context"].items():
                        f.write(f"- **{key}:** {value}\n")
                    f.write("\n")

                # Messages
                if round_data.get("messages"):
                    f.write("### Messages\n\n")
                    for i, msg in enumerate(round_data["messages"], 1):
                        role_emoji = {
                            "system": "üîß",
                            "user": "üë§",
                            "assistant": "ü§ñ",
                        }.get(msg["role"], "üìù")
                        f.write(
                            f"#### {role_emoji} {msg['role'].title()} Message {i}\n\n"
                        )
                        f.write(f"**Type:** {msg['type']}\n")
                        f.write(f"**Timestamp:** {msg['timestamp']}\n\n")
                        f.write("```\n")
                        f.write(msg["content"])
                        f.write("\n```\n\n")

                # Tool calls
                if round_data.get("tool_calls"):
                    f.write("### Tool Calls\n\n")
                    for i, tool_call in enumerate(round_data["tool_calls"], 1):
                        f.write(f"#### üõ†Ô∏è Tool Call {i}: {tool_call['name']}\n\n")
                        f.write(f"**ID:** {tool_call['id']}\n")
                        f.write(f"**Timestamp:** {tool_call['timestamp']}\n\n")
                        f.write("**Input:**\n")
                        f.write("```json\n")
                        f.write(
                            json.dumps(tool_call["input"], indent=2, ensure_ascii=False)
                        )
                        f.write("\n```\n\n")

                # Tool results
                if round_data.get("results"):
                    f.write("### Tool Results\n\n")
                    for i, result in enumerate(round_data["results"], 1):
                        f.write(f"#### üìä Result {i}: {result['tool_name']}\n\n")
                        f.write(f"**Timestamp:** {result['timestamp']}\n\n")
                        f.write("**Result:**\n")
                        f.write("```\n")
                        f.write(str(result["result"]))
                        f.write("\n```\n\n")

                # Memory Optimizations
                if round_data.get("memory_optimizations"):
                    f.write("### Memory Optimizations\n\n")
                    for i, opt in enumerate(round_data["memory_optimizations"], 1):
                        opt_data = opt["optimization_data"]
                        messages_before = opt["messages_before"]
                        messages_after = opt["messages_after"]

                        f.write(f"#### üßπ Memory Optimization {i}\n\n")
                        f.write(f"**Approach:** {opt_data['approach']}\n")
                        f.write(
                            f"**Messages Before:** {opt_data['messages_before_count']}\n"
                        )
                        f.write(
                            f"**Messages After:** {opt_data['messages_after_count']}\n"
                        )
                        f.write(
                            f"**Messages Removed:** {opt_data['messages_removed_count']}\n"
                        )
                        f.write(
                            f"**Compression Ratio:** {opt_data['compression_ratio']}\n"
                        )
                        f.write(f"**Timestamp:** {opt_data['timestamp']}\n\n")

                        # Show optimization stats
                        if opt_data.get("optimization_stats"):
                            f.write("**Optimization Statistics:**\n")
                            f.write("```json\n")
                            f.write(
                                json.dumps(
                                    opt_data["optimization_stats"],
                                    indent=2,
                                    ensure_ascii=False,
                                )
                            )
                            f.write("\n```\n\n")

                        # Show messages before optimization (limited to last 5 for readability)
                        if messages_before:
                            f.write("**Messages Before Optimization (last 5):**\n\n")
                            for j, msg in enumerate(messages_before[-5:], 1):
                                role = msg.get("role", "unknown")
                                content = msg.get("content", "")
                                # Truncate very long messages
                                if len(content) > 3000:
                                    content = content[:3000] + "...[truncated]"
                                f.write(
                                    f"- **{role} {j}:** {content[:3000]}{'...' if len(content) > 100 else ''}\n"
                                )
                            f.write("\n")

                        # Show messages after optimization
                        if messages_after:
                            f.write("**Messages After Optimization:**\n\n")
                            for j, msg in enumerate(messages_after, 1):
                                role = msg.get("role", "unknown")
                                content = msg.get("content", "")
                                # Truncate very long messages
                                if len(content) > 3000:
                                    content = content[:3000] + "...[truncated]"
                                f.write(
                                    f"- **{role} {j}:** {content[:3000]}{'...' if len(content) > 100 else ''}\n"
                                )
                            f.write("\n")

                        # Show what was removed
                        if len(messages_before) > len(messages_after):
                            removed_messages = (
                                messages_before[: -len(messages_after)]
                                if messages_after
                                else messages_before
                            )
                            f.write(
                                f"**Messages Removed ({len(removed_messages)}):**\n\n"
                            )
                            for j, msg in enumerate(
                                removed_messages[-3:], 1
                            ):  # Show last 3 removed
                                role = msg.get("role", "unknown")
                                content = msg.get("content", "")
                                if len(content) > 3000:
                                    content = content[:3000] + "...[truncated]"
                                f.write(f"- **{role} {j}:** {content}\n")
                            f.write("\n")

                        f.write("\n")

                # Metadata
                if round_data.get("metadata"):
                    f.write("### Metadata\n\n")
                    for key, value in round_data["metadata"].items():
                        if (
                            key != "memory_optimization"
                        ):  # Skip memory optimization metadata as it's shown above
                            f.write(f"- **{key}:** {value}\n")
                    f.write("\n")

                # Summary
                if round_data.get("summary"):
                    f.write("### Summary\n\n")
                    f.write(round_data["summary"])
                    f.write("\n\n")

                # Separator
                f.write("---\n\n")

        except Exception as e:
            print(f"‚ö†Ô∏è Failed to write round to log: {e}")

    def log_complete_exchange(
        self,
        system_prompt: str = "",
        user_message: str = "",
        assistant_response: str = "",
        tool_calls: List[Dict] = None,
        tool_results: List[Dict] = None,
        round_type: str = "exchange",
        context: Dict = None,
        summary: str = "",
    ):
        """
        Log a complete exchange in a single call

        Args:
            system_prompt: System prompt (optional)
            user_message: User message
            assistant_response: Assistant response
            tool_calls: Tool calls made
            tool_results: Tool execution results
            round_type: Type of round
            context: Additional context
            summary: Round summary
        """
        self.start_new_round(round_type, context)

        if system_prompt:
            self.log_system_prompt(system_prompt)

        if user_message:
            self.log_user_message(user_message)

        if assistant_response:
            self.log_assistant_response(assistant_response)

        if tool_calls:
            self.log_tool_calls(tool_calls)

        if tool_results:
            self.log_tool_results(tool_results)

        self.complete_round(summary)

    def get_session_stats(self) -> Dict[str, Any]:
        """Get session statistics"""
        return {
            "paper_id": self.paper_id,
            "session_start": self.session_start_time.isoformat(),
            "total_rounds": self.round_counter,
            "log_file": self.log_filepath,
            "session_duration": (
                datetime.now() - self.session_start_time
            ).total_seconds(),
        }

    def finalize_session(self, final_summary: str = ""):
        """
        Finalize the logging session

        Args:
            final_summary: Final session summary
        """
        try:
            with open(self.log_filepath, "a", encoding="utf-8") as f:
                f.write("\n## Session Summary\n\n")
                f.write(f"**Total Rounds:** {self.round_counter}\n")
                f.write(
                    f"**Session Duration:** {(datetime.now() - self.session_start_time).total_seconds():.2f} seconds\n"
                )
                f.write(
                    f"**End Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                )

                if final_summary:
                    f.write("### Final Summary\n\n")
                    f.write(final_summary)
                    f.write("\n\n")

                f.write("---\n\n")
                f.write("*End of Session*\n")

        except Exception as e:
            print(f"‚ö†Ô∏è Failed to finalize session: {e}")

        print(f"üéØ Session finalized: {self.round_counter} rounds logged")


# Utility functions for easy integration
def create_dialogue_logger(paper_id: str, base_path: str = None) -> DialogueLogger:
    """
    Create a dialogue logger for a specific paper

    Args:
        paper_id: Paper identifier
        base_path: Base path for logs

    Returns:
        DialogueLogger instance
    """
    return DialogueLogger(paper_id, base_path)


def extract_paper_id_from_path(path: str) -> str:
    """
    Extract paper ID from a file path

    Args:
        path: File path containing paper information

    Returns:
        Paper ID string
    """
    # Extract paper ID from path like "/data2/.../papers/1/initial_plan.txt"
    parts = path.split("/")
    for i, part in enumerate(parts):
        if part == "papers" and i + 1 < len(parts):
            return parts[i + 1]
    return "unknown"


# Example usage
if __name__ == "__main__":
    # Test the dialogue logger
    logger = DialogueLogger("1")

    # Log a complete exchange
    logger.log_complete_exchange(
        system_prompt="You are a code implementation assistant.",
        user_message="Implement the transformer model",
        assistant_response="I'll implement the transformer model step by step.",
        tool_calls=[
            {"id": "1", "name": "write_file", "input": {"filename": "transformer.py"}}
        ],
        tool_results=[
            {"tool_name": "write_file", "result": "File created successfully"}
        ],
        round_type="implementation",
        context={"files_implemented": 1},
        summary="Successfully implemented transformer model",
    )

    # Test memory optimization logging
    logger.start_new_round(
        "memory_optimization", {"trigger_reason": "write_file_detected"}
    )

    # Mock messages before and after optimization
    messages_before = [
        {"role": "user", "content": "Original message 1"},
        {"role": "assistant", "content": "Original response 1"},
        {"role": "user", "content": "Original message 2"},
        {"role": "assistant", "content": "Original response 2"},
        {"role": "user", "content": "Original message 3"},
    ]

    messages_after = [
        {"role": "user", "content": "Original message 1"},
        {"role": "assistant", "content": "Original response 1"},
        {"role": "user", "content": "Original message 3"},
    ]

    # Mock optimization stats
    optimization_stats = {
        "implemented_files_tracked": 2,
        "current_round": 5,
        "concise_mode_active": True,
    }

    # Log memory optimization
    logger.log_memory_optimization(
        messages_before=messages_before,
        messages_after=messages_after,
        optimization_stats=optimization_stats,
        approach="clear_after_write_file",
    )

    logger.complete_round("Memory optimization test completed")

    # Finalize session
    logger.finalize_session(
        "Test session with memory optimization logging completed successfully"
    )

    print("‚úÖ Dialogue logger test completed with memory optimization")


--- utils/file_processor.py ---
"""
File processing utilities for handling paper files and related operations.
"""

import json
import os
import re
from typing import Dict, List, Optional, Union


class FileProcessor:
    """
    A class to handle file processing operations including path extraction and file reading.
    """

    @staticmethod
    def extract_file_path(file_info: Union[str, Dict]) -> Optional[str]:
        """
        Extract paper directory path from the input information.

        Args:
            file_info: Either a JSON string or a dictionary containing file information

        Returns:
            Optional[str]: The extracted paper directory path or None if not found
        """
        try:
            # Handle direct file path input
            if isinstance(file_info, str):
                # Check if it's a file path (existing or not)
                if file_info.endswith(
                    (".md", ".pdf", ".txt", ".docx", ".doc", ".html", ".htm")
                ):
                    # It's a file path, return the directory
                    return os.path.dirname(os.path.abspath(file_info))
                elif os.path.exists(file_info):
                    if os.path.isfile(file_info):
                        return os.path.dirname(os.path.abspath(file_info))
                    elif os.path.isdir(file_info):
                        return os.path.abspath(file_info)

                # Try to parse as JSON
                try:
                    info_dict = json.loads(file_info)
                except json.JSONDecodeError:
                    # Â∞ùËØï‰ªéÊñáÊú¨‰∏≠ÊèêÂèñJSON
                    info_dict = FileProcessor.extract_json_from_text(file_info)
                    if not info_dict:
                        # If not JSON and doesn't look like a file path, raise error
                        raise ValueError(
                            f"Input is neither a valid file path nor JSON: {file_info}"
                        )
            else:
                info_dict = file_info

            # Extract paper path from dictionary
            paper_path = info_dict.get("paper_path")
            if not paper_path:
                raise ValueError("No paper_path found in input dictionary")

            # Get the directory path instead of the file path
            paper_dir = os.path.dirname(paper_path)

            # Convert to absolute path if relative
            if not os.path.isabs(paper_dir):
                paper_dir = os.path.abspath(paper_dir)

            return paper_dir

        except (AttributeError, TypeError) as e:
            raise ValueError(f"Invalid input format: {str(e)}")

    @staticmethod
    def find_markdown_file(directory: str) -> Optional[str]:
        """
        Find the first markdown file in the given directory.

        Args:
            directory: Directory path to search

        Returns:
            Optional[str]: Path to the markdown file or None if not found
        """
        if not os.path.isdir(directory):
            return None

        for file in os.listdir(directory):
            if file.endswith(".md"):
                return os.path.join(directory, file)
        return None

    @staticmethod
    def parse_markdown_sections(content: str) -> List[Dict[str, Union[str, int, List]]]:
        """
        Parse markdown content and organize it by sections based on headers.

        Args:
            content: The markdown content to parse

        Returns:
            List[Dict]: A list of sections, each containing:
                - level: The header level (1-6)
                - title: The section title
                - content: The section content
                - subsections: List of subsections
        """
        # Split content into lines
        lines = content.split("\n")
        sections = []
        current_section = None
        current_content = []

        for line in lines:
            # Check if line is a header
            header_match = re.match(r"^(#{1,6})\s+(.+)$", line)

            if header_match:
                # If we were building a section, save its content
                if current_section is not None:
                    current_section["content"] = "\n".join(current_content).strip()
                    sections.append(current_section)

                # Start a new section
                level = len(header_match.group(1))
                title = header_match.group(2).strip()
                current_section = {
                    "level": level,
                    "title": title,
                    "content": "",
                    "subsections": [],
                }
                current_content = []
            elif current_section is not None:
                current_content.append(line)

        # Don't forget to save the last section
        if current_section is not None:
            current_section["content"] = "\n".join(current_content).strip()
            sections.append(current_section)

        return FileProcessor._organize_sections(sections)

    @staticmethod
    def _organize_sections(sections: List[Dict]) -> List[Dict]:
        """
        Organize sections into a hierarchical structure based on their levels.

        Args:
            sections: List of sections with their levels

        Returns:
            List[Dict]: Organized hierarchical structure of sections
        """
        result = []
        section_stack = []

        for section in sections:
            while section_stack and section_stack[-1]["level"] >= section["level"]:
                section_stack.pop()

            if section_stack:
                section_stack[-1]["subsections"].append(section)
            else:
                result.append(section)

            section_stack.append(section)

        return result

    @staticmethod
    async def read_file_content(file_path: str) -> str:
        """
        Read the content of a file asynchronously.

        Args:
            file_path: Path to the file to read

        Returns:
            str: The content of the file

        Raises:
            FileNotFoundError: If the file doesn't exist
            IOError: If there's an error reading the file
        """
        try:
            # Ensure the file exists
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"File not found: {file_path}")

            # Check if file is actually a PDF by reading the first few bytes
            with open(file_path, "rb") as f:
                header = f.read(8)
                if header.startswith(b"%PDF"):
                    raise IOError(
                        f"File {file_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools."
                    )

            # Read file content
            # Note: Using async with would be better for large files
            # but for simplicity and compatibility, using regular file reading
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            return content

        except UnicodeDecodeError as e:
            raise IOError(
                f"Error reading file {file_path}: File encoding is not UTF-8. Original error: {str(e)}"
            )
        except Exception as e:
            raise IOError(f"Error reading file {file_path}: {str(e)}")

    @staticmethod
    def format_section_content(section: Dict) -> str:
        """
        Format a section's content with standardized spacing and structure.

        Args:
            section: Dictionary containing section information

        Returns:
            str: Formatted section content
        """
        # Start with section title
        formatted = f"\n{'#' * section['level']} {section['title']}\n"

        # Add section content if it exists
        if section["content"]:
            formatted += f"\n{section['content'].strip()}\n"

        # Process subsections
        if section["subsections"]:
            # Add a separator before subsections if there's content
            if section["content"]:
                formatted += "\n---\n"

            # Process each subsection
            for subsection in section["subsections"]:
                formatted += FileProcessor.format_section_content(subsection)

        # Add section separator
        formatted += "\n" + "=" * 80 + "\n"

        return formatted

    @staticmethod
    def standardize_output(sections: List[Dict]) -> str:
        """
        Convert structured sections into a standardized string format.

        Args:
            sections: List of section dictionaries

        Returns:
            str: Standardized string output
        """
        output = []

        # Process each top-level section
        for section in sections:
            output.append(FileProcessor.format_section_content(section))

        # Join all sections with clear separation
        return "\n".join(output)

    @classmethod
    async def process_file_input(
        cls, file_input: Union[str, Dict], base_dir: str = None
    ) -> Dict:
        """
        Process file input information and return the structured content.

        Args:
            file_input: File input information (JSON string, dict, or direct file path)
            base_dir: Optional base directory to use for creating paper directories (for sync support)

        Returns:
            Dict: The structured content with sections and standardized text
        """
        try:
            # È¶ñÂÖàÂ∞ùËØï‰ªéÂ≠óÁ¨¶‰∏≤‰∏≠ÊèêÂèñmarkdownÊñá‰ª∂Ë∑ØÂæÑ
            if isinstance(file_input, str):
                import re

                # Try to extract path from backticks first
                file_path_match = re.search(r"`([^`]+\.md)`", file_input)
                if file_path_match:
                    paper_path = file_path_match.group(1)
                    file_input = {"paper_path": paper_path}
                else:
                    # Try to extract from "Saved Path:" or similar patterns
                    path_patterns = [
                        r"[Ss]aved [Pp]ath[:\s]+([^\s\n]+\.md)",
                        r"[Pp]aper [Pp]ath[:\s]+([^\s\n]+\.md)",
                        r"[Ff]ile[:\s]+([^\s\n]+\.md)",
                        r"[Oo]utput[:\s]+([^\s\n]+\.md)",
                    ]
                    for pattern in path_patterns:
                        match = re.search(pattern, file_input)
                        if match:
                            paper_path = match.group(1)
                            file_input = {"paper_path": paper_path}
                            break

            # Extract paper directory path
            paper_dir = cls.extract_file_path(file_input)

            # If base_dir is provided, adjust paper_dir to be relative to base_dir
            if base_dir and paper_dir:
                # If paper_dir is using default location, move it to base_dir
                if paper_dir.endswith(("deepcode_lab", "agent_folders")):
                    paper_dir = base_dir
                else:
                    # Extract the relative part and combine with base_dir
                    paper_name = os.path.basename(paper_dir)
                    # ‰øùÊåÅÂéüÂßãÁõÆÂΩïÂêç‰∏çÂèòÔºå‰∏çÂÅö‰ªª‰ΩïÊõøÊç¢
                    paper_dir = os.path.join(base_dir, "papers", paper_name)

                # Ensure the directory exists
                os.makedirs(paper_dir, exist_ok=True)

            if not paper_dir:
                raise ValueError("Could not determine paper directory path")

            # Get the actual file path
            file_path = None
            if isinstance(file_input, str):
                # Â∞ùËØïËß£Êûê‰∏∫JSONÔºàÂ§ÑÁêÜ‰∏ãËΩΩÁªìÊûúÔºâ
                try:
                    parsed_json = json.loads(file_input)
                    if isinstance(parsed_json, dict) and "paper_path" in parsed_json:
                        file_path = parsed_json.get("paper_path")
                        # Â¶ÇÊûúÊñá‰ª∂‰∏çÂ≠òÂú®ÔºåÂ∞ùËØïÊü•ÊâæmarkdownÊñá‰ª∂
                        if file_path and not os.path.exists(file_path):
                            paper_dir = os.path.dirname(file_path)
                            if os.path.isdir(paper_dir):
                                file_path = cls.find_markdown_file(paper_dir)
                                if not file_path:
                                    raise ValueError(
                                        f"No markdown file found in directory: {paper_dir}"
                                    )
                    else:
                        raise ValueError("Invalid JSON format: missing paper_path")
                except json.JSONDecodeError:
                    # Â∞ùËØï‰ªéÊñáÊú¨‰∏≠ÊèêÂèñJSONÔºàÂ§ÑÁêÜÂåÖÂê´È¢ùÂ§ñÊñáÊú¨ÁöÑ‰∏ãËΩΩÁªìÊûúÔºâ
                    extracted_json = cls.extract_json_from_text(file_input)
                    if extracted_json and "paper_path" in extracted_json:
                        file_path = extracted_json.get("paper_path")
                        # Â¶ÇÊûúÊñá‰ª∂‰∏çÂ≠òÂú®ÔºåÂ∞ùËØïÊü•ÊâæmarkdownÊñá‰ª∂
                        if file_path and not os.path.exists(file_path):
                            paper_dir = os.path.dirname(file_path)
                            if os.path.isdir(paper_dir):
                                file_path = cls.find_markdown_file(paper_dir)
                                if not file_path:
                                    raise ValueError(
                                        f"No markdown file found in directory: {paper_dir}"
                                    )
                    else:
                        # ‰∏çÊòØJSONÔºåÊåâÊñá‰ª∂Ë∑ØÂæÑÂ§ÑÁêÜ
                        # Check if it's a file path (existing or not)
                        if file_input.endswith(
                            (".md", ".pdf", ".txt", ".docx", ".doc", ".html", ".htm")
                        ):
                            if os.path.exists(file_input):
                                file_path = file_input
                            else:
                                # File doesn't exist, try to find markdown in the directory
                                file_path = cls.find_markdown_file(paper_dir)
                                if not file_path:
                                    raise ValueError(
                                        f"No markdown file found in directory: {paper_dir}"
                                    )
                        elif os.path.exists(file_input):
                            if os.path.isfile(file_input):
                                file_path = file_input
                            elif os.path.isdir(file_input):
                                # If it's a directory, find the markdown file
                                file_path = cls.find_markdown_file(file_input)
                                if not file_path:
                                    raise ValueError(
                                        f"No markdown file found in directory: {file_input}"
                                    )
                        else:
                            raise ValueError(f"Invalid input: {file_input}")
            else:
                # Dictionary input
                file_path = file_input.get("paper_path")
                # If the file doesn't exist, try to find markdown in the directory
                if file_path and not os.path.exists(file_path):
                    paper_dir = os.path.dirname(file_path)
                    if os.path.isdir(paper_dir):
                        file_path = cls.find_markdown_file(paper_dir)
                        if not file_path:
                            raise ValueError(
                                f"No markdown file found in directory: {paper_dir}"
                            )

            if not file_path:
                raise ValueError("No valid file path found")

            # Read file content
            content = await cls.read_file_content(file_path)

            # Parse and structure the content
            structured_content = cls.parse_markdown_sections(content)

            # Generate standardized text output
            standardized_text = cls.standardize_output(structured_content)

            return {
                "paper_dir": paper_dir,
                "file_path": file_path,
                "sections": structured_content,
                "standardized_text": standardized_text,
            }

        except Exception as e:
            raise ValueError(f"Error processing file input: {str(e)}")

    @staticmethod
    def extract_json_from_text(text: str) -> Optional[Dict]:
        """
        Extract JSON from text that may contain markdown code blocks or other content.

        Args:
            text: Text that may contain JSON

        Returns:
            Optional[Dict]: Extracted JSON as dictionary or None if not found
        """
        import re

        # Try to find JSON in markdown code blocks
        json_pattern = r"```json\s*(\{.*?\})\s*```"
        match = re.search(json_pattern, text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass

        # Try to find standalone JSON
        json_pattern = r"(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})"
        matches = re.findall(json_pattern, text, re.DOTALL)
        for match in matches:
            try:
                parsed = json.loads(match)
                if isinstance(parsed, dict) and "paper_path" in parsed:
                    return parsed
            except json.JSONDecodeError:
                continue

        return None


--- utils/__init__.py ---
"""
Utils package for paper processing tools.
"""

from .file_processor import FileProcessor
from .dialogue_logger import (
    DialogueLogger,
    create_dialogue_logger,
    extract_paper_id_from_path,
)

__all__ = [
    "FileProcessor",
    "DialogueLogger",
    "create_dialogue_logger",
    "extract_paper_id_from_path",
]


--- utils/llm_utils.py ---
"""
LLM utility functions for DeepCode project.

This module provides common LLM-related utilities to avoid circular imports
and reduce code duplication across the project.
"""

import os
import yaml
from typing import Any, Type, Dict, Tuple


def get_api_keys(secrets_path: str = "mcp_agent.secrets.yaml") -> Dict[str, str]:
    """
    Get API keys from secrets file, with environment variables as fallback.

    Priority: secrets file > environment variables
    This ensures mcp_agent.secrets.yaml configuration is respected.

    Environment variable fallbacks (only used if secrets file has no value):
    - GOOGLE_API_KEY or GEMINI_API_KEY
    - ANTHROPIC_API_KEY
    - OPENAI_API_KEY

    Args:
        secrets_path: Path to the secrets YAML file

    Returns:
        Dict with 'google', 'anthropic', 'openai' keys
    """
    secrets = {}
    if os.path.exists(secrets_path):
        with open(secrets_path, "r", encoding="utf-8") as f:
            secrets = yaml.safe_load(f) or {}

    # Config file takes priority, env vars are fallback only
    return {
        "google": (
            secrets.get("google", {}).get("api_key", "")
            or os.environ.get("GOOGLE_API_KEY")
            or os.environ.get("GEMINI_API_KEY")
            or ""
        ).strip(),
        "anthropic": (
            secrets.get("anthropic", {}).get("api_key", "")
            or os.environ.get("ANTHROPIC_API_KEY")
            or ""
        ).strip(),
        "openai": (
            secrets.get("openai", {}).get("api_key", "")
            or os.environ.get("OPENAI_API_KEY")
            or ""
        ).strip(),
    }


def load_api_config(secrets_path: str = "mcp_agent.secrets.yaml") -> Dict[str, Any]:
    """
    Load API configuration with environment variable override.

    Environment variables take precedence over YAML values:
    - GOOGLE_API_KEY or GEMINI_API_KEY
    - ANTHROPIC_API_KEY
    - OPENAI_API_KEY

    Args:
        secrets_path: Path to the secrets YAML file

    Returns:
        Dict with provider configs including api_key values
    """
    # Load base config from YAML
    config = {}
    if os.path.exists(secrets_path):
        with open(secrets_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f) or {}

    # Get keys with env var override
    keys = get_api_keys(secrets_path)

    # Merge into config structure
    for provider, key in keys.items():
        if key:
            config.setdefault(provider, {})["api_key"] = key

    return config


def _get_llm_class(provider: str) -> Type[Any]:
    """Lazily import and return the LLM class for a given provider."""
    if provider == "anthropic":
        from mcp_agent.workflows.llm.augmented_llm_anthropic import (
            AnthropicAugmentedLLM,
        )

        return AnthropicAugmentedLLM
    elif provider == "openai":
        from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

        return OpenAIAugmentedLLM
    elif provider == "google":
        from mcp_agent.workflows.llm.augmented_llm_google import GoogleAugmentedLLM

        return GoogleAugmentedLLM
    else:
        raise ValueError(f"Unknown provider: {provider}")


def get_preferred_llm_class(config_path: str = "mcp_agent.secrets.yaml") -> Type[Any]:
    """
    Select the LLM class based on user preference and API key availability.

    Priority:
    1. Check mcp_agent.config.yaml for llm_provider preference
    2. Verify the preferred provider has API key
    3. Fallback to first available provider

    Args:
        config_path: Path to the secrets YAML configuration file

    Returns:
        class: The preferred LLM class
    """
    try:
        # Get API keys with environment variable override
        keys = get_api_keys(config_path)
        google_key = keys["google"]
        anthropic_key = keys["anthropic"]
        openai_key = keys["openai"]

        # Read user preference from main config (derive path from secrets path)
        secrets_dir = os.path.dirname(os.path.abspath(config_path))
        main_config_path = os.path.join(secrets_dir, "mcp_agent.config.yaml")
        preferred_provider = None
        if os.path.exists(main_config_path):
            with open(main_config_path, "r", encoding="utf-8") as f:
                main_config = yaml.safe_load(f)
                preferred_provider = main_config.get("llm_provider", "").strip().lower()

        # Map of providers to their keys and class names
        provider_keys = {
            "anthropic": (anthropic_key, "AnthropicAugmentedLLM"),
            "google": (google_key, "GoogleAugmentedLLM"),
            "openai": (openai_key, "OpenAIAugmentedLLM"),
        }

        # Try user's preferred provider first
        if preferred_provider and preferred_provider in provider_keys:
            api_key, class_name = provider_keys[preferred_provider]
            if api_key:
                print(f"ü§ñ Using {class_name} (user preference: {preferred_provider})")
                return _get_llm_class(preferred_provider)
            else:
                print(
                    f"‚ö†Ô∏è Preferred provider '{preferred_provider}' has no API key, checking alternatives..."
                )

        # Fallback: try providers in order of availability
        for provider, (api_key, class_name) in provider_keys.items():
            if api_key:
                print(f"ü§ñ Using {class_name} ({provider} API key found)")
                return _get_llm_class(provider)

        # No API keys found - default to google
        print("‚ö†Ô∏è No API keys configured, falling back to GoogleAugmentedLLM")
        return _get_llm_class("google")

    except Exception as e:
        print(f"ü§ñ Error reading config file {config_path}: {e}")
        print("ü§ñ Falling back to GoogleAugmentedLLM")
        return _get_llm_class("google")


def get_token_limits(config_path: str = "mcp_agent.config.yaml") -> Tuple[int, int]:
    """
    Get token limits from configuration.

    Args:
        config_path: Path to the main configuration file

    Returns:
        tuple: (base_max_tokens, retry_max_tokens)
    """
    # Default values that work with qwen/qwen-max (32768 total context)
    default_base = 20000
    default_retry = 15000

    try:
        if os.path.exists(config_path):
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            openai_config = config.get("openai", {})
            base_tokens = openai_config.get("base_max_tokens", default_base)
            retry_tokens = openai_config.get("retry_max_tokens", default_retry)

            print(
                f"‚öôÔ∏è Token limits from config: base={base_tokens}, retry={retry_tokens}"
            )
            return base_tokens, retry_tokens
        else:
            print(
                f"‚ö†Ô∏è Config file {config_path} not found, using defaults: base={default_base}, retry={default_retry}"
            )
            return default_base, default_retry
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading token config from {config_path}: {e}")
        print(
            f"üîß Falling back to default token limits: base={default_base}, retry={default_retry}"
        )
        return default_base, default_retry


def get_default_models(config_path: str = "mcp_agent.config.yaml"):
    """
    Get default models from configuration file.

    Args:
        config_path: Path to the configuration file

    Returns:
        dict: Dictionary with 'anthropic', 'openai', 'google' default models,
              plus 'google_planning' and 'google_implementation' for phase-specific models
    """
    try:
        if os.path.exists(config_path):
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            # Handle null values in config sections
            anthropic_config = config.get("anthropic") or {}
            openai_config = config.get("openai") or {}
            google_config = config.get("google") or {}

            anthropic_model = anthropic_config.get(
                "default_model", "claude-sonnet-4-20250514"
            )
            openai_model = openai_config.get("default_model", "o3-mini")
            google_model = google_config.get("default_model", "gemini-2.0-flash")

            # Phase-specific models (fall back to default if not specified)
            # Google
            google_planning = google_config.get("planning_model", google_model)
            google_implementation = google_config.get(
                "implementation_model", google_model
            )
            # Anthropic
            anthropic_planning = anthropic_config.get("planning_model", anthropic_model)
            anthropic_implementation = anthropic_config.get(
                "implementation_model", anthropic_model
            )
            # OpenAI
            openai_planning = openai_config.get("planning_model", openai_model)
            openai_implementation = openai_config.get(
                "implementation_model", openai_model
            )

            return {
                "anthropic": anthropic_model,
                "openai": openai_model,
                "google": google_model,
                "google_planning": google_planning,
                "google_implementation": google_implementation,
                "anthropic_planning": anthropic_planning,
                "anthropic_implementation": anthropic_implementation,
                "openai_planning": openai_planning,
                "openai_implementation": openai_implementation,
            }
        else:
            print(f"Config file {config_path} not found, using default models")
            return _get_fallback_models()

    except Exception as e:
        print(f"‚ùåError reading config file {config_path}: {e}")
        return _get_fallback_models()


def _get_fallback_models():
    """Return fallback model configuration when config file is unavailable."""
    google = "gemini-2.0-flash"
    anthropic = "claude-sonnet-4-20250514"
    openai = "o3-mini"
    return {
        "google": google,
        "google_planning": google,
        "google_implementation": google,
        "anthropic": anthropic,
        "anthropic_planning": anthropic,
        "anthropic_implementation": anthropic,
        "openai": openai,
        "openai_planning": openai,
        "openai_implementation": openai,
    }


def get_document_segmentation_config(
    config_path: str = "mcp_agent.config.yaml",
) -> Dict[str, Any]:
    """
    Get document segmentation configuration from config file.

    Args:
        config_path: Path to the main configuration file

    Returns:
        Dict containing segmentation configuration with default values
    """
    try:
        if os.path.exists(config_path):
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            # Get document segmentation config with defaults
            seg_config = config.get("document_segmentation", {})
            return {
                "enabled": seg_config.get("enabled", True),
                "size_threshold_chars": seg_config.get("size_threshold_chars", 50000),
            }
        else:
            print(
                f"üìÑ Config file {config_path} not found, using default segmentation settings"
            )
            return {"enabled": True, "size_threshold_chars": 50000}

    except Exception as e:
        print(f"üìÑ Error reading segmentation config from {config_path}: {e}")
        print("üìÑ Using default segmentation settings")
        return {"enabled": True, "size_threshold_chars": 50000}


def should_use_document_segmentation(
    document_content: str, config_path: str = "mcp_agent.config.yaml"
) -> Tuple[bool, str]:
    """
    Determine whether to use document segmentation based on configuration and document size.

    Args:
        document_content: The content of the document to analyze
        config_path: Path to the configuration file

    Returns:
        Tuple of (should_segment, reason) where:
        - should_segment: Boolean indicating whether to use segmentation
        - reason: String explaining the decision
    """
    seg_config = get_document_segmentation_config(config_path)

    if not seg_config["enabled"]:
        return False, "Document segmentation disabled in configuration"

    doc_size = len(document_content)
    threshold = seg_config["size_threshold_chars"]

    if doc_size > threshold:
        return (
            True,
            f"Document size ({doc_size:,} chars) exceeds threshold ({threshold:,} chars)",
        )
    else:
        return (
            False,
            f"Document size ({doc_size:,} chars) below threshold ({threshold:,} chars)",
        )


def get_adaptive_agent_config(
    use_segmentation: bool, search_server_names: list = None
) -> Dict[str, list]:
    """
    Get adaptive agent configuration based on whether to use document segmentation.

    Args:
        use_segmentation: Whether to include document-segmentation server
        search_server_names: Base search server names (from get_search_server_names)

    Returns:
        Dict containing server configurations for different agents
    """
    if search_server_names is None:
        search_server_names = []

    # Base configuration
    config = {
        "concept_analysis": [],
        "algorithm_analysis": search_server_names.copy(),
        "code_planner": search_server_names.copy(),
    }

    # Add document-segmentation server if needed
    if use_segmentation:
        config["concept_analysis"] = ["document-segmentation"]
        if "document-segmentation" not in config["algorithm_analysis"]:
            config["algorithm_analysis"].append("document-segmentation")
        if "document-segmentation" not in config["code_planner"]:
            config["code_planner"].append("document-segmentation")
    else:
        config["concept_analysis"] = ["filesystem"]
        if "filesystem" not in config["algorithm_analysis"]:
            config["algorithm_analysis"].append("filesystem")
        if "filesystem" not in config["code_planner"]:
            config["code_planner"].append("filesystem")

    return config


def get_adaptive_prompts(use_segmentation: bool) -> Dict[str, str]:
    """
    Get appropriate prompt versions based on segmentation usage.

    Args:
        use_segmentation: Whether to use segmented reading prompts

    Returns:
        Dict containing prompt configurations
    """
    # Import here to avoid circular imports
    from prompts.code_prompts import (
        PAPER_CONCEPT_ANALYSIS_PROMPT,
        PAPER_ALGORITHM_ANALYSIS_PROMPT,
        CODE_PLANNING_PROMPT,
        PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,
        PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,
        CODE_PLANNING_PROMPT_TRADITIONAL,
    )

    if use_segmentation:
        return {
            "concept_analysis": PAPER_CONCEPT_ANALYSIS_PROMPT,
            "algorithm_analysis": PAPER_ALGORITHM_ANALYSIS_PROMPT,
            "code_planning": CODE_PLANNING_PROMPT,
        }
    else:
        return {
            "concept_analysis": PAPER_CONCEPT_ANALYSIS_PROMPT_TRADITIONAL,
            "algorithm_analysis": PAPER_ALGORITHM_ANALYSIS_PROMPT_TRADITIONAL,
            "code_planning": CODE_PLANNING_PROMPT_TRADITIONAL,
        }


--- utils/simple_llm_logger.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ë∂ÖÁÆÄÂåñLLMÂìçÂ∫îÊó•ÂøóËÆ∞ÂΩïÂô®
‰∏ìÊ≥®‰∫éËÆ∞ÂΩïLLMÂõûÂ§çÁöÑÊ†∏ÂøÉÂÜÖÂÆπÔºåÈÖçÁΩÆÁÆÄÂçïÊòìÁî®
"""

import json
import os
import yaml
from datetime import datetime
from pathlib import Path
from typing import Dict, Any


class SimpleLLMLogger:
    """Ë∂ÖÁÆÄÂåñÁöÑLLMÂìçÂ∫îÊó•ÂøóËÆ∞ÂΩïÂô®"""

    def __init__(self, config_path: str = "mcp_agent.config.yaml"):
        """
        ÂàùÂßãÂåñÊó•ÂøóËÆ∞ÂΩïÂô®

        Args:
            config_path: ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ
        """
        self.config = self._load_config(config_path)
        self.llm_config = self.config.get("llm_logger", {})

        # Â¶ÇÊûúÁ¶ÅÁî®ÂàôÁõ¥Êé•ËøîÂõû
        if not self.llm_config.get("enabled", True):
            self.enabled = False
            return

        self.enabled = True
        self._setup_logger()

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Âä†ËΩΩÈÖçÁΩÆÊñá‰ª∂"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è ÈÖçÁΩÆÊñá‰ª∂Âä†ËΩΩÂ§±Ë¥•: {e}Ôºå‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆ")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÈªòËÆ§ÈÖçÁΩÆ"""
        return {
            "llm_logger": {
                "enabled": True,
                "output_format": "json",
                "log_level": "basic",
                "log_directory": "logs/llm_responses",
                "filename_pattern": "llm_responses_{timestamp}.jsonl",
                "include_models": ["claude-sonnet-4", "gpt-4", "o3-mini"],
                "min_response_length": 50,
            }
        }

    def _setup_logger(self):
        """ËÆæÁΩÆÊó•ÂøóËÆ∞ÂΩïÂô®"""
        log_dir = self.llm_config.get("log_directory", "logs/llm_responses")

        # ÂàõÂª∫Êó•ÂøóÁõÆÂΩï
        Path(log_dir).mkdir(parents=True, exist_ok=True)

        # ÁîüÊàêÊó•ÂøóÊñá‰ª∂Âêç
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename_pattern = self.llm_config.get(
            "filename_pattern", "llm_responses_{timestamp}.jsonl"
        )
        self.log_file = os.path.join(
            log_dir, filename_pattern.format(timestamp=timestamp)
        )

        print(f"üìù LLMÂìçÂ∫îÊó•Âøó: {self.log_file}")

    def log_response(self, content: str, model: str = "", agent: str = "", **kwargs):
        """
        ËÆ∞ÂΩïLLMÂìçÂ∫î - ÁÆÄÂåñÁâàÊú¨

        Args:
            content: LLMÂìçÂ∫îÂÜÖÂÆπ
            model: Ê®°ÂûãÂêçÁß∞
            agent: AgentÂêçÁß∞
            **kwargs: ÂÖ∂‰ªñÂèØÈÄâ‰ø°ÊÅØ
        """
        if not self.enabled:
            return

        # Ê£ÄÊü•ÊòØÂê¶Â∫îËØ•ËÆ∞ÂΩï
        if not self._should_log(content, model):
            return

        # ÊûÑÂª∫Êó•ÂøóËÆ∞ÂΩï
        log_entry = self._build_entry(content, model, agent, kwargs)

        # ÂÜôÂÖ•Êó•Âøó
        self._write_log(log_entry)

        # ÊéßÂà∂Âè∞ÊòæÁ§∫
        self._console_log(content, model, agent)

    def _should_log(self, content: str, model: str) -> bool:
        """Ê£ÄÊü•ÊòØÂê¶Â∫îËØ•ËÆ∞ÂΩï"""
        # Ê£ÄÊü•ÈïøÂ∫¶
        min_length = self.llm_config.get("min_response_length", 50)
        if len(content) < min_length:
            return False

        # Ê£ÄÊü•Ê®°Âûã
        include_models = self.llm_config.get("include_models", [])
        if include_models and not any(m in model for m in include_models):
            return False

        return True

    def _build_entry(self, content: str, model: str, agent: str, extra: Dict) -> Dict:
        """ÊûÑÂª∫Êó•ÂøóÊù°ÁõÆ"""
        log_level = self.llm_config.get("log_level", "basic")

        if log_level == "basic":
            # Âü∫Á°ÄÁ∫ßÂà´ÔºöÂè™ËÆ∞ÂΩïÊ†∏ÂøÉÂÜÖÂÆπ
            return {
                "timestamp": datetime.now().isoformat(),
                "content": content,
                "model": model,
            }
        else:
            # ËØ¶ÁªÜÁ∫ßÂà´ÔºöÂåÖÂê´Êõ¥Â§ö‰ø°ÊÅØ
            entry = {
                "timestamp": datetime.now().isoformat(),
                "content": content,
                "model": model,
                "agent": agent,
            }
            # Ê∑ªÂä†È¢ùÂ§ñ‰ø°ÊÅØ
            if "token_usage" in extra:
                entry["tokens"] = extra["token_usage"]
            if "session_id" in extra:
                entry["session"] = extra["session_id"]
            return entry

    def _write_log(self, entry: Dict):
        """ÂÜôÂÖ•Êó•ÂøóÊñá‰ª∂"""
        output_format = self.llm_config.get("output_format", "json")

        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                if output_format == "json":
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                elif output_format == "text":
                    timestamp = entry.get("timestamp", "")
                    model = entry.get("model", "")
                    content = entry.get("content", "")
                    f.write(f"[{timestamp}] {model}: {content}\n\n")
                elif output_format == "markdown":
                    timestamp = entry.get("timestamp", "")
                    model = entry.get("model", "")
                    content = entry.get("content", "")
                    f.write(f"**{timestamp}** | {model}\n\n{content}\n\n---\n\n")
        except Exception as e:
            print(f"‚ö†Ô∏è ÂÜôÂÖ•Êó•ÂøóÂ§±Ë¥•: {e}")

    def _console_log(self, content: str, model: str, agent: str):
        """ÊéßÂà∂Âè∞ÁÆÄË¶ÅÊòæÁ§∫"""
        preview = content[:80] + "..." if len(content) > 80 else content
        print(f"ü§ñ {model} ({agent}): {preview}")


# ÂÖ®Â±ÄÂÆû‰æã
_global_logger = None


def get_llm_logger() -> SimpleLLMLogger:
    """Ëé∑ÂèñÂÖ®Â±ÄLLMÊó•ÂøóËÆ∞ÂΩïÂô®ÂÆû‰æã"""
    global _global_logger
    if _global_logger is None:
        _global_logger = SimpleLLMLogger()
    return _global_logger


def log_llm_response(content: str, model: str = "", agent: str = "", **kwargs):
    """‰æøÊç∑ÂáΩÊï∞ÔºöËÆ∞ÂΩïLLMÂìçÂ∫î"""
    logger = get_llm_logger()
    logger.log_response(content, model, agent, **kwargs)


# Á§∫‰æã‰ΩøÁî®
if __name__ == "__main__":
    # ÊµãËØïÊó•ÂøóËÆ∞ÂΩï
    log_llm_response(
        content="ËøôÊòØ‰∏Ä‰∏™ÊµãËØïÁöÑLLMÂìçÂ∫îÂÜÖÂÆπÔºåÁî®‰∫éÈ™åËØÅÁÆÄÂåñÊó•ÂøóËÆ∞ÂΩïÂô®ÁöÑÂäüËÉΩÊòØÂê¶Ê≠£Â∏∏Â∑•‰Ωú„ÄÇ",
        model="claude-sonnet-4-20250514",
        agent="TestAgent",
    )

    print("‚úÖ ÁÆÄÂåñLLMÊó•ÂøóÊµãËØïÂÆåÊàê")


--- workflows/agent_orchestration_engine.py ---
"""
Intelligent Agent Orchestration Engine for Research-to-Code Automation

This module serves as the core orchestration engine that coordinates multiple specialized
AI agents to automate the complete research-to-code transformation pipeline:

1. Research Analysis Agent - Intelligent content processing and extraction
2. Workspace Infrastructure Agent - Automated environment synthesis
3. Code Architecture Agent - AI-driven design and planning
4. Reference Intelligence Agent - Automated knowledge discovery
5. Repository Acquisition Agent - Intelligent code repository management
6. Codebase Intelligence Agent - Advanced relationship analysis
7. Code Implementation Agent - AI-powered code synthesis

Core Features:
- Multi-agent coordination with intelligent task distribution
- Local environment automation for seamless deployment
- Real-time progress monitoring with comprehensive error handling
- Adaptive workflow optimization based on processing requirements
- Advanced intelligence analysis with configurable performance modes

Architecture:
- Async/await based high-performance agent coordination
- Modular agent design with specialized role separation
- Intelligent resource management and optimization
- Comprehensive logging and monitoring infrastructure
"""

import asyncio
import json
import os
import re
import yaml
from typing import Any, Callable, Dict, List, Optional, Tuple

# MCP Agent imports
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm import RequestParams
from mcp_agent.workflows.parallel.parallel_llm import ParallelLLM

# Local imports
from prompts.code_prompts import (
    PAPER_INPUT_ANALYZER_PROMPT,
    PAPER_DOWNLOADER_PROMPT,
    PAPER_REFERENCE_ANALYZER_PROMPT,
    CHAT_AGENT_PLANNING_PROMPT,
)
from utils.file_processor import FileProcessor
from workflows.code_implementation_workflow import CodeImplementationWorkflow
from tools.pdf_downloader import move_file_to, download_file_to
from workflows.code_implementation_workflow_index import (
    CodeImplementationWorkflowWithIndex,
)
from utils.llm_utils import (
    get_preferred_llm_class,
    should_use_document_segmentation,
    get_adaptive_agent_config,
    get_adaptive_prompts,
    get_token_limits,
)
from workflows.agents.document_segmentation_agent import prepare_document_segments
from workflows.agents.requirement_analysis_agent import RequirementAnalysisAgent

# Environment configuration
os.environ["PYTHONDONTWRITEBYTECODE"] = "1"  # Prevent .pyc file generation


def _assess_output_completeness(text: str) -> float:
    """
    Á≤æÂáÜËØÑ‰º∞YAMLÊ†ºÂºèÂÆûÁé∞ËÆ°ÂàíÁöÑÂÆåÊï¥ÊÄß

    Âü∫‰∫éCODE_PLANNING_PROMPT_TRADITIONALÁöÑÂÆûÈôÖË¶ÅÊ±ÇÔºö
    1. Ê£ÄÊü•5‰∏™ÂøÖÈúÄÁöÑYAML sectionsÊòØÂê¶ÈÉΩÂ≠òÂú®
    2. È™åËØÅYAMLÁªìÊûÑÁöÑÂÆåÊï¥ÊÄßÔºàÂºÄÂßãÂíåÁªìÊùüÊ†áËÆ∞Ôºâ
    3. Ê£ÄÊü•ÊúÄÂêé‰∏ÄË°åÊòØÂê¶Ë¢´Êà™Êñ≠
    4. È™åËØÅÊúÄÂ∞èÂêàÁêÜÈïøÂ∫¶

    Returns:
        float: ÂÆåÊï¥ÊÄßÂàÜÊï∞ (0.0-1.0)ÔºåË∂äÈ´òË°®Á§∫Ë∂äÂÆåÊï¥
    """
    if not text or len(text.strip()) < 500:
        return 0.0

    score = 0.0
    text_lower = text.lower()

    # 1. Ê£ÄÊü•5‰∏™ÂøÖÈúÄÁöÑYAML sections (ÊùÉÈáç: 0.5 - ÊúÄÈáçË¶Å)
    # ËøôÊòØpromptÊòéÁ°ÆË¶ÅÊ±ÇÁöÑ5‰∏™sections
    required_sections = [
        "file_structure:",
        "implementation_components:",
        "validation_approach:",
        "environment_setup:",
        "implementation_strategy:",
    ]

    sections_found = sum(1 for section in required_sections if section in text_lower)
    section_score = sections_found / len(required_sections)
    score += section_score * 0.5

    print(f"   üìã Required sections: {sections_found}/{len(required_sections)}")

    # 2. Ê£ÄÊü•YAMLÁªìÊûÑÂÆåÊï¥ÊÄß (ÊùÉÈáç: 0.2)
    has_yaml_start = any(
        marker in text
        for marker in ["```yaml", "complete_reproduction_plan:", "paper_info:"]
    )
    has_yaml_end = any(
        marker in text[-500:]
        for marker in ["```", "implementation_strategy:", "validation_approach:"]
    )

    if has_yaml_start and has_yaml_end:
        score += 0.2
    elif has_yaml_start:
        score += 0.1

    # 3. Ê£ÄÊü•ÊúÄÂêé‰∏ÄË°åÂÆåÊï¥ÊÄß (ÊùÉÈáç: 0.15)
    lines = text.strip().split("\n")
    if lines:
        last_line = lines[-1].strip()
        # YAMLÁöÑÊúÄÂêé‰∏ÄË°åÈÄöÂ∏∏ÊòØÁº©ËøõÁöÑÂÜÖÂÆπË°åÊàñÁªìÊùüÊ†áËÆ∞
        if (
            last_line.endswith(("```", ".", ":", "]", "}"))
            or last_line.startswith(("-", "*", " "))  # YAMLÂàóË°®È°πÊàñÁº©ËøõÂÜÖÂÆπ
            or (
                len(last_line) < 100 and not last_line.endswith(",")
            )  # Áü≠Ë°å‰∏î‰∏çÊòØË¢´Êà™Êñ≠ÁöÑ
        ):
            score += 0.15
        else:
            # ÈïøË°å‰∏îÊ≤°ÊúâÂêàÈÄÇÁöÑÁªìÂ∞æÔºåÂæàÂèØËÉΩË¢´Êà™Êñ≠
            print(f"   ‚ö†Ô∏è  Last line suspicious: '{last_line[-50:]}'")

    # 4. Ê£ÄÊü•ÂêàÁêÜÁöÑÊúÄÂ∞èÈïøÂ∫¶ (ÊùÉÈáç: 0.15)
    # ‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ5-sectionËÆ°ÂàíÂ∫îËØ•Ëá≥Â∞ë8000Â≠óÁ¨¶
    length = len(text)
    if length >= 10000:
        score += 0.15
    elif length >= 5000:
        score += 0.10
    elif length >= 2000:
        score += 0.05

    print(f"   üìè Content length: {length} chars")

    return min(score, 1.0)


def _adjust_params_for_retry(
    params: RequestParams, retry_count: int, config_path: str = "mcp_agent.config.yaml"
) -> RequestParams:
    """
    TokenÂáèÂ∞ëÁ≠ñÁï•‰ª•ÈÄÇÂ∫îÊ®°ÂûãcontextÈôêÂà∂

    Á≠ñÁï•ËØ¥ÊòéÔºàÈíàÂØπqwen/qwen-maxÁöÑ32768 tokenÈôêÂà∂ÔºâÔºö
    - Á¨¨1Ê¨°ÈáçËØïÔºöREDUCEÂà∞retry_max_tokensÔºà‰ªéconfigËØªÂèñÔºåÈªòËÆ§15000Ôºâ
    - Á¨¨2Ê¨°ÈáçËØïÔºöREDUCEÂà∞retry_max_tokensÁöÑ80%
    - Á¨¨3Ê¨°ÈáçËØïÔºöREDUCEÂà∞retry_max_tokensÁöÑ60%
    - Èôç‰ΩétemperatureÊèêÈ´òÁ®≥ÂÆöÊÄßÂíåÂèØÈ¢ÑÊµãÊÄß

    ‰∏∫‰ªÄ‰πàË¶ÅREDUCEËÄå‰∏çÊòØINCREASEÔºü
    - qwen/qwen-maxÊúÄÂ§ßcontext = 32768 tokens (input + output ÊÄªÂíå)
    - ÂΩìÈÅáÂà∞ "maximum context length exceeded" ÈîôËØØÊó∂ÔºåËØ¥Êòé input + requested_output > 32768
    - INCREASING max_tokensÂè™‰ºöËÆ©ÈóÆÈ¢òÊõ¥‰∏•ÈáçÔºÅ
    - Ê≠£Á°ÆÂÅöÊ≥ïÔºöDECREASE output tokensÔºå‰∏∫Êõ¥Â§öinputÁïôÂá∫Á©∫Èó¥
    - Ê®°ÂûãÂèØ‰ª•Áî®Êõ¥ÁÆÄÊ¥ÅÁöÑËæìÂá∫Ë°®ËææÁõ∏ÂêåÂÜÖÂÆπ
    """
    # ‰ªéÈÖçÁΩÆÊñá‰ª∂ËØªÂèñretry token limit
    _, retry_max_tokens = get_token_limits(config_path)

    # TokenÂáèÂ∞ëÁ≠ñÁï• - ‰∏∫inputËÖæÂá∫Êõ¥Â§öÁ©∫Èó¥
    if retry_count == 0:
        # Á¨¨‰∏ÄÊ¨°ÈáçËØïÔºö‰ΩøÁî®ÈÖçÁΩÆÁöÑretry_max_tokens
        new_max_tokens = retry_max_tokens
    elif retry_count == 1:
        # Á¨¨‰∫åÊ¨°ÈáçËØïÔºöÂáèÂ∞ëÂà∞retry_max_tokensÁöÑ80%
        new_max_tokens = int(retry_max_tokens * 0.9)
    else:
        # Á¨¨‰∏âÊ¨°Âèä‰ª•‰∏äÔºöÂáèÂ∞ëÂà∞retry_max_tokensÁöÑ60%
        new_max_tokens = int(retry_max_tokens * 0.8)

    # ÈöèÁùÄÈáçËØïÊ¨°Êï∞Â¢ûÂä†ÔºåÈôç‰Ωétemperature‰ª•Ëé∑ÂæóÊõ¥‰∏ÄËá¥„ÄÅÊõ¥ÂèØÈ¢ÑÊµãÁöÑËæìÂá∫
    new_temperature = max(params.temperature - (retry_count * 0.15), 0.05)

    print(f"üîß Adjusting parameters for retry {retry_count + 1}:")
    print(f"   Token limit: {params.maxTokens} ‚Üí {new_max_tokens}")
    print(f"   Temperature: {params.temperature:.2f} ‚Üí {new_temperature:.2f}")
    print(
        "   üí° Strategy: REDUCE output tokens to fit within model's total context limit"
    )

    # return RequestParams(
    #     maxTokens=new_max_tokens,  # Ê≥®ÊÑèÔºö‰ΩøÁî® camelCase
    #     temperature=new_temperature,
    # )
    return new_max_tokens, new_temperature


async def execute_requirement_analysis_workflow(
    user_input: str,
    analysis_mode: str,
    user_answers: Optional[Dict[str, str]] = None,
    logger=None,
    progress_callback: Optional[Callable[[int, str], None]] = None,
) -> Dict[str, Any]:
    """
    Lightweight orchestrator to run requirement-analysis-specific flows.
    """

    normalized_input = (user_input or "").strip()
    if not normalized_input:
        return {
            "status": "error",
            "error": "User requirement input cannot be empty.",
        }

    user_answers = user_answers or {}

    try:
        async with RequirementAnalysisAgent(logger=logger) as agent:
            if progress_callback:
                progress_callback(5, "ü§ñ Initializing requirement analysis agent...")

            if analysis_mode == "generate_questions":
                questions = await agent.generate_guiding_questions(normalized_input)
                if progress_callback:
                    progress_callback(100, "üß† Guiding questions generated.")
                return {
                    "status": "success",
                    "result": json.dumps(questions, ensure_ascii=False),
                }

            if analysis_mode == "summarize_requirements":
                summary = await agent.summarize_detailed_requirements(
                    normalized_input, user_answers
                )
                if progress_callback:
                    progress_callback(100, "üìÑ Requirement document created.")
                return {"status": "success", "result": summary}

            raise ValueError(f"Unsupported analysis_mode: {analysis_mode}")

    except Exception as exc:
        message = str(exc)
        if logger:
            try:
                logger.error("Requirement analysis workflow failed: %s", message)
            except Exception:
                pass
        return {"status": "error", "error": message}


def get_default_search_server(config_path: str = "mcp_agent.config.yaml"):
    """
    Get the default search server from configuration.

    Args:
        config_path: Path to the main configuration file

    Returns:
        str: The default search server name ("brave" or "bocha-mcp")
    """
    try:
        if os.path.exists(config_path):
            with open(config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            default_server = config.get("default_search_server", "brave")
            print(f"üîç Using search server: {default_server}")
            return default_server
        else:
            print(f"‚ö†Ô∏è Config file {config_path} not found, using default: brave")
            return "brave"
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading config file {config_path}: {e}")
        print("üîç Falling back to default search server: brave")
        return "brave"


def get_search_server_names(
    additional_servers: Optional[List[str]] = None,
) -> List[str]:
    """
    Get server names list with the configured default search server.

    Args:
        additional_servers: Optional list of additional servers to include

    Returns:
        List[str]: List of server names including the default search server
    """
    default_search = get_default_search_server()
    server_names = [default_search]

    if additional_servers:
        # Add additional servers, avoiding duplicates
        for server in additional_servers:
            if server not in server_names:
                server_names.append(server)

    return server_names


def extract_clean_json(llm_output: str) -> str:
    """
    Extract clean JSON from LLM output, removing all extra text and formatting.

    Args:
        llm_output: Raw LLM output

    Returns:
        str: Clean JSON string
    """
    try:
        # Try to parse the entire output as JSON first
        json.loads(llm_output.strip())
        return llm_output.strip()
    except json.JSONDecodeError:
        pass

    # Remove markdown code blocks
    if "```json" in llm_output:
        pattern = r"```json\s*(.*?)\s*```"
        match = re.search(pattern, llm_output, re.DOTALL)
        if match:
            json_text = match.group(1).strip()
            try:
                json.loads(json_text)
                return json_text
            except json.JSONDecodeError:
                pass

    # Find JSON object starting with {
    lines = llm_output.split("\n")
    json_lines = []
    in_json = False
    brace_count = 0

    for line in lines:
        stripped = line.strip()
        if not in_json and stripped.startswith("{"):
            in_json = True
            json_lines = [line]
            brace_count = stripped.count("{") - stripped.count("}")
        elif in_json:
            json_lines.append(line)
            brace_count += stripped.count("{") - stripped.count("}")
            if brace_count == 0:
                break

    if json_lines:
        json_text = "\n".join(json_lines).strip()
        try:
            json.loads(json_text)
            return json_text
        except json.JSONDecodeError:
            pass

    # Last attempt: use regex to find JSON
    pattern = r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}"
    matches = re.findall(pattern, llm_output, re.DOTALL)
    for match in matches:
        try:
            json.loads(match)
            return match
        except json.JSONDecodeError:
            continue

    # If all methods fail, return original output
    return llm_output


async def run_research_analyzer(prompt_text: str, logger) -> str:
    """
    Run the research analysis workflow using ResearchAnalyzerAgent.

    Args:
        prompt_text: Input prompt text containing research information
        logger: Logger instance for logging information

    Returns:
        str: Analysis result from the agent
    """
    try:
        # Log input information for debugging
        print("üìä Starting research analysis...")
        print(f"Input prompt length: {len(prompt_text) if prompt_text else 0}")
        print(f"Input preview: {prompt_text[:200] if prompt_text else 'None'}...")

        if not prompt_text or prompt_text.strip() == "":
            raise ValueError(
                "Empty or None prompt_text provided to run_research_analyzer"
            )

        analyzer_agent = Agent(
            name="ResearchAnalyzerAgent",
            instruction=PAPER_INPUT_ANALYZER_PROMPT,
            server_names=get_search_server_names(),
        )

        async with analyzer_agent:
            print("analyzer: Connected to server, calling list_tools...")
            try:
                tools = await analyzer_agent.list_tools()
                print(
                    "Tools available:",
                    tools.model_dump() if hasattr(tools, "model_dump") else str(tools),
                )
            except Exception as e:
                print(f"Failed to list tools: {e}")

            try:
                analyzer = await analyzer_agent.attach_llm(get_preferred_llm_class())
                print("‚úÖ LLM attached successfully")
            except Exception as e:
                print(f"‚ùå Failed to attach LLM: {e}")
                raise

            # Set higher token output for research analysis
            analysis_params = RequestParams(
                maxTokens=6144,  # ‰ΩøÁî® camelCase
                temperature=0.3,
            )

            print(
                f"üîÑ Making LLM request with params: maxTokens={analysis_params.maxTokens}, temperature={analysis_params.temperature}"
            )

            try:
                raw_result = await analyzer.generate_str(
                    message=prompt_text, request_params=analysis_params
                )

                print("‚úÖ LLM request completed")
                print(f"Raw result type: {type(raw_result)}")
                print(f"Raw result length: {len(raw_result) if raw_result else 0}")

                if not raw_result:
                    print("‚ùå CRITICAL: raw_result is empty or None!")
                    print("This could indicate:")
                    print("1. LLM API call failed silently")
                    print("2. API rate limiting or quota exceeded")
                    print("3. Network connectivity issues")
                    print("4. MCP server communication problems")
                    raise ValueError("LLM returned empty result")

            except Exception as e:
                print(f"‚ùå LLM generation failed: {e}")
                print(f"Exception type: {type(e)}")
                raise

            # Clean LLM output to ensure only pure JSON is returned
            try:
                clean_result = extract_clean_json(raw_result)
                print(f"Raw LLM output: {raw_result}")
                print(f"Cleaned JSON output: {clean_result}")

                # Log to SimpleLLMLogger
                if hasattr(logger, "log_response"):
                    logger.log_response(
                        clean_result,
                        model="ResearchAnalyzer",
                        agent="ResearchAnalyzerAgent",
                    )

                if not clean_result or clean_result.strip() == "":
                    print("‚ùå CRITICAL: clean_result is empty after JSON extraction!")
                    print(f"Original raw_result was: {raw_result}")
                    raise ValueError("JSON extraction resulted in empty output")

                return clean_result

            except Exception as e:
                print(f"‚ùå JSON extraction failed: {e}")
                print(f"Raw result was: {raw_result}")
                raise

    except Exception as e:
        print(f"‚ùå run_research_analyzer failed: {e}")
        print(f"Exception details: {type(e).__name__}: {str(e)}")
        raise


async def run_resource_processor(analysis_result: str, logger) -> str:
    """
    Run the resource processing workflow - deterministic file operations without LLM.

    This function handles file downloading/moving using direct logic rather than LLM,
    since the paper directory structure and ID are pre-computed and deterministic.

    Args:
        analysis_result: Result from the research analyzer (contains file path/URL)
        logger: Logger instance for logging information

    Returns:
        str: Processing result with paper directory path
    """
    # Pre-compute paper ID - deterministic, no LLM needed
    papers_dir = "./deepcode_lab/papers"
    os.makedirs(papers_dir, exist_ok=True)
    existing_ids = [
        int(d)
        for d in os.listdir(papers_dir)
        if os.path.isdir(os.path.join(papers_dir, d)) and d.isdigit()
    ]
    next_id = max(existing_ids) + 1 if existing_ids else 1
    paper_dir = os.path.join(papers_dir, str(next_id))
    os.makedirs(paper_dir, exist_ok=True)

    logger.info(f"üìã Paper ID: {next_id}")
    logger.info(f"üìÇ Paper directory: {paper_dir}")

    # Extract file path/URL from analysis_result - simple parsing, no LLM needed
    # The analysis_result should contain the path/URL identified by the analyzer
    try:
        # Parse the analysis result to extract path
        analysis_data = json.loads(analysis_result)
        source_path = analysis_data.get("path") or analysis_data.get("input_path")
        input_type = analysis_data.get("input_type", "unknown")

        logger.info(f"üì• Processing {input_type}: {source_path}")

        # Try direct function calls first - no LLM needed for deterministic operations
        direct_call_success = False
        operation_result = None

        # 1. Handle local file - direct copy
        if input_type == "file" and source_path and os.path.exists(source_path):
            logger.info(f"üìÑ Direct file copy: {source_path} -> {paper_dir}")
            try:
                operation_result = await move_file_to(
                    source=source_path, destination=paper_dir, filename=f"{next_id}.pdf"
                )
                # Check if operation succeeded
                if (
                    "[SUCCESS]" in operation_result
                    and "[ERROR]" not in operation_result
                ):
                    direct_call_success = True
                    logger.info(f"‚úÖ Direct file copy succeeded:\n{operation_result}")
                else:
                    logger.warning(f"‚ö†Ô∏è Direct file copy had issues: {operation_result}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Direct file copy failed: {e}")

        # 2. Handle URL - direct download
        elif input_type == "url" and source_path:
            logger.info(f"üåê Direct URL download: {source_path} -> {paper_dir}")
            try:
                operation_result = await download_file_to(
                    url=source_path,
                    destination=paper_dir,
                    filename=f"{next_id}.pdf",  # Default to PDF, conversion will handle it
                )
                # Check if operation succeeded
                if (
                    "[SUCCESS]" in operation_result
                    and "[ERROR]" not in operation_result
                ):
                    direct_call_success = True
                    logger.info(f"‚úÖ Direct download succeeded:\n{operation_result}")
                else:
                    logger.warning(f"‚ö†Ô∏è Direct download had issues: {operation_result}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Direct download failed: {e}")

        # 3. If direct call succeeded, format result
        if direct_call_success:
            dest_path = os.path.join(paper_dir, f"{next_id}.md")
            result = json.dumps(
                {
                    "status": "success",
                    "paper_id": next_id,
                    "paper_dir": paper_dir,
                    "file_path": dest_path,
                    "message": f"File successfully processed to {paper_dir}",
                    "operation_details": operation_result,
                }
            )
        else:
            # 4. Fallback to LLM agent if direct call failed or unsupported type
            logger.info(
                f"ü§ñ Falling back to LLM agent for: {input_type} - {source_path}"
            )
            processor_agent = Agent(
                name="ResourceProcessorAgent",
                instruction=PAPER_DOWNLOADER_PROMPT,
                server_names=["file-downloader"],
            )

            async with processor_agent:
                processor = await processor_agent.attach_llm(get_preferred_llm_class())
                processor_params = RequestParams(
                    maxTokens=4096,
                    temperature=0.2,
                    tool_filter={
                        "file-downloader": {"download_file_to", "move_file_to"}
                    },
                )

                # Provide context about what failed if available
                context = (
                    f"\nPrevious attempt result: {operation_result}"
                    if operation_result
                    else ""
                )
                message = f"""Download/move the file to paper directory: {paper_dir}
Source: {source_path}
Input Type: {input_type}
Paper ID: {next_id}
Target filename: {next_id}.md (after conversion){context}

Use the appropriate tool to complete this task."""

                result = await processor.generate_str(
                    message=message, request_params=processor_params
                )

        return result

    except (json.JSONDecodeError, KeyError, Exception) as e:
        logger.error(f"‚ùå Error processing resource: {e}")
        # Fallback - return paper directory for manual processing
        return json.dumps(
            {
                "status": "partial",
                "paper_id": next_id,
                "paper_dir": paper_dir,
                "message": f"Paper directory created at {paper_dir}, manual file placement may be needed",
            }
        )


async def run_code_analyzer(
    paper_dir: str, logger, use_segmentation: bool = True
) -> str:
    """
    Run the adaptive code analysis workflow with optimized file reading.

    This function minimizes LLM tool calls by:
    1. Reading paper file directly (deterministic, no LLM needed)
    2. Passing paper content directly to agents
    3. LLM only used for analysis and search decisions

    Orchestrates three specialized agents:
    - ConceptAnalysisAgent: Analyzes system architecture and conceptual framework
    - AlgorithmAnalysisAgent: Extracts algorithms, formulas, and technical details
    - CodePlannerAgent: Integrates outputs into a comprehensive implementation plan

    Args:
        paper_dir: Directory path containing the research paper and related resources
        logger: Logger instance for logging information
        use_segmentation: Whether to use document segmentation capabilities

    Returns:
        str: Comprehensive analysis result from the coordinated agents
    """
    print(
        f"üìä Code analysis mode: {'Segmented' if use_segmentation else 'Traditional'}"
    )
    print("   üîß Optimized workflow: Direct file reading, LLM only for analysis")

    # STEP 1: Read paper file directly - no LLM needed for deterministic file operations
    paper_content = None
    paper_file_path = None

    try:
        # Find .md file in paper directory - simple file system operation
        for filename in os.listdir(paper_dir):
            if filename.endswith(".md"):
                paper_file_path = os.path.join(paper_dir, filename)
                with open(paper_file_path, "r", encoding="utf-8") as f:
                    paper_content = f.read()
                logger.info(
                    f"üìÑ Paper file loaded: {paper_file_path} ({len(paper_content)} chars)"
                )
                break

        if not paper_content:
            logger.warning(
                f"‚ö†Ô∏è No .md file found in {paper_dir}, agents will search for it"
            )
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error reading paper file: {e}, agents will search for it")

    # STEP 2: Configure agents with minimal tool access
    search_server_names = get_search_server_names()
    agent_config = get_adaptive_agent_config(use_segmentation, search_server_names)
    prompts = get_adaptive_prompts(use_segmentation)

    if paper_content:
        # When paper content is already loaded, agents don't need search tools
        agent_config = {
            "concept_analysis": [],
            "algorithm_analysis": search_server_names,
            "code_planner": search_server_names,
        }
    else:
        agent_config = {
            "concept_analysis": ["filesystem"],
            "algorithm_analysis": search_server_names + ["filesystem"],
            "code_planner": search_server_names + ["filesystem"],
        }

    print(f"   Agent configurations: {agent_config}")

    concept_analysis_agent = Agent(
        name="ConceptAnalysisAgent",
        instruction=prompts["concept_analysis"],
        server_names=agent_config["concept_analysis"],
    )
    algorithm_analysis_agent = Agent(
        name="AlgorithmAnalysisAgent",
        instruction=prompts["algorithm_analysis"],
        server_names=agent_config["algorithm_analysis"],
    )
    code_planner_agent = Agent(
        name="CodePlannerAgent",
        instruction=prompts["code_planning"],
        server_names=agent_config["code_planner"],
    )

    code_aggregator_agent = ParallelLLM(
        fan_in_agent=code_planner_agent,
        fan_out_agents=[concept_analysis_agent, algorithm_analysis_agent],
        llm_factory=get_preferred_llm_class(),
    )

    base_max_tokens, _ = get_token_limits()

    # STEP 3: Configure parameters - minimal tool filter since paper content is provided
    if use_segmentation:
        max_tokens_limit = base_max_tokens
        temperature = 0.2
        max_iterations = 5
        print(
            f"üß† Using SEGMENTED mode: max_tokens={base_max_tokens} for complete YAML output"
        )

        # Segmentation mode: Only use segmentation tools if needed (paper content already provided)
        tool_filter = {
            "document-segmentation": {"read_document_segments", "get_document_overview"}
            if not paper_content
            else set(),  # Empty if paper already loaded
            # "brave" not in filter = all brave tools available for searching
        }
    else:
        max_tokens_limit = base_max_tokens
        temperature = 0.3
        max_iterations = 2
        print(
            f"üß† Using TRADITIONAL mode: max_tokens={base_max_tokens} for complete YAML output"
        )

        # Traditional mode: No filesystem tools needed (paper content already provided)
        if paper_content:
            tool_filter = {
                # Only brave search available - no filesystem tools needed
            }
        else:
            tool_filter = {
                "filesystem": {
                    "read_text_file",
                    "list_directory",
                }
            }

    enhanced_params = RequestParams(
        maxTokens=max_tokens_limit,
        temperature=temperature,
        max_iterations=max_iterations,
        tool_filter=tool_filter
        if tool_filter
        else None,  # None = all tools, empty dict = no filtering
    )

    # STEP 4: Construct message with paper content directly included
    if paper_content:
        # Paper content provided directly - LLM only needs to analyze, not read files
        message = f"""Analyze the research paper provided below. The paper file has been pre-loaded for you.

=== PAPER CONTENT START ===
{paper_content}
=== PAPER CONTENT END ===

Based on this paper, generate a comprehensive code reproduction plan that includes:

1. Complete system architecture and component breakdown
2. All algorithms, formulas, and implementation details
3. Detailed file structure and implementation roadmap

You may use web search (brave_web_search) if you need clarification on algorithms, methods, or concepts.

The goal is to create a reproduction plan detailed enough for independent implementation."""
    else:
        # Fallback: paper not found, agents will need to find it
        message = f"""Analyze the research paper in directory: {paper_dir}

Please locate and analyze the markdown (.md) file containing the research paper. Based on your analysis, generate a comprehensive code reproduction plan that includes:

1. Complete system architecture and component breakdown
2. All algorithms, formulas, and implementation details
3. Detailed file structure and implementation roadmap

The goal is to create a reproduction plan detailed enough for independent implementation."""

    max_retries = 3
    retry_count = 0

    while retry_count < max_retries:
        try:
            print(
                f"üöÄ Attempting code analysis (attempt {retry_count + 1}/{max_retries})"
            )
            result = await code_aggregator_agent.generate_str(
                message=message, request_params=enhanced_params
            )

            print(f"üîç Code analysis result:\n{result}")

            completeness_score = _assess_output_completeness(
                result
            )  # need to add file structure val
            print(f"üìä Output completeness score: {completeness_score:.2f}/1.0")

            if completeness_score >= 0.8:
                print(
                    f"‚úÖ Code analysis completed successfully (length: {len(result)} chars)"
                )
                return result
            else:
                print(
                    f"‚ö†Ô∏è Output appears truncated (score: {completeness_score:.2f}), retrying with enhanced parameters..."
                )
                new_max_tokens, new_temperature = _adjust_params_for_retry(
                    enhanced_params, retry_count
                )
                enhanced_params = RequestParams(
                    maxTokens=new_max_tokens,
                    temperature=new_temperature,
                    max_iterations=max_iterations,
                    tool_filter=tool_filter
                    if tool_filter
                    else None,  # None = all tools, empty dict = no filtering
                )
                retry_count += 1

        except Exception as e:
            print(f"‚ùå Error in code analysis attempt {retry_count + 1}: {e}")
            retry_count += 1
            if retry_count >= max_retries:
                raise

    print(f"‚ö†Ô∏è Returning potentially incomplete result after {max_retries} attempts")
    return result


async def github_repo_download(search_result: str, paper_dir: str, logger) -> str:
    """
    Download GitHub repositories based on search results.

    Args:
        search_result: Result from GitHub repository search
        paper_dir: Directory where the paper and its code will be stored
        logger: Logger instance for logging information

    Returns:
        str: Download result
    """
    github_download_agent = Agent(
        name="GithubDownloadAgent",
        instruction="Download github repo to the directory {paper_dir}/code_base".format(
            paper_dir=paper_dir
        ),
        server_names=["filesystem", "github-downloader"],
    )

    async with github_download_agent:
        print("GitHub downloader: Downloading repositories...")
        downloader = await github_download_agent.attach_llm(get_preferred_llm_class())

        # Set higher token output for GitHub download
        github_params = RequestParams(
            maxTokens=4096,  # ‰ΩøÁî® camelCase
            temperature=0.1,
        )

        return await downloader.generate_str(
            message=search_result, request_params=github_params
        )


async def paper_reference_analyzer(paper_dir: str, logger) -> str:
    """
    Run the paper reference analysis and GitHub repository workflow.

    Args:
        analysis_result: Result from the paper analyzer
        logger: Logger instance for logging information

    Returns:
        str: Reference analysis result
    """
    reference_analysis_agent = Agent(
        name="ReferenceAnalysisAgent",
        instruction=PAPER_REFERENCE_ANALYZER_PROMPT,
        server_names=["filesystem", "fetch"],
    )
    message = f"""Analyze the research paper in directory: {paper_dir}

Please locate and analyze the markdown (.md) file containing the research paper. **Focus specifically on the References/Bibliography section** to identify and analyze the 5 most relevant references that have GitHub repositories.

Goal: Find the most valuable GitHub repositories from the paper's reference list for code implementation reference."""

    async with reference_analysis_agent:
        print("Reference analyzer: Connected to server, analyzing references...")
        analyzer = await reference_analysis_agent.attach_llm(get_preferred_llm_class())

        # Filter tools to only essential ones for reference analysis
        reference_params = RequestParams(
            maxTokens=4096,
            temperature=0.2,
            tool_filter={
                "filesystem": {"read_text_file", "list_directory"},
                "fetch": {"fetch"},
            },
        )

        reference_result = await analyzer.generate_str(
            message=message, request_params=reference_params
        )
        return reference_result


async def _process_input_source(input_source: str, logger) -> str:
    """
    Process and validate input source (file path or URL).

    Args:
        input_source: Input source (file path or analysis result)
        logger: Logger instance

    Returns:
        str: Processed input source
    """
    if input_source.startswith("file://"):
        file_path = input_source[7:]
        if os.name == "nt" and file_path.startswith("/"):
            file_path = file_path.lstrip("/")
        return file_path
    return input_source


async def orchestrate_research_analysis_agent(
    input_source: str, logger, progress_callback: Optional[Callable] = None
) -> Tuple[str, str]:
    """
    Orchestrate intelligent research analysis and resource processing automation.

    This agent coordinates multiple AI components to analyze research content
    and process associated resources with automated workflow management.

    Args:
        input_source: Research input source for analysis
        logger: Logger instance for process tracking
        progress_callback: Progress callback function for workflow monitoring

    Returns:
        tuple: (analysis_result, resource_processing_result)
    """
    # Step 1: Research Analysis
    if progress_callback:
        progress_callback(
            10, "üìä Analyzing research content and extracting key information..."
        )
    analysis_result = await run_research_analyzer(input_source, logger)

    # Add brief pause for system stability
    await asyncio.sleep(5)

    # Step 2: Download Processing
    if progress_callback:
        progress_callback(
            25, "üì• Processing downloads and preparing document structure..."
        )
    download_result = await run_resource_processor(analysis_result, logger)
    print("download result:", download_result)

    return analysis_result, download_result


async def synthesize_workspace_infrastructure_agent(
    download_result: str, logger, workspace_dir: Optional[str] = None
) -> Dict[str, str]:
    """
    Synthesize intelligent research workspace infrastructure with automated structure generation.

    This agent autonomously creates and configures the optimal workspace architecture
    for research project implementation with AI-driven path optimization.

    Args:
        download_result: Resource processing result from analysis agent
        logger: Logger instance for infrastructure tracking
        workspace_dir: Optional workspace directory path for environment customization

    Returns:
        dict: Comprehensive workspace infrastructure metadata
    """
    # Parse download result to get file information
    result = await FileProcessor.process_file_input(
        download_result, base_dir=workspace_dir
    )
    paper_dir = result["paper_dir"]

    # Log workspace infrastructure synthesis
    print("üèóÔ∏è Intelligent workspace infrastructure synthesized:")
    print(f"   Base workspace environment: {workspace_dir or 'auto-detected'}")
    print(f"   Research workspace: {paper_dir}")
    print("   AI-driven path optimization: active")

    return {
        "paper_dir": paper_dir,
        "standardized_text": result["standardized_text"],
        "reference_path": os.path.join(paper_dir, "reference.txt"),
        "initial_plan_path": os.path.join(paper_dir, "initial_plan.txt"),
        "download_path": os.path.join(paper_dir, "github_download.txt"),
        "index_report_path": os.path.join(paper_dir, "codebase_index_report.txt"),
        "implementation_report_path": os.path.join(
            paper_dir, "code_implementation_report.txt"
        ),
        "workspace_dir": workspace_dir,
    }


async def orchestrate_reference_intelligence_agent(
    dir_info: Dict[str, str], logger, progress_callback: Optional[Callable] = None
) -> str:
    """
    Orchestrate intelligent reference analysis with automated research discovery.

    This agent autonomously processes research references and discovers
    related work using advanced AI-powered analysis algorithms.

    Args:
        dir_info: Workspace infrastructure metadata
        logger: Logger instance for intelligence tracking
        progress_callback: Progress callback function for monitoring

    Returns:
        str: Comprehensive reference intelligence analysis result
    """
    if progress_callback:
        progress_callback(50, "üß† Orchestrating reference intelligence discovery...")

    reference_path = dir_info["reference_path"]

    # Check if reference analysis already exists
    if os.path.exists(reference_path):
        print(f"Found existing reference analysis at {reference_path}")
        with open(reference_path, "r", encoding="utf-8") as f:
            return f.read()

    # Execute reference analysis
    reference_result = await paper_reference_analyzer(dir_info["paper_dir"], logger)

    # Save reference analysis result
    with open(reference_path, "w", encoding="utf-8") as f:
        f.write(reference_result)
    print(f"Reference analysis saved to {reference_path}")

    return reference_result


async def orchestrate_document_preprocessing_agent(
    dir_info: Dict[str, str], logger
) -> Dict[str, Any]:
    """
    Orchestrate adaptive document preprocessing with intelligent segmentation control.

    This agent autonomously determines whether to use document segmentation based on
    configuration settings and document size, then applies the appropriate processing strategy.

    Args:
        dir_info: Workspace infrastructure metadata
        logger: Logger instance for preprocessing tracking

    Returns:
        dict: Document preprocessing result with segmentation metadata
    """

    try:
        print("üîç Starting adaptive document preprocessing...")
        print(f"   Paper directory: {dir_info['paper_dir']}")

        # Step 1: Check if any markdown files exist
        md_files = []
        try:
            md_files = [
                f for f in os.listdir(dir_info["paper_dir"]) if f.endswith(".md")
            ]
        except Exception as e:
            print(f"‚ö†Ô∏è Error reading paper directory: {e}")

        if not md_files:
            print("‚ÑπÔ∏è No markdown files found - skipping document preprocessing")
            dir_info["segments_ready"] = False
            dir_info["use_segmentation"] = False
            return {
                "status": "skipped",
                "reason": "no_markdown_files",
                "paper_dir": dir_info["paper_dir"],
                "segments_ready": False,
                "use_segmentation": False,
            }

        # Step 2: Read document content to determine size
        md_path = os.path.join(dir_info["paper_dir"], md_files[0])
        try:
            # Check if file is actually a PDF by reading the first few bytes
            with open(md_path, "rb") as f:
                header = f.read(8)
                if header.startswith(b"%PDF"):
                    raise IOError(
                        f"File {md_path} is a PDF file, not a text file. Please convert it to markdown format or use PDF processing tools."
                    )

            with open(md_path, "r", encoding="utf-8") as f:
                document_content = f.read()
        except Exception as e:
            print(f"‚ö†Ô∏è Error reading document content: {e}")
            dir_info["segments_ready"] = False
            dir_info["use_segmentation"] = False
            return {
                "status": "error",
                "error_message": f"Failed to read document: {str(e)}",
                "paper_dir": dir_info["paper_dir"],
                "segments_ready": False,
                "use_segmentation": False,
            }

        # Step 3: Determine if segmentation should be used
        should_segment, reason = should_use_document_segmentation(document_content)
        print(f"üìä Segmentation decision: {should_segment}")
        print(f"   Reason: {reason}")

        # Store decision in dir_info for downstream agents
        dir_info["use_segmentation"] = should_segment

        if should_segment:
            print("üîß Using intelligent document segmentation workflow...")

            # Prepare document segments using the segmentation agent
            segmentation_result = await prepare_document_segments(
                paper_dir=dir_info["paper_dir"], logger=logger
            )

            if segmentation_result["status"] == "success":
                print("‚úÖ Document segmentation completed successfully!")
                print(f"   Segments directory: {segmentation_result['segments_dir']}")
                print("   üß† Intelligent segments ready for planning agents")

                # Add segment information to dir_info for downstream agents
                dir_info["segments_dir"] = segmentation_result["segments_dir"]
                dir_info["segments_ready"] = True

                return segmentation_result

            else:
                print(
                    f"‚ö†Ô∏è Document segmentation failed: {segmentation_result.get('error_message', 'Unknown error')}"
                )
                print("   Falling back to traditional full-document processing...")
                dir_info["segments_ready"] = False
                dir_info["use_segmentation"] = False

                return {
                    "status": "fallback_to_traditional",
                    "original_error": segmentation_result.get(
                        "error_message", "Unknown error"
                    ),
                    "paper_dir": dir_info["paper_dir"],
                    "segments_ready": False,
                    "use_segmentation": False,
                    "fallback_reason": "segmentation_failed",
                }
        else:
            print("üìñ Using traditional full-document reading workflow...")
            dir_info["segments_ready"] = False

            return {
                "status": "traditional",
                "reason": reason,
                "paper_dir": dir_info["paper_dir"],
                "segments_ready": False,
                "use_segmentation": False,
                "document_size": len(document_content),
            }

    except Exception as e:
        print(f"‚ùå Error during document preprocessing: {e}")
        print("   Continuing with traditional full-document processing...")

        # Ensure fallback settings
        dir_info["segments_ready"] = False
        dir_info["use_segmentation"] = False

        return {
            "status": "error",
            "paper_dir": dir_info["paper_dir"],
            "segments_ready": False,
            "use_segmentation": False,
            "error_message": str(e),
        }


async def orchestrate_code_planning_agent(
    dir_info: Dict[str, str], logger, progress_callback: Optional[Callable] = None
):
    """
    Orchestrate intelligent code planning with automated design analysis.

    This agent autonomously generates optimal code reproduction plans and implementation
    strategies using AI-driven code analysis and planning principles.

    Args:
        dir_info: Workspace infrastructure metadata
        logger: Logger instance for planning tracking
        progress_callback: Progress callback function for monitoring
    """
    if progress_callback:
        progress_callback(40, "üèóÔ∏è Synthesizing intelligent code architecture...")

    initial_plan_path = dir_info["initial_plan_path"]

    # Check if initial plan already exists
    if not os.path.exists(initial_plan_path):
        # Use segmentation setting from preprocessing phase
        use_segmentation = dir_info.get("use_segmentation", True)
        print(f"üìä Planning mode: {'Segmented' if use_segmentation else 'Traditional'}")

        initial_plan_result = await run_code_analyzer(
            dir_info["paper_dir"], logger, use_segmentation=use_segmentation
        )
        with open(initial_plan_path, "w", encoding="utf-8") as f:
            f.write(initial_plan_result)
        print(f"Initial plan saved to {initial_plan_path}")


async def automate_repository_acquisition_agent(
    reference_result: str,
    dir_info: Dict[str, str],
    logger,
    progress_callback: Optional[Callable] = None,
):
    """
    Automate intelligent repository acquisition with AI-guided selection.

    This agent autonomously identifies, evaluates, and acquires relevant
    repositories using intelligent filtering and automated download protocols.

    Args:
        reference_result: Reference intelligence analysis result
        dir_info: Workspace infrastructure metadata
        logger: Logger instance for acquisition tracking
        progress_callback: Progress callback function for monitoring
    """
    if progress_callback:
        progress_callback(60, "ü§ñ Automating intelligent repository acquisition...")

    await asyncio.sleep(5)  # Brief pause for stability

    try:
        download_result = await github_repo_download(
            reference_result, dir_info["paper_dir"], logger
        )

        # Save download results
        with open(dir_info["download_path"], "w", encoding="utf-8") as f:
            f.write(download_result)
        print(f"GitHub download results saved to {dir_info['download_path']}")

        # Verify if any repositories were actually downloaded
        code_base_path = os.path.join(dir_info["paper_dir"], "code_base")
        if os.path.exists(code_base_path):
            downloaded_repos = [
                d
                for d in os.listdir(code_base_path)
                if os.path.isdir(os.path.join(code_base_path, d))
                and not d.startswith(".")
            ]

            if downloaded_repos:
                print(
                    f"Successfully downloaded {len(downloaded_repos)} repositories: {downloaded_repos}"
                )
            else:
                print(
                    "GitHub download phase completed, but no repositories were found in the code_base directory"
                )
                print("This might indicate:")
                print(
                    "1. No relevant repositories were identified in the reference analysis"
                )
                print(
                    "2. Repository downloads failed due to access permissions or network issues"
                )
                print(
                    "3. The download agent encountered errors during the download process"
                )
        else:
            print(f"Code base directory was not created: {code_base_path}")

    except Exception as e:
        print(f"Error during GitHub repository download: {e}")
        # Still save the error information
        error_message = f"GitHub download failed: {str(e)}"
        with open(dir_info["download_path"], "w", encoding="utf-8") as f:
            f.write(error_message)
        print(f"GitHub download error saved to {dir_info['download_path']}")
        raise e  # Re-raise to be handled by the main pipeline


async def orchestrate_codebase_intelligence_agent(
    dir_info: Dict[str, str], logger, progress_callback: Optional[Callable] = None
) -> Dict:
    """
    Orchestrate intelligent codebase analysis with automated knowledge extraction.

    This agent autonomously processes and indexes codebases using advanced
    AI algorithms for intelligent relationship mapping and knowledge synthesis.

    Args:
        dir_info: Workspace infrastructure metadata
        logger: Logger instance for intelligence tracking
        progress_callback: Progress callback function for monitoring

    Returns:
        dict: Comprehensive codebase intelligence analysis result
    """
    if progress_callback:
        progress_callback(70, "üßÆ Orchestrating codebase intelligence analysis...")

    print(
        "Initiating intelligent codebase analysis with AI-powered relationship mapping..."
    )
    await asyncio.sleep(2)  # Brief pause before starting indexing

    # Check if code_base directory exists and has content
    code_base_path = os.path.join(dir_info["paper_dir"], "code_base")
    if not os.path.exists(code_base_path):
        print(f"Code base directory not found: {code_base_path}")
        return {
            "status": "skipped",
            "message": "No code base directory found - skipping indexing",
        }

    # Check if there are any repositories in the code_base directory
    try:
        repo_dirs = [
            d
            for d in os.listdir(code_base_path)
            if os.path.isdir(os.path.join(code_base_path, d)) and not d.startswith(".")
        ]

        if not repo_dirs:
            print(f"No repositories found in {code_base_path}")
            print("This might be because:")
            print("1. GitHub download phase didn't complete successfully")
            print("2. No relevant repositories were identified for download")
            print("3. Repository download failed due to access issues")
            print("Continuing with code implementation without codebase indexing...")

            # Save a report about the skipped indexing
            skip_report = {
                "status": "skipped",
                "reason": "no_repositories_found",
                "message": f"No repositories found in {code_base_path}",
                "suggestions": [
                    "Check if GitHub download phase completed successfully",
                    "Verify if relevant repositories were identified in reference analysis",
                    "Check network connectivity and GitHub access permissions",
                ],
            }

            with open(dir_info["index_report_path"], "w", encoding="utf-8") as f:
                f.write(str(skip_report))
            print(f"Indexing skip report saved to {dir_info['index_report_path']}")

            return skip_report

    except Exception as e:
        print(f"Error checking code base directory: {e}")
        return {
            "status": "error",
            "message": f"Error checking code base directory: {str(e)}",
        }

    try:
        from workflows.codebase_index_workflow import run_codebase_indexing

        print(f"Found {len(repo_dirs)} repositories to index: {repo_dirs}")

        # Run codebase index workflow
        index_result = await run_codebase_indexing(
            paper_dir=dir_info["paper_dir"],
            initial_plan_path=dir_info["initial_plan_path"],
            config_path="mcp_agent.secrets.yaml",
            logger=logger,
        )

        # Log indexing results
        if index_result["status"] == "success":
            print("Code indexing completed successfully!")
            print(
                f"Indexed {index_result['statistics']['total_repositories'] if index_result.get('statistics') else len(index_result['output_files'])} repositories"
            )
            print(f"Generated {len(index_result['output_files'])} index files")

            # Save indexing results to file
            with open(dir_info["index_report_path"], "w", encoding="utf-8") as f:
                f.write(str(index_result))
            print(f"Indexing report saved to {dir_info['index_report_path']}")

        elif index_result["status"] == "warning":
            print(f"Code indexing completed with warnings: {index_result['message']}")
        else:
            print(f"Code indexing failed: {index_result['message']}")

        return index_result

    except Exception as e:
        print(f"Error during codebase indexing workflow: {e}")
        print("Continuing with code implementation despite indexing failure...")

        # Save error report
        error_report = {
            "status": "error",
            "message": str(e),
            "phase": "codebase_indexing",
            "recovery_action": "continuing_with_code_implementation",
        }

        with open(dir_info["index_report_path"], "w", encoding="utf-8") as f:
            f.write(str(error_report))
        print(f"Indexing error report saved to {dir_info['index_report_path']}")

        return error_report


async def synthesize_code_implementation_agent(
    dir_info: Dict[str, str],
    logger,
    progress_callback: Optional[Callable] = None,
    enable_indexing: bool = True,
) -> Dict:
    """
    Synthesize intelligent code implementation with automated development.

    This agent autonomously generates high-quality code implementations using
    AI-powered development strategies and intelligent code synthesis algorithms.

    Args:
        dir_info: Workspace infrastructure metadata
        logger: Logger instance for implementation tracking
        progress_callback: Progress callback function for monitoring
        enable_indexing: Whether to enable code reference indexing for enhanced implementation

    Returns:
        dict: Comprehensive code implementation synthesis result
    """
    if progress_callback:
        progress_callback(85, "üî¨ Synthesizing intelligent code implementation...")

    print(
        "Launching intelligent code synthesis with AI-driven implementation strategies..."
    )
    await asyncio.sleep(3)  # Brief pause before starting implementation

    try:
        # Create code implementation workflow instance based on indexing preference
        if enable_indexing:
            print(
                "üîç Using enhanced code implementation workflow with reference indexing..."
            )
            code_workflow = CodeImplementationWorkflowWithIndex()
        else:
            print("‚ö° Using standard code implementation workflow (fast mode)...")
            code_workflow = CodeImplementationWorkflow()

        # Check if initial plan file exists
        if os.path.exists(dir_info["initial_plan_path"]):
            print(f"Using initial plan from {dir_info['initial_plan_path']}")

            # Run code implementation workflow with pure code mode
            implementation_result = await code_workflow.run_workflow(
                plan_file_path=dir_info["initial_plan_path"],
                target_directory=dir_info["paper_dir"],
                pure_code_mode=True,  # Focus on code implementation, skip testing
            )

            # Log implementation results
            if implementation_result["status"] == "success":
                print("Code implementation completed successfully!")
                print(f"Code directory: {implementation_result['code_directory']}")

                # Save implementation results to file
                with open(
                    dir_info["implementation_report_path"], "w", encoding="utf-8"
                ) as f:
                    f.write(str(implementation_result))
                print(
                    f"Implementation report saved to {dir_info['implementation_report_path']}"
                )

            else:
                print(
                    f"Code implementation failed: {implementation_result.get('message', 'Unknown error')}"
                )

            return implementation_result
        else:
            print(
                f"Initial plan file not found at {dir_info['initial_plan_path']}, skipping code implementation"
            )
            return {
                "status": "warning",
                "message": "Initial plan not found - code implementation skipped",
            }

    except Exception as e:
        print(f"Error during code implementation workflow: {e}")
        return {"status": "error", "message": str(e)}


async def run_chat_planning_agent(user_input: str, logger) -> str:
    """
    Run the chat-based planning agent for user-provided coding requirements.

    This agent transforms user's coding description into a comprehensive implementation plan
    that can be directly used for code generation. It handles both academic and engineering
    requirements with intelligent context adaptation.

    Args:
        user_input: User's coding requirements and description
        logger: Logger instance for logging information

    Returns:
        str: Comprehensive implementation plan in YAML format
    """
    try:
        print("üí¨ Starting chat-based planning agent...")
        print(f"Input length: {len(user_input) if user_input else 0}")
        print(f"Input preview: {user_input[:200] if user_input else 'None'}...")

        if not user_input or user_input.strip() == "":
            raise ValueError(
                "Empty or None user_input provided to run_chat_planning_agent"
            )

        # Create the chat planning agent
        chat_planning_agent = Agent(
            name="ChatPlanningAgent",
            instruction=CHAT_AGENT_PLANNING_PROMPT,
            server_names=get_search_server_names(),  # Dynamic search server configuration
        )

        async with chat_planning_agent:
            print("chat_planning: Connected to server, calling list_tools...")
            try:
                tools = await chat_planning_agent.list_tools()
                print(
                    "Tools available:",
                    tools.model_dump() if hasattr(tools, "model_dump") else str(tools),
                )
            except Exception as e:
                print(f"Failed to list tools: {e}")

            try:
                planner = await chat_planning_agent.attach_llm(
                    get_preferred_llm_class()
                )
                print("‚úÖ LLM attached successfully")
            except Exception as e:
                print(f"‚ùå Failed to attach LLM: {e}")
                raise

            # Set higher token output for comprehensive planning
            planning_params = RequestParams(
                maxTokens=8192,  # ‰ΩøÁî® camelCase - Higher token limit for detailed plans
                temperature=0.2,  # Lower temperature for more structured output
            )

            print(
                f"üîÑ Making LLM request with params: maxTokens={planning_params.maxTokens}, temperature={planning_params.temperature}"
            )

            # Format the input message for the agent
            formatted_message = f"""Please analyze the following coding requirements and generate a comprehensive implementation plan:

User Requirements:
{user_input}

Please provide a detailed implementation plan that covers all aspects needed for successful development."""

            try:
                raw_result = await planner.generate_str(
                    message=formatted_message, request_params=planning_params
                )

                print("‚úÖ Planning request completed")
                print(f"Raw result type: {type(raw_result)}")
                print(f"Raw result length: {len(raw_result) if raw_result else 0}")

                if not raw_result:
                    print("‚ùå CRITICAL: raw_result is empty or None!")
                    raise ValueError("Chat planning agent returned empty result")

            except Exception as e:
                print(f"‚ùå Planning generation failed: {e}")
                print(f"Exception type: {type(e)}")
                raise

            # Log to SimpleLLMLogger
            if hasattr(logger, "log_response"):
                logger.log_response(
                    raw_result, model="ChatPlanningAgent", agent="ChatPlanningAgent"
                )

            if not raw_result or raw_result.strip() == "":
                print("‚ùå CRITICAL: Planning result is empty!")
                raise ValueError("Chat planning agent produced empty output")

            print("üéØ Chat planning completed successfully")
            print(f"Planning result preview: {raw_result[:500]}...")

            return raw_result

    except Exception as e:
        print(f"‚ùå run_chat_planning_agent failed: {e}")
        print(f"Exception details: {type(e).__name__}: {str(e)}")
        raise


async def execute_multi_agent_research_pipeline(
    input_source: str,
    logger,
    progress_callback: Optional[Callable] = None,
    enable_indexing: bool = True,
) -> str:
    """
    Execute the complete intelligent multi-agent research orchestration pipeline.

    This is the main AI orchestration engine that coordinates autonomous research workflow agents:
    - Local workspace automation for seamless environment management
    - Intelligent research analysis with automated content processing
    - AI-driven code architecture synthesis and design automation
    - Reference intelligence discovery with automated knowledge extraction (optional)
    - Codebase intelligence orchestration with automated relationship analysis (optional)
    - Intelligent code implementation synthesis with AI-powered development

    Args:
        input_source: Research input source (file path, URL, or preprocessed analysis)
        logger: Logger instance for comprehensive workflow intelligence tracking
        progress_callback: Progress callback function for real-time monitoring
        enable_indexing: Whether to enable advanced intelligence analysis (default: True)

    Returns:
        str: The comprehensive pipeline execution result with status and outcomes
    """
    try:
        # Phase 0: Workspace Setup
        if progress_callback:
            progress_callback(5, "üîÑ Setting up workspace for file processing...")

        print("üöÄ Initializing intelligent multi-agent research orchestration system")

        # Setup local workspace directory
        workspace_dir = os.path.join(os.getcwd(), "deepcode_lab")
        os.makedirs(workspace_dir, exist_ok=True)

        print("üìÅ Working environment: local")
        print(f"üìÇ Workspace directory: {workspace_dir}")
        print("‚úÖ Workspace status: ready")

        # Log intelligence functionality status
        if enable_indexing:
            print("üß† Advanced intelligence analysis enabled - comprehensive workflow")
        else:
            print("‚ö° Optimized mode - advanced intelligence analysis disabled")

        # Phase 1: Input Processing and Validation
        input_source = await _process_input_source(input_source, logger)

        # Phase 2: Research Analysis and Resource Processing (if needed)
        if isinstance(input_source, str) and (
            input_source.endswith((".pdf", ".docx", ".txt", ".html", ".md"))
            or input_source.startswith(("http", "file://"))
        ):
            (
                analysis_result,
                download_result,
            ) = await orchestrate_research_analysis_agent(
                input_source, logger, progress_callback
            )
        else:
            download_result = input_source  # Use input directly if already processed

        # Phase 3: Workspace Infrastructure Synthesis
        if progress_callback:
            progress_callback(
                40, "üèóÔ∏è Synthesizing intelligent workspace infrastructure..."
            )

        dir_info = await synthesize_workspace_infrastructure_agent(
            download_result, logger, workspace_dir
        )
        await asyncio.sleep(5)

        # Phase 3.5: Document Segmentation and Preprocessing

        segmentation_result = await orchestrate_document_preprocessing_agent(
            dir_info, logger
        )

        # Handle segmentation result
        if segmentation_result["status"] == "success":
            print("‚úÖ Document preprocessing completed successfully!")
            print(
                f"   üìä Using segmentation: {dir_info.get('use_segmentation', False)}"
            )
            if dir_info.get("segments_ready", False):
                print(
                    f"   üìÅ Segments directory: {segmentation_result.get('segments_dir', 'N/A')}"
                )
        elif segmentation_result["status"] == "fallback_to_traditional":
            print("‚ö†Ô∏è Document segmentation failed, using traditional processing")
            print(
                f"   Original error: {segmentation_result.get('original_error', 'Unknown')}"
            )
        else:
            print(
                f"‚ö†Ô∏è Document preprocessing encountered issues: {segmentation_result.get('error_message', 'Unknown')}"
            )

        # Phase 4: Code Planning Orchestration
        await orchestrate_code_planning_agent(dir_info, logger, progress_callback)

        # Phase 5: Reference Intelligence (only when indexing is enabled)
        if enable_indexing:
            reference_result = await orchestrate_reference_intelligence_agent(
                dir_info, logger, progress_callback
            )
        else:
            print("üî∂ Skipping reference intelligence analysis (fast mode enabled)")
            # Create empty reference analysis result to maintain file structure consistency
            reference_result = "Reference intelligence analysis skipped - fast mode enabled for optimized processing"
            with open(dir_info["reference_path"], "w", encoding="utf-8") as f:
                f.write(reference_result)

        # Phase 6: Repository Acquisition Automation (optional)
        if enable_indexing:
            await automate_repository_acquisition_agent(
                reference_result, dir_info, logger, progress_callback
            )
        else:
            print("üî∂ Skipping automated repository acquisition (fast mode enabled)")
            # Create empty download result file to maintain file structure consistency
            with open(dir_info["download_path"], "w", encoding="utf-8") as f:
                f.write(
                    "Automated repository acquisition skipped - fast mode enabled for optimized processing"
                )

        # Phase 7: Codebase Intelligence Orchestration (optional)
        if enable_indexing:
            index_result = await orchestrate_codebase_intelligence_agent(
                dir_info, logger, progress_callback
            )
        else:
            print("üî∂ Skipping codebase intelligence orchestration (fast mode enabled)")
            # Create a skipped indexing result
            index_result = {
                "status": "skipped",
                "reason": "fast_mode_enabled",
                "message": "Codebase intelligence orchestration skipped for optimized processing",
            }
            with open(dir_info["index_report_path"], "w", encoding="utf-8") as f:
                f.write(str(index_result))

        # Phase 8: Code Implementation Synthesis
        implementation_result = await synthesize_code_implementation_agent(
            dir_info, logger, progress_callback, enable_indexing
        )

        # Final Status Report
        if enable_indexing:
            pipeline_summary = (
                f"Multi-agent research pipeline completed for {dir_info['paper_dir']}"
            )
        else:
            pipeline_summary = f"Multi-agent research pipeline completed (fast mode) for {dir_info['paper_dir']}"

        # Add indexing status to summary
        if not enable_indexing:
            pipeline_summary += (
                "\n‚ö° Fast mode: GitHub download and codebase indexing skipped"
            )
        elif index_result["status"] == "skipped":
            pipeline_summary += f"\nüî∂ Codebase indexing: {index_result['message']}"
        elif index_result["status"] == "error":
            pipeline_summary += (
                f"\n‚ùå Codebase indexing failed: {index_result['message']}"
            )
        elif index_result["status"] == "success":
            pipeline_summary += "\n‚úÖ Codebase indexing completed successfully"

        # Add implementation status to summary
        if implementation_result["status"] == "success":
            pipeline_summary += "\nüéâ Code implementation completed successfully!"
            pipeline_summary += (
                f"\nüìÅ Code generated in: {implementation_result['code_directory']}"
            )
            return pipeline_summary
        elif implementation_result["status"] == "warning":
            pipeline_summary += (
                f"\n‚ö†Ô∏è Code implementation: {implementation_result['message']}"
            )
            return pipeline_summary
        else:
            pipeline_summary += (
                f"\n‚ùå Code implementation failed: {implementation_result['message']}"
            )
            return pipeline_summary

    except Exception as e:
        print(f"Error in execute_multi_agent_research_pipeline: {e}")
        raise e


# Backward compatibility alias (deprecated)
async def paper_code_preparation(
    input_source: str, logger, progress_callback: Optional[Callable] = None
) -> str:
    """
    Deprecated: Use execute_multi_agent_research_pipeline instead.

    Args:
        input_source: Input source
        logger: Logger instance
        progress_callback: Progress callback function

    Returns:
        str: Pipeline result
    """
    print(
        "paper_code_preparation is deprecated. Use execute_multi_agent_research_pipeline instead."
    )
    return await execute_multi_agent_research_pipeline(
        input_source, logger, progress_callback
    )


async def execute_chat_based_planning_pipeline(
    user_input: str,
    logger,
    progress_callback: Optional[Callable] = None,
    enable_indexing: bool = True,
) -> str:
    """
    Execute the chat-based planning and implementation pipeline.

    This pipeline is designed for users who provide coding requirements directly through chat,
    bypassing the traditional paper analysis phases (Phase 0-7) and jumping directly to
    planning and code implementation.

    Pipeline Flow:
    - Chat Planning: Transform user input into implementation plan
    - Workspace Setup: Create necessary directory structure
    - Code Implementation: Generate code based on the plan

    Args:
        user_input: User's coding requirements and description
        logger: Logger instance for comprehensive workflow tracking
        progress_callback: Progress callback function for real-time monitoring
        enable_indexing: Whether to enable code reference indexing for enhanced implementation

    Returns:
        str: The pipeline execution result with status and outcomes
    """
    try:
        print("üöÄ Initializing chat-based planning and implementation pipeline")
        print("üí¨ Chat mode: Direct user requirements to code implementation")

        # Phase 0: Workspace Setup
        if progress_callback:
            progress_callback(5, "üîÑ Setting up workspace for file processing...")

        # Setup local workspace directory
        workspace_dir = os.path.join(os.getcwd(), "deepcode_lab")
        os.makedirs(workspace_dir, exist_ok=True)

        print("üìÅ Working environment: local")
        print(f"üìÇ Workspace directory: {workspace_dir}")
        print("‚úÖ Workspace status: ready")

        # Phase 1: Chat-Based Planning
        if progress_callback:
            progress_callback(
                30,
                "üí¨ Generating comprehensive implementation plan from user requirements...",
            )

        print("üß† Running chat-based planning agent...")
        planning_result = await run_chat_planning_agent(user_input, logger)

        # Phase 2: Workspace Infrastructure Synthesis
        if progress_callback:
            progress_callback(
                50, "üèóÔ∏è Synthesizing intelligent workspace infrastructure..."
            )

        # Create workspace directory structure for chat mode
        # First, let's create a temporary directory structure that mimics a paper workspace
        import time

        # Generate a unique paper directory name
        timestamp = str(int(time.time()))
        paper_name = f"chat_project_{timestamp}"

        # Use workspace directory
        chat_paper_dir = os.path.join(workspace_dir, "papers", paper_name)

        os.makedirs(chat_paper_dir, exist_ok=True)

        # Create a synthetic markdown file with user requirements
        markdown_content = f"""# User Coding Requirements

## Project Description
This is a coding project generated from user requirements via chat interface.

## User Requirements
{user_input}

## Generated Implementation Plan
The following implementation plan was generated by the AI chat planning agent:

```yaml
{planning_result}
```

## Project Metadata
- **Input Type**: Chat Input
- **Generation Method**: AI Chat Planning Agent
- **Timestamp**: {timestamp}
"""

        # Save the markdown file
        markdown_file_path = os.path.join(chat_paper_dir, f"{paper_name}.md")
        with open(markdown_file_path, "w", encoding="utf-8") as f:
            f.write(markdown_content)

        print(f"üíæ Created chat project workspace: {chat_paper_dir}")
        print(f"üìÑ Saved requirements to: {markdown_file_path}")

        # Create a download result that matches FileProcessor expectations
        synthetic_download_result = json.dumps(
            {
                "status": "success",
                "paper_path": markdown_file_path,
                "input_type": "chat_input",
                "paper_info": {
                    "title": "User-Provided Coding Requirements",
                    "source": "chat_input",
                    "description": "Implementation plan generated from user requirements",
                },
            }
        )

        dir_info = await synthesize_workspace_infrastructure_agent(
            synthetic_download_result, logger, workspace_dir
        )
        await asyncio.sleep(10)  # Brief pause for file system operations

        # Phase 3: Save Planning Result
        if progress_callback:
            progress_callback(70, "üìù Saving implementation plan...")

        # Save the planning result to the initial_plan.txt file (same location as Phase 4 in original pipeline)
        initial_plan_path = dir_info["initial_plan_path"]
        with open(initial_plan_path, "w", encoding="utf-8") as f:
            f.write(planning_result)
        print(f"üíæ Implementation plan saved to {initial_plan_path}")

        # Phase 4: Code Implementation Synthesis (same as Phase 8 in original pipeline)
        if progress_callback:
            progress_callback(85, "üî¨ Synthesizing intelligent code implementation...")

        implementation_result = await synthesize_code_implementation_agent(
            dir_info, logger, progress_callback, enable_indexing
        )

        # Final Status Report
        pipeline_summary = f"Chat-based planning and implementation pipeline completed for {dir_info['paper_dir']}"

        # Add implementation status to summary
        if implementation_result["status"] == "success":
            pipeline_summary += "\nüéâ Code implementation completed successfully!"
            pipeline_summary += (
                f"\nüìÅ Code generated in: {implementation_result['code_directory']}"
            )
            pipeline_summary += (
                "\nüí¨ Generated from user requirements via chat interface"
            )
            return pipeline_summary
        elif implementation_result["status"] == "warning":
            pipeline_summary += (
                f"\n‚ö†Ô∏è Code implementation: {implementation_result['message']}"
            )
            return pipeline_summary
        else:
            pipeline_summary += (
                f"\n‚ùå Code implementation failed: {implementation_result['message']}"
            )
            return pipeline_summary

    except Exception as e:
        print(f"Error in execute_chat_based_planning_pipeline: {e}")
        raise e


--- workflows/code_implementation_workflow.py ---
"""
Paper Code Implementation Workflow - MCP-compliant Iterative Development

Features:
1. File Tree Creation
2. Code Implementation - Based on aisi-basic-agent iterative development

MCP Architecture:
- MCP Server: tools/code_implementation_server.py
- MCP Client: Called through mcp_agent framework
- Configuration: mcp_agent.config.yaml
"""

import asyncio
import json
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, Any, Optional, List

# MCP Agent imports
from mcp_agent.agents.agent import Agent

# Local imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from prompts.code_prompts import STRUCTURE_GENERATOR_PROMPT
from prompts.code_prompts import (
    GENERAL_CODE_IMPLEMENTATION_SYSTEM_PROMPT,
)
from workflows.agents import CodeImplementationAgent
from workflows.agents.memory_agent_concise import ConciseMemoryAgent
from config.mcp_tool_definitions import get_mcp_tools
from utils.llm_utils import get_preferred_llm_class, get_default_models, load_api_config
# DialogueLogger removed - no longer needed


class CodeImplementationWorkflow:
    """
    Paper Code Implementation Workflow Manager

    Uses standard MCP architecture:
    1. Connect to code-implementation server via MCP client
    2. Use MCP protocol for tool calls
    3. Support workspace management and operation history tracking
    """

    # ==================== 1. Class Initialization and Configuration (Infrastructure Layer) ====================

    def __init__(self, config_path: str = "mcp_agent.secrets.yaml"):
        """Initialize workflow with configuration"""
        self.config_path = config_path
        # Derive main config path from secrets path (same directory)
        secrets_dir = os.path.dirname(os.path.abspath(config_path))
        self.main_config_path = os.path.join(secrets_dir, "mcp_agent.config.yaml")
        self.api_config = self._load_api_config()
        self.default_models = get_default_models(self.main_config_path)
        self.logger = self._create_logger()
        self.mcp_agent = None
        self.enable_read_tools = (
            True  # Default value, will be overridden by run_workflow parameter
        )

    def _load_api_config(self) -> Dict[str, Any]:
        """Load API configuration with environment variable override."""
        try:
            return load_api_config(self.config_path)
        except Exception as e:
            raise Exception(f"Failed to load API config: {e}")

    def _create_logger(self) -> logging.Logger:
        """Create and configure logger"""
        logger = logging.getLogger(__name__)
        # Don't add handlers to child loggers - let them propagate to root
        logger.setLevel(logging.INFO)
        return logger

    def _read_plan_file(self, plan_file_path: str) -> str:
        """Read implementation plan file"""
        plan_path = Path(plan_file_path)
        if not plan_path.exists():
            raise FileNotFoundError(
                f"Implementation plan file not found: {plan_file_path}"
            )

        with open(plan_path, "r", encoding="utf-8") as f:
            return f.read()

    def _check_file_tree_exists(self, target_directory: str) -> bool:
        """Check if file tree structure already exists"""
        code_directory = os.path.join(target_directory, "generate_code")
        return os.path.exists(code_directory) and len(os.listdir(code_directory)) > 0

    # ==================== 2. Public Interface Methods (External API Layer) ====================

    async def run_workflow(
        self,
        plan_file_path: str,
        target_directory: Optional[str] = None,
        pure_code_mode: bool = False,
        enable_read_tools: bool = True,
    ):
        """Run complete workflow - Main public interface"""
        # Set the read tools configuration
        self.enable_read_tools = enable_read_tools

        try:
            plan_content = self._read_plan_file(plan_file_path)

            if target_directory is None:
                target_directory = str(Path(plan_file_path).parent)

            # Calculate code directory for workspace alignment
            code_directory = os.path.join(target_directory, "generate_code")

            self.logger.info("=" * 80)
            self.logger.info("üöÄ STARTING CODE IMPLEMENTATION WORKFLOW")
            self.logger.info("=" * 80)
            self.logger.info(f"üìÑ Plan file: {plan_file_path}")
            self.logger.info(f"üìÇ Plan file parent: {target_directory}")
            self.logger.info(f"üéØ Code directory (MCP workspace): {code_directory}")
            self.logger.info(
                f"‚öôÔ∏è  Read tools: {'ENABLED' if self.enable_read_tools else 'DISABLED'}"
            )
            self.logger.info("=" * 80)

            results = {}

            # Check if file tree exists
            if self._check_file_tree_exists(target_directory):
                self.logger.info("File tree exists, skipping creation")
                results["file_tree"] = "Already exists, skipped creation"
            else:
                self.logger.info("Creating file tree...")
                results["file_tree"] = await self.create_file_structure(
                    plan_content, target_directory
                )

            # Code implementation
            if pure_code_mode:
                self.logger.info("Starting pure code implementation...")
                results["code_implementation"] = await self.implement_code_pure(
                    plan_content, target_directory, code_directory
                )
            else:
                pass

            self.logger.info("Workflow execution successful")

            return {
                "status": "success",
                "plan_file": plan_file_path,
                "target_directory": target_directory,
                "code_directory": os.path.join(target_directory, "generate_code"),
                "results": results,
                "mcp_architecture": "standard",
            }

        except Exception as e:
            self.logger.error(f"Workflow execution failed: {e}")

            return {"status": "error", "message": str(e), "plan_file": plan_file_path}
        finally:
            await self._cleanup_mcp_agent()

    async def create_file_structure(
        self, plan_content: str, target_directory: str
    ) -> str:
        """Create file tree structure based on implementation plan"""
        self.logger.info("Starting file tree creation...")

        structure_agent = Agent(
            name="StructureGeneratorAgent",
            instruction=STRUCTURE_GENERATOR_PROMPT,
            server_names=["command-executor"],
        )

        async with structure_agent:
            creator = await structure_agent.attach_llm(
                get_preferred_llm_class(self.config_path)
            )

            message = f"""Analyze the following implementation plan and generate shell commands to create the file tree structure.

Target Directory: {target_directory}/generate_code/

Implementation Plan:
{plan_content}

Tasks:
1. Find the file tree structure in the implementation plan
2. Generate shell commands (mkdir -p, touch) to create that structure
3. Use the execute_commands tool to run the commands and create the file structure

Requirements:
- Use mkdir -p to create directories
- Use touch to create files
- Include __init__.py file for Python packages
- Use relative paths to the target directory
- Execute commands to actually create the file structure"""

            result = await creator.generate_str(message=message)
            self.logger.info(f"LLM response: {result[:200]}...")  # Log first 200 chars

            # Verify directory was created, if not create it manually
            code_dir = os.path.join(target_directory, "generate_code")
            if not os.path.exists(code_dir):
                self.logger.warning(
                    "LLM did not create directory, creating manually..."
                )
                os.makedirs(code_dir, exist_ok=True)
                self.logger.info(f"‚úÖ Manually created directory: {code_dir}")
            else:
                self.logger.info(f"‚úÖ Directory exists: {code_dir}")

            return result

    async def implement_code_pure(
        self, plan_content: str, target_directory: str, code_directory: str = None
    ) -> str:
        """Pure code implementation - focus on code writing without testing"""
        self.logger.info("Starting pure code implementation (no testing)...")

        # Use provided code_directory or calculate it (for backwards compatibility)
        if code_directory is None:
            code_directory = os.path.join(target_directory, "generate_code")

        self.logger.info(f"üéØ Using code directory (MCP workspace): {code_directory}")

        if not os.path.exists(code_directory):
            self.logger.warning(
                f"Code directory does not exist, creating it: {code_directory}"
            )
            os.makedirs(code_directory, exist_ok=True)
            self.logger.info(f"‚úÖ Code directory created: {code_directory}")

        try:
            client, client_type = await self._initialize_llm_client()
            await self._initialize_mcp_agent(code_directory)

            tools = self._prepare_mcp_tool_definitions()
            system_message = GENERAL_CODE_IMPLEMENTATION_SYSTEM_PROMPT
            messages = []

            #             implementation_message = f"""**TASK: Implement Research Paper Reproduction Code**

            # You are implementing a complete, working codebase that reproduces the core algorithms, experiments, and methods described in a research paper. Your goal is to create functional code that can replicate the paper's key results and contributions.

            # **What you need to do:**
            # - Analyze the paper content and reproduction plan to understand requirements
            # - Implement all core algorithms mentioned in the main body of the paper
            # - Create the necessary components following the planned architecture
            # - Test each component to ensure functionality
            # - Integrate components into a cohesive, executable system
            # - Focus on reproducing main contributions rather than appendix-only experiments

            # **RESOURCES:**
            # - **Paper & Reproduction Plan**: `{target_directory}/` (contains .md paper files and initial_plan.txt with detailed implementation guidance)
            # - **Reference Code Indexes**: `{target_directory}/indexes/` (JSON files with implementation patterns from related codebases)
            # - **Implementation Directory**: `{code_directory}/` (your working directory for all code files)

            # **CURRENT OBJECTIVE:**
            # Start by reading the reproduction plan (`{target_directory}/initial_plan.txt`) to understand the implementation strategy, then examine the paper content to identify the first priority component to implement. Use the search_code tool to find relevant reference implementations from the indexes directory (`{target_directory}/indexes/*.json`) before coding.

            # ---
            # **START:** Review the plan above and begin implementation."""
            implementation_message = f"""**Task: Implement code based on the following reproduction plan**

**Code Reproduction Plan:**
{plan_content}

**Working Directory:** {code_directory}

**Current Objective:** Begin implementation by analyzing the plan structure, examining the current project layout, and implementing the first foundation file according to the plan's priority order."""

            messages.append({"role": "user", "content": implementation_message})

            result = await self._pure_code_implementation_loop(
                client,
                client_type,
                system_message,
                messages,
                tools,
                plan_content,
                target_directory,
            )

            return result

        finally:
            await self._cleanup_mcp_agent()

    # ==================== 3. Core Business Logic (Implementation Layer) ====================

    async def _pure_code_implementation_loop(
        self,
        client,
        client_type,
        system_message,
        messages,
        tools,
        plan_content,
        target_directory,
    ):
        """Pure code implementation loop with memory optimization and phase consistency"""
        max_iterations = 800
        iteration = 0
        start_time = time.time()
        max_time = 7200  # 120 minutes (2 hours)

        # Initialize specialized agents
        code_agent = CodeImplementationAgent(
            self.mcp_agent, self.logger, self.enable_read_tools
        )

        # Pass code_directory to memory agent for file extraction
        code_directory = os.path.join(target_directory, "generate_code")
        memory_agent = ConciseMemoryAgent(
            plan_content,
            self.logger,
            target_directory,
            self.default_models,
            code_directory,
        )

        # Log read tools configuration
        read_tools_status = "ENABLED" if self.enable_read_tools else "DISABLED"
        self.logger.info(
            f"üîß Read tools (read_file, read_code_mem): {read_tools_status}"
        )
        if not self.enable_read_tools:
            self.logger.info(
                "üö´ No read mode: read_file and read_code_mem tools will be skipped"
            )

        # Connect code agent with memory agent for summary generation
        # Note: Concise memory agent doesn't need LLM client for summary generation
        code_agent.set_memory_agent(memory_agent, client, client_type)

        # Initialize memory agent with iteration 0
        memory_agent.start_new_round(iteration=0)

        while iteration < max_iterations:
            iteration += 1
            elapsed_time = time.time() - start_time

            if elapsed_time > max_time:
                self.logger.warning(f"Time limit reached: {elapsed_time:.2f}s")
                break

            # # Test simplified memory approach if we have files implemented
            # if iteration == 5 and code_agent.get_files_implemented_count() > 0:
            #     self.logger.info("üß™ Testing simplified memory approach...")
            #     test_results = await memory_agent.test_simplified_memory_approach()
            #     self.logger.info(f"Memory test results: {test_results}")

            # self.logger.info(f"Pure code implementation iteration {iteration}: generating code")

            messages = self._validate_messages(messages)
            current_system_message = code_agent.get_system_prompt()

            # Round logging removed

            # Call LLM
            response = await self._call_llm_with_tools(
                client, client_type, current_system_message, messages, tools
            )

            response_content = response.get("content", "").strip()
            if not response_content:
                response_content = "Continue implementing code files..."

            messages.append({"role": "assistant", "content": response_content})

            # Handle tool calls
            if response.get("tool_calls"):
                tool_results = await code_agent.execute_tool_calls(
                    response["tool_calls"]
                )

                # Record essential tool results in concise memory agent
                for tool_call, tool_result in zip(response["tool_calls"], tool_results):
                    memory_agent.record_tool_result(
                        tool_name=tool_call["name"],
                        tool_input=tool_call["input"],
                        tool_result=tool_result.get("result"),
                    )

                # NEW LOGIC: Check if write_file was called and trigger memory optimization immediately

                # Determine guidance based on results
                has_error = self._check_tool_results_for_errors(tool_results)
                files_count = code_agent.get_files_implemented_count()

                if has_error:
                    guidance = self._generate_error_guidance()
                else:
                    guidance = self._generate_success_guidance(files_count)

                compiled_response = self._compile_user_response(tool_results, guidance)
                messages.append({"role": "user", "content": compiled_response})

                # NEW LOGIC: Apply memory optimization immediately after write_file detection
                if memory_agent.should_trigger_memory_optimization(
                    messages, code_agent.get_files_implemented_count()
                ):
                    # Memory optimization triggered

                    # Apply concise memory optimization
                    files_implemented_count = code_agent.get_files_implemented_count()
                    current_system_message = code_agent.get_system_prompt()
                    messages = memory_agent.apply_memory_optimization(
                        current_system_message, messages, files_implemented_count
                    )

                    # Memory optimization completed

            else:
                files_count = code_agent.get_files_implemented_count()
                no_tools_guidance = self._generate_no_tools_guidance(files_count)
                messages.append({"role": "user", "content": no_tools_guidance})

            # # Check for analysis loop and provide corrective guidance
            # if code_agent.is_in_analysis_loop():
            #     analysis_loop_guidance = code_agent.get_analysis_loop_guidance()
            #     messages.append({"role": "user", "content": analysis_loop_guidance})
            #     self.logger.warning(
            #         "Analysis loop detected and corrective guidance provided"
            #     )

            # Record file implementations in memory agent (for the current round)
            for file_info in code_agent.get_implementation_summary()["completed_files"]:
                memory_agent.record_file_implementation(file_info["file"])

            # REMOVED: Old memory optimization logic - now happens immediately after write_file
            # Memory optimization is now triggered immediately after write_file detection

            # Start new round for next iteration, sync with workflow iteration
            memory_agent.start_new_round(iteration=iteration)

            # Check completion based on actual unimplemented files list
            unimplemented_files = memory_agent.get_unimplemented_files()
            if not unimplemented_files:  # Empty list means all files implemented
                self.logger.info(
                    "‚úÖ Code implementation complete - All files implemented"
                )
                break

            # Emergency trim if too long
            if len(messages) > 50:
                self.logger.warning(
                    "Emergency message trim - applying concise memory optimization"
                )

                current_system_message = code_agent.get_system_prompt()
                files_implemented_count = code_agent.get_files_implemented_count()
                messages = memory_agent.apply_memory_optimization(
                    current_system_message, messages, files_implemented_count
                )

        return await self._generate_pure_code_final_report_with_concise_agents(
            iteration, time.time() - start_time, code_agent, memory_agent
        )

    # ==================== 4. MCP Agent and LLM Communication Management (Communication Layer) ====================

    async def _initialize_mcp_agent(self, code_directory: str):
        """Initialize MCP agent and connect to code-implementation server"""
        try:
            self.mcp_agent = Agent(
                name="CodeImplementationAgent",
                instruction="You are a code implementation assistant, using MCP tools to implement paper code replication.",
                server_names=["code-implementation", "code-reference-indexer"],
            )

            await self.mcp_agent.__aenter__()
            llm = await self.mcp_agent.attach_llm(
                get_preferred_llm_class(self.config_path)
            )

            # Set workspace to the target code directory
            workspace_result = await self.mcp_agent.call_tool(
                "set_workspace", {"workspace_path": code_directory}
            )
            self.logger.info(f"Workspace setup result: {workspace_result}")

            return llm

        except Exception as e:
            self.logger.error(f"Failed to initialize MCP agent: {e}")
            if self.mcp_agent:
                try:
                    await self.mcp_agent.__aexit__(None, None, None)
                except Exception:
                    pass
                self.mcp_agent = None
            raise

    async def _cleanup_mcp_agent(self):
        """Clean up MCP agent resources"""
        if self.mcp_agent:
            try:
                await self.mcp_agent.__aexit__(None, None, None)
                self.logger.info("MCP agent connection closed")
            except Exception as e:
                self.logger.warning(f"Error closing MCP agent: {e}")
            finally:
                self.mcp_agent = None

    async def _initialize_llm_client(self):
        """Initialize LLM client based on llm_provider preference and API key availability"""
        # Get API keys
        anthropic_key = self.api_config.get("anthropic", {}).get("api_key", "")
        openai_key = self.api_config.get("openai", {}).get("api_key", "")
        google_key = self.api_config.get("google", {}).get("api_key", "")

        # Read user preference from main config
        preferred_provider = None
        try:
            import yaml

            # Derive config path from secrets path (same directory)
            secrets_dir = os.path.dirname(os.path.abspath(self.config_path))
            config_path = os.path.join(secrets_dir, "mcp_agent.config.yaml")
            if os.path.exists(config_path):
                with open(config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                    preferred_provider = config.get("llm_provider", "").strip().lower()
        except Exception as e:
            self.logger.warning(f"Could not read llm_provider preference: {e}")

        # Define provider initialization functions
        async def init_anthropic():
            if not (anthropic_key and anthropic_key.strip()):
                return None
            try:
                from anthropic import AsyncAnthropic

                client = AsyncAnthropic(api_key=anthropic_key)
                await client.messages.create(
                    model=self.default_models["anthropic"],
                    max_tokens=20,
                    messages=[{"role": "user", "content": "test"}],
                )
                self.logger.info(
                    f"Using Anthropic API with model: {self.default_models['anthropic']}"
                )
                return client, "anthropic"
            except Exception as e:
                self.logger.warning(f"Anthropic API unavailable: {e}")
                return None

        async def init_google():
            if not (google_key and google_key.strip()):
                return None
            try:
                from google import genai

                client = genai.Client(api_key=google_key)
                try:
                    test_response = await client.aio.models.generate_content(
                        model=self.default_models.get("google", "gemini-2.0-flash"),
                        contents="test",
                    )
                    self.logger.info(
                        "Google API connection successful: " + str(test_response)
                    )
                except Exception as test_err:
                    self.logger.warning(
                        f"Could not test Google API: {test_err}, but will try to use client"
                    )

                self.logger.info(
                    f"Using Google API with model: {self.default_models.get('google', 'gemini-2.0-flash')}"
                )
                return client, "google"
            except Exception as e:
                self.logger.warning(f"Google API unavailable: {e}")
                return None

        async def init_openai():
            if not (openai_key and openai_key.strip()):
                return None
            try:
                from openai import AsyncOpenAI

                openai_config = self.api_config.get("openai", {})
                base_url = openai_config.get("base_url")

                if base_url:
                    client = AsyncOpenAI(api_key=openai_key, base_url=base_url)
                else:
                    client = AsyncOpenAI(api_key=openai_key)

                model_name = self.default_models.get("openai", "o3-mini")

                try:
                    await client.chat.completions.create(
                        model=model_name,
                        max_tokens=20,
                        messages=[{"role": "user", "content": "test"}],
                    )
                except Exception as e:
                    if "max_tokens" in str(e) and "max_completion_tokens" in str(e):
                        self.logger.info(
                            f"Model {model_name} requires max_completion_tokens parameter"
                        )
                        await client.chat.completions.create(
                            model=model_name,
                            max_completion_tokens=20,
                            messages=[{"role": "user", "content": "test"}],
                        )
                    else:
                        raise
                self.logger.info(f"Using OpenAI API with model: {model_name}")
                if base_url:
                    self.logger.info(f"Using custom base URL: {base_url}")
                return client, "openai"
            except Exception as e:
                self.logger.warning(f"OpenAI API unavailable: {e}")
                return None

        # Map providers to their init functions
        provider_init_map = {
            "anthropic": init_anthropic,
            "google": init_google,
            "openai": init_openai,
        }

        # Try preferred provider first
        if preferred_provider and preferred_provider in provider_init_map:
            self.logger.info(f"üéØ Trying preferred provider: {preferred_provider}")
            result = await provider_init_map[preferred_provider]()
            if result:
                return result
            else:
                self.logger.warning(
                    f"‚ö†Ô∏è Preferred provider '{preferred_provider}' unavailable, trying alternatives..."
                )

        # Fallback: try providers in order
        for provider_name, init_func in provider_init_map.items():
            if provider_name == preferred_provider:
                continue  # Already tried
            result = await init_func()
            if result:
                return result

        raise ValueError(
            "No available LLM API - please check your API keys in configuration"
        )

    async def _call_llm_with_tools(
        self, client, client_type, system_message, messages, tools, max_tokens=8192
    ):
        """Call LLM with tools"""
        try:
            if client_type == "anthropic":
                return await self._call_anthropic_with_tools(
                    client, system_message, messages, tools, max_tokens
                )
            elif client_type == "openai":
                return await self._call_openai_with_tools(
                    client, system_message, messages, tools, max_tokens
                )
            elif client_type == "google":
                return await self._call_google_with_tools(
                    client, system_message, messages, tools, max_tokens
                )
            else:
                raise ValueError(f"Unsupported client type: {client_type}")
        except Exception as e:
            self.logger.error(f"LLM call failed: {e}")
            raise

    async def _call_anthropic_with_tools(
        self, client, system_message, messages, tools, max_tokens
    ):
        """Call Anthropic API"""
        validated_messages = self._validate_messages(messages)
        if not validated_messages:
            validated_messages = [
                {"role": "user", "content": "Please continue implementing code"}
            ]

        try:
            # Use implementation-specific model for code generation
            impl_model = self.default_models.get(
                "anthropic_implementation", self.default_models["anthropic"]
            )
            self.logger.info(f"üîß Code generation using model: {impl_model}")
            response = await client.messages.create(
                model=impl_model,
                system=system_message,
                messages=validated_messages,
                tools=tools,
                max_tokens=max_tokens,
                temperature=0.2,
            )
        except Exception as e:
            self.logger.error(f"Anthropic API call failed: {e}")
            raise

        content = ""
        tool_calls = []

        for block in response.content:
            if block.type == "text":
                content += block.text
            elif block.type == "tool_use":
                tool_calls.append(
                    {"id": block.id, "name": block.name, "input": block.input}
                )

        return {"content": content, "tool_calls": tool_calls}

    async def _call_google_with_tools(
        self, client, system_message, messages, tools, max_tokens
    ):
        """
        Call Google Gemini API with tools

        Note: Google Gemini uses a completely different API structure.
        The client here is expected to be google.genai.Client from google-genai SDK.

        Reference: https://ai.google.dev/gemini-api/docs/function-calling
        """
        try:
            from google.genai import types
        except ImportError:
            raise ImportError("google-genai package is required for Google API calls")

        validated_messages = self._validate_messages(messages)
        if not validated_messages:
            validated_messages = [
                {"role": "user", "content": "Please continue implementing code"}
            ]

        # Convert messages to Google Gemini format (types.Content)
        # Gemini expects: role="user" or role="model" (not "assistant")
        gemini_messages = []
        for msg in validated_messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            # Convert role names: "assistant" -> "model"
            if role == "assistant":
                role = "model"
            elif role not in ["user", "model"]:
                # Skip unsupported roles or convert to user
                role = "user"

            gemini_messages.append(
                types.Content(role=role, parts=[types.Part.from_text(text=content)])
            )

        # Convert tools to Google Gemini format (types.Tool with FunctionDeclaration)
        # Following the EXACT pattern from GoogleAugmentedLLM line 92-103
        # IMPORTANT: Each tool should be wrapped in its own Tool object!
        gemini_tools = []
        if tools:
            for tool in tools:
                # Transform the input_schema to be Gemini-compatible
                parameters = self._transform_schema_for_gemini(tool["input_schema"])

                # Each tool gets its own Tool wrapper (not all in one!)
                gemini_tools.append(
                    types.Tool(
                        function_declarations=[
                            types.FunctionDeclaration(
                                name=tool["name"],
                                description=tool["description"],
                                parameters=parameters,
                            )
                        ]
                    )
                )

        # Create config with system instruction and tools
        config = types.GenerateContentConfig(
            max_output_tokens=max_tokens,
            temperature=0.2,
            system_instruction=system_message if system_message else None,
            tools=gemini_tools if gemini_tools else None,
            # Disable automatic function calling - we handle it manually
            automatic_function_calling=types.AutomaticFunctionCallingConfig(
                disable=True
            ),
        )

        try:
            # Google Gemini API call using the native SDK
            # client is google.genai.Client instance
            # Use implementation-specific model for code generation
            impl_model = self.default_models.get(
                "google_implementation", self.default_models["google"]
            )
            self.logger.info(f"üîß Code generation using model: {impl_model}")
            response = await client.aio.models.generate_content(
                model=impl_model,
                contents=gemini_messages,
                config=config,
            )
        except Exception as e:
            self.logger.error(f"Google API call failed: {e}")
            raise

        # Parse Gemini response (types.GenerateContentResponse)
        # Following the pattern from augmented_llm_google.py lines 145-165
        content = ""
        tool_calls = []

        if response and hasattr(response, "candidates") and response.candidates:
            candidate = response.candidates[0]

            if hasattr(candidate, "content") and candidate.content:
                if hasattr(candidate.content, "parts") and candidate.content.parts:
                    for part in candidate.content.parts:
                        # Handle text content
                        if hasattr(part, "text") and part.text:
                            content += part.text

                        # Handle function calls
                        # Check for function_call attribute, matching augmented_llm_google.py line 164
                        if hasattr(part, "function_call") and part.function_call:
                            fc = part.function_call
                            # Extract function call details
                            # Note: Gemini function_call has name and args attributes
                            tool_call = {
                                "id": getattr(
                                    fc, "id", getattr(fc, "name", "")
                                ),  # Use name as fallback for id
                                "name": fc.name if hasattr(fc, "name") else "",
                                "input": dict(fc.args)
                                if hasattr(fc, "args") and fc.args
                                else {},
                            }
                            self.logger.debug(
                                f"Google function_call parsed: {tool_call}"
                            )
                            tool_calls.append(tool_call)

        return {"content": content, "tool_calls": tool_calls}

    def _transform_schema_for_gemini(self, schema: dict) -> dict:
        """
        Transform JSON Schema to OpenAPI Schema format compatible with Gemini.

        This is based on the transform_mcp_tool_schema from GoogleAugmentedLLM.
        Key transformations:
        1. Convert camelCase to snake_case
        2. Remove unsupported fields (default, additionalProperties)
        3. Handle nullable types via anyOf
        """
        if not isinstance(schema, dict):
            return schema

        # Fields to exclude
        EXCLUDED_PROPERTIES = {"default", "additionalProperties"}

        # camelCase to snake_case mappings
        CAMEL_TO_SNAKE = {
            "anyOf": "any_of",
            "maxLength": "max_length",
            "minLength": "min_length",
            "minProperties": "min_properties",
            "maxProperties": "max_properties",
            "maxItems": "max_items",
            "minItems": "min_items",
        }

        result = {}

        for key, value in schema.items():
            # Skip excluded properties
            if key in EXCLUDED_PROPERTIES:
                continue

            # Convert camelCase to snake_case
            snake_key = CAMEL_TO_SNAKE.get(key, key)

            # Handle nested structures
            if key == "properties" and isinstance(value, dict):
                result[snake_key] = {
                    prop_k: self._transform_schema_for_gemini(prop_v)
                    for prop_k, prop_v in value.items()
                }
            elif key == "items" and isinstance(value, dict):
                result[snake_key] = self._transform_schema_for_gemini(value)
            elif key == "anyOf" and isinstance(value, list):
                # Handle nullable types (Type | None)
                has_null = any(
                    isinstance(item, dict) and item.get("type") == "null"
                    for item in value
                )
                if has_null:
                    result["nullable"] = True

                # Get first non-null schema
                for item in value:
                    if isinstance(item, dict) and item.get("type") != "null":
                        transformed = self._transform_schema_for_gemini(item)
                        for k, v in transformed.items():
                            if k not in result:
                                result[k] = v
                        break
            else:
                result[snake_key] = value

        return result

    def _repair_truncated_json(self, json_str: str, tool_name: str = "") -> dict:
        """
        Advanced JSON repair for truncated or malformed JSON from LLM responses.

        Handles:
        - Missing closing braces/brackets
        - Truncated string values
        - Missing required fields
        - Trailing commas
        """
        import re

        # Step 1: Try basic fixes first
        fixed = json_str.strip()

        # Remove trailing commas
        fixed = re.sub(r",\s*}", "}", fixed)
        fixed = re.sub(r",\s*]", "]", fixed)

        try:
            return json.loads(fixed)
        except json.JSONDecodeError as e:
            print("   üîß Attempting advanced JSON repair...")

            # Step 2: Check for truncation issues
            if e.msg == "Expecting value":
                # Likely truncated - try to close open structures
                fixed = self._close_json_structures(fixed)
                try:
                    return json.loads(fixed)
                except (json.JSONDecodeError, ValueError, TypeError):
                    pass

            # Step 3: Try to extract partial valid JSON
            if e.msg.startswith("Expecting") and e.pos:
                # Truncate at error position and try to close
                truncated = fixed[: e.pos]
                closed = self._close_json_structures(truncated)
                try:
                    partial = json.loads(closed)
                    print("   ‚úÖ Extracted partial JSON successfully")
                    return partial
                except (json.JSONDecodeError, ValueError, TypeError):
                    pass

            # Step 4: Tool-specific defaults for critical tools
            if tool_name == "write_file":
                # For write_file, try to extract at least file_path
                file_path_match = re.search(r'"file_path"\s*:\s*"([^"]*)"', fixed)
                if file_path_match:
                    print("   ‚ö†Ô∏è  write_file JSON truncated, using minimal structure")
                    return {
                        "file_path": file_path_match.group(1),
                        "content": "",  # Empty content is better than crashing
                    }

            # Step 5: Last resort - return error indicator
            print("   ‚ùå JSON repair failed completely")
            return None

    def _close_json_structures(self, json_str: str) -> str:
        """
        Intelligently close unclosed JSON structures.
        Counts braces and brackets to determine what needs closing.
        """
        # Count open structures
        open_braces = json_str.count("{") - json_str.count("}")
        open_brackets = json_str.count("[") - json_str.count("]")

        # Check if we're in the middle of a string
        quote_count = json_str.count('"')
        in_string = (quote_count % 2) != 0

        result = json_str

        # Close string if needed
        if in_string:
            result += '"'

        # Close brackets first (inner structures)
        result += "]" * open_brackets

        # Close braces
        result += "}" * open_braces

        return result

    async def _call_openai_with_tools(
        self, client, system_message, messages, tools, max_tokens
    ):
        """Call OpenAI API with robust JSON error handling and retry mechanism"""
        openai_tools = []
        for tool in tools:
            openai_tools.append(
                {
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool["description"],
                        "parameters": tool["input_schema"],
                    },
                }
            )

        openai_messages = [{"role": "system", "content": system_message}]
        openai_messages.extend(messages)

        # Retry mechanism for API calls
        max_retries = 3
        retry_delay = 2  # seconds

        # Use implementation-specific model for code generation
        impl_model = self.default_models.get(
            "openai_implementation", self.default_models["openai"]
        )
        self.logger.info(f"üîß Code generation using model: {impl_model}")

        for attempt in range(max_retries):
            try:
                # Try max_tokens first, fallback to max_completion_tokens if unsupported
                try:
                    response = await client.chat.completions.create(
                        model=impl_model,
                        messages=openai_messages,
                        tools=openai_tools if openai_tools else None,
                        max_tokens=max_tokens,
                        temperature=0.2,
                    )
                except Exception as e:
                    if "max_tokens" in str(e) and "max_completion_tokens" in str(e):
                        # Retry with max_completion_tokens for models that require it
                        response = await client.chat.completions.create(
                            model=impl_model,
                            messages=openai_messages,
                            tools=openai_tools if openai_tools else None,
                            max_completion_tokens=max_tokens,
                        )
                    else:
                        raise

                # Validate response structure
                if (
                    not response
                    or not hasattr(response, "choices")
                    or not response.choices
                ):
                    raise ValueError("Invalid API response: missing choices")

                if not response.choices[0] or not hasattr(
                    response.choices[0], "message"
                ):
                    raise ValueError("Invalid API response: missing message in choice")

                message = response.choices[0].message
                content = message.content or ""

                # Successfully got a valid response
                break

            except json.JSONDecodeError as e:
                print(
                    f"\n‚ùå JSON Decode Error in API response (attempt {attempt + 1}/{max_retries}):"
                )
                print(f"   Error: {e}")
                print(f"   Position: line {e.lineno}, column {e.colno}")

                if attempt < max_retries - 1:
                    print(f"   ‚è≥ Retrying in {retry_delay} seconds...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    print("   ‚ùå All retries exhausted")
                    raise

            except (ValueError, AttributeError, TypeError) as e:
                print(f"\n‚ùå API Response Error (attempt {attempt + 1}/{max_retries}):")
                print(f"   Error type: {type(e).__name__}")
                print(f"   Error: {e}")

                if attempt < max_retries - 1:
                    print(f"   ‚è≥ Retrying in {retry_delay} seconds...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    print("   ‚ùå All retries exhausted")
                    # Return empty response instead of crashing
                    return {
                        "content": "API error - unable to get valid response",
                        "tool_calls": [],
                    }

            except Exception as e:
                print(
                    f"\n‚ùå Unexpected API Error (attempt {attempt + 1}/{max_retries}):"
                )
                print(f"   Error type: {type(e).__name__}")
                print(f"   Error: {e}")

                if attempt < max_retries - 1:
                    print(f"   ‚è≥ Retrying in {retry_delay} seconds...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    print("   ‚ùå All retries exhausted")
                    raise

        tool_calls = []
        if message.tool_calls:
            for tool_call in message.tool_calls:
                try:
                    # Attempt to parse tool call arguments
                    parsed_input = json.loads(tool_call.function.arguments)
                    tool_calls.append(
                        {
                            "id": tool_call.id,
                            "name": tool_call.function.name,
                            "input": parsed_input,
                        }
                    )
                except json.JSONDecodeError as e:
                    # Detailed JSON parsing error logging
                    print("\n‚ùå JSON Parsing Error in tool call:")
                    print(f"   Tool: {tool_call.function.name}")
                    print(f"   Error: {e}")
                    print("   Raw arguments (first 500 chars):")
                    print(f"   {tool_call.function.arguments[:500]}")
                    print(f"   Error position: line {e.lineno}, column {e.colno}")
                    print(
                        f"   Problem at: ...{tool_call.function.arguments[max(0, e.pos-50):e.pos+50]}..."
                    )

                    # Attempt advanced JSON repair
                    repaired = self._repair_truncated_json(
                        tool_call.function.arguments, tool_call.function.name
                    )

                    if repaired:
                        print("   ‚úÖ JSON repaired successfully")
                        tool_calls.append(
                            {
                                "id": tool_call.id,
                                "name": tool_call.function.name,
                                "input": repaired,
                            }
                        )
                    else:
                        # Skip this tool call if repair failed
                        print("   ‚ö†Ô∏è  Skipping unrepairable tool call")
                        continue

        return {"content": content, "tool_calls": tool_calls}

    # ==================== 5. Tools and Utility Methods (Utility Layer) ====================

    def _validate_messages(self, messages: List[Dict]) -> List[Dict]:
        """Validate and clean message list"""
        valid_messages = []
        for msg in messages:
            content = msg.get("content", "").strip()
            if content:
                valid_messages.append(
                    {"role": msg.get("role", "user"), "content": content}
                )
            else:
                self.logger.warning(f"Skipping empty message: {msg}")
        return valid_messages

    def _prepare_mcp_tool_definitions(self) -> List[Dict[str, Any]]:
        """Prepare tool definitions in Anthropic API standard format"""
        return get_mcp_tools("code_implementation")

    def _check_tool_results_for_errors(self, tool_results: List[Dict]) -> bool:
        """Check tool results for errors with JSON repair capability"""
        for result in tool_results:
            try:
                if hasattr(result["result"], "content") and result["result"].content:
                    content_text = result["result"].content[0].text

                    # First attempt: try direct JSON parsing
                    try:
                        parsed_result = json.loads(content_text)
                        if parsed_result.get("status") == "error":
                            return True
                    except json.JSONDecodeError as e:
                        # JSON parsing failed - try to repair
                        print("\n‚ö†Ô∏è  JSON parsing failed in tool result check:")
                        print(f"   Error: {e}")
                        print(
                            f"   Position: line {e.lineno}, column {e.colno}, char {e.pos}"
                        )
                        print(f"   Content length: {len(content_text)} chars")
                        print(f"   First 300 chars: {content_text[:300]}")

                        # Attempt to repair the JSON
                        repaired = self._repair_truncated_json(content_text)
                        if repaired:
                            print("   ‚úÖ Tool result JSON repaired successfully")
                            if repaired.get("status") == "error":
                                return True
                        else:
                            # Fallback: check for "error" keyword in text
                            if "error" in content_text.lower():
                                return True

                elif isinstance(result["result"], str):
                    if "error" in result["result"].lower():
                        return True

            except (AttributeError, IndexError) as e:
                # Unexpected result structure
                print(f"\n‚ö†Ô∏è  Unexpected result structure: {type(e).__name__}: {e}")
                result_str = str(result["result"])
                if "error" in result_str.lower():
                    return True
        return False

    # ==================== 6. User Interaction and Feedback (Interaction Layer) ====================

    def _generate_success_guidance(self, files_count: int) -> str:
        """Generate concise success guidance for continuing implementation"""
        return f"""‚úÖ File implementation completed successfully!

üìä **Progress Status:** {files_count} files implemented

üéØ **Next Action:** Check if ALL files from the reproduction plan are implemented.

‚ö° **Decision Process:**
1. **If ALL files implemented:** Reply with "All files implemented" to complete the task
2. **If MORE files need implementation:** Continue with dependency-aware workflow:
   - **Use `write_file` to implement the new component"""

    def _generate_error_guidance(self) -> str:
        """Generate error guidance for handling issues"""
        return """‚ùå Error detected during file implementation.

üîß **Action Required:**
1. Review the error details above
2. Fix the identified issue
3. **Check if ALL files from the reproduction plan are implemented:**
   - **If YES:** Respond "**implementation complete**" to end the conversation
   - **If NO:** Continue with proper development cycle for next file:
     - **Use `write_file` to implement properly
4. Ensure proper error handling in future implementations"""

    def _generate_no_tools_guidance(self, files_count: int) -> str:
        """Generate concise guidance when no tools are called"""
        return f"""‚ö†Ô∏è No tool calls detected in your response.

üìä **Current Progress:** {files_count} files implemented

üö® **Action Required:** Check completion status NOW:

‚ö° **Decision Process:**
1. **If ALL files from plan are implemented:** Reply "All files implemented" to complete
2. **If MORE files need implementation:** Use tools to continue:
   - **Use `write_file` to implement the new component

üö® **Critical:** Don't just explain - either declare completion or use tools!"""

    def _compile_user_response(self, tool_results: List[Dict], guidance: str) -> str:
        """Compile tool results and guidance into a single user response"""
        response_parts = []

        if tool_results:
            response_parts.append("üîß **Tool Execution Results:**")
            for tool_result in tool_results:
                tool_name = tool_result["tool_name"]
                result_content = tool_result["result"]
                response_parts.append(
                    f"```\nTool: {tool_name}\nResult: {result_content}\n```"
                )

        if guidance:
            response_parts.append("\n" + guidance)

        return "\n\n".join(response_parts)

    # ==================== 7. Reporting and Output (Output Layer) ====================

    async def _generate_pure_code_final_report_with_concise_agents(
        self,
        iterations: int,
        elapsed_time: float,
        code_agent: CodeImplementationAgent,
        memory_agent: ConciseMemoryAgent,
    ):
        """Generate final report using concise agent statistics"""
        try:
            code_stats = code_agent.get_implementation_statistics()
            memory_stats = memory_agent.get_memory_statistics(
                code_stats["files_implemented_count"]
            )

            if self.mcp_agent:
                history_result = await self.mcp_agent.call_tool(
                    "get_operation_history", {"last_n": 30}
                )
                history_data = (
                    json.loads(history_result)
                    if isinstance(history_result, str)
                    else history_result
                )
            else:
                history_data = {"total_operations": 0, "history": []}

            write_operations = 0
            files_created = []
            if "history" in history_data:
                for item in history_data["history"]:
                    if item.get("action") == "write_file":
                        write_operations += 1
                        file_path = item.get("details", {}).get("file_path", "unknown")
                        files_created.append(file_path)

            report = f"""
# Pure Code Implementation Completion Report (Write-File-Based Memory Mode)

## Execution Summary
- Implementation iterations: {iterations}
- Total elapsed time: {elapsed_time:.2f} seconds
- Files implemented: {code_stats['total_files_implemented']}
- File write operations: {write_operations}
- Total MCP operations: {history_data.get('total_operations', 0)}

## Read Tools Configuration
- Read tools enabled: {code_stats['read_tools_status']['read_tools_enabled']}
- Status: {code_stats['read_tools_status']['status']}
- Tools affected: {', '.join(code_stats['read_tools_status']['tools_affected'])}

## Agent Performance
### Code Implementation Agent
- Files tracked: {code_stats['files_implemented_count']}
- Technical decisions: {code_stats['technical_decisions_count']}
- Constraints tracked: {code_stats['constraints_count']}
- Architecture notes: {code_stats['architecture_notes_count']}
- Dependency analysis performed: {code_stats['dependency_analysis_count']}
- Files read for dependencies: {code_stats['files_read_for_dependencies']}
- Last summary triggered at file count: {code_stats['last_summary_file_count']}

### Concise Memory Agent (Write-File-Based)
- Last write_file detected: {memory_stats['last_write_file_detected']}
- Should clear memory next: {memory_stats['should_clear_memory_next']}
- Files implemented count: {memory_stats['implemented_files_tracked']}
- Current round: {memory_stats['current_round']}
- Concise mode active: {memory_stats['concise_mode_active']}
- Current round tool results: {memory_stats['current_round_tool_results']}
- Essential tools recorded: {memory_stats['essential_tools_recorded']}

## Files Created
"""
            for file_path in files_created[-20:]:
                report += f"- {file_path}\n"

            if len(files_created) > 20:
                report += f"... and {len(files_created) - 20} more files\n"

            report += """
## Architecture Features
‚úÖ WRITE-FILE-BASED Memory Agent - Clear after each file generation
‚úÖ After write_file: Clear history ‚Üí Keep system prompt + initial plan + tool results
‚úÖ Tool accumulation: read_code_mem, read_file, search_reference_code until next write_file
‚úÖ Clean memory cycle: write_file ‚Üí clear ‚Üí accumulate ‚Üí write_file ‚Üí clear
‚úÖ Essential tool recording with write_file detection
‚úÖ Specialized agent separation for clean code organization
‚úÖ MCP-compliant tool execution
‚úÖ Production-grade code with comprehensive type hints
‚úÖ Intelligent dependency analysis and file reading
‚úÖ Automated read_file usage for implementation context
‚úÖ Eliminates conversation clutter between file generations
‚úÖ Focused memory for efficient next file generation
"""
            return report

        except Exception as e:
            self.logger.error(f"Failed to generate final report: {e}")
            return f"Failed to generate final report: {str(e)}"


async def main():
    """Main function for running the workflow"""
    # Configure root logger carefully to avoid duplicates
    root_logger = logging.getLogger()
    if not root_logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter("%(levelname)s:%(name)s:%(message)s")
        handler.setFormatter(formatter)
        root_logger.addHandler(handler)
        root_logger.setLevel(logging.INFO)

    workflow = CodeImplementationWorkflow()

    print("=" * 60)
    print("Code Implementation Workflow with UNIFIED Reference Indexer")
    print("=" * 60)
    print("Select mode:")
    print("1. Test Code Reference Indexer Integration")
    print("2. Run Full Implementation Workflow")
    print("3. Run Implementation with Pure Code Mode")
    print("4. Test Read Tools Configuration")

    # mode_choice = input("Enter choice (1-4, default: 3): ").strip()

    # For testing purposes, we'll run the test first
    # if mode_choice == "4":
    #     print("Testing Read Tools Configuration...")

    #     # Create a test workflow normally
    #     test_workflow = CodeImplementationWorkflow()

    #     # Create a mock code agent for testing
    #     print("\nüß™ Testing with read tools DISABLED:")
    #     test_agent_disabled = CodeImplementationAgent(None, enable_read_tools=False)
    #     await test_agent_disabled.test_read_tools_configuration()

    #     print("\nüß™ Testing with read tools ENABLED:")
    #     test_agent_enabled = CodeImplementationAgent(None, enable_read_tools=True)
    #     await test_agent_enabled.test_read_tools_configuration()

    #     print("‚úÖ Read tools configuration testing completed!")
    #     return

    # print("Running Code Reference Indexer Integration Test...")

    test_success = True
    if test_success:
        print("\n" + "=" * 60)
        print("üéâ UNIFIED Code Reference Indexer Integration Test PASSED!")
        print("üîß Three-step process successfully merged into ONE tool")
        print("=" * 60)

        # Ask if user wants to continue with actual workflow
        print("\nContinuing with workflow execution...")

        plan_file = os.path.join(
            os.getcwd(), "deepcode_lab", "papers", "2", "initial_plan.txt"
        )
        target_directory = os.path.join(os.getcwd(), "deepcode_lab", "papers", "2")
        print("Implementation Mode Selection:")
        print("1. Pure Code Implementation Mode (Recommended)")
        print("2. Iterative Implementation Mode")

        pure_code_mode = True
        mode_name = "Pure Code Implementation Mode with Memory Agent Architecture + Code Reference Indexer"
        print(f"Using: {mode_name}")

        # Configure read tools - modify this parameter to enable/disable read tools
        enable_read_tools = (
            True  # Set to False to disable read_file and read_code_mem tools
        )
        read_tools_status = "ENABLED" if enable_read_tools else "DISABLED"
        print(f"üîß Read tools (read_file, read_code_mem): {read_tools_status}")

        # NOTE: To test without read tools, change the line above to:
        # enable_read_tools = False

        result = await workflow.run_workflow(
            plan_file,
            target_directory=target_directory,
            pure_code_mode=pure_code_mode,
            enable_read_tools=enable_read_tools,
        )

        print("=" * 60)
        print("Workflow Execution Results:")
        print(f"Status: {result['status']}")
        print(f"Mode: {mode_name}")

        if result["status"] == "success":
            print(f"Code Directory: {result['code_directory']}")
            print(f"MCP Architecture: {result.get('mcp_architecture', 'unknown')}")
            print("Execution completed!")
        else:
            print(f"Error Message: {result['message']}")

        print("=" * 60)
        print(
            "‚úÖ Using Standard MCP Architecture with Memory Agent + Code Reference Indexer"
        )

    else:
        print("\n" + "=" * 60)
        print("‚ùå Code Reference Indexer Integration Test FAILED!")
        print("Please check the configuration and try again.")
        print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())


--- workflows/code_implementation_workflow_index.py ---
"""
Paper Code Implementation Workflow - MCP-compliant Iterative Development

Features:
1. File Tree Creation
2. Code Implementation - Based on aisi-basic-agent iterative development

MCP Architecture:
- MCP Server: tools/code_implementation_server.py
- MCP Client: Called through mcp_agent framework
- Configuration: mcp_agent.config.yaml
"""

import asyncio
import json
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, Any, Optional, List

# MCP Agent imports
from mcp_agent.agents.agent import Agent

# Local imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from prompts.code_prompts import STRUCTURE_GENERATOR_PROMPT
from prompts.code_prompts import (
    PURE_CODE_IMPLEMENTATION_SYSTEM_PROMPT_INDEX,
)
from workflows.agents import CodeImplementationAgent
from workflows.agents.memory_agent_concise import ConciseMemoryAgent
from config.mcp_tool_definitions_index import get_mcp_tools
from utils.llm_utils import get_preferred_llm_class, get_default_models, load_api_config
# DialogueLogger removed - no longer needed


class CodeImplementationWorkflowWithIndex:
    """
    Paper Code Implementation Workflow Manager with Code Reference Indexer

    Uses standard MCP architecture with enhanced indexing capabilities:
    1. Connect to code-implementation server via MCP client
    2. Use MCP protocol for tool calls
    3. Support workspace management and operation history tracking
    4. Integrated code reference indexer for enhanced code understanding
    """

    # ==================== 1. Class Initialization and Configuration (Infrastructure Layer) ====================

    def __init__(self, config_path: str = "mcp_agent.secrets.yaml"):
        """Initialize workflow with configuration"""
        self.config_path = config_path
        # Derive main config path from secrets path (same directory)
        secrets_dir = os.path.dirname(os.path.abspath(config_path))
        self.main_config_path = os.path.join(secrets_dir, "mcp_agent.config.yaml")
        self.api_config = self._load_api_config()
        self.default_models = get_default_models(self.main_config_path)
        self.logger = self._create_logger()
        self.mcp_agent = None
        self.enable_read_tools = (
            True  # Default value, will be overridden by run_workflow parameter
        )

    def _load_api_config(self) -> Dict[str, Any]:
        """Load API configuration with environment variable override."""
        try:
            return load_api_config(self.config_path)
        except Exception as e:
            raise Exception(f"Failed to load API config: {e}")

    def _create_logger(self) -> logging.Logger:
        """Create and configure logger"""
        logger = logging.getLogger(__name__)
        # Don't add handlers to child loggers - let them propagate to root
        logger.setLevel(logging.INFO)
        return logger

    def _read_plan_file(self, plan_file_path: str) -> str:
        """Read implementation plan file"""
        plan_path = Path(plan_file_path)
        if not plan_path.exists():
            raise FileNotFoundError(
                f"Implementation plan file not found: {plan_file_path}"
            )

        with open(plan_path, "r", encoding="utf-8") as f:
            return f.read()

    def _check_file_tree_exists(self, target_directory: str) -> bool:
        """Check if file tree structure already exists"""
        code_directory = os.path.join(target_directory, "generate_code")
        return os.path.exists(code_directory) and len(os.listdir(code_directory)) > 0

    # ==================== 2. Public Interface Methods (External API Layer) ====================

    async def run_workflow(
        self,
        plan_file_path: str,
        target_directory: Optional[str] = None,
        pure_code_mode: bool = False,
        enable_read_tools: bool = True,
    ):
        """Run complete workflow - Main public interface"""
        # Set the read tools configuration
        self.enable_read_tools = enable_read_tools

        try:
            plan_content = self._read_plan_file(plan_file_path)

            if target_directory is None:
                target_directory = str(Path(plan_file_path).parent)

            # Calculate code directory for workspace alignment
            code_directory = os.path.join(target_directory, "generate_code")

            self.logger.info("=" * 80)
            self.logger.info("üöÄ STARTING CODE IMPLEMENTATION WORKFLOW")
            self.logger.info("=" * 80)
            self.logger.info(f"üìÑ Plan file: {plan_file_path}")
            self.logger.info(f"üìÇ Plan file parent: {target_directory}")
            self.logger.info(f"üéØ Code directory (MCP workspace): {code_directory}")
            self.logger.info(
                f"‚öôÔ∏è  Read tools: {'ENABLED' if self.enable_read_tools else 'DISABLED'}"
            )
            self.logger.info("=" * 80)

            results = {}

            # Check if file tree exists
            if self._check_file_tree_exists(target_directory):
                self.logger.info("File tree exists, skipping creation")
                results["file_tree"] = "Already exists, skipped creation"
            else:
                self.logger.info("Creating file tree...")
                results["file_tree"] = await self.create_file_structure(
                    plan_content, target_directory
                )

            # Code implementation
            if pure_code_mode:
                self.logger.info("Starting pure code implementation...")
                results["code_implementation"] = await self.implement_code_pure(
                    plan_content, target_directory, code_directory
                )
            else:
                pass

            self.logger.info("Workflow execution successful")

            return {
                "status": "success",
                "plan_file": plan_file_path,
                "target_directory": target_directory,
                "code_directory": os.path.join(target_directory, "generate_code"),
                "results": results,
                "mcp_architecture": "standard",
            }

        except Exception as e:
            self.logger.error(f"Workflow execution failed: {e}")

            return {"status": "error", "message": str(e), "plan_file": plan_file_path}
        finally:
            await self._cleanup_mcp_agent()

    async def create_file_structure(
        self, plan_content: str, target_directory: str
    ) -> str:
        """Create file tree structure based on implementation plan"""
        self.logger.info("Starting file tree creation...")

        structure_agent = Agent(
            name="StructureGeneratorAgent",
            instruction=STRUCTURE_GENERATOR_PROMPT,
            server_names=["command-executor"],
        )

        async with structure_agent:
            creator = await structure_agent.attach_llm(
                get_preferred_llm_class(self.config_path)
            )

            message = f"""Analyze the following implementation plan and generate shell commands to create the file tree structure.

Target Directory: {target_directory}/generate_code

Implementation Plan:
{plan_content}

Tasks:
1. Find the file tree structure in the implementation plan
2. Generate shell commands (mkdir -p, touch) to create that structure
3. Use the execute_commands tool to run the commands and create the file structure

Requirements:
- Use mkdir -p to create directories
- Use touch to create files
- Include __init__.py file for Python packages
- Use relative paths to the target directory
- Execute commands to actually create the file structure"""

            result = await creator.generate_str(message=message)
            self.logger.info("File tree structure creation completed")
            return result

    async def implement_code_pure(
        self, plan_content: str, target_directory: str, code_directory: str = None
    ) -> str:
        """Pure code implementation - focus on code writing without testing"""
        self.logger.info("Starting pure code implementation (no testing)...")

        # Use provided code_directory or calculate it (for backwards compatibility)
        if code_directory is None:
            code_directory = os.path.join(target_directory, "generate_code")

        self.logger.info(f"üéØ Using code directory (MCP workspace): {code_directory}")

        if not os.path.exists(code_directory):
            self.logger.warning(
                f"Code directory does not exist, creating it: {code_directory}"
            )
            os.makedirs(code_directory, exist_ok=True)
            self.logger.info(f"‚úÖ Code directory created: {code_directory}")

        try:
            client, client_type = await self._initialize_llm_client()
            await self._initialize_mcp_agent(code_directory)

            tools = self._prepare_mcp_tool_definitions()
            system_message = PURE_CODE_IMPLEMENTATION_SYSTEM_PROMPT_INDEX
            messages = []

            #             implementation_message = f"""**TASK: Implement Research Paper Reproduction Code**

            # You are implementing a complete, working codebase that reproduces the core algorithms, experiments, and methods described in a research paper. Your goal is to create functional code that can replicate the paper's key results and contributions.

            # **What you need to do:**
            # - Analyze the paper content and reproduction plan to understand requirements
            # - Implement all core algorithms mentioned in the main body of the paper
            # - Create the necessary components following the planned architecture
            # - Test each component to ensure functionality
            # - Integrate components into a cohesive, executable system
            # - Focus on reproducing main contributions rather than appendix-only experiments

            # **RESOURCES:**
            # - **Paper & Reproduction Plan**: `{target_directory}/` (contains .md paper files and initial_plan.txt with detailed implementation guidance)
            # - **Reference Code Indexes**: `{target_directory}/indexes/` (JSON files with implementation patterns from related codebases)
            # - **Implementation Directory**: `{code_directory}/` (your working directory for all code files)

            # **CURRENT OBJECTIVE:**
            # Start by reading the reproduction plan (`{target_directory}/initial_plan.txt`) to understand the implementation strategy, then examine the paper content to identify the first priority component to implement. Use the search_code tool to find relevant reference implementations from the indexes directory (`{target_directory}/indexes/*.json`) before coding.

            # ---
            # **START:** Review the plan above and begin implementation."""
            implementation_message = f"""**Task: Implement code based on the following reproduction plan**

**Code Reproduction Plan:**
{plan_content}

**Working Directory:** {code_directory}

**Current Objective:** Begin implementation by analyzing the plan structure, examining the current project layout, and implementing the first foundation file according to the plan's priority order."""

            messages.append({"role": "user", "content": implementation_message})

            result = await self._pure_code_implementation_loop(
                client,
                client_type,
                system_message,
                messages,
                tools,
                plan_content,
                target_directory,
            )

            return result

        finally:
            await self._cleanup_mcp_agent()

    # ==================== 3. Core Business Logic (Implementation Layer) ====================

    async def _pure_code_implementation_loop(
        self,
        client,
        client_type,
        system_message,
        messages,
        tools,
        plan_content,
        target_directory,
    ):
        """Pure code implementation loop with memory optimization and phase consistency"""
        max_iterations = 800
        iteration = 0
        start_time = time.time()
        max_time = 7200  # 120 minutes (2 hours)

        # Initialize specialized agents
        code_agent = CodeImplementationAgent(
            self.mcp_agent, self.logger, self.enable_read_tools
        )

        # Pass code_directory to memory agent for file extraction
        code_directory = os.path.join(target_directory, "generate_code")
        memory_agent = ConciseMemoryAgent(
            plan_content,
            self.logger,
            target_directory,
            self.default_models,
            code_directory,
        )

        # Log read tools configuration
        read_tools_status = "ENABLED" if self.enable_read_tools else "DISABLED"
        self.logger.info(
            f"üîß Read tools (read_file, read_code_mem): {read_tools_status}"
        )
        if not self.enable_read_tools:
            self.logger.info(
                "üö´ No read mode: read_file and read_code_mem tools will be skipped"
            )

        # Connect code agent with memory agent for summary generation
        # Note: Concise memory agent doesn't need LLM client for summary generation
        code_agent.set_memory_agent(memory_agent, client, client_type)

        # Initialize memory agent with iteration 0
        memory_agent.start_new_round(iteration=0)

        while iteration < max_iterations:
            iteration += 1
            elapsed_time = time.time() - start_time

            if elapsed_time > max_time:
                self.logger.warning(f"Time limit reached: {elapsed_time:.2f}s")
                break

            # # Test simplified memory approach if we have files implemented
            # if iteration == 5 and code_agent.get_files_implemented_count() > 0:
            #     self.logger.info("üß™ Testing simplified memory approach...")
            #     test_results = await memory_agent.test_simplified_memory_approach()
            #     self.logger.info(f"Memory test results: {test_results}")

            # self.logger.info(f"Pure code implementation iteration {iteration}: generating code")

            messages = self._validate_messages(messages)
            current_system_message = code_agent.get_system_prompt()

            # Round logging removed

            # Call LLM
            response = await self._call_llm_with_tools(
                client, client_type, current_system_message, messages, tools
            )

            response_content = response.get("content", "").strip()
            if not response_content:
                response_content = "Continue implementing code files..."

            messages.append({"role": "assistant", "content": response_content})

            # Handle tool calls
            if response.get("tool_calls"):
                tool_results = await code_agent.execute_tool_calls(
                    response["tool_calls"]
                )

                # Record essential tool results in concise memory agent
                for tool_call, tool_result in zip(response["tool_calls"], tool_results):
                    memory_agent.record_tool_result(
                        tool_name=tool_call["name"],
                        tool_input=tool_call["input"],
                        tool_result=tool_result.get("result"),
                    )

                # NEW LOGIC: Check if write_file was called and trigger memory optimization immediately

                # Determine guidance based on results
                has_error = self._check_tool_results_for_errors(tool_results)
                files_count = code_agent.get_files_implemented_count()

                if has_error:
                    guidance = self._generate_error_guidance()
                else:
                    guidance = self._generate_success_guidance(files_count)

                compiled_response = self._compile_user_response(tool_results, guidance)
                messages.append({"role": "user", "content": compiled_response})

                # NEW LOGIC: Apply memory optimization immediately after write_file detection
                if memory_agent.should_trigger_memory_optimization(
                    messages, code_agent.get_files_implemented_count()
                ):
                    # Memory optimization triggered

                    # Apply concise memory optimization
                    files_implemented_count = code_agent.get_files_implemented_count()
                    current_system_message = code_agent.get_system_prompt()
                    messages = memory_agent.apply_memory_optimization(
                        current_system_message, messages, files_implemented_count
                    )

                    # Memory optimization completed

            else:
                files_count = code_agent.get_files_implemented_count()
                no_tools_guidance = self._generate_no_tools_guidance(files_count)
                messages.append({"role": "user", "content": no_tools_guidance})

            # Check for analysis loop and provide corrective guidance
            # if code_agent.is_in_analysis_loop():
            #     analysis_loop_guidance = code_agent.get_analysis_loop_guidance()
            #     messages.append({"role": "user", "content": analysis_loop_guidance})
            #     self.logger.warning(
            #         "Analysis loop detected and corrective guidance provided"
            #     )

            # Record file implementations in memory agent (for the current round)
            for file_info in code_agent.get_implementation_summary()["completed_files"]:
                memory_agent.record_file_implementation(file_info["file"])

            # REMOVED: Old memory optimization logic - now happens immediately after write_file
            # Memory optimization is now triggered immediately after write_file detection

            # Start new round for next iteration, sync with workflow iteration
            memory_agent.start_new_round(iteration=iteration)

            # Check completion based on actual unimplemented files list
            unimplemented_files = memory_agent.get_unimplemented_files()
            if not unimplemented_files:  # Empty list means all files implemented
                self.logger.info(
                    "‚úÖ Code implementation complete - All files implemented"
                )
                break

            # Emergency trim if too long
            if len(messages) > 50:
                self.logger.warning(
                    "Emergency message trim - applying concise memory optimization"
                )

                current_system_message = code_agent.get_system_prompt()
                files_implemented_count = code_agent.get_files_implemented_count()
                messages = memory_agent.apply_memory_optimization(
                    current_system_message, messages, files_implemented_count
                )

        return await self._generate_pure_code_final_report_with_concise_agents(
            iteration, time.time() - start_time, code_agent, memory_agent
        )

    # ==================== 4. MCP Agent and LLM Communication Management (Communication Layer) ====================

    async def _initialize_mcp_agent(self, code_directory: str):
        """Initialize MCP agent and connect to code-implementation server"""
        try:
            self.mcp_agent = Agent(
                name="CodeImplementationAgent",
                instruction="You are a code implementation assistant, using MCP tools to implement paper code replication.",
                server_names=["code-implementation", "code-reference-indexer"],
            )

            await self.mcp_agent.__aenter__()
            llm = await self.mcp_agent.attach_llm(
                get_preferred_llm_class(self.config_path)
            )

            # Set workspace to the target code directory
            workspace_result = await self.mcp_agent.call_tool(
                "set_workspace", {"workspace_path": code_directory}
            )
            self.logger.info(f"Workspace setup result: {workspace_result}")

            return llm

        except Exception as e:
            self.logger.error(f"Failed to initialize MCP agent: {e}")
            if self.mcp_agent:
                try:
                    await self.mcp_agent.__aexit__(None, None, None)
                except Exception:
                    pass
                self.mcp_agent = None
            raise

    async def _cleanup_mcp_agent(self):
        """Clean up MCP agent resources"""
        if self.mcp_agent:
            try:
                await self.mcp_agent.__aexit__(None, None, None)
                self.logger.info("MCP agent connection closed")
            except Exception as e:
                self.logger.warning(f"Error closing MCP agent: {e}")
            finally:
                self.mcp_agent = None

    async def _initialize_llm_client(self):
        """Initialize LLM client based on llm_provider preference and API key availability"""
        # Get API keys
        anthropic_key = self.api_config.get("anthropic", {}).get("api_key", "")
        openai_key = self.api_config.get("openai", {}).get("api_key", "")
        google_key = self.api_config.get("google", {}).get("api_key", "")

        # Read user preference from main config
        preferred_provider = None
        try:
            import yaml

            # Derive config path from secrets path (same directory)
            secrets_dir = os.path.dirname(os.path.abspath(self.config_path))
            config_path = os.path.join(secrets_dir, "mcp_agent.config.yaml")
            if os.path.exists(config_path):
                with open(config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                    preferred_provider = config.get("llm_provider", "").strip().lower()
        except Exception as e:
            self.logger.warning(f"Could not read llm_provider preference: {e}")

        # Define provider initialization functions
        async def init_anthropic():
            if not (anthropic_key and anthropic_key.strip()):
                return None
            try:
                from anthropic import AsyncAnthropic

                client = AsyncAnthropic(api_key=anthropic_key)
                await client.messages.create(
                    model=self.default_models["anthropic"],
                    max_tokens=20,
                    messages=[{"role": "user", "content": "test"}],
                )
                self.logger.info(
                    f"Using Anthropic API with model: {self.default_models['anthropic']}"
                )
                return client, "anthropic"
            except Exception as e:
                self.logger.warning(f"Anthropic API unavailable: {e}")
                return None

        async def init_google():
            if not (google_key and google_key.strip()):
                return None
            try:
                from google import genai

                client = genai.Client(api_key=google_key)
                try:
                    test_response = await client.aio.models.generate_content(
                        model=self.default_models.get("google", "gemini-2.0-flash"),
                        contents="test",
                    )

                    self.logger.info(
                        "Google API connection successful: " + str(test_response)
                    )
                except Exception as test_err:
                    self.logger.warning(
                        f"Could not test Google API: {test_err}, but will try to use client"
                    )

                self.logger.info(
                    f"Using Google API with model: {self.default_models.get('google', 'gemini-2.0-flash')}"
                )
                return client, "google"
            except Exception as e:
                self.logger.warning(f"Google API unavailable: {e}")
                return None

        async def init_openai():
            if not (openai_key and openai_key.strip()):
                return None
            try:
                from openai import AsyncOpenAI

                openai_config = self.api_config.get("openai", {})
                base_url = openai_config.get("base_url")

                if base_url:
                    client = AsyncOpenAI(api_key=openai_key, base_url=base_url)
                else:
                    client = AsyncOpenAI(api_key=openai_key)

                model_name = self.default_models.get("openai", "o3-mini")

                try:
                    await client.chat.completions.create(
                        model=model_name,
                        max_tokens=20,
                        messages=[{"role": "user", "content": "test"}],
                    )
                except Exception as e:
                    if "max_tokens" in str(e) and "max_completion_tokens" in str(e):
                        self.logger.info(
                            f"Model {model_name} requires max_completion_tokens parameter"
                        )
                        await client.chat.completions.create(
                            model=model_name,
                            max_completion_tokens=20,
                            messages=[{"role": "user", "content": "test"}],
                        )
                    else:
                        raise
                self.logger.info(f"Using OpenAI API with model: {model_name}")
                if base_url:
                    self.logger.info(f"Using custom base URL: {base_url}")
                return client, "openai"
            except Exception as e:
                self.logger.warning(f"OpenAI API unavailable: {e}")
                return None

        # Map providers to their init functions
        provider_init_map = {
            "anthropic": init_anthropic,
            "google": init_google,
            "openai": init_openai,
        }

        # Try preferred provider first
        if preferred_provider and preferred_provider in provider_init_map:
            self.logger.info(f"üéØ Trying preferred provider: {preferred_provider}")
            result = await provider_init_map[preferred_provider]()
            if result:
                return result
            else:
                self.logger.warning(
                    f"‚ö†Ô∏è Preferred provider '{preferred_provider}' unavailable, trying alternatives..."
                )

        # Fallback: try providers in order
        for provider_name, init_func in provider_init_map.items():
            if provider_name == preferred_provider:
                continue  # Already tried
            result = await init_func()
            if result:
                return result

        raise ValueError(
            "No available LLM API - please check your API keys in configuration"
        )

    async def _call_llm_with_tools(
        self, client, client_type, system_message, messages, tools, max_tokens=8192
    ):
        """Call LLM with tools"""
        try:
            if client_type == "anthropic":
                return await self._call_anthropic_with_tools(
                    client, system_message, messages, tools, max_tokens
                )
            elif client_type == "openai":
                return await self._call_openai_with_tools(
                    client, system_message, messages, tools, max_tokens
                )
            elif client_type == "google":
                return await self._call_google_with_tools(
                    client, system_message, messages, tools, max_tokens
                )
            else:
                raise ValueError(f"Unsupported client type: {client_type}")
        except Exception as e:
            self.logger.error(f"LLM call failed: {e}")
            raise

    async def _call_anthropic_with_tools(
        self, client, system_message, messages, tools, max_tokens
    ):
        """Call Anthropic API"""
        validated_messages = self._validate_messages(messages)
        if not validated_messages:
            validated_messages = [
                {"role": "user", "content": "Please continue implementing code"}
            ]

        try:
            # Use implementation-specific model for code generation
            impl_model = self.default_models.get(
                "anthropic_implementation", self.default_models["anthropic"]
            )
            self.logger.info(f"üîß Code generation using model: {impl_model}")
            response = await client.messages.create(
                model=impl_model,
                system=system_message,
                messages=validated_messages,
                tools=tools,
                max_tokens=max_tokens,
                temperature=0.2,
            )
        except Exception as e:
            self.logger.error(f"Anthropic API call failed: {e}")
            raise

        content = ""
        tool_calls = []

        for block in response.content:
            if block.type == "text":
                content += block.text
            elif block.type == "tool_use":
                tool_calls.append(
                    {"id": block.id, "name": block.name, "input": block.input}
                )

        return {"content": content, "tool_calls": tool_calls}

    async def _call_google_with_tools(
        self, client, system_message, messages, tools, max_tokens
    ):
        """
        Call Google Gemini API with tools

        Note: Google Gemini uses a completely different API structure.
        The client here is expected to be google.genai.Client from google-genai SDK.

        Reference: https://ai.google.dev/gemini-api/docs/function-calling
        """
        try:
            from google.genai import types
        except ImportError:
            raise ImportError("google-genai package is required for Google API calls")

        validated_messages = self._validate_messages(messages)
        if not validated_messages:
            validated_messages = [
                {"role": "user", "content": "Please continue implementing code"}
            ]

        # Convert messages to Google Gemini format (types.Content)
        # Gemini expects: role="user" or role="model" (not "assistant")
        gemini_messages = []
        for msg in validated_messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            # Convert role names: "assistant" -> "model"
            if role == "assistant":
                role = "model"
            elif role not in ["user", "model"]:
                # Skip unsupported roles or convert to user
                role = "user"

            gemini_messages.append(
                types.Content(role=role, parts=[types.Part.from_text(text=content)])
            )

        # Convert tools to Google Gemini format (types.Tool with FunctionDeclaration)
        # Following the EXACT pattern from GoogleAugmentedLLM line 92-103
        # IMPORTANT: Each tool should be wrapped in its own Tool object!
        gemini_tools = []
        if tools:
            for tool in tools:
                # Transform the input_schema to be Gemini-compatible
                parameters = self._transform_schema_for_gemini(tool["input_schema"])

                # Each tool gets its own Tool wrapper (not all in one!)
                gemini_tools.append(
                    types.Tool(
                        function_declarations=[
                            types.FunctionDeclaration(
                                name=tool["name"],
                                description=tool["description"],
                                parameters=parameters,
                            )
                        ]
                    )
                )

        # Create config with system instruction and tools
        config = types.GenerateContentConfig(
            max_output_tokens=max_tokens,
            temperature=0.2,
            system_instruction=system_message if system_message else None,
            tools=gemini_tools if gemini_tools else None,
            # Disable automatic function calling - we handle it manually
            automatic_function_calling=types.AutomaticFunctionCallingConfig(
                disable=True
            ),
        )

        try:
            # Google Gemini API call using the native SDK
            # client is google.genai.Client instance
            # Use implementation-specific model for code generation
            impl_model = self.default_models.get(
                "google_implementation", self.default_models["google"]
            )
            self.logger.info(f"üîß Code generation using model: {impl_model}")
            response = await client.aio.models.generate_content(
                model=impl_model,
                contents=gemini_messages,
                config=config,
            )
        except Exception as e:
            self.logger.error(f"Google API call failed: {e}")
            raise

        # Parse Gemini response (types.GenerateContentResponse)
        # Following the pattern from augmented_llm_google.py lines 145-165
        content = ""
        tool_calls = []

        if response and hasattr(response, "candidates") and response.candidates:
            candidate = response.candidates[0]

            if hasattr(candidate, "content") and candidate.content:
                if hasattr(candidate.content, "parts") and candidate.content.parts:
                    for part in candidate.content.parts:
                        # Handle text content
                        if hasattr(part, "text") and part.text:
                            content += part.text

                        # Handle function calls
                        # Check for function_call attribute, matching augmented_llm_google.py line 164
                        if hasattr(part, "function_call") and part.function_call:
                            fc = part.function_call
                            # Extract function call details
                            # Note: Gemini function_call has name and args attributes
                            tool_call = {
                                "id": getattr(
                                    fc, "id", getattr(fc, "name", "")
                                ),  # Use name as fallback for id
                                "name": fc.name if hasattr(fc, "name") else "",
                                "input": dict(fc.args)
                                if hasattr(fc, "args") and fc.args
                                else {},
                            }
                            self.logger.debug(
                                f"Google function_call parsed: {tool_call}"
                            )
                            tool_calls.append(tool_call)

        return {"content": content, "tool_calls": tool_calls}

    def _transform_schema_for_gemini(self, schema: dict) -> dict:
        """
        Transform JSON Schema to OpenAPI Schema format compatible with Gemini.

        This is based on the transform_mcp_tool_schema from GoogleAugmentedLLM.
        Key transformations:
        1. Convert camelCase to snake_case
        2. Remove unsupported fields (default, additionalProperties)
        3. Handle nullable types via anyOf
        """
        if not isinstance(schema, dict):
            return schema

        # Fields to exclude
        EXCLUDED_PROPERTIES = {"default", "additionalProperties"}

        # camelCase to snake_case mappings
        CAMEL_TO_SNAKE = {
            "anyOf": "any_of",
            "maxLength": "max_length",
            "minLength": "min_length",
            "minProperties": "min_properties",
            "maxProperties": "max_properties",
            "maxItems": "max_items",
            "minItems": "min_items",
        }

        result = {}

        for key, value in schema.items():
            # Skip excluded properties
            if key in EXCLUDED_PROPERTIES:
                continue

            # Convert camelCase to snake_case
            snake_key = CAMEL_TO_SNAKE.get(key, key)

            # Handle nested structures
            if key == "properties" and isinstance(value, dict):
                result[snake_key] = {
                    prop_k: self._transform_schema_for_gemini(prop_v)
                    for prop_k, prop_v in value.items()
                }
            elif key == "items" and isinstance(value, dict):
                result[snake_key] = self._transform_schema_for_gemini(value)
            elif key == "anyOf" and isinstance(value, list):
                # Handle nullable types (Type | None)
                has_null = any(
                    isinstance(item, dict) and item.get("type") == "null"
                    for item in value
                )
                if has_null:
                    result["nullable"] = True

                # Get first non-null schema
                for item in value:
                    if isinstance(item, dict) and item.get("type") != "null":
                        transformed = self._transform_schema_for_gemini(item)
                        for k, v in transformed.items():
                            if k not in result:
                                result[k] = v
                        break
            else:
                result[snake_key] = value

        return result

    def _repair_truncated_json(self, json_str: str, tool_name: str = "") -> dict:
        """
        Advanced JSON repair for truncated or malformed JSON from LLM responses.

        Handles:
        - Missing closing braces/brackets
        - Truncated string values
        - Missing required fields
        - Trailing commas
        """
        import re

        # Step 1: Try basic fixes first
        fixed = json_str.strip()

        # Remove trailing commas
        fixed = re.sub(r",\s*}", "}", fixed)
        fixed = re.sub(r",\s*]", "]", fixed)

        try:
            return json.loads(fixed)
        except json.JSONDecodeError as e:
            print("   üîß Attempting advanced JSON repair...")

            # Step 2: Check for truncation issues
            if e.msg == "Expecting value":
                # Likely truncated - try to close open structures
                fixed = self._close_json_structures(fixed)
                try:
                    return json.loads(fixed)
                except (json.JSONDecodeError, ValueError, TypeError):
                    pass

            # Step 3: Try to extract partial valid JSON
            if e.msg.startswith("Expecting") and e.pos:
                # Truncate at error position and try to close
                truncated = fixed[: e.pos]
                closed = self._close_json_structures(truncated)
                try:
                    partial = json.loads(closed)
                    print("   ‚úÖ Extracted partial JSON successfully")
                    return partial
                except (json.JSONDecodeError, ValueError, TypeError):
                    pass

            # Step 4: Tool-specific defaults for critical tools
            if tool_name == "write_file":
                # For write_file, try to extract at least file_path
                file_path_match = re.search(r'"file_path"\s*:\s*"([^"]*)"', fixed)
                if file_path_match:
                    print("   ‚ö†Ô∏è  write_file JSON truncated, using minimal structure")
                    return {
                        "file_path": file_path_match.group(1),
                        "content": "",  # Empty content is better than crashing
                    }

            # Step 5: Last resort - return error indicator
            print("   ‚ùå JSON repair failed completely")
            return None

    def _close_json_structures(self, json_str: str) -> str:
        """
        Intelligently close unclosed JSON structures.
        Counts braces and brackets to determine what needs closing.
        """
        # Count open structures
        open_braces = json_str.count("{") - json_str.count("}")
        open_brackets = json_str.count("[") - json_str.count("]")

        # Check if we're in the middle of a string
        quote_count = json_str.count('"')
        in_string = (quote_count % 2) != 0

        result = json_str

        # Close string if needed
        if in_string:
            result += '"'

        # Close brackets first (inner structures)
        result += "]" * open_brackets

        # Close braces
        result += "}" * open_braces

        return result

    async def _call_openai_with_tools(
        self, client, system_message, messages, tools, max_tokens
    ):
        """Call OpenAI API with robust JSON error handling and retry mechanism"""
        openai_tools = []
        for tool in tools:
            openai_tools.append(
                {
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool["description"],
                        "parameters": tool["input_schema"],
                    },
                }
            )

        openai_messages = [{"role": "system", "content": system_message}]
        openai_messages.extend(messages)

        # Retry mechanism for API calls
        max_retries = 3
        retry_delay = 2  # seconds

        # Use implementation-specific model for code generation
        impl_model = self.default_models.get(
            "openai_implementation", self.default_models["openai"]
        )
        self.logger.info(f"üîß Code generation using model: {impl_model}")

        for attempt in range(max_retries):
            try:
                # Try max_tokens first, fallback to max_completion_tokens if unsupported
                try:
                    response = await client.chat.completions.create(
                        model=impl_model,
                        messages=openai_messages,
                        tools=openai_tools if openai_tools else None,
                        max_tokens=max_tokens,
                        temperature=0.2,
                    )
                except Exception as e:
                    if "max_tokens" in str(e) and "max_completion_tokens" in str(e):
                        # Retry with max_completion_tokens for models that require it
                        response = await client.chat.completions.create(
                            model=impl_model,
                            messages=openai_messages,
                            tools=openai_tools if openai_tools else None,
                            max_completion_tokens=max_tokens,
                        )
                    else:
                        raise

                # Validate response structure
                if (
                    not response
                    or not hasattr(response, "choices")
                    or not response.choices
                ):
                    raise ValueError("Invalid API response: missing choices")

                if not response.choices[0] or not hasattr(
                    response.choices[0], "message"
                ):
                    raise ValueError("Invalid API response: missing message in choice")

                message = response.choices[0].message
                content = message.content or ""

                # Successfully got a valid response
                break

            except json.JSONDecodeError as e:
                print(
                    f"\n‚ùå JSON Decode Error in API response (attempt {attempt + 1}/{max_retries}):"
                )
                print(f"   Error: {e}")
                print(f"   Position: line {e.lineno}, column {e.colno}")

                if attempt < max_retries - 1:
                    print(f"   ‚è≥ Retrying in {retry_delay} seconds...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    print("   ‚ùå All retries exhausted")
                    raise

            except (ValueError, AttributeError, TypeError) as e:
                print(f"\n‚ùå API Response Error (attempt {attempt + 1}/{max_retries}):")
                print(f"   Error type: {type(e).__name__}")
                print(f"   Error: {e}")

                if attempt < max_retries - 1:
                    print(f"   ‚è≥ Retrying in {retry_delay} seconds...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    print("   ‚ùå All retries exhausted")
                    # Return empty response instead of crashing
                    return {
                        "content": "API error - unable to get valid response",
                        "tool_calls": [],
                    }

            except Exception as e:
                print(
                    f"\n‚ùå Unexpected API Error (attempt {attempt + 1}/{max_retries}):"
                )
                print(f"   Error type: {type(e).__name__}")
                print(f"   Error: {e}")

                if attempt < max_retries - 1:
                    print(f"   ‚è≥ Retrying in {retry_delay} seconds...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    print("   ‚ùå All retries exhausted")
                    raise

        tool_calls = []
        if message.tool_calls:
            for tool_call in message.tool_calls:
                try:
                    # Attempt to parse tool call arguments
                    parsed_input = json.loads(tool_call.function.arguments)
                    tool_calls.append(
                        {
                            "id": tool_call.id,
                            "name": tool_call.function.name,
                            "input": parsed_input,
                        }
                    )
                except json.JSONDecodeError as e:
                    # Detailed JSON parsing error logging
                    print("\n‚ùå JSON Parsing Error in tool call:")
                    print(f"   Tool: {tool_call.function.name}")
                    print(f"   Error: {e}")
                    print("   Raw arguments (first 500 chars):")
                    print(f"   {tool_call.function.arguments[:500]}")
                    print(f"   Error position: line {e.lineno}, column {e.colno}")
                    print(
                        f"   Problem at: ...{tool_call.function.arguments[max(0, e.pos-50):e.pos+50]}..."
                    )

                    # Attempt advanced JSON repair
                    repaired = self._repair_truncated_json(
                        tool_call.function.arguments, tool_call.function.name
                    )

                    if repaired:
                        print("   ‚úÖ JSON repaired successfully")
                        tool_calls.append(
                            {
                                "id": tool_call.id,
                                "name": tool_call.function.name,
                                "input": repaired,
                            }
                        )
                    else:
                        # Skip this tool call if repair failed
                        print("   ‚ö†Ô∏è  Skipping unrepairable tool call")
                        continue

        return {"content": content, "tool_calls": tool_calls}

    # ==================== 5. Tools and Utility Methods (Utility Layer) ====================

    def _validate_messages(self, messages: List[Dict]) -> List[Dict]:
        """Validate and clean message list"""
        valid_messages = []
        for msg in messages:
            content = msg.get("content", "").strip()
            if content:
                valid_messages.append(
                    {"role": msg.get("role", "user"), "content": content}
                )
            else:
                self.logger.warning(f"Skipping empty message: {msg}")
        return valid_messages

    def _prepare_mcp_tool_definitions(self) -> List[Dict[str, Any]]:
        """Prepare tool definitions in Anthropic API standard format with filtering"""
        # Get all available tools
        all_tools = get_mcp_tools("code_implementation")

        # Define essential tools for code implementation
        essential_tool_names = {"write_file", "search_code_references"}

        # Filter to only essential tools
        filtered_tools = [
            tool for tool in all_tools if tool.get("name") in essential_tool_names
        ]

        self.logger.info(
            f"üîß Tool filtering: {len(filtered_tools)}/{len(all_tools)} tools enabled"
        )
        self.logger.info(
            f"   Available tools: {[tool.get('name') for tool in filtered_tools]}"
        )

        return filtered_tools

        # return get_mcp_tools("code_implementation")

    def _check_tool_results_for_errors(self, tool_results: List[Dict]) -> bool:
        """Check tool results for errors with JSON repair capability"""
        for result in tool_results:
            try:
                if hasattr(result["result"], "content") and result["result"].content:
                    content_text = result["result"].content[0].text

                    # First attempt: try direct JSON parsing
                    try:
                        parsed_result = json.loads(content_text)
                        if parsed_result.get("status") == "error":
                            return True
                    except json.JSONDecodeError as e:
                        # JSON parsing failed - try to repair
                        print("\n‚ö†Ô∏è  JSON parsing failed in tool result check:")
                        print(f"   Error: {e}")
                        print(
                            f"   Position: line {e.lineno}, column {e.colno}, char {e.pos}"
                        )
                        print(f"   Content length: {len(content_text)} chars")
                        print(f"   First 300 chars: {content_text[:300]}")

                        # Attempt to repair the JSON
                        repaired = self._repair_truncated_json(content_text)
                        if repaired:
                            print("   ‚úÖ Tool result JSON repaired successfully")
                            if repaired.get("status") == "error":
                                return True
                        else:
                            # Fallback: check for "error" keyword in text
                            if "error" in content_text.lower():
                                return True

                elif isinstance(result["result"], str):
                    if "error" in result["result"].lower():
                        return True

            except (AttributeError, IndexError) as e:
                # Unexpected result structure
                print(f"\n‚ö†Ô∏è  Unexpected result structure: {type(e).__name__}: {e}")
                result_str = str(result["result"])
                if "error" in result_str.lower():
                    return True
        return False

    # ==================== 6. User Interaction and Feedback (Interaction Layer) ====================

    def _generate_success_guidance(self, files_count: int) -> str:
        """Generate concise success guidance for continuing implementation"""
        return f"""‚úÖ File implementation completed successfully!

üìä **Progress Status:** {files_count} files implemented

üéØ **Next Action:** Check if ALL files from the reproduction plan are implemented.

‚ö° **Decision Process:**
1. **If ALL files are implemented:** Use `execute_python` or `execute_bash` to test the complete implementation, then respond "**implementation complete**" to end the conversation
2. **If MORE files need implementation:** Continue with dependency-aware workflow:
   - **Start with `read_code_mem`** to understand existing implementations and dependencies
   - **Optionally use `search_code_references`** for reference patterns (OPTIONAL - use for inspiration only, original paper specs take priority)
   - **Then `write_file`** to implement the new component
   - **Finally: Test** if needed

üí° **Key Point:** Always verify completion status before continuing with new file creation."""

    def _generate_error_guidance(self) -> str:
        """Generate error guidance for handling issues"""
        return """‚ùå Error detected during file implementation.

üîß **Action Required:**
1. Review the error details above
2. Fix the identified issue
3. **Check if ALL files from the reproduction plan are implemented:**
   - **If YES:** Use `execute_python` or `execute_bash` to test the complete implementation, then respond "**implementation complete**" to end the conversation
   - **If NO:** Continue with proper development cycle for next file:
     - **Start with `read_code_mem`** to understand existing implementations
     - **Optionally use `search_code_references`** for reference patterns (OPTIONAL - for inspiration only)
     - **Then `write_file`** to implement properly
     - **Test** if needed
4. Ensure proper error handling in future implementations

üí° **Remember:** Always verify if all planned files are implemented before continuing with new file creation."""

    def _generate_no_tools_guidance(self, files_count: int) -> str:
        """Generate concise guidance when no tools are called"""
        return f"""‚ö†Ô∏è No tool calls detected in your response.

üìä **Current Progress:** {files_count} files implemented

üö® **Action Required:** You must use tools. **FIRST check if ALL files from the reproduction plan are implemented:**

‚ö° **Decision Process:**
1. **If ALL files are implemented:** Use `execute_python` or `execute_bash` to test the complete implementation, then respond "**implementation complete**" to end the conversation
2. **If MORE files need implementation:** Follow the development cycle:
   - **Start with `read_code_mem`** to understand existing implementations
   - **Optionally use `search_code_references`** for reference patterns (OPTIONAL - for inspiration only)
   - **Then `write_file`** to implement the new component
   - **Finally: Test** if needed

üö® **Critical:** Always verify completion status first, then use appropriate tools - not just explanations!"""

    def _compile_user_response(self, tool_results: List[Dict], guidance: str) -> str:
        """Compile tool results and guidance into a single user response"""
        response_parts = []

        if tool_results:
            response_parts.append("üîß **Tool Execution Results:**")
            for tool_result in tool_results:
                tool_name = tool_result["tool_name"]
                result_content = tool_result["result"]
                response_parts.append(
                    f"```\nTool: {tool_name}\nResult: {result_content}\n```"
                )

        if guidance:
            response_parts.append("\n" + guidance)

        return "\n\n".join(response_parts)

    # ==================== 7. Reporting and Output (Output Layer) ====================

    async def _generate_pure_code_final_report_with_concise_agents(
        self,
        iterations: int,
        elapsed_time: float,
        code_agent: CodeImplementationAgent,
        memory_agent: ConciseMemoryAgent,
    ):
        """Generate final report using concise agent statistics"""
        try:
            code_stats = code_agent.get_implementation_statistics()
            memory_stats = memory_agent.get_memory_statistics(
                code_stats["files_implemented_count"]
            )

            if self.mcp_agent:
                history_result = await self.mcp_agent.call_tool(
                    "get_operation_history", {"last_n": 30}
                )
                history_data = (
                    json.loads(history_result)
                    if isinstance(history_result, str)
                    else history_result
                )
            else:
                history_data = {"total_operations": 0, "history": []}

            write_operations = 0
            files_created = []
            if "history" in history_data:
                for item in history_data["history"]:
                    if item.get("action") == "write_file":
                        write_operations += 1
                        file_path = item.get("details", {}).get("file_path", "unknown")
                        files_created.append(file_path)

            report = f"""
# Pure Code Implementation Completion Report (Write-File-Based Memory Mode)

## Execution Summary
- Implementation iterations: {iterations}
- Total elapsed time: {elapsed_time:.2f} seconds
- Files implemented: {code_stats['total_files_implemented']}
- File write operations: {write_operations}
- Total MCP operations: {history_data.get('total_operations', 0)}

## Read Tools Configuration
- Read tools enabled: {code_stats['read_tools_status']['read_tools_enabled']}
- Status: {code_stats['read_tools_status']['status']}
- Tools affected: {', '.join(code_stats['read_tools_status']['tools_affected'])}

## Agent Performance
### Code Implementation Agent
- Files tracked: {code_stats['files_implemented_count']}
- Technical decisions: {code_stats['technical_decisions_count']}
- Constraints tracked: {code_stats['constraints_count']}
- Architecture notes: {code_stats['architecture_notes_count']}
- Dependency analysis performed: {code_stats['dependency_analysis_count']}
- Files read for dependencies: {code_stats['files_read_for_dependencies']}
- Last summary triggered at file count: {code_stats['last_summary_file_count']}

### Concise Memory Agent (Write-File-Based)
- Last write_file detected: {memory_stats['last_write_file_detected']}
- Should clear memory next: {memory_stats['should_clear_memory_next']}
- Files implemented count: {memory_stats['implemented_files_tracked']}
- Current round: {memory_stats['current_round']}
- Concise mode active: {memory_stats['concise_mode_active']}
- Current round tool results: {memory_stats['current_round_tool_results']}
- Essential tools recorded: {memory_stats['essential_tools_recorded']}

## Files Created
"""
            for file_path in files_created[-20:]:
                report += f"- {file_path}\n"

            if len(files_created) > 20:
                report += f"... and {len(files_created) - 20} more files\n"

            report += """
## Architecture Features
‚úÖ WRITE-FILE-BASED Memory Agent - Clear after each file generation
‚úÖ After write_file: Clear history ‚Üí Keep system prompt + initial plan + tool results
‚úÖ Tool accumulation: read_code_mem, read_file, search_reference_code until next write_file
‚úÖ Clean memory cycle: write_file ‚Üí clear ‚Üí accumulate ‚Üí write_file ‚Üí clear
‚úÖ Essential tool recording with write_file detection
‚úÖ Specialized agent separation for clean code organization
‚úÖ MCP-compliant tool execution
‚úÖ Production-grade code with comprehensive type hints
‚úÖ Intelligent dependency analysis and file reading
‚úÖ Automated read_file usage for implementation context
‚úÖ Eliminates conversation clutter between file generations
‚úÖ Focused memory for efficient next file generation
"""
            return report

        except Exception as e:
            self.logger.error(f"Failed to generate final report: {e}")
            return f"Failed to generate final report: {str(e)}"


async def main():
    """Main function for running the workflow"""
    # Configure root logger carefully to avoid duplicates
    root_logger = logging.getLogger()
    if not root_logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter("%(levelname)s:%(name)s:%(message)s")
        handler.setFormatter(formatter)
        root_logger.addHandler(handler)
        root_logger.setLevel(logging.INFO)

    workflow = CodeImplementationWorkflowWithIndex()

    print("=" * 60)
    print("Code Implementation Workflow with UNIFIED Reference Indexer")
    print("=" * 60)
    print("Select mode:")
    print("1. Test Code Reference Indexer Integration")
    print("2. Run Full Implementation Workflow")
    print("3. Run Implementation with Pure Code Mode")
    print("4. Test Read Tools Configuration")

    # mode_choice = input("Enter choice (1-4, default: 3): ").strip()

    # For testing purposes, we'll run the test first
    # if mode_choice == "4":
    #     print("Testing Read Tools Configuration...")

    #     # Create a test workflow normally
    #     test_workflow = CodeImplementationWorkflow()

    #     # Create a mock code agent for testing
    #     print("\nüß™ Testing with read tools DISABLED:")
    #     test_agent_disabled = CodeImplementationAgent(None, enable_read_tools=False)
    #     await test_agent_disabled.test_read_tools_configuration()

    #     print("\nüß™ Testing with read tools ENABLED:")
    #     test_agent_enabled = CodeImplementationAgent(None, enable_read_tools=True)
    #     await test_agent_enabled.test_read_tools_configuration()

    #     print("‚úÖ Read tools configuration testing completed!")
    #     return

    # print("Running Code Reference Indexer Integration Test...")

    test_success = True
    if test_success:
        print("\n" + "=" * 60)
        print("üéâ UNIFIED Code Reference Indexer Integration Test PASSED!")
        print("üîß Three-step process successfully merged into ONE tool")
        print("=" * 60)

        # Ask if user wants to continue with actual workflow
        print("\nContinuing with workflow execution...")

        plan_file = "/data2/bjdwhzzh/project-hku/Deepcode_collections/DeepCode/deepcode_lab/papers/54_only_code_gen/initial_plan.txt"
        # plan_file = "/data2/bjdwhzzh/project-hku/Code-Agent2.0/Code-Agent/deepcode-mcp/agent_folders/papers/1/initial_plan.txt"
        target_directory = "/data2/bjdwhzzh/project-hku/Deepcode_collections/DeepCode/deepcode_lab/papers/54_only_code_gen/"
        print("Implementation Mode Selection:")
        print("1. Pure Code Implementation Mode (Recommended)")
        print("2. Iterative Implementation Mode")

        pure_code_mode = True
        mode_name = "Pure Code Implementation Mode with Memory Agent Architecture + Code Reference Indexer"
        print(f"Using: {mode_name}")

        # Configure read tools - modify this parameter to enable/disable read tools
        enable_read_tools = (
            True  # Set to False to disable read_file and read_code_mem tools
        )
        read_tools_status = "ENABLED" if enable_read_tools else "DISABLED"
        print(f"üîß Read tools (read_file, read_code_mem): {read_tools_status}")

        # NOTE: To test without read tools, change the line above to:
        # enable_read_tools = False

        result = await workflow.run_workflow(
            plan_file,
            target_directory=target_directory,
            pure_code_mode=pure_code_mode,
            enable_read_tools=enable_read_tools,
        )

        print("=" * 60)
        print("Workflow Execution Results:")
        print(f"Status: {result['status']}")
        print(f"Mode: {mode_name}")

        if result["status"] == "success":
            print(f"Code Directory: {result['code_directory']}")
            print(f"MCP Architecture: {result.get('mcp_architecture', 'unknown')}")
            print("Execution completed!")
        else:
            print(f"Error Message: {result['message']}")

        print("=" * 60)
        print(
            "‚úÖ Using Standard MCP Architecture with Memory Agent + Code Reference Indexer"
        )

    else:
        print("\n" + "=" * 60)
        print("‚ùå Code Reference Indexer Integration Test FAILED!")
        print("Please check the configuration and try again.")
        print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())


--- workflows/codebase_index_workflow.py ---
"""
Codebase Index Workflow

This workflow integrates the functionality of run_indexer.py and code_indexer.py
to build intelligent relationships between existing codebase and target structure.

Features:
- Extract target file structure from initial_plan.txt
- Analyze codebase and build indexes
- Generate relationship mappings and statistical reports
- Provide reference basis for code reproduction
"""

import asyncio
import json
import logging
import os
import re
import sys
from pathlib import Path
from typing import Dict, Any, Optional
import yaml

# Add tools directory to path
sys.path.append(str(Path(__file__).parent.parent / "tools"))

from tools.code_indexer import CodeIndexer


class CodebaseIndexWorkflow:
    """Codebase Index Workflow Class"""

    def __init__(self, logger=None):
        """
        Initialize workflow

        Args:
            logger: Logger instance
        """
        self.logger = logger or self._setup_default_logger()
        self.indexer = None

    def _setup_default_logger(self) -> logging.Logger:
        """Setup default logger"""
        logger = logging.getLogger("CodebaseIndexWorkflow")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def extract_file_tree_from_plan(self, plan_content: str) -> Optional[str]:
        """
        Extract file tree structure from initial_plan.txt content

        Args:
            plan_content: Content of the initial_plan.txt file

        Returns:
            Extracted file tree structure as string
        """
        # Look for file structure section, specifically "## File Structure" format
        file_structure_pattern = r"## File Structure[^\n]*\n```[^\n]*\n(.*?)\n```"

        match = re.search(file_structure_pattern, plan_content, re.DOTALL)
        if match:
            file_tree = match.group(1).strip()
            lines = file_tree.split("\n")

            # Clean tree structure - remove empty lines and comments not part of structure
            cleaned_lines = []
            for line in lines:
                # Keep tree structure lines
                if line.strip() and (
                    any(char in line for char in ["‚îú‚îÄ‚îÄ", "‚îî‚îÄ‚îÄ", "‚îÇ"])
                    or line.strip().endswith("/")
                    or "." in line.split("/")[-1]  # has file extension
                    or line.strip().endswith(".py")
                    or line.strip().endswith(".txt")
                    or line.strip().endswith(".md")
                    or line.strip().endswith(".yaml")
                ):
                    cleaned_lines.append(line)

            if len(cleaned_lines) >= 5:
                file_tree = "\n".join(cleaned_lines)
                self.logger.info(
                    f"üìä Extracted file tree structure from ## File Structure section ({len(cleaned_lines)} lines)"
                )
                return file_tree

        # Fallback: look for any code block containing project structure
        code_block_patterns = [
            r"```[^\n]*\n(project/.*?(?:‚îú‚îÄ‚îÄ|‚îî‚îÄ‚îÄ).*?)\n```",
            r"```[^\n]*\n(src/.*?(?:‚îú‚îÄ‚îÄ|‚îî‚îÄ‚îÄ).*?)\n```",
            r"```[^\n]*\n(core/.*?(?:‚îú‚îÄ‚îÄ|‚îî‚îÄ‚îÄ).*?)\n```",
            r"```[^\n]*\n(.*?(?:‚îú‚îÄ‚îÄ|‚îî‚îÄ‚îÄ).*?(?:\.py|\.txt|\.md|\.yaml).*?)\n```",
        ]

        for pattern in code_block_patterns:
            match = re.search(pattern, plan_content, re.DOTALL)
            if match:
                file_tree = match.group(1).strip()
                lines = [line for line in file_tree.split("\n") if line.strip()]
                if len(lines) >= 5:
                    self.logger.info(
                        f"üìä Extracted file tree structure from code block ({len(lines)} lines)"
                    )
                    return file_tree

        # Final fallback: extract file paths from file mentions and create basic structure
        self.logger.warning(
            "‚ö†Ô∏è No standard file tree found, trying to extract from file mentions..."
        )

        # Search for file paths in backticks throughout the document
        file_mentions = re.findall(
            r"`([^`]*(?:\.py|\.txt|\.md|\.yaml|\.yml)[^`]*)`", plan_content
        )

        if file_mentions:
            # Organize files into directory structure
            dirs = set()
            files_by_dir = {}

            for file_path in file_mentions:
                file_path = file_path.strip()
                if "/" in file_path:
                    dir_path = "/".join(file_path.split("/")[:-1])
                    filename = file_path.split("/")[-1]
                    dirs.add(dir_path)
                    if dir_path not in files_by_dir:
                        files_by_dir[dir_path] = []
                    files_by_dir[dir_path].append(filename)
                else:
                    if "root" not in files_by_dir:
                        files_by_dir["root"] = []
                    files_by_dir["root"].append(file_path)

            # Create tree structure
            structure_lines = []

            # Determine root directory name from common patterns
            if any("src/" in f for f in file_mentions):
                root_name = "src"
            elif any("core/" in f for f in file_mentions):
                root_name = "core"
            elif any("lib/" in f for f in file_mentions):
                root_name = "lib"
            else:
                root_name = "project"
            structure_lines.append(f"{root_name}/")

            # Add directories and files
            sorted_dirs = sorted(dirs) if dirs else []
            for i, dir_path in enumerate(sorted_dirs):
                is_last_dir = i == len(sorted_dirs) - 1
                prefix = "‚îî‚îÄ‚îÄ" if is_last_dir else "‚îú‚îÄ‚îÄ"
                structure_lines.append(f"{prefix} {dir_path}/")

                if dir_path in files_by_dir:
                    files = sorted(files_by_dir[dir_path])
                    for j, filename in enumerate(files):
                        is_last_file = j == len(files) - 1
                        if is_last_dir:
                            file_prefix = "    ‚îî‚îÄ‚îÄ" if is_last_file else "    ‚îú‚îÄ‚îÄ"
                        else:
                            file_prefix = "‚îÇ   ‚îî‚îÄ‚îÄ" if is_last_file else "‚îÇ   ‚îú‚îÄ‚îÄ"
                        structure_lines.append(f"{file_prefix} {filename}")

            # Add root files (if any)
            if "root" in files_by_dir:
                root_files = sorted(files_by_dir["root"])
                for i, filename in enumerate(root_files):
                    is_last = (i == len(root_files) - 1) and not sorted_dirs
                    prefix = "‚îî‚îÄ‚îÄ" if is_last else "‚îú‚îÄ‚îÄ"
                    structure_lines.append(f"{prefix} {filename}")

            if len(structure_lines) >= 3:
                file_tree = "\n".join(structure_lines)
                self.logger.info(
                    f"üìä Generated file tree from file mentions ({len(structure_lines)} lines)"
                )
                return file_tree

        # If no file tree found, return None
        self.logger.warning("‚ö†Ô∏è No file tree structure found in initial plan")
        return None

    def load_target_structure_from_plan(self, plan_path: str) -> str:
        """
        Load target structure from initial_plan.txt and extract file tree

        Args:
            plan_path: Path to initial_plan.txt file

        Returns:
            Extracted file tree structure
        """
        try:
            # Load complete plan content
            with open(plan_path, "r", encoding="utf-8") as f:
                plan_content = f.read()

            self.logger.info(f"üìÑ Loaded initial plan ({len(plan_content)} characters)")

            # Extract file tree structure
            file_tree = self.extract_file_tree_from_plan(plan_content)

            if file_tree:
                self.logger.info(
                    "‚úÖ Successfully extracted file tree from initial plan"
                )
                self.logger.info("üìã Extracted structure preview:")
                # Show first few lines of extracted tree
                preview_lines = file_tree.split("\n")[:8]
                for line in preview_lines:
                    self.logger.info(f"   {line}")
                if len(file_tree.split("\n")) > 8:
                    self.logger.info(
                        f"   ... {len(file_tree.split('\n')) - 8} more lines"
                    )
                return file_tree
            else:
                self.logger.warning("‚ö†Ô∏è Unable to extract file tree from initial plan")
                self.logger.info("üîÑ Falling back to default target structure")
                return self.get_default_target_structure()

        except Exception as e:
            self.logger.error(f"‚ùå Failed to load initial plan file {plan_path}: {e}")
            self.logger.info("üîÑ Falling back to default target structure")
            return self.get_default_target_structure()

    def get_default_target_structure(self) -> str:
        """Get default target structure"""
        return """
project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gcn.py        # GCN encoder
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diffusion.py  # forward/reverse processes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ denoiser.py   # denoising MLP
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fusion.py     # fusion combiner
‚îÇ   ‚îú‚îÄ‚îÄ models/           # model wrapper classes
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recdiff.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.py       # loading & preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predictor.py  # scoring functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loss.py       # loss functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py    # NDCG, Recall etc.
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sched.py      # beta/alpha schedule utils
‚îÇ   ‚îî‚îÄ‚îÄ configs/
‚îÇ       ‚îî‚îÄ‚îÄ default.yaml  # hyperparameters, paths
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_gcn.py
‚îÇ   ‚îú‚îÄ‚îÄ test_diffusion.py
‚îÇ   ‚îú‚îÄ‚îÄ test_denoiser.py
‚îÇ   ‚îú‚îÄ‚îÄ test_loss.py
‚îÇ   ‚îî‚îÄ‚îÄ test_pipeline.py
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md
‚îÇ   ‚îú‚îÄ‚îÄ api_reference.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îú‚îÄ‚îÄ run_experiment.py
‚îÇ   ‚îî‚îÄ‚îÄ notebooks/
‚îÇ       ‚îî‚îÄ‚îÄ analysis.ipynb
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ setup.py
"""

    def load_or_create_indexer_config(self, paper_dir: str) -> Dict[str, Any]:
        """
        Load or create indexer configuration

        Args:
            paper_dir: Paper directory path

        Returns:
            Configuration dictionary
        """
        # Try to load existing configuration file
        config_path = Path(__file__).parent.parent / "tools" / "indexer_config.yaml"

        try:
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)

                # Update path configuration to current paper directory
                if "paths" not in config:
                    config["paths"] = {}
                config["paths"]["code_base_path"] = os.path.join(paper_dir, "code_base")
                config["paths"]["output_dir"] = os.path.join(paper_dir, "indexes")

                # Adjust performance settings for workflow
                if "performance" in config:
                    config["performance"]["enable_concurrent_analysis"] = (
                        False  # Disable concurrency to avoid API limits
                    )
                if "debug" in config:
                    config["debug"]["verbose_output"] = True  # Enable verbose output
                if "llm" in config:
                    config["llm"]["request_delay"] = 0.5  # Increase request delay

                self.logger.info(f"Loaded configuration file: {config_path}")
                return config

        except Exception as e:
            self.logger.warning(f"Failed to load configuration file: {e}")

        # If loading fails, use default configuration
        self.logger.info("Using default configuration")
        default_config = {
            "paths": {
                "code_base_path": os.path.join(paper_dir, "code_base"),
                "output_dir": os.path.join(paper_dir, "indexes"),
            },
            "llm": {
                "model_provider": "anthropic",
                "max_tokens": 4000,
                "temperature": 0.3,
                "request_delay": 0.5,  # Increase request delay
                "max_retries": 3,
                "retry_delay": 1.0,
            },
            "file_analysis": {
                "max_file_size": 1048576,  # 1MB
                "max_content_length": 3000,
                "supported_extensions": [
                    ".py",
                    ".js",
                    ".ts",
                    ".java",
                    ".cpp",
                    ".c",
                    ".h",
                    ".hpp",
                    ".cs",
                    ".php",
                    ".rb",
                    ".go",
                    ".rs",
                    ".scala",
                    ".kt",
                    ".yaml",
                    ".yml",
                    ".json",
                    ".xml",
                    ".toml",
                    ".md",
                    ".txt",
                ],
                "skip_directories": [
                    "__pycache__",
                    "node_modules",
                    "target",
                    "build",
                    "dist",
                    "venv",
                    "env",
                    ".git",
                    ".svn",
                    "data",
                    "datasets",
                ],
            },
            "relationships": {
                "min_confidence_score": 0.3,
                "high_confidence_threshold": 0.7,
                "relationship_types": {
                    "direct_match": 1.0,
                    "partial_match": 0.8,
                    "reference": 0.6,
                    "utility": 0.4,
                },
            },
            "performance": {
                "enable_concurrent_analysis": False,  # Disable concurrency to avoid API limits
                "max_concurrent_files": 3,
                "enable_content_caching": True,
                "max_cache_size": 100,
            },
            "debug": {
                "verbose_output": True,
                "save_raw_responses": False,
                "mock_llm_responses": False,
            },
            "output": {
                "generate_summary": True,
                "generate_statistics": True,
                "include_metadata": True,
                "json_indent": 2,
            },
            "logging": {"level": "INFO", "log_to_file": False},
        }

        return default_config

    async def run_indexing_workflow(
        self,
        paper_dir: str,
        initial_plan_path: Optional[str] = None,
        config_path: str = "mcp_agent.secrets.yaml",
    ) -> Dict[str, Any]:
        """
        Run the complete code indexing workflow

        Args:
            paper_dir: Paper directory path
            initial_plan_path: Initial plan file path (optional)
            config_path: API configuration file path

        Returns:
            Index result dictionary
        """
        try:
            self.logger.info("üöÄ Starting codebase index workflow...")

            # Step 1: Determine initial plan file path
            if not initial_plan_path:
                initial_plan_path = os.path.join(paper_dir, "initial_plan.txt")

            # Step 2: Load target structure
            if os.path.exists(initial_plan_path):
                self.logger.info(
                    f"üìê Loading target structure from {initial_plan_path}"
                )
                target_structure = self.load_target_structure_from_plan(
                    initial_plan_path
                )
            else:
                self.logger.warning(
                    f"‚ö†Ô∏è Initial plan file does not exist: {initial_plan_path}"
                )
                self.logger.info("üìê Using default target structure")
                target_structure = self.get_default_target_structure()

            # Step 3: Check codebase path
            code_base_path = os.path.join(paper_dir, "code_base")
            if not os.path.exists(code_base_path):
                self.logger.error(f"‚ùå Codebase path does not exist: {code_base_path}")
                return {
                    "status": "error",
                    "message": f"Code base path does not exist: {code_base_path}",
                    "output_files": {},
                }

            # Step 4: Create output directory
            output_dir = os.path.join(paper_dir, "indexes")
            os.makedirs(output_dir, exist_ok=True)

            # Step 5: Load configuration
            indexer_config = self.load_or_create_indexer_config(paper_dir)

            self.logger.info(f"üìÅ Codebase path: {code_base_path}")
            self.logger.info(f"üì§ Output directory: {output_dir}")

            # Step 6: Create code indexer
            self.indexer = CodeIndexer(
                code_base_path=code_base_path,
                target_structure=target_structure,
                output_dir=output_dir,
                config_path=config_path,
                enable_pre_filtering=True,
            )

            # Apply configuration settings
            self.indexer.indexer_config = indexer_config

            # Directly set configuration attributes to indexer
            if "file_analysis" in indexer_config:
                file_config = indexer_config["file_analysis"]
                self.indexer.supported_extensions = set(
                    file_config.get(
                        "supported_extensions", self.indexer.supported_extensions
                    )
                )
                self.indexer.skip_directories = set(
                    file_config.get("skip_directories", self.indexer.skip_directories)
                )
                self.indexer.max_file_size = file_config.get(
                    "max_file_size", self.indexer.max_file_size
                )
                self.indexer.max_content_length = file_config.get(
                    "max_content_length", self.indexer.max_content_length
                )

            if "llm" in indexer_config:
                llm_config = indexer_config["llm"]
                self.indexer.model_provider = llm_config.get(
                    "model_provider", self.indexer.model_provider
                )
                self.indexer.llm_max_tokens = llm_config.get(
                    "max_tokens", self.indexer.llm_max_tokens
                )
                self.indexer.llm_temperature = llm_config.get(
                    "temperature", self.indexer.llm_temperature
                )
                self.indexer.request_delay = llm_config.get(
                    "request_delay", self.indexer.request_delay
                )
                self.indexer.max_retries = llm_config.get(
                    "max_retries", self.indexer.max_retries
                )
                self.indexer.retry_delay = llm_config.get(
                    "retry_delay", self.indexer.retry_delay
                )

            if "relationships" in indexer_config:
                rel_config = indexer_config["relationships"]
                self.indexer.min_confidence_score = rel_config.get(
                    "min_confidence_score", self.indexer.min_confidence_score
                )
                self.indexer.high_confidence_threshold = rel_config.get(
                    "high_confidence_threshold", self.indexer.high_confidence_threshold
                )
                self.indexer.relationship_types = rel_config.get(
                    "relationship_types", self.indexer.relationship_types
                )

            if "performance" in indexer_config:
                perf_config = indexer_config["performance"]
                self.indexer.enable_concurrent_analysis = perf_config.get(
                    "enable_concurrent_analysis",
                    self.indexer.enable_concurrent_analysis,
                )
                self.indexer.max_concurrent_files = perf_config.get(
                    "max_concurrent_files", self.indexer.max_concurrent_files
                )
                self.indexer.enable_content_caching = perf_config.get(
                    "enable_content_caching", self.indexer.enable_content_caching
                )
                self.indexer.max_cache_size = perf_config.get(
                    "max_cache_size", self.indexer.max_cache_size
                )

            if "debug" in indexer_config:
                debug_config = indexer_config["debug"]
                self.indexer.verbose_output = debug_config.get(
                    "verbose_output", self.indexer.verbose_output
                )
                self.indexer.save_raw_responses = debug_config.get(
                    "save_raw_responses", self.indexer.save_raw_responses
                )
                self.indexer.mock_llm_responses = debug_config.get(
                    "mock_llm_responses", self.indexer.mock_llm_responses
                )

            if "output" in indexer_config:
                output_config = indexer_config["output"]
                self.indexer.generate_summary = output_config.get(
                    "generate_summary", self.indexer.generate_summary
                )
                self.indexer.generate_statistics = output_config.get(
                    "generate_statistics", self.indexer.generate_statistics
                )
                self.indexer.include_metadata = output_config.get(
                    "include_metadata", self.indexer.include_metadata
                )

            self.logger.info("üîß Indexer configuration completed")
            self.logger.info(f"ü§ñ Model provider: {self.indexer.model_provider}")
            self.logger.info(
                f"‚ö° Concurrent analysis: {'Enabled' if self.indexer.enable_concurrent_analysis else 'Disabled'}"
            )
            self.logger.info(
                f"üóÑÔ∏è Content caching: {'Enabled' if self.indexer.enable_content_caching else 'Disabled'}"
            )
            self.logger.info(
                f"üîç Pre-filtering: {'Enabled' if self.indexer.enable_pre_filtering else 'Disabled'}"
            )

            self.logger.info("=" * 60)
            self.logger.info("üöÄ Starting code indexing process...")

            # Step 7: Build all indexes
            output_files = await self.indexer.build_all_indexes()

            # Step 8: Generate summary report
            if output_files:
                summary_report = self.indexer.generate_summary_report(output_files)

                self.logger.info("=" * 60)
                self.logger.info("‚úÖ Indexing completed successfully!")
                self.logger.info(f"üìä Processed {len(output_files)} repositories")
                self.logger.info("üìÅ Generated index files:")
                for repo_name, file_path in output_files.items():
                    self.logger.info(f"   üìÑ {repo_name}: {file_path}")
                self.logger.info(f"üìã Summary report: {summary_report}")

                # Statistics (if enabled)
                if self.indexer.generate_statistics:
                    self.logger.info("\nüìà Processing statistics:")
                    total_relationships = 0
                    high_confidence_relationships = 0

                    for file_path in output_files.values():
                        try:
                            with open(file_path, "r", encoding="utf-8") as f:
                                index_data = json.load(f)
                                relationships = index_data.get("relationships", [])
                                total_relationships += len(relationships)
                                high_confidence_relationships += len(
                                    [
                                        r
                                        for r in relationships
                                        if r.get("confidence_score", 0)
                                        > self.indexer.high_confidence_threshold
                                    ]
                                )
                        except Exception as e:
                            self.logger.warning(
                                f"   ‚ö†Ô∏è Unable to load statistics from {file_path}: {e}"
                            )

                    self.logger.info(
                        f"   üîó Total relationships found: {total_relationships}"
                    )
                    self.logger.info(
                        f"   ‚≠ê High confidence relationships: {high_confidence_relationships}"
                    )
                    self.logger.info(
                        f"   üìä Average relationships per repository: {total_relationships / len(output_files) if output_files else 0:.1f}"
                    )

                self.logger.info("\nüéâ Code indexing process completed successfully!")

                return {
                    "status": "success",
                    "message": f"Successfully indexed {len(output_files)} repositories",
                    "output_files": output_files,
                    "summary_report": summary_report,
                    "statistics": {
                        "total_repositories": len(output_files),
                        "total_relationships": total_relationships,
                        "high_confidence_relationships": high_confidence_relationships,
                    }
                    if self.indexer.generate_statistics
                    else None,
                }
            else:
                self.logger.warning("‚ö†Ô∏è No index files generated")
                return {
                    "status": "warning",
                    "message": "No index files were generated",
                    "output_files": {},
                }

        except Exception as e:
            self.logger.error(f"‚ùå Index workflow failed: {e}")
            # If there are detailed error messages, log them
            import traceback

            self.logger.error(f"Detailed error information: {traceback.format_exc()}")
            return {"status": "error", "message": str(e), "output_files": {}}

    def print_banner(self):
        """Print application banner"""
        banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    üîç Codebase Index Workflow v1.0                   ‚ïë
‚ïë              Intelligent Code Relationship Analysis Tool              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  üìÅ Analyzes existing codebases                                      ‚ïë
‚ïë  üîó Builds intelligent relationships with target structure           ‚ïë
‚ïë  ü§ñ Powered by LLM analysis                                          ‚ïë
‚ïë  üìä Generates detailed JSON indexes                                   ‚ïë
‚ïë  üéØ Provides reference for code reproduction                          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        """
        print(banner)


# Convenience function for direct workflow invocation
async def run_codebase_indexing(
    paper_dir: str,
    initial_plan_path: Optional[str] = None,
    config_path: str = "mcp_agent.secrets.yaml",
    logger=None,
) -> Dict[str, Any]:
    """
    Convenience function to run codebase indexing

    Args:
        paper_dir: Paper directory path
        initial_plan_path: Initial plan file path (optional)
        config_path: API configuration file path
        logger: Logger instance (optional)

    Returns:
        Index result dictionary
    """
    workflow = CodebaseIndexWorkflow(logger=logger)
    workflow.print_banner()

    return await workflow.run_indexing_workflow(
        paper_dir=paper_dir,
        initial_plan_path=initial_plan_path,
        config_path=config_path,
    )


# Main function for testing
async def main():
    """Main function for testing workflow"""
    import logging

    # Setup logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # Test parameters
    paper_dir = "./deepcode_lab/papers/1"
    initial_plan_path = os.path.join(paper_dir, "initial_plan.txt")

    # Run workflow
    result = await run_codebase_indexing(
        paper_dir=paper_dir, initial_plan_path=initial_plan_path, logger=logger
    )

    logger.info(f"Index result: {result}")


if __name__ == "__main__":
    asyncio.run(main())


--- workflows/__init__.py ---
"""
Intelligent Agent Orchestration Workflows for Research-to-Code Automation.

This package provides advanced AI-driven workflow orchestration capabilities
for automated research analysis and code implementation synthesis.
"""

from .agent_orchestration_engine import (
    run_research_analyzer,
    run_resource_processor,
    run_code_analyzer,
    github_repo_download,
    paper_reference_analyzer,
    execute_multi_agent_research_pipeline,
    paper_code_preparation,  # Deprecated, for backward compatibility
)

from .code_implementation_workflow import CodeImplementationWorkflow

__all__ = [
    # Initial workflows
    "run_research_analyzer",
    "run_resource_processor",
    "run_code_analyzer",
    "github_repo_download",
    "paper_reference_analyzer",
    "execute_multi_agent_research_pipeline",  # Main multi-agent pipeline function
    "paper_code_preparation",  # Deprecated, for backward compatibility
    # Code implementation workflows
    "CodeImplementationWorkflow",
]


--- workflows/plugins/base.py ---
"""
User-in-Loop Plugin System - Base Classes

This module provides a plugin-based architecture for adding user interaction
points to workflows without modifying core workflow code.

Design Philosophy:
- Plugins are registered at specific "hook points" in the workflow
- Each plugin decides if it should trigger based on context
- Plugins are completely optional and can be enabled/disabled via config
- Zero changes to core workflow code - just call `await plugins.run_hook(...)`

Usage:
    from workflows.plugins import PluginRegistry, InteractionPoint

    # Initialize registry with interaction callback
    plugins = PluginRegistry(interaction_callback=my_callback)

    # In workflow, call hooks at specific points
    context = await plugins.run_hook(
        InteractionPoint.BEFORE_PLANNING,
        context={"user_input": user_input, "task_id": task_id}
    )
"""

import asyncio
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Awaitable
import logging


class InteractionPoint(Enum):
    """
    Defines hook points where plugins can be inserted in the workflow.

    Hook points are named by their position relative to workflow phases:
    - BEFORE_* : Before a phase starts
    - AFTER_*  : After a phase completes
    """

    # Chat Planning Pipeline hooks
    BEFORE_PLANNING = "before_planning"  # Before generating implementation plan
    AFTER_PLANNING = "after_planning"  # After plan is generated, before implementation

    # Paper-to-Code Pipeline hooks
    BEFORE_RESEARCH_ANALYSIS = "before_research_analysis"  # Before analyzing paper
    AFTER_RESEARCH_ANALYSIS = "after_research_analysis"  # After paper analysis
    AFTER_CODE_PLANNING = "after_code_planning"  # After code plan generated

    # Common hooks
    BEFORE_IMPLEMENTATION = "before_implementation"  # Before code generation starts
    AFTER_IMPLEMENTATION = "after_implementation"  # After code is generated


@dataclass
class InteractionRequest:
    """Data structure for requesting user interaction"""

    interaction_type: str  # Type of interaction (e.g., "questions", "plan_review")
    title: str  # Display title
    description: str  # Description for user
    data: Dict[str, Any]  # Interaction-specific data
    options: Dict[str, str] = field(default_factory=dict)  # Available actions
    required: bool = False  # If True, cannot be skipped
    timeout_seconds: int = 300  # Timeout for response (5 min default)


@dataclass
class InteractionResponse:
    """Data structure for user's response to interaction"""

    action: str  # User's action (e.g., "confirm", "modify", "skip")
    data: Dict[str, Any] = field(default_factory=dict)  # Response data
    skipped: bool = False  # True if user chose to skip


class InteractionPlugin(ABC):
    """
    Base class for User-in-Loop plugins.

    Each plugin implements:
    1. should_trigger() - Decides if plugin should run based on context
    2. create_interaction() - Creates the interaction request
    3. process_response() - Handles user's response and updates context

    Example:
        class MyPlugin(InteractionPlugin):
            name = "my_plugin"
            hook_point = InteractionPoint.AFTER_PLANNING

            async def should_trigger(self, context):
                return context.get("enable_my_plugin", True)

            async def create_interaction(self, context):
                return InteractionRequest(...)

            async def process_response(self, response, context):
                context["my_result"] = response.data
                return context
    """

    # Plugin metadata - override in subclass
    name: str = "base_plugin"
    description: str = "Base plugin"
    hook_point: InteractionPoint = InteractionPoint.BEFORE_PLANNING
    priority: int = 100  # Lower number = higher priority (runs first)

    def __init__(self, enabled: bool = True, config: Optional[Dict] = None):
        self.enabled = enabled
        self.config = config or {}
        self.logger = logging.getLogger(f"plugin.{self.name}")

    @abstractmethod
    async def should_trigger(self, context: Dict[str, Any]) -> bool:
        """
        Determine if this plugin should trigger.

        Args:
            context: Current workflow context

        Returns:
            True if plugin should run, False to skip
        """
        pass

    @abstractmethod
    async def create_interaction(self, context: Dict[str, Any]) -> InteractionRequest:
        """
        Create the interaction request to send to user.

        Args:
            context: Current workflow context

        Returns:
            InteractionRequest with data for user interface
        """
        pass

    @abstractmethod
    async def process_response(
        self, response: InteractionResponse, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Process user's response and update context.

        Args:
            response: User's response
            context: Current workflow context

        Returns:
            Updated context dictionary
        """
        pass

    async def on_skip(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Called when user skips the interaction.
        Override to provide default behavior.

        Args:
            context: Current workflow context

        Returns:
            Updated context (default: unchanged)
        """
        self.logger.info(f"Plugin {self.name} skipped by user")
        return context

    async def on_timeout(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Called when interaction times out.
        Override to provide timeout behavior.

        Args:
            context: Current workflow context

        Returns:
            Updated context (default: same as skip)
        """
        self.logger.warning(f"Plugin {self.name} timed out")
        return await self.on_skip(context)


# Type alias for interaction callback
InteractionCallback = Callable[
    [str, InteractionRequest],  # (task_id, request)
    Awaitable[InteractionResponse],  # Returns response
]


class PluginRegistry:
    """
    Registry for managing and executing User-in-Loop plugins.

    Features:
    - Register plugins at specific hook points
    - Enable/disable plugins dynamically
    - Execute all plugins at a hook point in priority order
    - Handle interaction callbacks to frontend

    Usage:
        # Create registry
        registry = PluginRegistry()

        # Register plugins
        registry.register(RequirementAnalysisPlugin())
        registry.register(PlanReviewPlugin(enabled=False))

        # Set interaction callback (connects to WebSocket/API)
        registry.set_interaction_callback(my_callback)

        # Run hooks in workflow
        context = await registry.run_hook(InteractionPoint.BEFORE_PLANNING, context)
    """

    def __init__(self, interaction_callback: Optional[InteractionCallback] = None):
        self._plugins: Dict[InteractionPoint, List[InteractionPlugin]] = {
            point: [] for point in InteractionPoint
        }
        self._interaction_callback = interaction_callback
        self.logger = logging.getLogger("plugin.registry")

    def register(self, plugin: InteractionPlugin) -> None:
        """Register a plugin at its hook point."""
        hook_point = plugin.hook_point
        self._plugins[hook_point].append(plugin)
        # Sort by priority (lower number first)
        self._plugins[hook_point].sort(key=lambda p: p.priority)
        self.logger.info(f"Registered plugin '{plugin.name}' at {hook_point.value}")

    def unregister(self, plugin_name: str) -> bool:
        """Unregister a plugin by name."""
        for hook_point, plugins in self._plugins.items():
            for plugin in plugins:
                if plugin.name == plugin_name:
                    plugins.remove(plugin)
                    self.logger.info(f"Unregistered plugin '{plugin_name}'")
                    return True
        return False

    def enable(self, plugin_name: str) -> bool:
        """Enable a plugin by name."""
        for plugins in self._plugins.values():
            for plugin in plugins:
                if plugin.name == plugin_name:
                    plugin.enabled = True
                    self.logger.info(f"Enabled plugin '{plugin_name}'")
                    return True
        return False

    def disable(self, plugin_name: str) -> bool:
        """Disable a plugin by name."""
        for plugins in self._plugins.values():
            for plugin in plugins:
                if plugin.name == plugin_name:
                    plugin.enabled = False
                    self.logger.info(f"Disabled plugin '{plugin_name}'")
                    return True
        return False

    def set_interaction_callback(self, callback: InteractionCallback) -> None:
        """Set the callback function for user interactions."""
        self._interaction_callback = callback

    def get_plugins(self, hook_point: InteractionPoint) -> List[InteractionPlugin]:
        """Get all plugins registered at a hook point."""
        return self._plugins.get(hook_point, [])

    async def run_hook(
        self,
        hook_point: InteractionPoint,
        context: Dict[str, Any],
        task_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Execute all enabled plugins at a hook point.

        Plugins are executed in priority order. Each plugin can:
        - Modify the context
        - Request user interaction
        - Be skipped by the user

        Args:
            hook_point: The hook point to execute
            context: Current workflow context
            task_id: Task ID for interaction callbacks

        Returns:
            Updated context after all plugins have run
        """
        plugins = self._plugins.get(hook_point, [])

        if not plugins:
            self.logger.debug(f"No plugins registered at {hook_point.value}")
            return context

        self.logger.info(
            f"Running hook {hook_point.value} with {len(plugins)} plugin(s)"
        )

        for plugin in plugins:
            if not plugin.enabled:
                self.logger.debug(f"Plugin '{plugin.name}' is disabled, skipping")
                continue

            try:
                # Check if plugin should trigger
                if not await plugin.should_trigger(context):
                    self.logger.debug(f"Plugin '{plugin.name}' chose not to trigger")
                    continue

                self.logger.info(f"Running plugin '{plugin.name}'")

                # Create interaction request
                interaction = await plugin.create_interaction(context)

                # If we have a callback, request user interaction
                if self._interaction_callback and task_id:
                    try:
                        response = await asyncio.wait_for(
                            self._interaction_callback(task_id, interaction),
                            timeout=interaction.timeout_seconds,
                        )

                        if response.skipped:
                            context = await plugin.on_skip(context)
                        else:
                            context = await plugin.process_response(response, context)

                    except asyncio.TimeoutError:
                        self.logger.warning(
                            f"Plugin '{plugin.name}' interaction timed out"
                        )
                        context = await plugin.on_timeout(context)
                else:
                    # No callback - auto-skip non-required interactions
                    if not interaction.required:
                        self.logger.info(
                            f"No callback, auto-skipping plugin '{plugin.name}'"
                        )
                        context = await plugin.on_skip(context)
                    else:
                        raise RuntimeError(
                            f"Plugin '{plugin.name}' requires interaction but no callback provided"
                        )

            except Exception as e:
                self.logger.error(f"Plugin '{plugin.name}' failed: {e}")
                # Continue with other plugins
                continue

        return context


# Global default registry
_default_registry: Optional[PluginRegistry] = None


def get_default_registry(auto_register: bool = True) -> PluginRegistry:
    """
    Get or create the default plugin registry.

    Args:
        auto_register: If True, auto-register default plugins. Set to False to avoid
                       circular imports when called from plugin modules.
    """
    global _default_registry
    if _default_registry is None:
        _default_registry = PluginRegistry()

        if auto_register:
            # Lazy import to avoid circular imports
            try:
                from .requirement_analysis import RequirementAnalysisPlugin
                from .plan_review import PlanReviewPlugin

                _default_registry.register(RequirementAnalysisPlugin())
                _default_registry.register(PlanReviewPlugin())
            except ImportError as e:
                logging.getLogger("plugin.registry").warning(
                    f"Could not auto-register default plugins: {e}"
                )

    return _default_registry


def reset_registry() -> None:
    """Reset the default registry (useful for testing)."""
    global _default_registry
    _default_registry = None


--- workflows/agents/code_implementation_agent.py ---
"""
Code Implementation Agent for File-by-File Development

Handles systematic code implementation with progress tracking and
memory optimization for long-running development sessions.
"""

import json
import time
import logging
from typing import Dict, Any, List, Optional

# Import tiktoken for token calculation
try:
    import tiktoken

    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False

# Import prompts from code_prompts
import sys
import os

sys.path.insert(
    0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
)
from prompts.code_prompts import (
    GENERAL_CODE_IMPLEMENTATION_SYSTEM_PROMPT,
)


class CodeImplementationAgent:
    """
    Code Implementation Agent for systematic file-by-file development

    Responsibilities:
    - Track file implementation progress
    - Execute MCP tool calls for code generation
    - Monitor implementation status
    - Coordinate with Summary Agent for memory optimization
    - Calculate token usage for context management
    """

    def __init__(
        self,
        mcp_agent,
        logger: Optional[logging.Logger] = None,
        enable_read_tools: bool = True,
    ):
        """
        Initialize Code Implementation Agent

        Args:
            mcp_agent: MCP agent instance for tool calls
            logger: Logger instance for tracking operations
            enable_read_tools: Whether to enable read_file and read_code_mem tools (default: True)
        """
        self.mcp_agent = mcp_agent
        self.logger = logger or self._create_default_logger()
        self.enable_read_tools = enable_read_tools  # Control read tools execution

        self.implementation_summary = {
            "completed_files": [],
            "technical_decisions": [],
            "important_constraints": [],
            "architecture_notes": [],
            "dependency_analysis": [],  # Track dependency analysis and file reads
        }
        self.files_implemented_count = 0
        self.implemented_files_set = (
            set()
        )  # Track unique file paths to avoid duplicate counting
        self.files_read_for_dependencies = (
            set()
        )  # Track files read for dependency analysis
        self.last_summary_file_count = (
            0  # Track the file count when last summary was triggered
        )

        # Token calculation settings
        self.max_context_tokens = (
            200000  # Default max context tokens for Claude-3.5-Sonnet
        )
        self.token_buffer = 10000  # Safety buffer before reaching max
        self.summary_trigger_tokens = (
            self.max_context_tokens - self.token_buffer
        )  # Trigger summary when approaching limit
        self.last_summary_token_count = (
            0  # Track token count when last summary was triggered
        )

        # Initialize tokenizer
        if TIKTOKEN_AVAILABLE:
            try:
                # Use Claude-3 tokenizer (approximation with OpenAI's o200k_base)
                self.tokenizer = tiktoken.get_encoding("o200k_base")
                self.logger.info("Token calculation enabled with o200k_base encoding")
            except Exception as e:
                self.tokenizer = None
                self.logger.warning(f"Failed to initialize tokenizer: {e}")
        else:
            self.tokenizer = None
            self.logger.warning(
                "tiktoken not available, token-based summary triggering disabled"
            )

        # Analysis loop detection
        self.recent_tool_calls = []  # Track recent tool calls to detect analysis loops
        self.max_read_without_write = 5  # Max read_file calls without write_file

        # Memory agent integration
        self.memory_agent = None  # Will be set externally
        self.llm_client = None  # Will be set externally
        self.llm_client_type = None  # Will be set externally

        # Log read tools configuration
        read_tools_status = "ENABLED" if self.enable_read_tools else "DISABLED"
        self.logger.info(
            f"üîß Code Implementation Agent initialized - Read tools: {read_tools_status}"
        )
        if not self.enable_read_tools:
            self.logger.info(
                "üö´ Testing mode: read_file and read_code_mem will be skipped when called"
            )

    def _create_default_logger(self) -> logging.Logger:
        """Create default logger if none provided"""
        logger = logging.getLogger(f"{__name__}.CodeImplementationAgent")
        # Don't add handlers to child loggers - let them propagate to root
        logger.setLevel(logging.INFO)
        return logger

    def get_system_prompt(self) -> str:
        """
        Get the system prompt for code implementation
        """
        return GENERAL_CODE_IMPLEMENTATION_SYSTEM_PROMPT

    def set_memory_agent(self, memory_agent, llm_client=None, llm_client_type=None):
        """
        Set memory agent for code summary generation

        Args:
            memory_agent: Memory agent instance
            llm_client: LLM client for summary generation
            llm_client_type: Type of LLM client ("anthropic" or "openai")
        """
        self.memory_agent = memory_agent
        self.llm_client = llm_client
        self.llm_client_type = llm_client_type
        self.logger.info("Memory agent integration configured")

    async def execute_tool_calls(self, tool_calls: List[Dict]) -> List[Dict]:
        """
        Execute MCP tool calls and track implementation progress

        Args:
            tool_calls: List of tool calls to execute

        Returns:
            List of tool execution results
        """
        results = []

        for tool_call in tool_calls:
            tool_name = tool_call["name"]
            tool_input = tool_call["input"]

            self.logger.info(f"Executing MCP tool: {tool_name}")

            try:
                # Check if read tools are disabled
                if not self.enable_read_tools and tool_name in [
                    "read_file",
                    "read_code_mem",
                ]:
                    # self.logger.info(f"üö´ SKIPPING {tool_name} - Read tools disabled for testing")
                    # Return a mock result indicating the tool was skipped
                    mock_result = json.dumps(
                        {
                            "status": "skipped",
                            "message": f"{tool_name} tool disabled for testing",
                            "tool_disabled": True,
                            "original_input": tool_input,
                        },
                        ensure_ascii=False,
                    )

                    results.append(
                        {
                            "tool_id": tool_call["id"],
                            "tool_name": tool_name,
                            "result": mock_result,
                        }
                    )
                    continue

                # read_code_mem is now a proper MCP tool, no special handling needed

                # INTERCEPT read_file calls - redirect to read_code_mem first if memory agent is available
                if tool_name == "read_file":
                    file_path = tool_call["input"].get("file_path", "unknown")
                    self.logger.info(f"üîç READ_FILE CALL DETECTED: {file_path}")
                    self.logger.info(
                        f"üìä Files implemented count: {self.files_implemented_count}"
                    )
                    self.logger.info(
                        f"üß† Memory agent available: {self.memory_agent is not None}"
                    )

                    # Enable optimization if memory agent is available (more aggressive approach)
                    if self.memory_agent is not None:
                        self.logger.info(
                            f"üîÑ INTERCEPTING read_file call for {file_path} (memory agent available)"
                        )
                        result = await self._handle_read_file_with_memory_optimization(
                            tool_call
                        )
                        results.append(result)
                        continue
                    else:
                        self.logger.info(
                            "üìÅ NO INTERCEPTION: no memory agent available"
                        )

                if self.mcp_agent:
                    # Execute tool call through MCP protocol
                    result = await self.mcp_agent.call_tool(tool_name, tool_input)

                    # Track file implementation progress
                    if tool_name == "write_file":
                        await self._track_file_implementation_with_summary(
                            tool_call, result
                        )
                    elif tool_name == "read_file":
                        self._track_dependency_analysis(tool_call, result)

                    # Track tool calls for analysis loop detection
                    self._track_tool_call_for_loop_detection(tool_name)

                    results.append(
                        {
                            "tool_id": tool_call["id"],
                            "tool_name": tool_name,
                            "result": result,
                        }
                    )
                else:
                    results.append(
                        {
                            "tool_id": tool_call["id"],
                            "tool_name": tool_name,
                            "result": json.dumps(
                                {
                                    "status": "error",
                                    "message": "MCP agent not initialized",
                                },
                                ensure_ascii=False,
                            ),
                        }
                    )

            except Exception as e:
                self.logger.error(f"MCP tool execution failed: {e}")
                results.append(
                    {
                        "tool_id": tool_call["id"],
                        "tool_name": tool_name,
                        "result": json.dumps(
                            {"status": "error", "message": str(e)}, ensure_ascii=False
                        ),
                    }
                )

        return results

    # _handle_read_code_mem method removed - read_code_mem is now a proper MCP tool

    async def _handle_read_file_with_memory_optimization(self, tool_call: Dict) -> Dict:
        """
        Intercept read_file calls and redirect to read_code_mem if a summary exists.
        This prevents unnecessary file reads if the summary is already available.
        """
        file_path = tool_call["input"].get("file_path")
        if not file_path:
            return {
                "tool_id": tool_call["id"],
                "tool_name": "read_file",
                "result": json.dumps(
                    {"status": "error", "message": "file_path parameter is required"},
                    ensure_ascii=False,
                ),
            }

        # Check if a summary exists for this file using read_code_mem MCP tool
        should_use_summary = False
        if self.memory_agent and self.mcp_agent:
            try:
                # Use read_code_mem MCP tool to check if summary exists (pass file path as list)
                read_code_mem_result = await self.mcp_agent.call_tool(
                    "read_code_mem", {"file_paths": [file_path]}
                )

                # Parse the result to check if summary was found
                import json

                if isinstance(read_code_mem_result, str):
                    try:
                        result_data = json.loads(read_code_mem_result)
                        # Check if any summaries were found in the results
                        should_use_summary = (
                            result_data.get("status")
                            in ["all_summaries_found", "partial_summaries_found"]
                            and result_data.get("summaries_found", 0) > 0
                        )
                    except json.JSONDecodeError:
                        should_use_summary = False
            except Exception as e:
                self.logger.debug(f"read_code_mem check failed for {file_path}: {e}")
                should_use_summary = False

        if should_use_summary:
            self.logger.info(f"üîÑ READ_FILE INTERCEPTED: Using summary for {file_path}")

            # Use the MCP agent to call read_code_mem tool
            if self.mcp_agent:
                result = await self.mcp_agent.call_tool(
                    "read_code_mem", {"file_paths": [file_path]}
                )

                # Modify the result to indicate it was originally a read_file call
                import json

                try:
                    result_data = (
                        json.loads(result) if isinstance(result, str) else result
                    )
                    if isinstance(result_data, dict):
                        # Extract the specific file result for the single file we requested
                        file_results = result_data.get("results", [])
                        if file_results and len(file_results) > 0:
                            specific_result = file_results[
                                0
                            ]  # Get the first (and only) result
                            # Transform to match the old single-file format for backward compatibility
                            transformed_result = {
                                "status": specific_result.get("status", "no_summary"),
                                "file_path": specific_result.get(
                                    "file_path", file_path
                                ),
                                "summary_content": specific_result.get(
                                    "summary_content"
                                ),
                                "message": specific_result.get("message", ""),
                                "original_tool": "read_file",
                                "optimization": "redirected_to_read_code_mem",
                            }
                            final_result = json.dumps(
                                transformed_result, ensure_ascii=False
                            )
                        else:
                            # Fallback if no results
                            result_data["original_tool"] = "read_file"
                            result_data["optimization"] = "redirected_to_read_code_mem"
                            final_result = json.dumps(result_data, ensure_ascii=False)
                    else:
                        final_result = result
                except (json.JSONDecodeError, TypeError):
                    final_result = result

                return {
                    "tool_id": tool_call["id"],
                    "tool_name": "read_file",  # Keep original tool name for tracking
                    "result": final_result,
                }
            else:
                self.logger.warning(
                    "MCP agent not available for read_code_mem optimization"
                )
        else:
            self.logger.info(
                f"üìÅ READ_FILE: No summary for {file_path}, using actual file"
            )

            # Execute the original read_file call
            if self.mcp_agent:
                result = await self.mcp_agent.call_tool("read_file", tool_call["input"])

                # Track dependency analysis for the actual file read
                self._track_dependency_analysis(tool_call, result)

                # Track tool calls for analysis loop detection
                self._track_tool_call_for_loop_detection("read_file")

                return {
                    "tool_id": tool_call["id"],
                    "tool_name": "read_file",
                    "result": result,
                }
            else:
                return {
                    "tool_id": tool_call["id"],
                    "tool_name": "read_file",
                    "result": json.dumps(
                        {"status": "error", "message": "MCP agent not initialized"},
                        ensure_ascii=False,
                    ),
                }

    async def _track_file_implementation_with_summary(
        self, tool_call: Dict, result: Any
    ):
        """
        Track file implementation and create code summary

        Args:
            tool_call: The write_file tool call
            result: Result of the tool execution
        """
        # First do the regular tracking
        self._track_file_implementation(tool_call, result)

        # Then create and save code summary if memory agent is available
        if self.memory_agent and self.llm_client and self.llm_client_type:
            try:
                file_path = tool_call["input"].get("file_path")
                file_content = tool_call["input"].get("content", "")

                if file_path and file_content:
                    # Create code implementation summary
                    summary = await self.memory_agent.create_code_implementation_summary(
                        self.llm_client,
                        self.llm_client_type,
                        file_path,
                        file_content,
                        self.get_files_implemented_count(),  # Pass the current file count
                    )

                    self.logger.info(
                        f"Created code summary for implemented file: {file_path}, summary: {summary[:100]}..."
                    )
                else:
                    self.logger.warning(
                        "Missing file path or content for summary generation"
                    )

            except Exception as e:
                self.logger.error(f"Failed to create code summary: {e}")

    def _track_file_implementation(self, tool_call: Dict, result: Any):
        """
        Track file implementation progress
        """
        try:
            # Handle different result types from MCP
            result_data = None

            # Check if result is a CallToolResult object
            if hasattr(result, "content"):
                # Extract content from CallToolResult
                if hasattr(result.content, "text"):
                    result_content = result.content.text
                else:
                    result_content = str(result.content)

                # Try to parse as JSON
                try:
                    result_data = json.loads(result_content)
                except json.JSONDecodeError:
                    # If not JSON, create a structure
                    result_data = {
                        "status": "success",
                        "file_path": tool_call["input"].get("file_path", "unknown"),
                    }
            elif isinstance(result, str):
                # Try to parse string result
                try:
                    result_data = json.loads(result)
                except json.JSONDecodeError:
                    result_data = {
                        "status": "success",
                        "file_path": tool_call["input"].get("file_path", "unknown"),
                    }
            elif isinstance(result, dict):
                # Direct dictionary result
                result_data = result
            else:
                # Fallback: assume success and extract file path from input
                result_data = {
                    "status": "success",
                    "file_path": tool_call["input"].get("file_path", "unknown"),
                }

            # Extract file path for tracking
            file_path = None
            if result_data and result_data.get("status") == "success":
                file_path = result_data.get(
                    "file_path", tool_call["input"].get("file_path", "unknown")
                )
            else:
                file_path = tool_call["input"].get("file_path")

            # Only count unique files, not repeated tool calls on same file
            if file_path and file_path not in self.implemented_files_set:
                # This is a new file implementation
                self.implemented_files_set.add(file_path)
                self.files_implemented_count += 1
                # self.logger.info(f"New file implementation tracked: count={self.files_implemented_count}, file={file_path}")
                # print(f"New file implementation tracked: count={self.files_implemented_count}, file={file_path}")

                # Add to completed files list
                self.implementation_summary["completed_files"].append(
                    {
                        "file": file_path,
                        "iteration": self.files_implemented_count,
                        "timestamp": time.time(),
                        "size": result_data.get("size", 0) if result_data else 0,
                    }
                )

                # self.logger.info(
                #     f"New file implementation tracked: count={self.files_implemented_count}, file={file_path}"
                # )
                # print(f"üìù NEW FILE IMPLEMENTED: count={self.files_implemented_count}, file={file_path}")
                # print(f"üîß OPTIMIZATION NOW ENABLED: files_implemented_count > 0 = {self.files_implemented_count > 0}")
            elif file_path and file_path in self.implemented_files_set:
                # This file was already implemented (duplicate tool call)
                self.logger.debug(
                    f"File already tracked, skipping duplicate count: {file_path}"
                )
            else:
                # No valid file path found
                self.logger.warning("No valid file path found for tracking")

        except Exception as e:
            self.logger.warning(f"Failed to track file implementation: {e}")
            # Even if tracking fails, try to count based on tool input (but check for duplicates)

            file_path = tool_call["input"].get("file_path")
            if file_path and file_path not in self.implemented_files_set:
                self.implemented_files_set.add(file_path)
                self.files_implemented_count += 1
                self.logger.info(
                    f"File implementation counted (emergency fallback): count={self.files_implemented_count}, file={file_path}"
                )

    def _track_dependency_analysis(self, tool_call: Dict, result: Any):
        """
        Track dependency analysis through read_file calls
        """
        try:
            file_path = tool_call["input"].get("file_path")
            if file_path:
                # Track unique files read for dependency analysis
                if file_path not in self.files_read_for_dependencies:
                    self.files_read_for_dependencies.add(file_path)

                    # Add to dependency analysis summary
                    self.implementation_summary["dependency_analysis"].append(
                        {
                            "file_read": file_path,
                            "timestamp": time.time(),
                            "purpose": "dependency_analysis",
                        }
                    )

                    self.logger.info(
                        f"Dependency analysis tracked: file_read={file_path}"
                    )

        except Exception as e:
            self.logger.warning(f"Failed to track dependency analysis: {e}")

    def calculate_messages_token_count(self, messages: List[Dict]) -> int:
        """
        Calculate total token count for a list of messages

        Args:
            messages: List of chat messages with 'role' and 'content' keys

        Returns:
            Total token count
        """
        if not self.tokenizer:
            # Fallback: rough estimation based on character count
            total_chars = sum(len(str(msg.get("content", ""))) for msg in messages)
            # Rough approximation: 1 token ‚âà 4 characters
            return total_chars // 4

        try:
            total_tokens = 0
            for message in messages:
                content = str(message.get("content", ""))
                role = message.get("role", "")

                # Count tokens for content
                if content:
                    content_tokens = len(
                        self.tokenizer.encode(content, disallowed_special=())
                    )
                    total_tokens += content_tokens

                # Add tokens for role and message structure
                role_tokens = len(self.tokenizer.encode(role, disallowed_special=()))
                total_tokens += role_tokens + 4  # Extra tokens for message formatting

            return total_tokens

        except Exception as e:
            self.logger.warning(f"Token calculation failed: {e}")
            # Fallback estimation
            total_chars = sum(len(str(msg.get("content", ""))) for msg in messages)
            return total_chars // 4

    def should_trigger_summary_by_tokens(self, messages: List[Dict]) -> bool:
        """
        Check if summary should be triggered based on token count

        Args:
            messages: Current conversation messages

        Returns:
            True if summary should be triggered based on token count
        """
        if not messages:
            return False

        # Calculate current token count / ËÆ°ÁÆóÂΩìÂâçtokenÊï∞
        current_token_count = self.calculate_messages_token_count(messages)

        # Check if we should trigger summary / Ê£ÄÊü•ÊòØÂê¶Â∫îËß¶ÂèëÊÄªÁªì
        should_trigger = (
            current_token_count > self.summary_trigger_tokens
            and current_token_count
            > self.last_summary_token_count
            + 10000  # Minimum 10k tokens between summaries / ÊÄªÁªìÈó¥ÊúÄÂ∞ë10k tokens
        )

        if should_trigger:
            self.logger.info(
                f"Token-based summary trigger: current={current_token_count:,}, "
                f"threshold={self.summary_trigger_tokens:,}, "
                f"last_summary={self.last_summary_token_count:,}"
            )

        return should_trigger

    def should_trigger_summary(
        self, summary_trigger: int = 5, messages: List[Dict] = None
    ) -> bool:
        """
        Check if summary should be triggered based on token count (preferred) or file count (fallback)
        Ê†πÊçÆtokenÊï∞ÔºàÈ¶ñÈÄâÔºâÊàñÊñá‰ª∂Êï∞ÔºàÂõûÈÄÄÔºâÊ£ÄÊü•ÊòØÂê¶Â∫îËß¶ÂèëÊÄªÁªì

        Args:
            summary_trigger: Number of files after which to trigger summary (fallback)
            messages: Current conversation messages for token calculation

        Returns:
            True if summary should be triggered
        """
        # Primary: Token-based triggering / ‰∏ªË¶ÅÔºöÂü∫‰∫étokenÁöÑËß¶Âèë
        if messages and self.tokenizer:
            return self.should_trigger_summary_by_tokens(messages)

        # Fallback: File-based triggering (original logic) / ÂõûÈÄÄÔºöÂü∫‰∫éÊñá‰ª∂ÁöÑËß¶ÂèëÔºàÂéüÂßãÈÄªËæëÔºâ
        self.logger.info("Using fallback file-based summary triggering")
        should_trigger = (
            self.files_implemented_count > 0
            and self.files_implemented_count % summary_trigger == 0
            and self.files_implemented_count > self.last_summary_file_count
        )

        return should_trigger

    def mark_summary_triggered(self, messages: List[Dict] = None):
        """
        Mark that summary has been triggered for current state
        Ê†áËÆ∞ÂΩìÂâçÁä∂ÊÄÅÁöÑÊÄªÁªìÂ∑≤Ë¢´Ëß¶Âèë

        Args:
            messages: Current conversation messages for token tracking
        """
        # Update file-based tracking / Êõ¥Êñ∞Âü∫‰∫éÊñá‰ª∂ÁöÑË∑üË∏™
        self.last_summary_file_count = self.files_implemented_count

        # Update token-based tracking / Êõ¥Êñ∞Âü∫‰∫étokenÁöÑË∑üË∏™
        if messages and self.tokenizer:
            self.last_summary_token_count = self.calculate_messages_token_count(
                messages
            )
            self.logger.info(
                f"Summary marked as triggered - file_count: {self.files_implemented_count}, "
                f"token_count: {self.last_summary_token_count:,}"
            )
        else:
            self.logger.info(
                f"Summary marked as triggered for file count: {self.files_implemented_count}"
            )

    def get_implementation_summary(self) -> Dict[str, Any]:
        """
        Get current implementation summary
        Ëé∑ÂèñÂΩìÂâçÂÆûÁé∞ÊÄªÁªì
        """
        return self.implementation_summary.copy()

    def get_files_implemented_count(self) -> int:
        """
        Get the number of files implemented so far
        Ëé∑ÂèñÂà∞ÁõÆÂâç‰∏∫Ê≠¢ÂÆûÁé∞ÁöÑÊñá‰ª∂Êï∞Èáè
        """
        return self.files_implemented_count

    def get_read_tools_status(self) -> Dict[str, Any]:
        """
        Get read tools configuration status
        Ëé∑ÂèñËØªÂèñÂ∑•ÂÖ∑ÈÖçÁΩÆÁä∂ÊÄÅ

        Returns:
            Dictionary with read tools status information
        """
        return {
            "read_tools_enabled": self.enable_read_tools,
            "status": "ENABLED" if self.enable_read_tools else "DISABLED",
            "tools_affected": ["read_file", "read_code_mem"],
            "description": "Read tools configuration for testing purposes",
        }

    def add_technical_decision(self, decision: str, context: str = ""):
        """
        Add a technical decision to the implementation summary
        ÂêëÂÆûÁé∞ÊÄªÁªìÊ∑ªÂä†ÊäÄÊúØÂÜ≥Á≠ñ

        Args:
            decision: Description of the technical decision
            context: Additional context for the decision
        """
        self.implementation_summary["technical_decisions"].append(
            {"decision": decision, "context": context, "timestamp": time.time()}
        )
        self.logger.info(f"Technical decision recorded: {decision}")

    def add_constraint(self, constraint: str, impact: str = ""):
        """
        Add an important constraint to the implementation summary
        ÂêëÂÆûÁé∞ÊÄªÁªìÊ∑ªÂä†ÈáçË¶ÅÁ∫¶Êùü

        Args:
            constraint: Description of the constraint
            impact: Impact of the constraint on implementation
        """
        self.implementation_summary["important_constraints"].append(
            {"constraint": constraint, "impact": impact, "timestamp": time.time()}
        )
        self.logger.info(f"Constraint recorded: {constraint}")

    def add_architecture_note(self, note: str, component: str = ""):
        """
        Add an architecture note to the implementation summary
        ÂêëÂÆûÁé∞ÊÄªÁªìÊ∑ªÂä†Êû∂ÊûÑÊ≥®Èáä

        Args:
            note: Architecture note description
            component: Related component or module
        """
        self.implementation_summary["architecture_notes"].append(
            {"note": note, "component": component, "timestamp": time.time()}
        )
        self.logger.info(f"Architecture note recorded: {note}")

    def get_implementation_statistics(self) -> Dict[str, Any]:
        """
        Get comprehensive implementation statistics
        Ëé∑ÂèñÂÖ®Èù¢ÁöÑÂÆûÁé∞ÁªüËÆ°‰ø°ÊÅØ
        """
        return {
            "total_files_implemented": self.files_implemented_count,
            "files_implemented_count": self.files_implemented_count,
            "technical_decisions_count": len(
                self.implementation_summary["technical_decisions"]
            ),
            "constraints_count": len(
                self.implementation_summary["important_constraints"]
            ),
            "architecture_notes_count": len(
                self.implementation_summary["architecture_notes"]
            ),
            "dependency_analysis_count": len(
                self.implementation_summary["dependency_analysis"]
            ),
            "files_read_for_dependencies": len(self.files_read_for_dependencies),
            "unique_files_implemented": len(self.implemented_files_set),
            "completed_files_list": [
                f["file"] for f in self.implementation_summary["completed_files"]
            ],
            "dependency_files_read": list(self.files_read_for_dependencies),
            "last_summary_file_count": self.last_summary_file_count,
            "read_tools_status": self.get_read_tools_status(),  # Include read tools configuration
        }

    def force_enable_optimization(self):
        """
        Force enable optimization for testing purposes
        Âº∫Âà∂ÂêØÁî®‰ºòÂåñÁî®‰∫éÊµãËØïÁõÆÁöÑ
        """
        self.files_implemented_count = 1
        self.logger.info(
            f"üîß OPTIMIZATION FORCE ENABLED: files_implemented_count set to {self.files_implemented_count}"
        )
        print(
            f"üîß OPTIMIZATION FORCE ENABLED: files_implemented_count set to {self.files_implemented_count}"
        )

    def reset_implementation_tracking(self):
        """
        Reset implementation tracking (useful for new sessions)
        ÈáçÁΩÆÂÆûÁé∞Ë∑üË∏™ÔºàÂØπÊñ∞‰ºöËØùÊúâÁî®Ôºâ
        """
        self.implementation_summary = {
            "completed_files": [],
            "technical_decisions": [],
            "important_constraints": [],
            "architecture_notes": [],
            "dependency_analysis": [],  # Reset dependency analysis and file reads
        }
        self.files_implemented_count = 0
        self.implemented_files_set = (
            set()
        )  # Reset the unique files set / ÈáçÁΩÆÂîØ‰∏ÄÊñá‰ª∂ÈõÜÂêà
        self.files_read_for_dependencies = (
            set()
        )  # Reset files read for dependency analysis / ÈáçÁΩÆ‰∏∫‰æùËµñÂàÜÊûêËÄåËØªÂèñÁöÑÊñá‰ª∂
        self.last_summary_file_count = 0  # Reset the file count when last summary was triggered / ÈáçÁΩÆ‰∏äÊ¨°Ëß¶ÂèëÊÄªÁªìÊó∂ÁöÑÊñá‰ª∂Êï∞
        self.last_summary_token_count = 0  # Reset token count when last summary was triggered / ÈáçÁΩÆ‰∏äÊ¨°Ëß¶ÂèëÊÄªÁªìÊó∂ÁöÑtokenÊï∞
        self.logger.info("Implementation tracking reset")

        # Reset analysis loop detection / ÈáçÁΩÆÂàÜÊûêÂæ™ÁéØÊ£ÄÊµã
        self.recent_tool_calls = []
        self.logger.info("Analysis loop detection reset")

    def _track_tool_call_for_loop_detection(self, tool_name: str):
        """
        Track tool calls for analysis loop detection
        Ë∑üË∏™Â∑•ÂÖ∑Ë∞ÉÁî®‰ª•Ê£ÄÊµãÂàÜÊûêÂæ™ÁéØ

        Args:
            tool_name: Name of the tool called
        """
        self.recent_tool_calls.append(tool_name)
        if len(self.recent_tool_calls) > self.max_read_without_write:
            self.recent_tool_calls.pop(0)

        if len(set(self.recent_tool_calls)) == 1:
            self.logger.warning("Analysis loop detected")

    def is_in_analysis_loop(self) -> bool:
        """
        Check if the agent is in an analysis loop (only reading files, not writing)
        Ê£ÄÊü•‰ª£ÁêÜÊòØÂê¶Âú®ÂàÜÊûêÂæ™ÁéØ‰∏≠ÔºàÂè™ËØªÊñá‰ª∂Ôºå‰∏çÂÜôÊñá‰ª∂Ôºâ

        Returns:
            True if in analysis loop
        """
        if len(self.recent_tool_calls) < self.max_read_without_write:
            return False

        # Check if recent calls are all read_file or search_reference_code / Ê£ÄÊü•ÊúÄËøëÁöÑË∞ÉÁî®ÊòØÂê¶ÈÉΩÊòØread_fileÊàñsearch_reference_code
        analysis_tools = {
            "read_file",
            "search_reference_code",
            "get_all_available_references",
        }
        recent_calls_set = set(self.recent_tool_calls)

        # If all recent calls are analysis tools, we're in an analysis loop / Â¶ÇÊûúÊúÄËøëÁöÑË∞ÉÁî®ÈÉΩÊòØÂàÜÊûêÂ∑•ÂÖ∑ÔºåÊàë‰ª¨Âú®ÂàÜÊûêÂæ™ÁéØ‰∏≠
        in_loop = (
            recent_calls_set.issubset(analysis_tools) and len(recent_calls_set) >= 1
        )

        if in_loop:
            self.logger.warning(
                f"Analysis loop detected! Recent calls: {self.recent_tool_calls}"
            )

        return in_loop

    def get_analysis_loop_guidance(self) -> str:
        """
        Get guidance to break out of analysis loop

        Returns:
            Guidance message to encourage implementation
        """
        return f"""üö® **ANALYSIS LOOP DETECTED - IMMEDIATE ACTION REQUIRED**

**Problem**: You've been reading/analyzing files for {len(self.recent_tool_calls)} consecutive calls without writing code.
**Recent tool calls**: {' ‚Üí '.join(self.recent_tool_calls)}

**SOLUTION - IMPLEMENT CODE NOW**:
1. **STOP ANALYZING** - You have enough information
2. **Use write_file** to create the next code file according to the implementation plan
3. **Choose ANY file** from the plan that hasn't been implemented yet
4. **Write complete, working code** - don't ask for permission or clarification

**Files implemented so far**: {self.files_implemented_count}
**Your goal**: Implement MORE files, not analyze existing ones!

**CRITICAL**: Your next response MUST use write_file to create a new code file!"""

    async def test_summary_functionality(self, test_file_path: str = None):
        """
        Test if the code summary functionality is working correctly
        ÊµãËØï‰ª£Á†ÅÊÄªÁªìÂäüËÉΩÊòØÂê¶Ê≠£Â∏∏Â∑•‰Ωú

        Args:
            test_file_path: Specific file to test, if None will test all implemented files
        """
        if not self.memory_agent:
            self.logger.warning("No memory agent available for testing")
            return

        if test_file_path:
            files_to_test = [test_file_path]
        else:
            # Use implemented files from tracking
            files_to_test = list(self.implemented_files_set)[
                :3
            ]  # Limit to first 3 files

        if not files_to_test:
            self.logger.warning("No implemented files to test")
            return

        # Test each file silently
        summary_files_found = 0

        for file_path in files_to_test:
            if self.mcp_agent:
                try:
                    result = await self.mcp_agent.call_tool(
                        "read_code_mem", {"file_paths": [file_path]}
                    )

                    # Parse the result to check if summary was found
                    import json

                    result_data = (
                        json.loads(result) if isinstance(result, str) else result
                    )

                    if (
                        result_data.get("status")
                        in ["all_summaries_found", "partial_summaries_found"]
                        and result_data.get("summaries_found", 0) > 0
                    ):
                        summary_files_found += 1
                except Exception as e:
                    self.logger.warning(
                        f"Failed to test read_code_mem for {file_path}: {e}"
                    )
            else:
                self.logger.warning("MCP agent not available for testing")

        self.logger.info(
            f"üìã Summary testing: {summary_files_found}/{len(files_to_test)} files have summaries"
        )

    async def test_automatic_read_file_optimization(self):
        """
        Test the automatic read_file optimization that redirects to read_code_mem
        ÊµãËØïËá™Âä®read_file‰ºòÂåñÔºåÈáçÂÆöÂêëÂà∞read_code_mem
        """
        print("=" * 80)
        print("üîÑ TESTING AUTOMATIC READ_FILE OPTIMIZATION")
        print("=" * 80)

        # Simulate that at least one file has been implemented (to trigger optimization)
        self.files_implemented_count = 1

        # Test with a generic config file that should have a summary
        test_file = "config.py"

        print(f"üìÅ Testing automatic optimization for: {test_file}")
        print(f"üìä Files implemented count: {self.files_implemented_count}")
        print(
            f"üîß Optimization should be: {'ENABLED' if self.files_implemented_count > 0 else 'DISABLED'}"
        )

        # Create a simulated read_file tool call
        simulated_read_file_call = {
            "id": "test_read_file_optimization",
            "name": "read_file",
            "input": {"file_path": test_file},
        }

        print("\nüîÑ Simulating read_file call:")
        print(f"   Tool: {simulated_read_file_call['name']}")
        print(f"   File: {simulated_read_file_call['input']['file_path']}")

        # Execute the tool call (this should trigger automatic optimization)
        results = await self.execute_tool_calls([simulated_read_file_call])

        if results:
            result = results[0]
            print("\n‚úÖ Tool execution completed:")
            print(f"   Tool name: {result.get('tool_name', 'N/A')}")
            print(f"   Tool ID: {result.get('tool_id', 'N/A')}")

            # Parse the result to check if optimization occurred
            import json

            try:
                result_data = json.loads(result.get("result", "{}"))
                if result_data.get("optimization") == "redirected_to_read_code_mem":
                    print("üéâ SUCCESS: read_file was automatically optimized!")
                    print(
                        f"   Original tool: {result_data.get('original_tool', 'N/A')}"
                    )
                    print(f"   Status: {result_data.get('status', 'N/A')}")
                elif result_data.get("status") == "summary_found":
                    print("üéâ SUCCESS: Summary was found and returned!")
                else:
                    print("‚ÑπÔ∏è  INFO: No optimization occurred (no summary available)")
            except json.JSONDecodeError:
                print("‚ö†Ô∏è  WARNING: Could not parse result as JSON")
        else:
            print("‚ùå ERROR: No results returned from tool execution")

        print("\n" + "=" * 80)
        print("üîÑ AUTOMATIC READ_FILE OPTIMIZATION TEST COMPLETE")
        print("=" * 80)

    async def test_summary_optimization(self, test_file_path: str = "config.py"):
        """
        Test the summary optimization functionality with a specific file
        ÊµãËØïÁâπÂÆöÊñá‰ª∂ÁöÑÊÄªÁªì‰ºòÂåñÂäüËÉΩ

        Args:
            test_file_path: File path to test (default: config.py which should be in summary)
        """
        if not self.mcp_agent:
            return False

        try:
            # Use MCP agent to call read_code_mem tool
            result = await self.mcp_agent.call_tool(
                "read_code_mem", {"file_paths": [test_file_path]}
            )

            # Parse the result to check if summary was found
            import json

            result_data = json.loads(result) if isinstance(result, str) else result

            return (
                result_data.get("status")
                in ["all_summaries_found", "partial_summaries_found"]
                and result_data.get("summaries_found", 0) > 0
            )
        except Exception as e:
            self.logger.warning(f"Failed to test read_code_mem optimization: {e}")
            return False

    async def test_read_tools_configuration(self):
        """
        Test the read tools configuration to verify enabling/disabling works correctly
        ÊµãËØïËØªÂèñÂ∑•ÂÖ∑ÈÖçÁΩÆ‰ª•È™åËØÅÂêØÁî®/Á¶ÅÁî®ÊòØÂê¶Ê≠£Â∏∏Â∑•‰Ωú
        """
        print("=" * 60)
        print("üß™ TESTING READ TOOLS CONFIGURATION")
        print("=" * 60)

        status = self.get_read_tools_status()
        print(f"Read tools enabled: {status['read_tools_enabled']}")
        print(f"Status: {status['status']}")
        print(f"Tools affected: {status['tools_affected']}")

        # Test with mock tool calls
        test_tools = [
            {
                "id": "test_read_file",
                "name": "read_file",
                "input": {"file_path": "test.py"},
            },
            {
                "id": "test_read_code_mem",
                "name": "read_code_mem",
                "input": {"file_path": "test.py"},
            },
            {
                "id": "test_write_file",
                "name": "write_file",
                "input": {"file_path": "test.py", "content": "# test"},
            },
        ]

        print(
            f"\nüîÑ Testing tool execution with read_tools_enabled={self.enable_read_tools}"
        )

        for tool_call in test_tools:
            tool_name = tool_call["name"]
            if not self.enable_read_tools and tool_name in [
                "read_file",
                "read_code_mem",
            ]:
                print(f"üö´ {tool_name}: Would be SKIPPED (disabled)")
            else:
                print(f"‚úÖ {tool_name}: Would be EXECUTED")

        print("=" * 60)
        print("üß™ READ TOOLS CONFIGURATION TEST COMPLETE")
        print("=" * 60)

        return status


--- workflows/agents/document_segmentation_agent.py ---
"""
Document Segmentation Agent

A lightweight agent that coordinates with the document segmentation MCP server
to analyze document structure and prepare segments for other agents.
"""

import os
import logging
from typing import Dict, Any, Optional

from mcp_agent.agents.agent import Agent
from utils.llm_utils import get_preferred_llm_class


class DocumentSegmentationAgent:
    """
    Intelligent document segmentation agent with semantic analysis capabilities.

    This enhanced agent provides:
    1. **Semantic Document Classification**: Content-based document type identification
    2. **Adaptive Segmentation Strategy**: Algorithm integrity and semantic coherence preservation
    3. **Planning Agent Optimization**: Segment preparation specifically optimized for downstream agents
    4. **Quality Intelligence Validation**: Advanced metrics for completeness and technical accuracy
    5. **Algorithm Completeness Protection**: Ensures critical algorithms and formulas remain intact

    Key improvements over traditional segmentation:
    - Semantic content analysis vs mechanical structure splitting
    - Dynamic character limits based on content complexity
    - Enhanced relevance scoring for planning agents
    - Algorithm and formula integrity preservation
    - Content type-aware segmentation strategies
    """

    def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or self._create_default_logger()
        self.mcp_agent = None

    def _create_default_logger(self) -> logging.Logger:
        """Create default logger if none provided"""
        logger = logging.getLogger(f"{__name__}.DocumentSegmentationAgent")
        logger.setLevel(logging.INFO)
        return logger

    async def __aenter__(self):
        """Async context manager entry"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.cleanup()

    async def initialize(self):
        """Initialize the MCP agent connection"""
        try:
            self.mcp_agent = Agent(
                name="DocumentSegmentationCoordinator",
                instruction="""You are an intelligent document segmentation coordinator that leverages advanced semantic analysis for optimal document processing.

Your enhanced capabilities include:
1. **Semantic Content Analysis**: Coordinate intelligent document type classification based on content semantics rather than structural patterns
2. **Algorithm Integrity Protection**: Ensure algorithm blocks, formulas, and related content maintain logical coherence
3. **Adaptive Segmentation Strategy**: Select optimal segmentation approaches (semantic_research_focused, algorithm_preserve_integrity, concept_implementation_hybrid, etc.)
4. **Quality Intelligence Validation**: Assess segmentation quality using enhanced metrics for completeness, relevance, and technical accuracy
5. **Planning Agent Optimization**: Ensure segments are specifically optimized for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent needs

**Key Principles**:
- Prioritize content semantics over mechanical structure
- Preserve algorithm and formula completeness
- Optimize for downstream agent token efficiency
- Ensure technical content integrity
- Provide actionable quality assessments

Use the enhanced document-segmentation tools to deliver superior segmentation results that significantly improve planning agent performance.""",
                server_names=["document-segmentation"],
            )

            # Initialize the agent context
            await self.mcp_agent.__aenter__()

            # Attach LLM
            self.llm = await self.mcp_agent.attach_llm(get_preferred_llm_class())

            self.logger.info("DocumentSegmentationAgent initialized successfully")

        except Exception as e:
            self.logger.error(f"Failed to initialize DocumentSegmentationAgent: {e}")
            raise

    async def cleanup(self):
        """Cleanup resources"""
        if self.mcp_agent:
            try:
                await self.mcp_agent.__aexit__(None, None, None)
            except Exception as e:
                self.logger.warning(f"Error during cleanup: {e}")

    async def analyze_and_prepare_document(
        self, paper_dir: str, force_refresh: bool = False
    ) -> Dict[str, Any]:
        """
        Perform intelligent semantic analysis and create optimized document segments.

        This method coordinates with the enhanced document segmentation server to:
        - Classify document type using semantic content analysis
        - Select optimal segmentation strategy (semantic_research_focused, algorithm_preserve_integrity, etc.)
        - Preserve algorithm and formula integrity
        - Optimize segments for downstream planning agents

        Args:
            paper_dir: Path to the paper directory
            force_refresh: Whether to force re-analysis with latest algorithms

        Returns:
            Dict containing enhanced analysis results and intelligent segment information
        """
        try:
            self.logger.info(f"Starting document analysis for: {paper_dir}")

            # Check if markdown file exists
            md_files = [f for f in os.listdir(paper_dir) if f.endswith(".md")]
            if not md_files:
                raise ValueError(f"No markdown file found in {paper_dir}")

            # Use the enhanced document segmentation tool
            message = f"""Please perform intelligent semantic analysis and segmentation for the document in directory: {paper_dir}

Use the analyze_and_segment_document tool with these parameters:
- paper_dir: {paper_dir}
- force_refresh: {force_refresh}

**Focus on these enhanced objectives**:
1. **Semantic Document Classification**: Identify document type using content semantics (research_paper, algorithm_focused, technical_doc, etc.)
2. **Intelligent Segmentation Strategy**: Select the optimal strategy based on content analysis:
   - `semantic_research_focused` for research papers with high algorithm density
   - `algorithm_preserve_integrity` for algorithm-heavy documents
   - `concept_implementation_hybrid` for mixed concept/implementation content
3. **Algorithm Completeness**: Ensure algorithm blocks, formulas, and related descriptions remain logically connected
4. **Planning Agent Optimization**: Create segments that maximize effectiveness for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent

After segmentation, get a document overview and provide:
- Quality assessment of semantic segmentation approach
- Algorithm/formula integrity verification
- Recommendations for planning agent optimization
- Technical content completeness evaluation"""

            result = await self.llm.generate_str(message=message)

            self.logger.info("Document analysis completed successfully")

            # Parse the result and return structured information
            return {
                "status": "success",
                "paper_dir": paper_dir,
                "analysis_result": result,
                "segments_available": True,
            }

        except Exception as e:
            self.logger.error(f"Error in document analysis: {e}")
            return {
                "status": "error",
                "paper_dir": paper_dir,
                "error_message": str(e),
                "segments_available": False,
            }

    async def get_document_overview(self, paper_dir: str) -> Dict[str, Any]:
        """
        Get overview of document structure and segments.

        Args:
            paper_dir: Path to the paper directory

        Returns:
            Dict containing document overview information
        """
        try:
            message = f"""Please provide an intelligent overview of the enhanced document segmentation for: {paper_dir}

Use the get_document_overview tool to retrieve:
- **Semantic Document Classification**: Document type and confidence score
- **Adaptive Segmentation Strategy**: Strategy used and reasoning
- **Segment Intelligence**: Total segments with enhanced metadata
- **Content Type Distribution**: Breakdown by algorithm, concept, formula, implementation content
- **Quality Intelligence Assessment**: Completeness, coherence, and planning agent optimization

Provide a comprehensive analysis focusing on:
1. Semantic vs structural segmentation quality
2. Algorithm and formula integrity preservation
3. Segment relevance for downstream planning agents
4. Technical content distribution and completeness"""

            result = await self.llm.generate_str(message=message)

            return {
                "status": "success",
                "paper_dir": paper_dir,
                "overview_result": result,
            }

        except Exception as e:
            self.logger.error(f"Error getting document overview: {e}")
            return {"status": "error", "paper_dir": paper_dir, "error_message": str(e)}

    async def validate_segmentation_quality(self, paper_dir: str) -> Dict[str, Any]:
        """
        Validate the quality of document segmentation.

        Args:
            paper_dir: Path to the paper directory

        Returns:
            Dict containing validation results
        """
        try:
            # Get overview first
            overview_result = await self.get_document_overview(paper_dir)

            if overview_result["status"] != "success":
                return overview_result

            # Analyze enhanced segmentation quality
            message = f"""Based on the intelligent document overview for {paper_dir}, please evaluate the enhanced segmentation quality using advanced criteria.

**Enhanced Quality Assessment Factors**:
1. **Semantic Coherence**: Do segments maintain logical content boundaries vs mechanical structural splits?
2. **Algorithm Integrity**: Are algorithm blocks, formulas, and related explanations kept together?
3. **Content Type Optimization**: Are different content types (algorithm, concept, formula, implementation) properly identified and scored?
4. **Planning Agent Effectiveness**: Will ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent receive optimal information?
5. **Dynamic Sizing**: Are segments adaptively sized based on content complexity rather than fixed limits?
6. **Technical Completeness**: Are critical technical details preserved without fragmentation?

**Provide specific recommendations for**:
- Semantic segmentation improvements
- Algorithm/formula integrity enhancements
- Planning agent optimization opportunities
- Content distribution balance adjustments"""

            validation_result = await self.llm.generate_str(message=message)

            return {
                "status": "success",
                "paper_dir": paper_dir,
                "validation_result": validation_result,
                "overview_data": overview_result,
            }

        except Exception as e:
            self.logger.error(f"Error validating segmentation quality: {e}")
            return {"status": "error", "paper_dir": paper_dir, "error_message": str(e)}


async def run_document_segmentation_analysis(
    paper_dir: str, logger: Optional[logging.Logger] = None, force_refresh: bool = False
) -> Dict[str, Any]:
    """
    Convenience function to run document segmentation analysis.

    Args:
        paper_dir: Path to the paper directory
        logger: Optional logger instance
        force_refresh: Whether to force re-analysis

    Returns:
        Dict containing analysis results
    """
    async with DocumentSegmentationAgent(logger=logger) as agent:
        # Analyze and prepare document
        analysis_result = await agent.analyze_and_prepare_document(
            paper_dir, force_refresh=force_refresh
        )

        if analysis_result["status"] == "success":
            # Validate segmentation quality
            validation_result = await agent.validate_segmentation_quality(paper_dir)
            analysis_result["validation"] = validation_result

        return analysis_result


# Utility function for integration with existing workflow
async def prepare_document_segments(
    paper_dir: str, logger: Optional[logging.Logger] = None
) -> Dict[str, Any]:
    """
    Prepare intelligent document segments optimized for planning agents.

    This enhanced function leverages semantic analysis to create segments that:
    - Preserve algorithm and formula integrity
    - Optimize for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent
    - Use adaptive character limits based on content complexity
    - Maintain technical content completeness

    Called from the orchestration engine (Phase 3.5) to prepare documents
    before the planning phase with superior segmentation quality.

    Args:
        paper_dir: Path to the paper directory containing markdown file
        logger: Optional logger instance for tracking

    Returns:
        Dict containing enhanced preparation results and intelligent metadata
    """
    try:
        logger = logger or logging.getLogger(__name__)
        logger.info(f"Preparing document segments for: {paper_dir}")

        # Run analysis
        result = await run_document_segmentation_analysis(
            paper_dir=paper_dir,
            logger=logger,
            force_refresh=False,  # Use cached analysis if available
        )

        if result["status"] == "success":
            logger.info("Document segments prepared successfully")

            # Create metadata for downstream agents
            segments_dir = os.path.join(paper_dir, "document_segments")

            return {
                "status": "success",
                "paper_dir": paper_dir,
                "segments_dir": segments_dir,
                "segments_ready": True,
                "analysis_summary": result.get("analysis_result", ""),
                "validation_summary": result.get("validation", {}).get(
                    "validation_result", ""
                ),
            }
        else:
            logger.error(
                f"Document segmentation failed: {result.get('error_message', 'Unknown error')}"
            )
            return {
                "status": "error",
                "paper_dir": paper_dir,
                "segments_ready": False,
                "error_message": result.get(
                    "error_message", "Document segmentation failed"
                ),
            }

    except Exception as e:
        logger.error(f"Error preparing document segments: {e}")
        return {
            "status": "error",
            "paper_dir": paper_dir,
            "segments_ready": False,
            "error_message": str(e),
        }


--- workflows/agents/__init__.py ---
"""
Agents Package for Code Implementation Workflow

This package contains specialized agents for different aspects of code implementation:
- CodeImplementationAgent: Handles file-by-file code generation
- ConciseMemoryAgent: Manages memory optimization and consistency across phases
"""

from .code_implementation_agent import CodeImplementationAgent
from .memory_agent_concise import ConciseMemoryAgent as MemoryAgent

__all__ = ["CodeImplementationAgent", "MemoryAgent"]


--- workflows/plugins/__init__.py ---
# User-in-Loop Plugin System
from .base import InteractionPlugin, InteractionPoint, PluginRegistry
from .requirement_analysis import RequirementAnalysisPlugin
from .plan_review import PlanReviewPlugin

__all__ = [
    "InteractionPlugin",
    "InteractionPoint",
    "PluginRegistry",
    "RequirementAnalysisPlugin",
    "PlanReviewPlugin",
]
